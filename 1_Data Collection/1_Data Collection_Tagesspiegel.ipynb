{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac01bf71",
   "metadata": {},
   "source": [
    "# Datensammlung von Nachrichtenseiten per Web Scraping - Tagesspiegel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5808309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation Bibliotheken\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "221a9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time period: 2023-12-01 until 2023-12-28 with 7 requests\n"
     ]
    }
   ],
   "source": [
    "#Definition der Parameter für das Web Scraping \n",
    "\n",
    "#Suchwörter in Google Search API\n",
    "search_queries = [\"Künstliche Intelligenz\", \"AI\", \"Artificial Intelligence\", \"KI\"]\n",
    "\n",
    "#Lege ein Startdatum für die automatische Datenerfassung fest\n",
    "last_day_str = \"2023-12-28\"\n",
    "\n",
    "#Zeitintervall von x Tagen vor dem Zieldatum\n",
    "request_days = 3\n",
    "\n",
    "#Limit für API-Anfragen\n",
    "api_request_limit = 7\n",
    "\n",
    "#Pausenzeit des Data Scraping über Beautiful Soup in Sekunden \n",
    "scrap_pause = 3\n",
    "\n",
    "#Bereinigung der Datumsangabe\n",
    "last_day = datetime.strptime(last_day_str, \"%Y-%m-%d\")\n",
    "\n",
    "#Berechnung des letzten Tages\n",
    "first_day = last_day - timedelta(days=request_days*api_request_limit) - ((timedelta(days=1)*api_request_limit)-timedelta(days=1))\n",
    "\n",
    "#Ausgabe des Zeitraums\n",
    "print(f\"Time period: {first_day.strftime('%Y-%m-%d')} until {last_day_str} with {api_request_limit} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "250d9529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Search Query        Quelle       Datum  \\\n",
      "0    Künstliche Intelligenz  Tagesspiegel  2023-12-25   \n",
      "1    Künstliche Intelligenz  Tagesspiegel  2023-12-27   \n",
      "2                        AI  Tagesspiegel  2023-12-28   \n",
      "3                        KI  Tagesspiegel  2023-12-25   \n",
      "4                        KI  Tagesspiegel  2023-12-27   \n",
      "5    Künstliche Intelligenz  Tagesspiegel  2023-12-21   \n",
      "6    Künstliche Intelligenz  Tagesspiegel  2023-12-21   \n",
      "7    Künstliche Intelligenz  Tagesspiegel  2023-12-21   \n",
      "8                        AI  Tagesspiegel  2023-12-21   \n",
      "9                        KI  Tagesspiegel  2023-12-21   \n",
      "10                       KI  Tagesspiegel  2023-12-21   \n",
      "11                       KI  Tagesspiegel  2023-12-21   \n",
      "12   Künstliche Intelligenz  Tagesspiegel  2023-12-17   \n",
      "13                       KI  Tagesspiegel  2023-12-17   \n",
      "14   Künstliche Intelligenz  Tagesspiegel  2023-12-13   \n",
      "15                       AI  Tagesspiegel  2023-12-14   \n",
      "16  Artificial Intelligence  Tagesspiegel  2023-12-14   \n",
      "17                       KI  Tagesspiegel  2023-12-13   \n",
      "18   Künstliche Intelligenz  Tagesspiegel  2023-12-10   \n",
      "19   Künstliche Intelligenz  Tagesspiegel  2023-12-09   \n",
      "20                       AI  Tagesspiegel  2023-12-09   \n",
      "21                       KI  Tagesspiegel  2023-12-11   \n",
      "22                       KI  Tagesspiegel  2023-12-09   \n",
      "23   Künstliche Intelligenz  Tagesspiegel  2023-12-08   \n",
      "24   Künstliche Intelligenz  Tagesspiegel  2023-12-06   \n",
      "25   Künstliche Intelligenz  Tagesspiegel  2023-12-08   \n",
      "26   Künstliche Intelligenz  Tagesspiegel  2023-12-05   \n",
      "27                       AI  Tagesspiegel  2023-12-08   \n",
      "28                       AI  Tagesspiegel  2023-12-05   \n",
      "29                       KI  Tagesspiegel  2023-12-08   \n",
      "30                       KI  Tagesspiegel  2023-12-06   \n",
      "31                       KI  Tagesspiegel  2023-12-08   \n",
      "32                       KI  Tagesspiegel  2023-12-05   \n",
      "33                       AI  Tagesspiegel  2023-12-04   \n",
      "34                       AI  Tagesspiegel  2023-12-02   \n",
      "35                       AI  Tagesspiegel  2023-12-02   \n",
      "36                       KI  Tagesspiegel  2023-12-04   \n",
      "\n",
      "                                                 Link  \\\n",
      "0   https://www.tagesspiegel.de/wissen/deep-fakes-...   \n",
      "1   https://www.tagesspiegel.de/wirtschaft/karrier...   \n",
      "2   https://www.tagesspiegel.de/wirtschaft/bestand...   \n",
      "3   https://www.tagesspiegel.de/wissen/deep-fakes-...   \n",
      "4   https://www.tagesspiegel.de/wirtschaft/karrier...   \n",
      "5   https://www.tagesspiegel.de/wissen/die-welt-is...   \n",
      "6   https://www.tagesspiegel.de/innovationschefin-...   \n",
      "7   https://www.tagesspiegel.de/wirtschaft/kunstli...   \n",
      "8   https://www.tagesspiegel.de/wirtschaft/eu-eini...   \n",
      "9   https://www.tagesspiegel.de/wissen/die-welt-is...   \n",
      "10  https://www.tagesspiegel.de/innovationschefin-...   \n",
      "11  https://www.tagesspiegel.de/wirtschaft/kunstli...   \n",
      "12  https://www.tagesspiegel.de/potsdam/brandenbur...   \n",
      "13  https://www.tagesspiegel.de/potsdam/brandenbur...   \n",
      "14  https://www.tagesspiegel.de/wirtschaft/kunstli...   \n",
      "15  https://www.tagesspiegel.de/berlin/start-in-be...   \n",
      "16  https://www.tagesspiegel.de/berlin/start-in-be...   \n",
      "17  https://www.tagesspiegel.de/wirtschaft/kunstli...   \n",
      "18  https://www.tagesspiegel.de/berlin/folge-der-k...   \n",
      "19  https://www.tagesspiegel.de/internationales/di...   \n",
      "20  https://www.tagesspiegel.de/internationales/di...   \n",
      "21  https://www.tagesspiegel.de/berlin/schule/exze...   \n",
      "22  https://www.tagesspiegel.de/internationales/di...   \n",
      "23  https://www.tagesspiegel.de/kultur/springer-na...   \n",
      "24  https://www.tagesspiegel.de/meinung/wenn-man-s...   \n",
      "25  https://www.tagesspiegel.de/wirtschaft/das-ren...   \n",
      "26  https://www.tagesspiegel.de/kultur/ai-act-zur-...   \n",
      "27  https://www.tagesspiegel.de/wirtschaft/das-ren...   \n",
      "28  https://www.tagesspiegel.de/kultur/ai-act-zur-...   \n",
      "29  https://www.tagesspiegel.de/kultur/springer-na...   \n",
      "30  https://www.tagesspiegel.de/meinung/wenn-man-s...   \n",
      "31  https://www.tagesspiegel.de/wirtschaft/das-ren...   \n",
      "32  https://www.tagesspiegel.de/kultur/ai-act-zur-...   \n",
      "33  https://www.tagesspiegel.de/internationales/kr...   \n",
      "34  https://www.tagesspiegel.de/kultur/liebe-kann-...   \n",
      "35  https://www.tagesspiegel.de/internationales/zu...   \n",
      "36  https://www.tagesspiegel.de/politik/digitalisi...   \n",
      "\n",
      "                                                Titel  \\\n",
      "0   Deep Fakes, deep feelings?: Was KI-generierte ...   \n",
      "1   KI entdeckt verborgene Talente: So hilft Künst...   \n",
      "2   Bestandsaufnahme der Deutschen Wirtschaft: Bei...   \n",
      "3   Deep Fakes, deep feelings?: Was KI-generierte ...   \n",
      "4   KI entdeckt verborgene Talente: So hilft Künst...   \n",
      "5   Die Welt ist nicht genug: Der Datenhunger der ...   \n",
      "6   Innovationschefin über KI: Warum die Telekom a...   \n",
      "7   Künstliche Intelligenz: Telekom setzt auf Chat...   \n",
      "8   EU erzielt Einigung: Was die neuen Schuldenreg...   \n",
      "9   Die Welt ist nicht genug: Der Datenhunger der ...   \n",
      "10  Innovationschefin über KI: Warum die Telekom a...   \n",
      "11  Künstliche Intelligenz: Telekom setzt auf Chat...   \n",
      "12  Mit Maschinen sprechen: Brandenburger Start-up...   \n",
      "13  Mit Maschinen sprechen: Brandenburger Start-up...   \n",
      "14  Künstliche Intelligenz im Journalismus: Axel S...   \n",
      "15  Start in Berlin für Frühjahr 2024 geplant: Aut...   \n",
      "16  Start in Berlin für Frühjahr 2024 geplant: Aut...   \n",
      "17  Künstliche Intelligenz im Journalismus: Axel S...   \n",
      "18  Folge der Künstlichen Intelligenz: Supermärkte...   \n",
      "19  Digitalisierung: Europäische Union will Künstl...   \n",
      "20  Digitalisierung: Europäische Union will Künstl...   \n",
      "21  Exzellente digitale Schule in Berlin: „Inzwisc...   \n",
      "22  Digitalisierung: Europäische Union will Künstl...   \n",
      "23  Springer-Nachrichten-App: Upday bald komplett ...   \n",
      "24  Die perfekte Fälschung: Macht Künstliche Intel...   \n",
      "25  Das Rennen um die KI-Vorherrschaft: Wie Google...   \n",
      "26  „AI Act“ zur Künstlichen Intelligenz : Urheber...   \n",
      "27  Das Rennen um die KI-Vorherrschaft: Wie Google...   \n",
      "28  „AI Act“ zur Künstlichen Intelligenz : Urheber...   \n",
      "29  Springer-Nachrichten-App: Upday bald komplett ...   \n",
      "30  Die perfekte Fälschung: Macht Künstliche Intel...   \n",
      "31  Das Rennen um die KI-Vorherrschaft: Wie Google...   \n",
      "32  „AI Act“ zur Künstlichen Intelligenz : Urheber...   \n",
      "33  Kriegsschiff im Südchinesischen Meer: Peking w...   \n",
      "34  Nach über 20 Jahren Arbeit: Peter Gabriels neu...   \n",
      "35  Zur Zerstörung der Hamas: USA soll Israel 100 ...   \n",
      "36  Umstrittene Pläne der EU-Kommission: Buschmann...   \n",
      "\n",
      "                                                 Text  \n",
      "0   Die Bilder von der Verhaftung Donald Trumps, d...  \n",
      "1   „Das ist ein Match“, befand die Künstliche Int...  \n",
      "2   Fast im Tagesrhythmus hat der Dax im Dezember ...  \n",
      "3   Die Bilder von der Verhaftung Donald Trumps, d...  \n",
      "4   „Das ist ein Match“, befand die Künstliche Int...  \n",
      "5   ChatGPT und seine Verwandten sind gefräßig: si...  \n",
      "6   Frau Nemat, Künstliche Intelligenz (KI) war da...  \n",
      "7   Die Deutsche Telekom setzt bei Kundenanfragen ...  \n",
      "8   Die EU-Finanzminister haben sich auf neue euro...  \n",
      "9   ChatGPT und seine Verwandten sind gefräßig: si...  \n",
      "10  Frau Nemat, Künstliche Intelligenz (KI) war da...  \n",
      "11  Die Deutsche Telekom setzt bei Kundenanfragen ...  \n",
      "12  Das Cottbuser Start-up Zander Laboratories Gmb...  \n",
      "13  Das Cottbuser Start-up Zander Laboratories Gmb...  \n",
      "14  Der ChatGPT-Entwickler OpenAI und das Verlagsh...  \n",
      "15  Berlin soll im kommenden Jahr den ersten auton...  \n",
      "16  Berlin soll im kommenden Jahr den ersten auton...  \n",
      "17  Der ChatGPT-Entwickler OpenAI und das Verlagsh...  \n",
      "18  Bei den Pilzen und der Paprika sieht’s sehr gu...  \n",
      "19  Nach 38 Stunden ist der Verhandlungsmarathon i...  \n",
      "20  Nach 38 Stunden ist der Verhandlungsmarathon i...  \n",
      "21  Herr Hog, die FSAS wurde dieses Jahr – zusamme...  \n",
      "22  Nach 38 Stunden ist der Verhandlungsmarathon i...  \n",
      "23  Die Nachrichten-App Upday aus dem Verlagshaus ...  \n",
      "24  Nur weil etwas im Internet auftaucht, ist es n...  \n",
      "25  Google will mit einem neuen Modell für Künstli...  \n",
      "26  Eine für bestimmte Aufgaben trainierte Basis-K...  \n",
      "27  Google will mit einem neuen Modell für Künstli...  \n",
      "28  Eine für bestimmte Aufgaben trainierte Basis-K...  \n",
      "29  Die Nachrichten-App Upday aus dem Verlagshaus ...  \n",
      "30  Nur weil etwas im Internet auftaucht, ist es n...  \n",
      "31  Google will mit einem neuen Modell für Künstli...  \n",
      "32  Eine für bestimmte Aufgaben trainierte Basis-K...  \n",
      "33  China hat sich über ein „illegales“ Eindringen...  \n",
      "34  Aus den metallisch schnarrenden, leicht flamen...  \n",
      "35  Die USA haben Israel für den Krieg gegen die i...  \n",
      "36  Justizminister Marco Buschmann spricht sich we...  \n"
     ]
    }
   ],
   "source": [
    "#API-Anfrage\n",
    "\n",
    "#API Schlüssel\n",
    "api_key = \"???\"\n",
    "\n",
    "#DataFrame für die Ergebnisse erstellen\n",
    "df_data_all_queries = []\n",
    "\n",
    "#Berechnung Zeitintervall für jede Schleife\n",
    "loop_interval = timedelta(days=request_days)\n",
    "\n",
    "#Schleife bis zur Erreichung der Grenze an API-Anfragen\n",
    "for i in range(api_request_limit):\n",
    "    #Berechnen des Start- und Enddatums für die aktuelle Schleife\n",
    "    end_date = last_day - (loop_interval * i) - (timedelta(days=1) * i) if i > 0 else last_day - (loop_interval * i)\n",
    "    start_date = end_date - timedelta(days=request_days)\n",
    "\n",
    "    #Bereinigung von Datumsangaben\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    #Schleife über die Suchbegriffe\n",
    "    for search_query in search_queries:\n",
    "        #API-Endpunkt und Parameter\n",
    "        api_endpoint = \"https://www.googleapis.com/customsearch/v1\"\n",
    "        api_params = {\n",
    "            \"key\": api_key,\n",
    "            \"cx\": \"???\",\n",
    "            \"q\": f\"{search_query} before:{end_date_str}\",\n",
    "            \"sort\": \"date\",\n",
    "        }\n",
    "\n",
    "        #API-Aufruf\n",
    "        response = requests.get(api_endpoint, params=api_params)\n",
    "        data = response.json()\n",
    "\n",
    "        #Daten im DataFrame speichern\n",
    "        if \"items\" in data:\n",
    "            for item in data[\"items\"]:\n",
    "                link = item[\"link\"]\n",
    "                title = item[\"title\"]\n",
    "\n",
    "                #Extrahieren von HTML-Inhalten aus der Website\n",
    "                article_response = requests.get(link)\n",
    "\n",
    "                #Pause von x Sekunden\n",
    "                time.sleep(scrap_pause)\n",
    "\n",
    "                soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "                '''Individueller Teil je Quelle'''\n",
    "\n",
    "                #Extrahiere den Inhalt\n",
    "                #Suche nach dem entsprechenden HTML-Abschnitt\n",
    "                story_elements = soup.find('div', {'id': 'story-elements'})\n",
    "\n",
    "                #Prüfen, ob der Abschnitt gefunden wird\n",
    "                if story_elements:\n",
    "                    #Alle Absätze innerhalb des Abschnitts finden\n",
    "                    article_elements = story_elements.find_all('p')\n",
    "\n",
    "                    #Text aus Absätzen extrahieren und verbinden\n",
    "                    article_text = '\\n'.join([element.text.strip() for element in article_elements])\n",
    "\n",
    "                    #Speichere den Artikeltext in der Variablen \"content\"\n",
    "                    content = article_text\n",
    "\n",
    "                #Extrahieren des Datums aus dem HTML-Inhalt\n",
    "                #Verwende BeautifulSoup, um das Datum zu extrahieren\n",
    "                date_tag = soup.find('meta', {'content': re.compile(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3}Z')})\n",
    "\n",
    "                #Extrahieren des Datums aus dem Inhalt des Meta-Tags\n",
    "                if date_tag:\n",
    "                    article_date = date_tag['content']\n",
    "                    formatted_date = article_date.split('T')[0]\n",
    "                else:\n",
    "                    #Wenn article_date nicht vorhanden ist, wird formatted_date auf einen leeren String gesetzt\n",
    "                    formatted_date = \"1900-01-01\"\n",
    "                    \n",
    "                #Extrahiere den Titel aus der HTML-Datei\n",
    "                article_title = soup.title.text if soup.title else \"\"\n",
    "\n",
    "                '''Individueller Teil je Quelle'''\n",
    "\n",
    "                #Prüfen, ob das Datum mit dem Enddatum übereinstimmt\n",
    "                if datetime.strptime(start_date_str, \"%Y-%m-%d\") <= datetime.strptime(formatted_date, \"%Y-%m-%d\") <= datetime.strptime(end_date_str, \"%Y-%m-%d\"):\n",
    "                    df_data_all_queries.append({\n",
    "                        \"Search Query\": search_query,\n",
    "                        \"Quelle\": \"Tagesspiegel\",\n",
    "                        \"Datum\": formatted_date,\n",
    "                        \"Link\": link,\n",
    "                        \"Titel\": article_title,\n",
    "                        \"Text\": content\n",
    "                    })\n",
    "\n",
    "#DataFrame für alle Suchbegriffe erstellen\n",
    "df_all_queries = pd.DataFrame(df_data_all_queries)\n",
    "\n",
    "#DataFrame anzeigen mit Prüfen, ob der DataFrame leer ist\n",
    "if df_all_queries.empty:\n",
    "    print(f\"No entries found!\")\n",
    "else:\n",
    "    print(df_all_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a26df35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neue Daten in einem Hauptdatensatz in CSV speichern\n",
    "\n",
    "#Prüfen, ob die CSV-Datei bereits existiert\n",
    "try:\n",
    "    #Versuche, die vorhandene CSV-Datei zu lesen\n",
    "    existing_df = pd.read_csv(\"2_Daten_Tagesspiegel_V2.csv\")\n",
    "except FileNotFoundError:\n",
    "    #Wenn die Datei nicht existiert, erstelle einen leeren DataFrame\n",
    "    existing_df = pd.DataFrame()\n",
    "\n",
    "#Anhängen der neuen Daten an den bestehenden DataFrame\n",
    "df = pd.DataFrame(df_all_queries)\n",
    "df_to_append = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "#Duplikate auf Basis aller Spalten entfernen\n",
    "df_to_append = df_to_append.drop_duplicates()\n",
    "\n",
    "#Speichern des kombinierten DataFrame in der CSV-Datei\n",
    "df_to_append.to_csv(\"2_Daten_Tagesspiegel_V2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683465dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
