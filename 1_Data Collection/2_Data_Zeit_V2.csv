Suchbegriff,Quelle,Datum,Link,Titel,Text
Künstliche Intelligenz,Zeit,2024-04-30,https://www.zeit.de/kultur/film/2024-04/the-fall-guy-ryan-gosling-emily-blunt-film,"Emily Blunt & Ryan Gosling: ""Der Job tut immer weh"" | ZEIT ONLINE","Wer Anfang der Achtzigerjahre in Westdeutschland geboren wurde und vor dem Fernseher aufgewachsen ist, wollte entweder Außerirdischer oder Stuntman werden oder eine paramilitärische Terrororganisation gründen. Das Kinder-TV der damaligen Zeit bestand im Wesentlichen aus den US-Serien Alf, Das A-Team und ein Ein Colt für alle Fälle. Man wurde also in dem Glauben groß, dass jederzeit ein Atomunfall passieren könnte, wie auf Alfs Heimatplaneten Melmac, oder man selbst eine Atombombe bauen müsste, wie in gefühlt allen Folgen des A-Teams, aus ein paar alten Benzinkanistern, Klebeband und anderen Dingen, die man an Tankstellen findet, im Wald oder bei einer rivalisierenden Terrororganisation. Wer wollte nicht voller Nostalgie auf diese Zeit zurückblicken?

Vier Leute, die ganz sicher wollen: Emily Blunt, Ryan Gosling, David Leitch und Kelly McCormick. Blunt und Gosling waren letztes Jahr Kitty und Ken, die oscarnominierten Sidekicks der Barbenheimer-Saga. Bei der Verleihung der Academy Awards gingen sie leer aus, hatten aber mit einer gemeinsamen Hommage an die Stuntmen der Filmbranche den zweitbesten Auftritt des Abends. (Den besten hatte bekanntlich Gosling allein.) Die Aktion geschah nicht ohne Hintergedanken: Drei Monate später sind Gosling und Blunt die Stars in The Fall Guy, einer Kinoadaption der Actionserie Ein Colt für alle Fälle über den Stuntman Colt Seavers. Mitte der Achtziger lief die Show im ZDF und ebnete den deutschen Fernsehweg für das A-Team einerseits und lächerliche Poser wie MacGyver und Renegade andererseits.

David Leitch und Kelly McCormick sind der Regisseur und die maßgebliche Produzentin von The Fall Guy, die Masterminds außerdem hinter Filmen wie Atomic Blonde, Deadpool 2 und Bullet Train. Mitte April kann man sie und ihre Hauptdarsteller in Berlin-Mitte treffen, im ersten Stock eines Luxushotels, der weitgehend freigeräumt wurde für die blockbustergroßen Delegationen von Filmverleih und PR-Agentur. Wie immer, wenn es nach Hollywood riecht, kommt auch Steven Gätjen die Treppe hochgejoggt.

Hier ist der Deal: Man darf Gosling und Blunt zusammen interviewen, wenn man auch McCormick und Leitch zusammen interviewt. Man soll Fragen bitte nur zum Film stellen, keine Quizze oder Trinkspiele anregen und gar nicht erst auf die Idee kommen, um ein Foto zu bitten, ein Autogramm oder eine Ansage für die eigene Mailbox. Die Interviewzeit beträgt jeweils sechs Minuten, wobei damit eigentlich fünf Minuten gemeint sind, denn die sechs Minuten beinhalten auch den sogenannten Turnaround für das nächste Interview. Alle Gespräche werden vor einer bunten Fall-Guy-Tapete gefilmt, gleich danach erhält man das Material in einem kleinen Umschlag auf personalisierten Chipkarten.

Zeit ist wahnsinnig kostbar bei einer solchen Veranstaltung und dann auch wieder gar nicht. Alles erscheint minutengenau durchgeplant, die ganze Hoteletage wuselt um wartende Journalistinnen und Journalisten herum, plötzlich verschiebt sich alles um eine Stunde nach hinten. Es geht zu wie auf einer hochfunktionalen Ameisenfarm oder in der Notaufnahme eines Großstadtkrankenhauses am Freitagabend. Niemand interessiert sich dafür, dass vor dem Eingang zum Hotel auch der Mannschaftsbus des deutschen Vizemeisters FC Bayern München geparkt hat.

Dann ist es so weit und gleich mal enttäuschend: Wer Ryan Gosling zum Interview trifft, trifft eigentlich gar nicht Ryan Gosling, sondern Ryan Gosling verkleidet als Stuntman Colt Seavers. Wie auch in mehreren Szenen aus The Fall Guy trägt er Lederjacke und Stiefel, die ihn auf drollige Weise extrabreit und -markig erscheinen lassen – man wundert sich fast, dass er keinen Zahnstocher im Mund hat. Gosling ist ebenso seidenglänzend geschminkt wie im Film, hat noch immer die straßenköterblond gefärbten Haare seiner Figur und strahlt professionelle Zugewandtheit aus. Für Emily Blunt gibt es solche Auflagen offenbar nicht: In einem witterungstechnisch bedenklichen Sommer-Onesie sitzt sie neben Gosling und ist aus unerklärlichen Gründen geradezu ekstatisch gut drauf.

Blunt und Gosling sind zwar nicht vor westdeutschen Fernsehern, aber mit und im Fall von Gosling sogar im Fernsehen der späten Achtziger und frühen Neunziger aufgewachsen. ""Den Filmen, die damals liefen, sah man ziemlich schnell an, dass es jetzt nicht gerade Arnold Schwarzenegger selbst war, der das Motorrad fuhr"", sagt der frühere Kinderstar über die Aufgabe von Stuntmen. ""Heute wird so etwas natürlich besser kaschiert."" Blunt findet das beinahe unfair: ""Einer meiner ersten Drehs war vor 20 Jahren in Rumänien, da wurde ich gleich mal auf ein Pferd gesetzt, obwohl ich überhaupt nicht reiten konnte. Einer Kollegin von mir ging es ähnlich – und als sie vom Pferd fiel, war es der einzige Stuntman am Set, der sie auffing und ihr wahrscheinlich das Leben rettete.""

Stuntmen sind also Helden, diese Botschaft gehört zur Mission von Blunt und Gosling, aber auch tragische Figuren, wie die Schauspielerin sagt. ""Je besser sie ihren Job machen, desto weniger nimmt man sie wahr. Sie tauchen auf, vollbringen Unglaubliches und verschwinden wieder in der Dunkelheit."" Nicht allerdings in The Fall Guy. Der Film sei als Verbeugung vor allen Stuntmen und ihrer Community gedacht, erfährt man im Interview nebenan von dem Regisseur und ehemaligen Stuntman David Leitch. ""Denn natürlich steckt in jedem Stuntman auch ein Performer. Niemand lässt sich durch einen zerbrechlichen Tisch werfen oder von einem Hochhaus schubsen, ohne dafür eine Reaktion vom Publikum zu erwarten.""
"
Künstliche Intelligenz,Zeit,2024-04-30,https://www.zeit.de/news/2024-04/30/financial-times-gewaehrt-chatgpt-zugriff-auf-ihre-texte,"Künstliche Intelligenz: ""Financial Times"" gewährt ChatGPT Zugriff auf ihre Texte | ZEIT ONLINE","Der populäre Chatbot ChatGPT wird künftig auch mit Artikeln der «Financial Times» trainiert. Die Entwicklerfirma OpenAI schloss dafür eine Lizenzvereinbarung mit der britischen Wirtschaftszeitung. ChatGPT soll damit auch Fragen von Nutzern mit Informationen aus Artikeln der «Financial Times» samt Links zur Webseite beantworten können, wie OpenAI mitteilte.

In den vergangenen Monaten hatten unter anderem der deutsche Konzern Axel Springer und die französische Zeitung «Le Monde» Vereinbarungen mit OpenAI geschlossen. Die «New York Times» dagegen zog im Dezember vor Gericht mit dem Vorwurf, ChatGPT sei ohne Erlaubnis mit Artikeln der Zeitung angelernt worden.

ChatGPT löste vor über einem Jahr den Hype um Künstliche Intelligenz aus. Solche KI-Chatbots werden mit gewaltigen Mengen an Informationen trainiert und können Texte auf dem sprachlichen Niveau eines Menschen formulieren, Software-Code schreiben und Informationen zusammenfassen. Das Prinzip dahinter ist, dass sie Wort für Wort abschätzen, wie ein Satz weitergehen sollte.

© dpa-infocom, dpa:240430-99-857748/2
"
Künstliche Intelligenz,Zeit,2024-04-30,https://www.zeit.de/geld/2024-04/mario-goetze-fussball-start-up-investment-geld,"Mario Götze: Mario Götze, wie legen Sie Ihr Geld an? | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-04-29,https://www.zeit.de/news/2024-04/29/datenschuetzer-beschwerde-gegen-openai-und-chatgpt,Künstliche Intelligenz: Datenschützer: Beschwerde gegen OpenAI und ChatGPT | ZEIT ONLINE,"Die europäische Datenschutz-Organisation Noyb hat zusammen mit einem betroffenen europäischen Bürger eine Datenschutz-Beschwerde gegen den ChatGPT-Anbieter OpenAI wegen des Verstoßes gegen die Europäische Datenschutz-Grundverordnung (DSGVO) eingereicht. 

Die von Datenschutz-Aktivist Max Schrems mitbegründete Organisation warf OpenAI unter anderem vor, im Fall einer namentlich nicht genannten «Person des öffentlichen Lebens» falsche Angabe zu persönlichen Daten zu machen, ohne die gesetzlich vorgeschriebene Möglichkeit einer Berichtigung oder Löschung einzuräumen.

Schrems hatte zuvor bereits den Facebook-Konzern Meta in zwei Klagen das Fürchten gelehrt und dabei zweimal vor dem Europäischen Gerichtshof wichtige Datenabkommen zwischen den USA und Europa gekippt.

In der Auseinandersetzung mit ChatGPT-Entwickler OpenAI wirft Noyb dem US-Unternehmen vor, den Menschen in Europa ihre Rechte nach der DSGVO zu verweigern. Im konkreten Fall, in dem es auch um ein falsches Geburtsdatum ging, habe OpenAI damit argumentiert, dass eine Korrektur der Daten nicht möglich sei. Man könne zwar Daten bei bestimmten Anfragen blockieren, etwa den Namen des Prominenten. Man könne ChatGPT aber nicht daran zu hindern, alle Informationen über den Beschwerdeführer zu filtern.

Noyb warf OpenAI weiterhin vor, nicht angemessen auf das Auskunftsersuchen des Beschwerdeführers reagiert zu haben. Obwohl die DSGVO den Nutzerinnen und Nutzern das Recht einräume, eine Kopie aller persönlichen Daten zu verlangen, habe es OpenAI versäumt die verarbeiteten Daten, ihre Quellen oder Empfänger offenzulegen.

Maartje de Graaf, Datenschutzjuristin bei Noyb, sagte, die Verpflichtung, einem Auskunftsersuchen nachzukommen, gelte für alle Unternehmen. «Es ist selbstverständlich möglich, die verwendeten Trainingsdaten zu protokollieren, um zumindest eine Vorstellung von den Informationsquellen zu erhalten. Es scheint, dass mit jeder 'Innovation' eine andere Gruppe von Unternehmen meint, dass ihre Produkte nicht mit dem Gesetz übereinstimmen müssen.»

Noyb und der Betroffene forderten nun die österreichische Datenschutzbehörde (DSB) zu einer Untersuchung der Datenverarbeitungspraktiken von OpenAI auf. Von besonderem Interesse sei dabei die Frage, welche Maßnahmen das Start-up zur Sicherstellung der Richtigkeit persönlicher Daten getroffen habe. Gegen OpenAI müsse ein Bußgeld verhängt werden, um die zukünftige Einhaltung der Vorschriften sicherzustellen.

© dpa-infocom, dpa:240429-99-846833/3
"
Künstliche Intelligenz,Zeit,2024-04-27,https://www.zeit.de/2024/18/daenemark-moewen-tourismus-sonderborg,Dänemark: Kiu Kiu! | ZEIT ONLINE,
AI,Zeit,2024-04-29,https://www.zeit.de/digital/datenschutz/2024-04/datenschutz-chatgpt-open-ai-beschwerde,ChatGPT: Datenschützer reichen Beschwerde gegen OpenAI ein | ZEIT ONLINE,"Datenschützer in Österreich
haben eine Beschwerde gegen den ChatGPT-Entwickler OpenAI eingereicht, da das
KI-Programm falsche Informationen über Personen erfinde. Dies sei
inakzeptabel, da es die Europäische Datenschutz-Grundverordnung verletze,
teilte die Datenschutzorganisation None of Your Business (Noyb) mit Sitz in
Wien mit.

Wenn ein System keine genauen
und transparenten Ergebnisse liefern könne, dürfe es nicht dazu verwendet
werden, Daten über Einzelpersonen zu generieren, sagte Maartje de Graaf,
Datenschutzanwältin bei Noyb. ""Die Technologie muss den rechtlichen Anforderungen
folgen, nicht umgekehrt.""

""ChatGPT halluziniert
und nicht mal OpenAI kann es stoppen"", teilte Noyb mit. Der
Datenschutzorganisation zufolge ist sich das Unternehmen aus den USA des
Problems bewusst. ""Das Unternehmen gibt
offen zu, falsche Informationen auf ChatGPT nicht korrigieren zu können."" Das Unternehmen wisse ""nicht einmal, woher die Daten stammen oder
welche Daten ChatGPT über einzelne Personen speichert"".

Die Organisation warf
 OpenAI unter anderem vor, im Fall einer namentlich nicht 
genannten ""Person des öffentlichen Lebens"" falsche Angabe zu 
persönlichen Daten zu machen, ohne die gesetzlich vorgeschriebene 
Möglichkeit einer Berichtigung oder Löschung einzuräumen.

OpenAI habe zudem nicht angemessen auf das Auskunftsersuchen des Beschwerdeführers 
reagiert. Obwohl die Datenschutz-Grundverordnung den Nutzerinnen und Nutzern das 
Recht einräume, eine Kopie aller persönlichen Daten zu verlangen, habe 
es OpenAI versäumt, die verarbeiteten Daten, ihre Quellen oder Empfänger 
offenzulegen.  

Noyb und der Betroffene forderten die österreichische Datenschutzbehörde 
(DSB) zu einer Untersuchung der Datenverarbeitungspraktiken von OpenAI 
auf. Von besonderem Interesse sei dabei die Frage, welche Maßnahmen das 
Start-up zur Sicherstellung der Richtigkeit persönlicher Daten getroffen
 habe. Gegen OpenAI müsse ein Bußgeld verhängt werden, um die zukünftige
 Einhaltung der Vorschriften sicherzustellen. 
"
AI,Zeit,2024-04-29,https://www.zeit.de/digital/internet/2024-04/ki-generierte-produkte-programmier-bots-newsletter-kuenstliche-intelligenz,"KI-generierte Produkte: Trau keiner KI-Demo, die du nicht selbst gefälscht hast | ZEIT ONLINE","Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 18. April 2024. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Haben Sie schon mal jemandem
erzählt, wie toll Ihr Projekt läuft, obwohl Sie in Wirklichkeit gerade erst mit
der Arbeit angefangen haben? Dann wissen Sie, dass das ein riskantes
Unterfangen ist. Fake it till you make it, wie diese Taktik in der Start-up-Welt
oft genannt wird, funktioniert nur, wenn man nicht dabei erwischt wird.

Die Firma Cognition Labs wurde vor
einigen Tagen genau dabei ertappt. Das Produkt des Unternehmens ist noch nicht
öffentlich verfügbar, erlebte aber bereits einen kleinen Hype und war vor vier
Wochen schon einmal Thema in diesem Newsletter: Der Bot Devin soll eigenständig
Programmier-Aufgaben lösen können.

Ein Video, in dem das demonstriert
wird, bekam in sozialen Medien viel Aufmerksamkeit und inspirierte direkt wilde
Spekulationen. Programmiererinnen würden bald massenweise arbeitslos, weil die
KI sie nicht nur bei ihrer Arbeit unterstützen, sondern gleich komplett
ersetzen könnte.

Meine Kollegin Marie Kilg war in
ihrer Bewertung vorsichtiger und sie behielt recht. Der YouTuber Internet of
Bugs zeichnet in einem Video detailliert nach, dass das Demovideo mindestens
stark übertrieben, wenn nicht komplett fake ist.

In für die KI-Welt angenehm
unaufgeregter Art zeigt er zum Beispiel, dass der Bot nicht wirklich Fehler in
dem Code korrigiert, der ihm vorgegeben wurde, sondern in dem Code, den er
selbst vorher produziert hatte. Der YouTuber erklärt, dass das System Dinge
tut, die falsch sind oder zumindest nicht den aktuellen Standards guter
Programmierung entsprechen. Und er weist darauf hin, dass Devin viele Stunden
für die Aufgabe brauchte, er selbst sie aber in gut 30 Minuten lösen konnte.

Er sei nicht gegen KI, sagt der
YouTuber in dem Video, er benutze ChatGPT und andere Tools als Hilfe beim
Programmieren. Und auch Devin leiste durchaus Beeindruckendes. ""Aber sie
mussten ja so tun, als könnte es noch viel mehr"", sagt er. ""Solche
Lügen erzeugen echten Schaden.""

Der Fall ist ein gutes Beispiel für
das Problem, dass wir in der Techwelt oft so tun, als wäre die Zukunft schon
da. Ich schreibe ""wir"", weil auch Journalistinnen und Journalisten
nicht davor gefeit sind, den Hype zu befeuern.

Es liegt ein Reiz darin, das, was
man heute sieht, direkt weiterzudenken. Weil es sich visionär und weitsichtig
anfühlt. Wenn ChatGPT einigermaßen sinnvollen Programmcode generieren kann,
dann überlegen wir sofort, welche Regeln eine Welt braucht, in der KI im
Alleingang ganze Apps entwickelt.

Einerseits ist das richtig. Wir
sollten nicht blind in die Zukunft manövrieren, ohne über sie nachzudenken.
Aber je konkreter Unternehmen Versprechungen über diese Zukunft mit Produkten
verknüpfen, für die man schon heute Geld bezahlen kann, desto größer ist die
Gefahr einer Enttäuschung.

Ein anderes aktuelles Beispiel
dafür ist der Humane Ai Pin. Der KI-Anstecker wurde im vergangenen Jahr
angekündigt. In Produktvideos entstand der Eindruck, dass hier der
Star-Trek-Communicator für 700 Dollar vorzubestellen ist: ein Gerät, das auf
alles eine Antwort hat, Unterhaltungen simultan übersetzen kann und per Laser
ein Display auf die Hand projiziert. Die Zukunft ist jetzt, dachte auch ich.

Nun, da der Pin in den USA
tatsächlich ausgeliefert wird, blamiert er sich in den Tests von
Techjournalisten. Er gibt falsche oder gar keine Antworten oder braucht 13
Sekunden, um zu reagieren, seine Batterie wird zu heiß oder die KI dreht durch,
weil im Namen von Beyoncé ein Accent auf dem ""e"" ist.

Das heißt nicht, dass der KI-Pin
oder Programmier-Bots nicht funktionieren können. Aber sie sind bei Weitem
nicht fertig und nicht so gut, wie ihre Macher uns glauben lassen. Es sind
eigentlich eher Ideen als Produkte. 

Erste Anzeichen lassen erahnen,
dass eine ähnliche Enttäuschung auch mit dem KI-Produkt droht, das eigentlich
der ganz große Geldbringer für die Konzerne werden soll: Büro-Assistenten wie
Microsofts Copilot.

Meine eigenen – eher ernüchternden
– Tests mit dem System passen zu Artikeln wie dem des Techmediums The
Information, in dem es heißt, dass Vertreter von Microsoft, Amazon und Google
Schwierigkeiten haben, ihre KI-Assistenten an Unternehmen zu verkaufen.
Angesichts der hohen Kosten und nach wie vor problematischen Ungenauigkeiten
seien Kunden zurückhaltend mit neuen Ausgaben für KI.

Das ist erst mal nur ein Hinweis,
aber er trägt zu meinem Gefühl bei, dass uns in der KI-Welt eine Zeit der
Ernüchterung bevorsteht.
"
Artificial Intelligence,Zeit,2024-04-29,https://www.zeit.de/zeit-wissen/2024/03/ki-gesundheitsbereich-diagnose-arzt-patient/komplettansicht,KI im Gesundheitsbereich: Wie geht’s uns denn morgen? | ZEIT ONLINE,"Auf der Intensivstation ist es niemals still. Das langsam tiefer werdende Tut-tut-tut des Pulsoxymeters kündigt einen drohenden Sauerstoffmangel an. Eine leere Medikamentenpumpe bittet mit quäkenden Tönen um Aufmerksamkeit. Am alarmierendsten ist das Fiepen des Notfallalarms. Jetzt zählt jede Minute. Wie viele Ärzte haben schweißgebadet vor einem kritisch kranken Patienten gestanden und sich gewünscht, sie hätten diesen Notfall vorhersehen können? 

Der Neurologe Nils Schweingruber arbeitet daran, diesen Wunsch zu erfüllen. Er versucht, in die Zukunft zu schauen – zumindest für ein paar Stunden. Es ist nicht lange her, da arbeitete er selbst auf einer Intensivstation. Am Universitätsklinikum Hamburg-Eppendorf (UKE) kümmerte er sich um Patienten mit schweren Schlaganfällen und Hirnblutungen. Patienten, deren Gehirn anschwoll, gegen das Innere ihres Schädels drückte und sich selbst zu zerquetschen drohte. Normalerweise herrscht im Gehirn ein Druck von etwa 0,013 bar. Durch eine Schwellung, einen Tumor oder eine Blutung kann der Hirndruck dramatisch steigen. Spätestens ab 0,05 bar besteht Lebensgefahr. Eine spezielle Beatmung, abschwellende Medikamente und eine Narkose können den Hirndruck senken. Als letzte Maßnahme entfernen Neurochirurgen einen Teil der Schädeldecke, um dem Gehirn Platz zu verschaffen.

Am besten wäre es, man könnte eine Hirndruckkrise erkennen, bevor sie entsteht, doch selbst erfahrenen Intensivmedizinern fällt das schwer. Schweingruber kam ein Gedanke: Die Monitore, Beatmungsgeräte und Sensoren, mit denen die Patienten auf der Intensivstation verkabelt sind, produzieren nicht nur Alarmtöne, sondern auch unendlich viele Daten. In der digitalen Patientenakte werden diese mit Laborwerten und medizinischen Notizen verbunden. Könnte die Lösung in diesem Datenmeer versteckt sein?

Das UKE hat sich früh digitalisiert und hütet einen seltenen Schatz, nämlich die Daten von fast 1.400 Hirndruck-Patienten, die im vergangenen Jahrzehnt auf den Intensivstationen behandelt wurden. Das entspricht 63 Behandlungsjahren. Ein perfektes Trainingslager für die künstliche Intelligenz (KI). Nils Schweingruber ließ einen KI-Algorithmus in den Daten nach Mustern suchen, und tatsächlich fand die KI einen Zusammenhang zwischen den Vital-, Labor- und Gerätedaten einerseits und dem Hirndruck auf der anderen Seite. Das Programm kann eine Hirndruckkrise in den folgenden sechs Stunden mit einer Treffsicherheit von 85 Prozent vorhersagen. Das ist genug Zeit, um die richtige Behandlung einzuleiten.

Ein britisches Team programmierte einen ""KI-Arzt"", der die Daten von 90.000 Patienten mit einer schweren Blutvergiftung (Sepsis) durchforstete. In einem virtuellen Vergleich entschied er deutlich treffsicherer als ein menschlicher Kollege und hätte in der Realität viele Menschenleben gerettet. Wissenschaftler aus der Schweiz fütterten eine KI mit den Daten von über 55.000 intensivmedizinischen Patienten, um ein Kreislaufversagen zu prognostizieren. Die KI sagte in einem Testlauf 82 Prozent der Fälle mehr als zwei Stunden vor dem Ereignis korrekt vorher.

Damit solche KI-Prognosen zuverlässig sind, müssen drei Faktoren zusammenkommen, sagt der Medizinethiker Vince Madai, Leiter des Projektteams ""Verantwortungsvolle Algorithmen"" des Berliner Instituts für Gesundheitsforschung, das zur Charité gehört. Erstens eine Menge an Daten. Meist gilt: Je mehr, desto besser. Zweitens arbeiten KIs mit Zahlen. Es ist für eine KI viel einfacher, einen Schlaganfall oder einen Knochenbruch zu erkennen als eine Depression, weil sie jeden Bildpunkt einer Röntgen- oder MRT-Aufnahme als Zahl lesen kann, traurige Gedanken aber nicht. Der dritte Faktor ist die Verfügbarkeit der Daten: Aufgrund der strengen deutschen Datenschutzrichtlinien waren viele Bereiche für die KI bisher nicht zugänglich. Für intensivmedizinische Patienten gibt es aber eine Vielzahl an frei zugänglichen Datenbanken.

Manche KI ist in der Praxis allerdings kläglich gescheitert. Etwa die Sepsis-Prognose eines großen amerikanischen Unternehmens für Gesundheitssoftware, die die versprochene 80-prozentige Trefferquote aus der Testphase weit verfehlte. Stattdessen erkannte sie im Krankenhausalltag nur ein Drittel der Blutvergiftungen, gleichzeitig waren 90 Prozent der Warnungen Fehlalarme. Was war geschehen? Das Hauptproblem war ein Datenleck. So nennt man es, wenn eine KI mit Daten trainiert wird, die in der Praxis gar nicht vorhanden sind. In diesem Fall war die Verabreichung von Antibiotika in das KI-Training mit eingeflossen. Wenn ein Patient Antibiotika bekommt, heißt das jedoch, dass das Ärzteteam eine Infektion bereits in Betracht gezogen hat. Die KI erreichte im Test eine hohe Treffsicherheit, versagte aber in der Praxis dabei, eine Sepsis früher als die Fachleute zu erkennen. Sie hatte geschummelt.

Selbst wenn es nicht zu einem Datenleck kommt, können die Ergebnisse problematische Verzerrungen enthalten, englisch: Bias. Denn in den Gesundheitsdaten sind Benachteiligungen aufgrund von Einkommen, Geschlecht oder Ethnie eingraviert. 2019 stellten Forschende fest, dass eine KI, die in den USA das Schicksal von Millionen von Patienten mitbestimmte, schwarze Menschen systematisch benachteiligte. Der Algorithmus diente dazu, Patienten mit Krankheiten wie Diabetes und einer hohen Wahrscheinlichkeit für einen schlechten Verlauf zu identifizieren und ihnen Unterstützung anzubieten. Für die Vorhersage nutzte die KI unter anderem die Kosten, die eine Person im Gesundheitssystem verursacht. Je weniger Kosten, desto gesünder, las sie aus den Daten. 

Falsch. Schwarze Menschen haben in den USA oft nur eingeschränkten Zugang zum Gesundheitssystem. Die KI unterschätzte, wie krank schwarze Patienten wirklich waren. Nachdem die Forschenden die Kostendaten aus der Berechnung entfernt hatten, stieg der Anteil der Schwarzen, für die die KI ein Gesundheitsprogramm empfahl, von 18 auf 47 Prozent. 

Wenn Verzerrungen im Gesundheitssystem in eine KI eingeschleust werden, verstärken sich Ungleichheiten womöglich. Man könne den Spieß aber umdrehen, argumentiert der Medizinethiker Vince Madai. Der Bias einzelner Ärzte lasse sich nicht testen oder korrigieren, der einer KI schon. Dadurch könnte das Gesundheitssystem insgesamt gerechter werden.

Das Problem ist die mangelnde Transparenz der Algorithmen. Die KI ist eine Blackbox. Wie kam sie zu einer Prognose? Welche Faktoren wurden berücksichtigt, und wie wurden sie gewichtet? Man weiß es nicht. Doch die beste KI ist wertlos, wenn die Nutzer dem Ergebnis nicht trauen. In einer dänischen Studie sollte eine KI die Dispatcher in einer Rettungsstelle – das sind die, die den Notruf annehmen – dabei unterstützen, einen Herzstillstand zu erkennen. Ein Machine-Learning-Programm hörte dafür fast 170.000 Notrufe mit, analysierte dann Gespräche in Echtzeit und gab eine Einschätzung ab, ob der Grund für den Anruf ein Herzstillstand sein könnte. Die KI erkannte 85 Prozent der Herzstillstände, fast zehn Prozent mehr als die Dispatcher allein. 54 Menschen hätten durch das Tool die richtige Diagnose und sofortige Hilfe erhalten können. Haben sie aber nicht. Denn die Dispatcher ignorierten den KI-Ratschlag. ""Die Studie zeigt, was passiert, wenn die Nutzer eines KI-Programms zu wenig in die Entwicklung eingebunden sind"", sagt Madai. Ein Dispatcher, der seit 20 Jahren Notrufe annimmt, lässt sich nicht von irgendeinem Programm die Entscheidung abnehmen. Es gibt auch ein Fachwort dafür: Algorithm Aversion Bias. Eine Art Software-Allergie.

Das Gegenteil davon ist der Automation Bias: Die Nutzer machen immer genau das, was eine Entscheidungshilfe vorgibt. Maschine an, Gehirn aus. ""Automation Bias ist der Grund dafür, dass immer mal wieder Menschen mit dem Auto in einen Fluss fahren, weil sie blind ihrem Navigationssystem folgen"", sagt Madai. Ein Assistenzarzt wagt vielleicht nicht, eine KI-Entscheidung zu hinterfragen. Wer ist er, dem elektronischen Superhirn zu widersprechen?

""Es kommt nicht nur darauf an, welche Daten in ein Programm einfließen, sondern auch, wie das Ergebnis präsentiert wird"", sagt Madai. Es muss erkennbar sein, wie die KI zu ihrem Ergebnis kam und wie sicher es ist. Im Klinikalltag kann das Personal keine langen Anleitungen und keinen Computercode lesen. Madai wünscht sich verlässliche Daten, am besten aus Studien, wie sie auch für die Zulassung von Medikamenten üblich sind. Aktuell ist noch nicht geregelt, unter welchen Bedingungen eine KI eingesetzt werden darf. 

Und was passiert, wenn aufgrund einer KI-Empfehlung Fehler passieren? ""Hier könnte es ein responsibility gap geben"", sagt der Wissenschaftsphilosoph Claus Beisbart vom Center for Artificial Intelligence in Medicine (CAIM) in Bern. Eine Verantwortlichkeitslücke. Die Softwareingenieure können argumentieren, dass sie nur für die Programmierung zuständig sind, die Krankenhausleitung, dass sie das Programm lediglich zur Verfügung stellt, die Ärzte, dass die KI für sie eine Blackbox ist. ""Der Gesetzgeber muss eine Lösung dafür finden, wie die Verantwortung verteilt wird"", sagt Beisbart. Er plädiert für eine Art KI-Versicherung: ""Die Frage nach einer Strafe ist manchmal wenig produktiv, man muss Ausgleich schaffen."" Der Einsatz von KI im Gesundheitswesen kann einen Haufen Geld sparen: durch schnellere Diagnosen und eine bessere Früherkennung. Das eingesparte Geld könnte in einen Versicherungsfonds eingezahlt und im Falle eines KI-bedingten Schadens ausgeschüttet werden, sagt Beisbart. Die KI gibt, die KI nimmt.

Eine andere Möglichkeit gefällt Beisbart noch besser: dass der Patient ins Zentrum der Entscheidung rückt. Eine KI kann schließlich auch Patienten mit Informationen versorgen, verschiedene Therapieoptionen beleuchten und mit Zahlen unterfüttern – ohne eine Entscheidung vorwegzunehmen. Die Meinung der KI-Ärztin wäre dann nur eine von vielen, die Patienten bei ihren Überlegungen berücksichtigen können.

Mehr noch, die KI-Ärztin wäre eine medizinische Expertin, die endlich Rede und Antwort steht. Denn Ärzte sprechen nicht gern über Prognosen. Sie haben Sorge, dass die Arzt-Patient-Beziehung darunter leidet oder Patienten damit nicht umgehen können. Ein weiteres Problem ist, dass Ärzte oft danebenliegen. Eine Übersichtsarbeit von 2022 zeigte, dass der Verlauf einer Erkrankung nur in elf Prozent der Studien richtig vorausgesehen wurde (siehe Grafik auf Seite 73). In den meisten Studien sind die Fachleute zu pessimistisch und die Patienten zu optimistisch. So oder so, die Therapieentscheidung kann darunter leiden. Pessimisten beenden lebensverlängernde Maßnahmen womöglich zu früh, Optimisten riskieren unnötige und belastende Therapien und deren Nebenwirkungen. Eine KI dagegen könne individuelle Prognosen für jeden einzelnen Patienten ausgeben, sagt Beisbart. ""Das verändert das Verhältnis zwischen Arzt und Patient."" Wissen ist Macht, auch in der Medizin.

Natürlich sei es wichtig, wie eine Prognose überbracht werde, sagt Beisbart. Aber selbst auf diesem vermeintlich urmenschlichen Gebiet, Mitgefühl, schlägt die KI den Menschen. Ein Forschungsteam kopierte von der Website ""Ask a doctor"", auf der Ärzte medizinische Fragen beantworten, 195 Fragen und beauftragte ChatGPT, sie ebenfalls zu beantworten. Drei ärztliche Gutachter bewerteten die Antworten von ChatGPT und Fachleuten hinsichtlich Qualität und Empathie, ohne zu wissen, welche von einem Menschen und welche vom Chatbot verfasst worden waren. In fast 80 Prozent der Fälle schnitt ChatGPT besser ab. Nur fünf Prozent der ärztlichen, aber fast 50 Prozent der KI-Antworten wurden als empathisch eingestuft. 

Auch Ärzte oder Pflegepersonal könnten von einer KI-generierten Zweitmeinung profitieren, wenn sie auf Visite oder im Nachtdienst sind, sagt Claus Beisbart. ""Schon heute werden die meisten medizinischen Entscheidungen im Team getroffen.""

Wie geht es jetzt weiter? In naher Zukunft wird im Krankenhaus keine KI über Leben und Tod entscheiden, aber die Programme werden in den Klinikalltag Einzug halten. Raphael Sznitman leitet das Artorg Center for Biomedical Engineering in Bern und teilt KI-Prognoseprogramme in drei Level ein: Das erste Level betrifft medizinische Entscheidungen, die Ärzte heute schon präzise treffen können, wie das Bewerten von MRT- oder Röntgenbildern. Etliche Bildgebungsgeräte sind bereits mit KI-Software ausgestattet, die Radiologen etwa die Umrisse eines Organs oder eines Tumors vorschlägt. Das spart Zeit: Schwedische Forschende konnten an Daten von mehr als 80.000 Frauen nachweisen, dass Ärzte Brustkrebs mit einer KI-unterstützten Mammografie genauso sicher, aber viel schneller erkannten als ohne. Es ist eine der größten Studien bisher. Im zweiten Level werden KI-Tools Ereignisse vorhersagen, die Mediziner aktuell nur schätzen können – wie das individuelle Risiko einer Hirndruckkrise.

Spannend wird es laut Sznitman beim dritten Level: wenn eine KI völlig neue, dem Menschen unzugängliche Zusammenhänge entdeckt. Ein Forschungsteam in den USA programmierte eine KI, die anhand der Netzhaut eines Menschen ziemlich gut vorhersagen kann, wie hoch dessen Risiko für einen Herzinfarkt oder Schlaganfall in den kommenden fünf Jahren ist. Selbst der erfahrenste Augenarzt wäre dazu nicht imstande. Eine andere KI fand Hinweise für das Auftreten von Bauchspeicheldrüsenkrebs drei Jahre vor Ausbruch der Erkrankung. Dafür durchsuchte das Tool die Gesundheitsdaten von sechs Millionen dänischen und drei Millionen amerikanischen Patienten. KI-Programme dieser Art könnten die Sichtweise auf Gesundheit und Krankheit grundlegend verändern. Doch es wird dauern, bis sie in der Lebenswirklichkeit ankommen.

Nils Schweingruber glaubt, dass sein Hirndruck-Prognoseprogramm in zwei Jahren routinemäßig auf der Intensivstation vor einer Hirndruckkrise warnen kann. Dann wird das Alarm-Orchester um ein weiteres Instrument erweitert: Achtung, Lebensgefahr in 6:00 Stunden, 5:59, 5:58, 5:57, 5:56, 5:57 ...
"
KI,Zeit,2024-04-30,https://www.zeit.de/kultur/film/2024-04/the-fall-guy-ryan-gosling-emily-blunt-film,"Emily Blunt & Ryan Gosling: ""Der Job tut immer weh"" | ZEIT ONLINE","Wer Anfang der Achtzigerjahre in Westdeutschland geboren wurde und vor dem Fernseher aufgewachsen ist, wollte entweder Außerirdischer oder Stuntman werden oder eine paramilitärische Terrororganisation gründen. Das Kinder-TV der damaligen Zeit bestand im Wesentlichen aus den US-Serien Alf, Das A-Team und ein Ein Colt für alle Fälle. Man wurde also in dem Glauben groß, dass jederzeit ein Atomunfall passieren könnte, wie auf Alfs Heimatplaneten Melmac, oder man selbst eine Atombombe bauen müsste, wie in gefühlt allen Folgen des A-Teams, aus ein paar alten Benzinkanistern, Klebeband und anderen Dingen, die man an Tankstellen findet, im Wald oder bei einer rivalisierenden Terrororganisation. Wer wollte nicht voller Nostalgie auf diese Zeit zurückblicken?

Vier Leute, die ganz sicher wollen: Emily Blunt, Ryan Gosling, David Leitch und Kelly McCormick. Blunt und Gosling waren letztes Jahr Kitty und Ken, die oscarnominierten Sidekicks der Barbenheimer-Saga. Bei der Verleihung der Academy Awards gingen sie leer aus, hatten aber mit einer gemeinsamen Hommage an die Stuntmen der Filmbranche den zweitbesten Auftritt des Abends. (Den besten hatte bekanntlich Gosling allein.) Die Aktion geschah nicht ohne Hintergedanken: Drei Monate später sind Gosling und Blunt die Stars in The Fall Guy, einer Kinoadaption der Actionserie Ein Colt für alle Fälle über den Stuntman Colt Seavers. Mitte der Achtziger lief die Show im ZDF und ebnete den deutschen Fernsehweg für das A-Team einerseits und lächerliche Poser wie MacGyver und Renegade andererseits.

David Leitch und Kelly McCormick sind der Regisseur und die maßgebliche Produzentin von The Fall Guy, die Masterminds außerdem hinter Filmen wie Atomic Blonde, Deadpool 2 und Bullet Train. Mitte April kann man sie und ihre Hauptdarsteller in Berlin-Mitte treffen, im ersten Stock eines Luxushotels, der weitgehend freigeräumt wurde für die blockbustergroßen Delegationen von Filmverleih und PR-Agentur. Wie immer, wenn es nach Hollywood riecht, kommt auch Steven Gätjen die Treppe hochgejoggt.

Hier ist der Deal: Man darf Gosling und Blunt zusammen interviewen, wenn man auch McCormick und Leitch zusammen interviewt. Man soll Fragen bitte nur zum Film stellen, keine Quizze oder Trinkspiele anregen und gar nicht erst auf die Idee kommen, um ein Foto zu bitten, ein Autogramm oder eine Ansage für die eigene Mailbox. Die Interviewzeit beträgt jeweils sechs Minuten, wobei damit eigentlich fünf Minuten gemeint sind, denn die sechs Minuten beinhalten auch den sogenannten Turnaround für das nächste Interview. Alle Gespräche werden vor einer bunten Fall-Guy-Tapete gefilmt, gleich danach erhält man das Material in einem kleinen Umschlag auf personalisierten Chipkarten.

Zeit ist wahnsinnig kostbar bei einer solchen Veranstaltung und dann auch wieder gar nicht. Alles erscheint minutengenau durchgeplant, die ganze Hoteletage wuselt um wartende Journalistinnen und Journalisten herum, plötzlich verschiebt sich alles um eine Stunde nach hinten. Es geht zu wie auf einer hochfunktionalen Ameisenfarm oder in der Notaufnahme eines Großstadtkrankenhauses am Freitagabend. Niemand interessiert sich dafür, dass vor dem Eingang zum Hotel auch der Mannschaftsbus des deutschen Vizemeisters FC Bayern München geparkt hat.

Dann ist es so weit und gleich mal enttäuschend: Wer Ryan Gosling zum Interview trifft, trifft eigentlich gar nicht Ryan Gosling, sondern Ryan Gosling verkleidet als Stuntman Colt Seavers. Wie auch in mehreren Szenen aus The Fall Guy trägt er Lederjacke und Stiefel, die ihn auf drollige Weise extrabreit und -markig erscheinen lassen – man wundert sich fast, dass er keinen Zahnstocher im Mund hat. Gosling ist ebenso seidenglänzend geschminkt wie im Film, hat noch immer die straßenköterblond gefärbten Haare seiner Figur und strahlt professionelle Zugewandtheit aus. Für Emily Blunt gibt es solche Auflagen offenbar nicht: In einem witterungstechnisch bedenklichen Sommer-Onesie sitzt sie neben Gosling und ist aus unerklärlichen Gründen geradezu ekstatisch gut drauf.

Blunt und Gosling sind zwar nicht vor westdeutschen Fernsehern, aber mit und im Fall von Gosling sogar im Fernsehen der späten Achtziger und frühen Neunziger aufgewachsen. ""Den Filmen, die damals liefen, sah man ziemlich schnell an, dass es jetzt nicht gerade Arnold Schwarzenegger selbst war, der das Motorrad fuhr"", sagt der frühere Kinderstar über die Aufgabe von Stuntmen. ""Heute wird so etwas natürlich besser kaschiert."" Blunt findet das beinahe unfair: ""Einer meiner ersten Drehs war vor 20 Jahren in Rumänien, da wurde ich gleich mal auf ein Pferd gesetzt, obwohl ich überhaupt nicht reiten konnte. Einer Kollegin von mir ging es ähnlich – und als sie vom Pferd fiel, war es der einzige Stuntman am Set, der sie auffing und ihr wahrscheinlich das Leben rettete.""

Stuntmen sind also Helden, diese Botschaft gehört zur Mission von Blunt und Gosling, aber auch tragische Figuren, wie die Schauspielerin sagt. ""Je besser sie ihren Job machen, desto weniger nimmt man sie wahr. Sie tauchen auf, vollbringen Unglaubliches und verschwinden wieder in der Dunkelheit."" Nicht allerdings in The Fall Guy. Der Film sei als Verbeugung vor allen Stuntmen und ihrer Community gedacht, erfährt man im Interview nebenan von dem Regisseur und ehemaligen Stuntman David Leitch. ""Denn natürlich steckt in jedem Stuntman auch ein Performer. Niemand lässt sich durch einen zerbrechlichen Tisch werfen oder von einem Hochhaus schubsen, ohne dafür eine Reaktion vom Publikum zu erwarten.""
"
KI,Zeit,2024-04-29,https://www.zeit.de/zeit-wissen/2024/03/ki-gesundheitsbereich-diagnose-arzt-patient/komplettansicht,KI im Gesundheitsbereich: Wie geht’s uns denn morgen? | ZEIT ONLINE,"Auf der Intensivstation ist es niemals still. Das langsam tiefer werdende Tut-tut-tut des Pulsoxymeters kündigt einen drohenden Sauerstoffmangel an. Eine leere Medikamentenpumpe bittet mit quäkenden Tönen um Aufmerksamkeit. Am alarmierendsten ist das Fiepen des Notfallalarms. Jetzt zählt jede Minute. Wie viele Ärzte haben schweißgebadet vor einem kritisch kranken Patienten gestanden und sich gewünscht, sie hätten diesen Notfall vorhersehen können? 

Der Neurologe Nils Schweingruber arbeitet daran, diesen Wunsch zu erfüllen. Er versucht, in die Zukunft zu schauen – zumindest für ein paar Stunden. Es ist nicht lange her, da arbeitete er selbst auf einer Intensivstation. Am Universitätsklinikum Hamburg-Eppendorf (UKE) kümmerte er sich um Patienten mit schweren Schlaganfällen und Hirnblutungen. Patienten, deren Gehirn anschwoll, gegen das Innere ihres Schädels drückte und sich selbst zu zerquetschen drohte. Normalerweise herrscht im Gehirn ein Druck von etwa 0,013 bar. Durch eine Schwellung, einen Tumor oder eine Blutung kann der Hirndruck dramatisch steigen. Spätestens ab 0,05 bar besteht Lebensgefahr. Eine spezielle Beatmung, abschwellende Medikamente und eine Narkose können den Hirndruck senken. Als letzte Maßnahme entfernen Neurochirurgen einen Teil der Schädeldecke, um dem Gehirn Platz zu verschaffen.

Am besten wäre es, man könnte eine Hirndruckkrise erkennen, bevor sie entsteht, doch selbst erfahrenen Intensivmedizinern fällt das schwer. Schweingruber kam ein Gedanke: Die Monitore, Beatmungsgeräte und Sensoren, mit denen die Patienten auf der Intensivstation verkabelt sind, produzieren nicht nur Alarmtöne, sondern auch unendlich viele Daten. In der digitalen Patientenakte werden diese mit Laborwerten und medizinischen Notizen verbunden. Könnte die Lösung in diesem Datenmeer versteckt sein?

Das UKE hat sich früh digitalisiert und hütet einen seltenen Schatz, nämlich die Daten von fast 1.400 Hirndruck-Patienten, die im vergangenen Jahrzehnt auf den Intensivstationen behandelt wurden. Das entspricht 63 Behandlungsjahren. Ein perfektes Trainingslager für die künstliche Intelligenz (KI). Nils Schweingruber ließ einen KI-Algorithmus in den Daten nach Mustern suchen, und tatsächlich fand die KI einen Zusammenhang zwischen den Vital-, Labor- und Gerätedaten einerseits und dem Hirndruck auf der anderen Seite. Das Programm kann eine Hirndruckkrise in den folgenden sechs Stunden mit einer Treffsicherheit von 85 Prozent vorhersagen. Das ist genug Zeit, um die richtige Behandlung einzuleiten.

Ein britisches Team programmierte einen ""KI-Arzt"", der die Daten von 90.000 Patienten mit einer schweren Blutvergiftung (Sepsis) durchforstete. In einem virtuellen Vergleich entschied er deutlich treffsicherer als ein menschlicher Kollege und hätte in der Realität viele Menschenleben gerettet. Wissenschaftler aus der Schweiz fütterten eine KI mit den Daten von über 55.000 intensivmedizinischen Patienten, um ein Kreislaufversagen zu prognostizieren. Die KI sagte in einem Testlauf 82 Prozent der Fälle mehr als zwei Stunden vor dem Ereignis korrekt vorher.

Damit solche KI-Prognosen zuverlässig sind, müssen drei Faktoren zusammenkommen, sagt der Medizinethiker Vince Madai, Leiter des Projektteams ""Verantwortungsvolle Algorithmen"" des Berliner Instituts für Gesundheitsforschung, das zur Charité gehört. Erstens eine Menge an Daten. Meist gilt: Je mehr, desto besser. Zweitens arbeiten KIs mit Zahlen. Es ist für eine KI viel einfacher, einen Schlaganfall oder einen Knochenbruch zu erkennen als eine Depression, weil sie jeden Bildpunkt einer Röntgen- oder MRT-Aufnahme als Zahl lesen kann, traurige Gedanken aber nicht. Der dritte Faktor ist die Verfügbarkeit der Daten: Aufgrund der strengen deutschen Datenschutzrichtlinien waren viele Bereiche für die KI bisher nicht zugänglich. Für intensivmedizinische Patienten gibt es aber eine Vielzahl an frei zugänglichen Datenbanken.

Manche KI ist in der Praxis allerdings kläglich gescheitert. Etwa die Sepsis-Prognose eines großen amerikanischen Unternehmens für Gesundheitssoftware, die die versprochene 80-prozentige Trefferquote aus der Testphase weit verfehlte. Stattdessen erkannte sie im Krankenhausalltag nur ein Drittel der Blutvergiftungen, gleichzeitig waren 90 Prozent der Warnungen Fehlalarme. Was war geschehen? Das Hauptproblem war ein Datenleck. So nennt man es, wenn eine KI mit Daten trainiert wird, die in der Praxis gar nicht vorhanden sind. In diesem Fall war die Verabreichung von Antibiotika in das KI-Training mit eingeflossen. Wenn ein Patient Antibiotika bekommt, heißt das jedoch, dass das Ärzteteam eine Infektion bereits in Betracht gezogen hat. Die KI erreichte im Test eine hohe Treffsicherheit, versagte aber in der Praxis dabei, eine Sepsis früher als die Fachleute zu erkennen. Sie hatte geschummelt.

Selbst wenn es nicht zu einem Datenleck kommt, können die Ergebnisse problematische Verzerrungen enthalten, englisch: Bias. Denn in den Gesundheitsdaten sind Benachteiligungen aufgrund von Einkommen, Geschlecht oder Ethnie eingraviert. 2019 stellten Forschende fest, dass eine KI, die in den USA das Schicksal von Millionen von Patienten mitbestimmte, schwarze Menschen systematisch benachteiligte. Der Algorithmus diente dazu, Patienten mit Krankheiten wie Diabetes und einer hohen Wahrscheinlichkeit für einen schlechten Verlauf zu identifizieren und ihnen Unterstützung anzubieten. Für die Vorhersage nutzte die KI unter anderem die Kosten, die eine Person im Gesundheitssystem verursacht. Je weniger Kosten, desto gesünder, las sie aus den Daten. 

Falsch. Schwarze Menschen haben in den USA oft nur eingeschränkten Zugang zum Gesundheitssystem. Die KI unterschätzte, wie krank schwarze Patienten wirklich waren. Nachdem die Forschenden die Kostendaten aus der Berechnung entfernt hatten, stieg der Anteil der Schwarzen, für die die KI ein Gesundheitsprogramm empfahl, von 18 auf 47 Prozent. 

Wenn Verzerrungen im Gesundheitssystem in eine KI eingeschleust werden, verstärken sich Ungleichheiten womöglich. Man könne den Spieß aber umdrehen, argumentiert der Medizinethiker Vince Madai. Der Bias einzelner Ärzte lasse sich nicht testen oder korrigieren, der einer KI schon. Dadurch könnte das Gesundheitssystem insgesamt gerechter werden.

Das Problem ist die mangelnde Transparenz der Algorithmen. Die KI ist eine Blackbox. Wie kam sie zu einer Prognose? Welche Faktoren wurden berücksichtigt, und wie wurden sie gewichtet? Man weiß es nicht. Doch die beste KI ist wertlos, wenn die Nutzer dem Ergebnis nicht trauen. In einer dänischen Studie sollte eine KI die Dispatcher in einer Rettungsstelle – das sind die, die den Notruf annehmen – dabei unterstützen, einen Herzstillstand zu erkennen. Ein Machine-Learning-Programm hörte dafür fast 170.000 Notrufe mit, analysierte dann Gespräche in Echtzeit und gab eine Einschätzung ab, ob der Grund für den Anruf ein Herzstillstand sein könnte. Die KI erkannte 85 Prozent der Herzstillstände, fast zehn Prozent mehr als die Dispatcher allein. 54 Menschen hätten durch das Tool die richtige Diagnose und sofortige Hilfe erhalten können. Haben sie aber nicht. Denn die Dispatcher ignorierten den KI-Ratschlag. ""Die Studie zeigt, was passiert, wenn die Nutzer eines KI-Programms zu wenig in die Entwicklung eingebunden sind"", sagt Madai. Ein Dispatcher, der seit 20 Jahren Notrufe annimmt, lässt sich nicht von irgendeinem Programm die Entscheidung abnehmen. Es gibt auch ein Fachwort dafür: Algorithm Aversion Bias. Eine Art Software-Allergie.

Das Gegenteil davon ist der Automation Bias: Die Nutzer machen immer genau das, was eine Entscheidungshilfe vorgibt. Maschine an, Gehirn aus. ""Automation Bias ist der Grund dafür, dass immer mal wieder Menschen mit dem Auto in einen Fluss fahren, weil sie blind ihrem Navigationssystem folgen"", sagt Madai. Ein Assistenzarzt wagt vielleicht nicht, eine KI-Entscheidung zu hinterfragen. Wer ist er, dem elektronischen Superhirn zu widersprechen?

""Es kommt nicht nur darauf an, welche Daten in ein Programm einfließen, sondern auch, wie das Ergebnis präsentiert wird"", sagt Madai. Es muss erkennbar sein, wie die KI zu ihrem Ergebnis kam und wie sicher es ist. Im Klinikalltag kann das Personal keine langen Anleitungen und keinen Computercode lesen. Madai wünscht sich verlässliche Daten, am besten aus Studien, wie sie auch für die Zulassung von Medikamenten üblich sind. Aktuell ist noch nicht geregelt, unter welchen Bedingungen eine KI eingesetzt werden darf. 

Und was passiert, wenn aufgrund einer KI-Empfehlung Fehler passieren? ""Hier könnte es ein responsibility gap geben"", sagt der Wissenschaftsphilosoph Claus Beisbart vom Center for Artificial Intelligence in Medicine (CAIM) in Bern. Eine Verantwortlichkeitslücke. Die Softwareingenieure können argumentieren, dass sie nur für die Programmierung zuständig sind, die Krankenhausleitung, dass sie das Programm lediglich zur Verfügung stellt, die Ärzte, dass die KI für sie eine Blackbox ist. ""Der Gesetzgeber muss eine Lösung dafür finden, wie die Verantwortung verteilt wird"", sagt Beisbart. Er plädiert für eine Art KI-Versicherung: ""Die Frage nach einer Strafe ist manchmal wenig produktiv, man muss Ausgleich schaffen."" Der Einsatz von KI im Gesundheitswesen kann einen Haufen Geld sparen: durch schnellere Diagnosen und eine bessere Früherkennung. Das eingesparte Geld könnte in einen Versicherungsfonds eingezahlt und im Falle eines KI-bedingten Schadens ausgeschüttet werden, sagt Beisbart. Die KI gibt, die KI nimmt.

Eine andere Möglichkeit gefällt Beisbart noch besser: dass der Patient ins Zentrum der Entscheidung rückt. Eine KI kann schließlich auch Patienten mit Informationen versorgen, verschiedene Therapieoptionen beleuchten und mit Zahlen unterfüttern – ohne eine Entscheidung vorwegzunehmen. Die Meinung der KI-Ärztin wäre dann nur eine von vielen, die Patienten bei ihren Überlegungen berücksichtigen können.

Mehr noch, die KI-Ärztin wäre eine medizinische Expertin, die endlich Rede und Antwort steht. Denn Ärzte sprechen nicht gern über Prognosen. Sie haben Sorge, dass die Arzt-Patient-Beziehung darunter leidet oder Patienten damit nicht umgehen können. Ein weiteres Problem ist, dass Ärzte oft danebenliegen. Eine Übersichtsarbeit von 2022 zeigte, dass der Verlauf einer Erkrankung nur in elf Prozent der Studien richtig vorausgesehen wurde (siehe Grafik auf Seite 73). In den meisten Studien sind die Fachleute zu pessimistisch und die Patienten zu optimistisch. So oder so, die Therapieentscheidung kann darunter leiden. Pessimisten beenden lebensverlängernde Maßnahmen womöglich zu früh, Optimisten riskieren unnötige und belastende Therapien und deren Nebenwirkungen. Eine KI dagegen könne individuelle Prognosen für jeden einzelnen Patienten ausgeben, sagt Beisbart. ""Das verändert das Verhältnis zwischen Arzt und Patient."" Wissen ist Macht, auch in der Medizin.

Natürlich sei es wichtig, wie eine Prognose überbracht werde, sagt Beisbart. Aber selbst auf diesem vermeintlich urmenschlichen Gebiet, Mitgefühl, schlägt die KI den Menschen. Ein Forschungsteam kopierte von der Website ""Ask a doctor"", auf der Ärzte medizinische Fragen beantworten, 195 Fragen und beauftragte ChatGPT, sie ebenfalls zu beantworten. Drei ärztliche Gutachter bewerteten die Antworten von ChatGPT und Fachleuten hinsichtlich Qualität und Empathie, ohne zu wissen, welche von einem Menschen und welche vom Chatbot verfasst worden waren. In fast 80 Prozent der Fälle schnitt ChatGPT besser ab. Nur fünf Prozent der ärztlichen, aber fast 50 Prozent der KI-Antworten wurden als empathisch eingestuft. 

Auch Ärzte oder Pflegepersonal könnten von einer KI-generierten Zweitmeinung profitieren, wenn sie auf Visite oder im Nachtdienst sind, sagt Claus Beisbart. ""Schon heute werden die meisten medizinischen Entscheidungen im Team getroffen.""

Wie geht es jetzt weiter? In naher Zukunft wird im Krankenhaus keine KI über Leben und Tod entscheiden, aber die Programme werden in den Klinikalltag Einzug halten. Raphael Sznitman leitet das Artorg Center for Biomedical Engineering in Bern und teilt KI-Prognoseprogramme in drei Level ein: Das erste Level betrifft medizinische Entscheidungen, die Ärzte heute schon präzise treffen können, wie das Bewerten von MRT- oder Röntgenbildern. Etliche Bildgebungsgeräte sind bereits mit KI-Software ausgestattet, die Radiologen etwa die Umrisse eines Organs oder eines Tumors vorschlägt. Das spart Zeit: Schwedische Forschende konnten an Daten von mehr als 80.000 Frauen nachweisen, dass Ärzte Brustkrebs mit einer KI-unterstützten Mammografie genauso sicher, aber viel schneller erkannten als ohne. Es ist eine der größten Studien bisher. Im zweiten Level werden KI-Tools Ereignisse vorhersagen, die Mediziner aktuell nur schätzen können – wie das individuelle Risiko einer Hirndruckkrise.

Spannend wird es laut Sznitman beim dritten Level: wenn eine KI völlig neue, dem Menschen unzugängliche Zusammenhänge entdeckt. Ein Forschungsteam in den USA programmierte eine KI, die anhand der Netzhaut eines Menschen ziemlich gut vorhersagen kann, wie hoch dessen Risiko für einen Herzinfarkt oder Schlaganfall in den kommenden fünf Jahren ist. Selbst der erfahrenste Augenarzt wäre dazu nicht imstande. Eine andere KI fand Hinweise für das Auftreten von Bauchspeicheldrüsenkrebs drei Jahre vor Ausbruch der Erkrankung. Dafür durchsuchte das Tool die Gesundheitsdaten von sechs Millionen dänischen und drei Millionen amerikanischen Patienten. KI-Programme dieser Art könnten die Sichtweise auf Gesundheit und Krankheit grundlegend verändern. Doch es wird dauern, bis sie in der Lebenswirklichkeit ankommen.

Nils Schweingruber glaubt, dass sein Hirndruck-Prognoseprogramm in zwei Jahren routinemäßig auf der Intensivstation vor einer Hirndruckkrise warnen kann. Dann wird das Alarm-Orchester um ein weiteres Instrument erweitert: Achtung, Lebensgefahr in 6:00 Stunden, 5:59, 5:58, 5:57, 5:56, 5:57 ...
"
KI,Zeit,2024-04-29,https://www.zeit.de/zeit-wissen/2024/03/ki-gesundheitsbereich-diagnose-arzt-patient,KI im Gesundheitsbereich: Wie geht’s uns denn morgen? | ZEIT ONLINE,"Auf der Intensivstation ist es niemals still. Das langsam tiefer werdende Tut-tut-tut des Pulsoxymeters kündigt einen drohenden Sauerstoffmangel an. Eine leere Medikamentenpumpe bittet mit quäkenden Tönen um Aufmerksamkeit. Am alarmierendsten ist das Fiepen des Notfallalarms. Jetzt zählt jede Minute. Wie viele Ärzte haben schweißgebadet vor einem kritisch kranken Patienten gestanden und sich gewünscht, sie hätten diesen Notfall vorhersehen können? 

Der Neurologe Nils Schweingruber arbeitet daran, diesen Wunsch zu erfüllen. Er versucht, in die Zukunft zu schauen – zumindest für ein paar Stunden. Es ist nicht lange her, da arbeitete er selbst auf einer Intensivstation. Am Universitätsklinikum Hamburg-Eppendorf (UKE) kümmerte er sich um Patienten mit schweren Schlaganfällen und Hirnblutungen. Patienten, deren Gehirn anschwoll, gegen das Innere ihres Schädels drückte und sich selbst zu zerquetschen drohte. Normalerweise herrscht im Gehirn ein Druck von etwa 0,013 bar. Durch eine Schwellung, einen Tumor oder eine Blutung kann der Hirndruck dramatisch steigen. Spätestens ab 0,05 bar besteht Lebensgefahr. Eine spezielle Beatmung, abschwellende Medikamente und eine Narkose können den Hirndruck senken. Als letzte Maßnahme entfernen Neurochirurgen einen Teil der Schädeldecke, um dem Gehirn Platz zu verschaffen.

Am besten wäre es, man könnte eine Hirndruckkrise erkennen, bevor sie entsteht, doch selbst erfahrenen Intensivmedizinern fällt das schwer. Schweingruber kam ein Gedanke: Die Monitore, Beatmungsgeräte und Sensoren, mit denen die Patienten auf der Intensivstation verkabelt sind, produzieren nicht nur Alarmtöne, sondern auch unendlich viele Daten. In der digitalen Patientenakte werden diese mit Laborwerten und medizinischen Notizen verbunden. Könnte die Lösung in diesem Datenmeer versteckt sein?

Das UKE hat sich früh digitalisiert und hütet einen seltenen Schatz, nämlich die Daten von fast 1.400 Hirndruck-Patienten, die im vergangenen Jahrzehnt auf den Intensivstationen behandelt wurden. Das entspricht 63 Behandlungsjahren. Ein perfektes Trainingslager für die künstliche Intelligenz (KI). Nils Schweingruber ließ einen KI-Algorithmus in den Daten nach Mustern suchen, und tatsächlich fand die KI einen Zusammenhang zwischen den Vital-, Labor- und Gerätedaten einerseits und dem Hirndruck auf der anderen Seite. Das Programm kann eine Hirndruckkrise in den folgenden sechs Stunden mit einer Treffsicherheit von 85 Prozent vorhersagen. Das ist genug Zeit, um die richtige Behandlung einzuleiten.

Ein britisches Team programmierte einen ""KI-Arzt"", der die Daten von 90.000 Patienten mit einer schweren Blutvergiftung (Sepsis) durchforstete. In einem virtuellen Vergleich entschied er deutlich treffsicherer als ein menschlicher Kollege und hätte in der Realität viele Menschenleben gerettet. Wissenschaftler aus der Schweiz fütterten eine KI mit den Daten von über 55.000 intensivmedizinischen Patienten, um ein Kreislaufversagen zu prognostizieren. Die KI sagte in einem Testlauf 82 Prozent der Fälle mehr als zwei Stunden vor dem Ereignis korrekt vorher.

Damit solche KI-Prognosen zuverlässig sind, müssen drei Faktoren zusammenkommen, sagt der Medizinethiker Vince Madai, Leiter des Projektteams ""Verantwortungsvolle Algorithmen"" des Berliner Instituts für Gesundheitsforschung, das zur Charité gehört. Erstens eine Menge an Daten. Meist gilt: Je mehr, desto besser. Zweitens arbeiten KIs mit Zahlen. Es ist für eine KI viel einfacher, einen Schlaganfall oder einen Knochenbruch zu erkennen als eine Depression, weil sie jeden Bildpunkt einer Röntgen- oder MRT-Aufnahme als Zahl lesen kann, traurige Gedanken aber nicht. Der dritte Faktor ist die Verfügbarkeit der Daten: Aufgrund der strengen deutschen Datenschutzrichtlinien waren viele Bereiche für die KI bisher nicht zugänglich. Für intensivmedizinische Patienten gibt es aber eine Vielzahl an frei zugänglichen Datenbanken.

Manche KI ist in der Praxis allerdings kläglich gescheitert. Etwa die Sepsis-Prognose eines großen amerikanischen Unternehmens für Gesundheitssoftware, die die versprochene 80-prozentige Trefferquote aus der Testphase weit verfehlte. Stattdessen erkannte sie im Krankenhausalltag nur ein Drittel der Blutvergiftungen, gleichzeitig waren 90 Prozent der Warnungen Fehlalarme. Was war geschehen? Das Hauptproblem war ein Datenleck. So nennt man es, wenn eine KI mit Daten trainiert wird, die in der Praxis gar nicht vorhanden sind. In diesem Fall war die Verabreichung von Antibiotika in das KI-Training mit eingeflossen. Wenn ein Patient Antibiotika bekommt, heißt das jedoch, dass das Ärzteteam eine Infektion bereits in Betracht gezogen hat. Die KI erreichte im Test eine hohe Treffsicherheit, versagte aber in der Praxis dabei, eine Sepsis früher als die Fachleute zu erkennen. Sie hatte geschummelt.
"
KI,Zeit,2024-04-29,https://www.zeit.de/digital/internet/2024-04/ki-generierte-produkte-programmier-bots-newsletter-kuenstliche-intelligenz,"KI-generierte Produkte: Trau keiner KI-Demo, die du nicht selbst gefälscht hast | ZEIT ONLINE","Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 18. April 2024. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Haben Sie schon mal jemandem
erzählt, wie toll Ihr Projekt läuft, obwohl Sie in Wirklichkeit gerade erst mit
der Arbeit angefangen haben? Dann wissen Sie, dass das ein riskantes
Unterfangen ist. Fake it till you make it, wie diese Taktik in der Start-up-Welt
oft genannt wird, funktioniert nur, wenn man nicht dabei erwischt wird.

Die Firma Cognition Labs wurde vor
einigen Tagen genau dabei ertappt. Das Produkt des Unternehmens ist noch nicht
öffentlich verfügbar, erlebte aber bereits einen kleinen Hype und war vor vier
Wochen schon einmal Thema in diesem Newsletter: Der Bot Devin soll eigenständig
Programmier-Aufgaben lösen können.

Ein Video, in dem das demonstriert
wird, bekam in sozialen Medien viel Aufmerksamkeit und inspirierte direkt wilde
Spekulationen. Programmiererinnen würden bald massenweise arbeitslos, weil die
KI sie nicht nur bei ihrer Arbeit unterstützen, sondern gleich komplett
ersetzen könnte.

Meine Kollegin Marie Kilg war in
ihrer Bewertung vorsichtiger und sie behielt recht. Der YouTuber Internet of
Bugs zeichnet in einem Video detailliert nach, dass das Demovideo mindestens
stark übertrieben, wenn nicht komplett fake ist.

In für die KI-Welt angenehm
unaufgeregter Art zeigt er zum Beispiel, dass der Bot nicht wirklich Fehler in
dem Code korrigiert, der ihm vorgegeben wurde, sondern in dem Code, den er
selbst vorher produziert hatte. Der YouTuber erklärt, dass das System Dinge
tut, die falsch sind oder zumindest nicht den aktuellen Standards guter
Programmierung entsprechen. Und er weist darauf hin, dass Devin viele Stunden
für die Aufgabe brauchte, er selbst sie aber in gut 30 Minuten lösen konnte.

Er sei nicht gegen KI, sagt der
YouTuber in dem Video, er benutze ChatGPT und andere Tools als Hilfe beim
Programmieren. Und auch Devin leiste durchaus Beeindruckendes. ""Aber sie
mussten ja so tun, als könnte es noch viel mehr"", sagt er. ""Solche
Lügen erzeugen echten Schaden.""

Der Fall ist ein gutes Beispiel für
das Problem, dass wir in der Techwelt oft so tun, als wäre die Zukunft schon
da. Ich schreibe ""wir"", weil auch Journalistinnen und Journalisten
nicht davor gefeit sind, den Hype zu befeuern.

Es liegt ein Reiz darin, das, was
man heute sieht, direkt weiterzudenken. Weil es sich visionär und weitsichtig
anfühlt. Wenn ChatGPT einigermaßen sinnvollen Programmcode generieren kann,
dann überlegen wir sofort, welche Regeln eine Welt braucht, in der KI im
Alleingang ganze Apps entwickelt.

Einerseits ist das richtig. Wir
sollten nicht blind in die Zukunft manövrieren, ohne über sie nachzudenken.
Aber je konkreter Unternehmen Versprechungen über diese Zukunft mit Produkten
verknüpfen, für die man schon heute Geld bezahlen kann, desto größer ist die
Gefahr einer Enttäuschung.

Ein anderes aktuelles Beispiel
dafür ist der Humane Ai Pin. Der KI-Anstecker wurde im vergangenen Jahr
angekündigt. In Produktvideos entstand der Eindruck, dass hier der
Star-Trek-Communicator für 700 Dollar vorzubestellen ist: ein Gerät, das auf
alles eine Antwort hat, Unterhaltungen simultan übersetzen kann und per Laser
ein Display auf die Hand projiziert. Die Zukunft ist jetzt, dachte auch ich.

Nun, da der Pin in den USA
tatsächlich ausgeliefert wird, blamiert er sich in den Tests von
Techjournalisten. Er gibt falsche oder gar keine Antworten oder braucht 13
Sekunden, um zu reagieren, seine Batterie wird zu heiß oder die KI dreht durch,
weil im Namen von Beyoncé ein Accent auf dem ""e"" ist.

Das heißt nicht, dass der KI-Pin
oder Programmier-Bots nicht funktionieren können. Aber sie sind bei Weitem
nicht fertig und nicht so gut, wie ihre Macher uns glauben lassen. Es sind
eigentlich eher Ideen als Produkte. 

Erste Anzeichen lassen erahnen,
dass eine ähnliche Enttäuschung auch mit dem KI-Produkt droht, das eigentlich
der ganz große Geldbringer für die Konzerne werden soll: Büro-Assistenten wie
Microsofts Copilot.

Meine eigenen – eher ernüchternden
– Tests mit dem System passen zu Artikeln wie dem des Techmediums The
Information, in dem es heißt, dass Vertreter von Microsoft, Amazon und Google
Schwierigkeiten haben, ihre KI-Assistenten an Unternehmen zu verkaufen.
Angesichts der hohen Kosten und nach wie vor problematischen Ungenauigkeiten
seien Kunden zurückhaltend mit neuen Ausgaben für KI.

Das ist erst mal nur ein Hinweis,
aber er trägt zu meinem Gefühl bei, dass uns in der KI-Welt eine Zeit der
Ernüchterung bevorsteht.
"
KI,Zeit,2024-04-27,https://www.zeit.de/digital/internet/2024-04/ki-bilder-shrimp-jesus-facebook-betrug,KI-Bilder: Wieso hat dieser Krabben-Jesus 200.000 Likes? | ZEIT ONLINE,
KI,Zeit,2024-04-27,https://www.zeit.de/2024/18/daenemark-moewen-tourismus-sonderborg,Dänemark: Kiu Kiu! | ZEIT ONLINE,
Künstliche Intelligenz,Zeit,2024-04-26,https://www.zeit.de/digital/2024-04/google-microsoft-deutliche-gewinne-herausforderungen-kuenstliche-intelligenz,Künstliche Intelligenz: Google und Microsoft verzeichnen deutlich mehr Umsatz und Gewinn | ZEIT ONLINE,"Die konkurrierenden Unternehmen Google und Microsoft haben trotz KI-Herausforderern zuletzt deutliche Gewinne verzeichnet. Damit übertrafen sie Erwartungen von Expertinnen und Experten. Microsofts Umsatz stieg um 17 Prozent auf knapp 62 Milliarden Dollar (57,82 Milliarden Euro) im Ende März abgeschlossenen dritten Geschäftsquartal im Jahresvergleich. Auch die Anzeigeerlöse von Google nahmen im vergangenen Quartal im Jahresvergleich um etwa 13 Prozent zu und liegen damit bei 61,66 Milliarden Dollar (57,49 Milliarden  Euro).

Microsoft arbeitet eng mit dem ChatGPT-Entwickler OpenAI zusammen und integriert auf dieser Basis KI-Funktionen in immer mehr seiner Produkte. Der Bedarf an Rechenleistung für künstliche Intelligenz treibt wiederum Microsofts Cloud-Geschäft an. 

Die aktuellen Geschäftszahlen deuten darauf hin, dass sich Microsofts Investitionen in Cloud und künstliche Intelligenz auszahlen. Auch der Umsatz der konzerneigenen
Cloud-Sparte Azure stieg um 31 Prozent auf 26,7
Milliarden Dollar (24,9 Milliarden Euro). Der Gewinn von Microsoft wuchs insgesamt um ein Fünftel auf fast 22 Milliarden Dollar (20,52 Milliarden  Euro).

Indes legte die Aktie des Konzerns um gut vier Prozent zu. Microsoft ist mit einem 
Marktwert von rund drei Billionen Dollar (2,8 Billionen Euro) aktuell das wertvollste 
börsennotierte Unternehmen der Welt.

Indes wird die Entwicklung von Googles Werbegeschäft, insbesondere seiner Websuche, sehr genau beobachtet. Die zentrale Frage ist, ob KI-Versuche von Konkurrenzunternehmen, direkt Antworten statt Links anzuzeigen, dem Geschäft der Google-Suchmaschine schaden könnten. Die aktuellen Zahlen allerdings legen nahe, dass Googles Werbegeschäft sich bisher problemlos vor den KI-Herausforderern behaupten konnte. Wie seine Konkurrenten versucht auch Google selbst, die Websuche mit KI-Funktionen aufzubessern.

Bei der Google-Mutter Alphabet insgesamt stieg der Umsatz im 
Jahresvergleich um 15 Prozent auf 80,5 Milliarden Dollar (75,07 
Milliarden Euro). Damit wuchs der Konzerngewinn von gut 15 Milliarden 
Dollar im vergangenen Jahr auf aktuell 23,66 Milliarden Dollar (22,06 
Milliarden Euro). Dazu beigetragen hatte auch das Wachstum des Geschäfts
 mit Software und Rechenleistung aus der Cloud.

Bei einem Anstieg der Alphabet-Aktie im nachbörslichen Handel um gut elf Prozent kündigte das Unternehmen erstmals in seiner Geschichte eine Dividende von 20 Cent pro Aktie an. Auch für die Zukunft stellte das Unternehmen Ausschüttungen in Sicht. Der Internetkonzern will zudem zusätzliche eigene Aktien im Volumen von 70 Milliarden Dollar zurückkaufen.

Neben seiner Suchmaschine ist auch die Videotochter YouTube eine wichtige Einnahmequelle für Google. Die Plattform steuerte rund acht Milliarden Dollar zu Googles Anzeigenerlösen bei, das waren rund 21 Prozent mehr als im Vorjahresquartal. 
"
Künstliche Intelligenz,Zeit,2024-04-25,https://www.zeit.de/arbeit/2024-04/chatgpt-remote-work-portale-stellenausschreibungen-einfluss,Künstliche Intelligenz: Welche Jobs ChatGPT bereits ersetzt | ZEIT Arbeit,
AI,Zeit,2024-04-26,https://www.zeit.de/geld/2024-04/aenderungen-mai-2024-steuer-fluege-ki-regeln-meta,"Änderungen im Mai 2024: Höhere Flugsteuern, KI-Regeln für Facebook und Instagram | ZEIT ONLINE","Reisende müssen sich im Mai auf ein paar Änderungen einstellen. So schaltet die Bahn ihre App für Störmeldungen ab. Flugreisen ab Deutschland könnten teurer werden, und Berliner Busse, Trams und U-Bahnen nehmen keine E-Scooter mehr mit.

Autofahrer hingegen können beim Kauf ihres nächsten Autos auf transparentere Angaben zur Energieeffizienz hoffen – und sollten sich künftig auch in der Schweiz an die Verkehrsregeln halten. Außerdem steigt der Mindestlohn für Pflegekräfte in der Altenpflege, und die Meta-Plattformen Facebook und Instagram wollen auf KI-generierte Inhalte hinweisen. Alle Änderungen im Überblick:

Ab Mai müssen sich Flugreisende auf höhere Ticketpreise einstellen. Die Bundesregierung hat die Luftverkehrsabgabe für Passagierflüge von allen deutschen Flughäfen erhöht. Die Airlines müssen dann höhere Steuern zahlen und können die Mehrbelastung über die Ticketpreise an die Kunden weitergeben. Die Fluggesellschaften zahlen bisher für Flugverbindungen von bis zu 2.500 Kilometer 13,03 Euro pro Passagier an den Staat, für Entfernungen von mehr als 6.000 Kilometer 59,43 Euro. Ab Mai steigen die Steuern dann auf 15,53 bis 70,83 Euro an.

Der sogenannte Streckenagent ist eine App, mit der sich Fahrgäste über aktuelle Störungen auf der Zugstrecke informieren lassen können. Die Deutsche Bahn will den Dienst ab dem 2. Mai einstellen und künftig in die DB-Navigator-App integrieren. Damit sollen die Suche nach Zugverbindungen, die Ticketbuchung und die Echtzeitinformation gebündelt werden.

Eine Skala von A bis G, unterlegt mit den Farben von Dunkelgrün bis Dunkelrot: Die Effizienzklassen, die man von Kühlschränken oder Waschmaschinen kennt, gibt es auch schon länger für Autos. Ab dem 1. Mai sollen sie Autokäufern transparentere Informationen über die Energieeffizienz der Modelle geben. Bislang zählten bei der Eingruppierung in die Effizienzklassen nicht die absoluten CO₂-Emissionen, sondern die Emissionen im Verhältnis zum Gewicht des Autos. Ein SUV, der doppelt so schwer ist wie ein Kleinwagen, aber nicht ganz doppelt so viel CO₂ ausstößt, konnte mit einer besseren Energieeffizienz werben als der Kleinwagen. Künftig werden die Effizienzlabels einzig auf Grundlage der absoluten CO₂-Emissionen pro gefahrenem Kilometer berechnet, der Bezug zum Gewicht entfällt.

Autofahrer, die in der Schweiz gegen die Verkehrsregeln verstoßen, können ab dem 1. Mai auch in Deutschland dafür bestraft werden. Grundlage dafür ist die Reform des Polizeivertrags zwischen Deutschland und der Schweiz. Belangt werden sollen Strafen ab 80 Schweizer Franken, was etwa 70 Euro entspricht. Punkte in Flensburg gibt es für Verkehrsverstöße in der Schweiz nicht. Auch in der Schweiz verhängte Fahrverbote gelten nur innerhalb der Schweiz.

Der Mindestlohn in der Altenpflege steigt im Mai. Die Pflegekräfte sollen dann mindestens 15,50 Euro verdienen. Für qualifizierte Pflegekräfte wird der Mindestlohn auf 16,50 Euro und für Fachkräfte auf 19,50 Euro angehoben.

Facebooks und Instagrams Mutterkonzern Meta will ab Mai den Umgang mit KI-generierten Inhalten auf seinen Onlineplattformen neu regulieren. Die von künstlicher Intelligenz geschaffenen oder veränderten Audioaufnahmen, Bilder und Videos sollen künftig nicht mehr gelöscht, sondern mit dem Label ""Made with AI"" gekennzeichnet werden.  

In manchen Großstädten 
wie München und Leipzig ist die Mitnahme von E-Rollern in öffentlichen 
Nahverkehrsmitteln bereits verboten. Ab Mai verbieten nun auch die Berliner Verkehrsbetriebe (BVG) die elektrisch betriebenen Roller in allen U-Bahnen, Trams und Bussen. 
Auch auf Bahnhöfen werden die E-Roller untersagt. Grund für die neue 
Regelung sind Bedenken über die Sicherheit der in den Rollern verbauten 
Lithium-Ionen-Akkus. Zulässig sind aber weiterhin Elektrofahrräder, 
E-Rollstühle und Seniorenscooter.

Mit Material der Nachrichtenagentur AFP
"
AI,Zeit,2024-04-26,https://www.zeit.de/news/2024-04/26/google-und-microsoft-mit-deutlich-mehr-umsatz-und-gewinn,Quartalszahlen: Google und Microsoft mit deutlich mehr Umsatz und Gewinn | ZEIT ONLINE,"Googles Werbegeschäft trotzt bisher problemlos den KI-Herausforderern. Im vergangenen Quartal stiegen die Anzeigenerlöse von Google im Jahresvergleich um gut 13 Prozent auf 61,66 Milliarden Dollar (57,49 Mrd Euro). Die Videotochter YouTube steuerte dazu gut acht Milliarden Dollar bei - rund 21 Prozent mehr als im Vorjahresquartal.

Die Entwicklung von Googles Werbegeschäft - vor allem in der Websuche als wichtigstem Geldbringer - wird sehr genau beobachtet. Eine zentrale Frage ist, ob Versuche von Konkurrenten, mit Hilfe Künstlicher Intelligenz direkte Antworten statt Links anzuzeigen, eine Spur bei der seit Jahren dominierenden Suchmaschine hinterlassen. Einige Investoren wetten darauf: So wurde der Herausforderer Perplexity AI in einer jüngsten Finanzierungsrunde mit mehr als einer Milliarde Dollar bewertet.

Google versucht unterdessen auch selbst, die Websuche mit KI-Funktionen aufzubessern. Zum Beispiel kann man auf Samsungs neuem Top-Smartphone Galaxy S24 und neueren Modellen von Googles eigenen Telefonen der Marke Pixel Suchanfragen auslösen, indem man ein Objekt oder ein Wort auf dem Display einkreist.

Zuletzt seien solche Angebote verstärkt genutzt worden, sagte Konzernchef Sundar Pichai am Donnerstag. Er zeigte sich überzeugt, dass es Google gelingen werde, das Geschäftsmodell an die neuen KI-Möglichkeiten anzupassen.

Bei der Google-Mutter Alphabet insgesamt stieg der Umsatz im Jahresvergleich um 15 Prozent auf 80,5 Milliarden Dollar. Analysten hatten im Schnitt nur mit rund 79 Milliarden Dollar gerechnet. Der Konzerngewinn wuchs auf 23,66 Milliarden Dollar von gut 15 Milliarden Dollar ein Jahr zuvor.

Dazu trug auch das Wachstum des Geschäfts mit Software und Rechenleistung aus der Cloud bei. Der Bereich steigerte den operativen Gewinn auf 900 Millionen Dollar von 191 Millionen im Vorjahresquartal.

Auch der Rivale Microsoft gab ein klares Signal, dass sich seine Investitionen in Cloud und Künstliche Intelligenz auszahlen. Der Software-Riese schloss einen Pakt mit dem ChatGPT-Entwickler OpenAI und integriert auf dieser Basis KI-Funktionen in immer mehr seiner Produkte. Der Bedarf an Rechenleistung für Künstliche Intelligenz treibt wiederum Microsofts Cloud-Geschäft an.

Microsoft steigerte den Umsatz im Ende März abgeschlossenen dritten Geschäftsquartal im Jahresvergleich um 17 Prozent auf knapp 62 Milliarden Dollar. Der Gewinn wuchs um ein Fünftel auf fast 22 Milliarden Dollar. Beides übertraf die Erwartungen von Experten. Bei Microsofts Cloud-Plattform Azure stieg der Umsatz um 31 Prozent.

Bei Alphabet stieg der Umsatz der sogenannten «anderen Wetten» - Zukunftsprojekten wie selbstfahrende Autos oder Lieferdrohnen - insgesamt von 288 auf 495 Millionen Dollar. Der operative Verlust der Sparte wurde gedrückt - von 1,22 Milliarden Dollar vor einem Jahr auf jetzt gut eine Milliarde Dollar.

Die Alphabet-Aktie sprang im nachbörslichen Handel um gut elf Prozent hoch. Alphabet kündigte erstmals eine Dividende von 20 Cent pro Aktie an - und stellte auch für die Zukunft Ausschüttungen in Aussicht. Microsofts Aktie legte um gut vier Prozent zu. Microsoft ist mit einem Marktwert von rund drei Billionen Dollar aktuell das wertvollste börsennotierte Unternehmen der Welt.


© dpa-infocom, dpa:240426-99-814117/2
"
AI,Zeit,2024-04-25,https://www.zeit.de/news/2024-04/25/zuckerberg-will-meta-zur-nummer-eins-bei-ki-machen,Facebook-Konzern : Zuckerberg will Meta zur Nummer eins bei KI machen | ZEIT ONLINE,"Mark Zuckerberg hat ein neues Ziel: Er will seinen Facebook-Konzern Meta zur Nummer eins bei Künstlicher Intelligenz machen. Der hauseigene Assistent Meta AI solle zum «weltweit führenden KI-Dienst sowohl bei der Qualität als auch bei der Nutzung werden», verkündete der Facebook-Gründer am Mittwoch. Zugleich stimmte Zuckerberg die Anleger darauf ein, dass die KI-Offensive mit Investitionen in Software und Technik teuer werde - es aber Jahre dauern könnte, bis der Konzern damit Geld verdient. 

Solche KI-Ambitionen vernahm die Wall Street mit Schrecken: Die Aktie rauschte im frühen US-Handel am Donnerstag um mehr als zwölf Prozent nach unten, obwohl das Werbegeschäft im vergangenen Quartal weiter auf Hochtouren lief. Zweieinhalb Jahre nachdem Zuckerberg den Konzern von Facebook in Meta umbenennen ließ, um den Fokus auf die verlustreichen virtuellen «Metaverse»-Welten zu betonen, scheint die Aussicht auf einen weiteren Umbruch Investoren den Angstschweiß auf die Stirn zu treiben.

Dabei wirkte Zuckerbergs Programmrede in einer Telefonkonferenz mit Analysten wie das Werben um einen Vertrauensvorschuss der Börse. Er verwies darauf, dass der Ansatz des Facebook-Konzerns, zunächst bei Nutzern beliebte Dienste aufzubauen und erst dann ans Geldverdienen zu denken, immer wieder funktioniert habe. Geschäftsmöglichkeiten sieht er unter anderem in der Kommunikation zwischen Unternehmen und ihren Kunden auf Metas Plattformen wie Facebook, Instagram und WhatsApp.

Auch stellt sich Zuckerberg «KI-Agenten» vor, die anders als heutige Chatbots nicht nur einzelne Fragen beantworten, sondern für die Nutzer auch komplexere Aufgaben übernehmen könnten, die im Hintergrund eigenes Handeln und Recherche erforderten.

Zuckerbergs große KI-Pläne bringen den Konzern stärker in Wettbewerb mit dem ChatGPT-Entwickler OpenAI sowie anderen Tech-Schwergewichten wie Microsoft, Google und Amazon, die alle eine führende Rolle bei Künstlicher Intelligenz spielen wollen. Zugleich hat Meta als strategischen Vorteil die Basis von 3,24 Milliarden Nutzer, die zuletzt täglich auf mindestens eine App des Konzerns zugriffen. Der Konzern muss Szenarien finden, die KI sinnvoll in ihren Alltag integrieren.

Seit vergangener Woche tauchen in den Meta-Apps in den USA und mehreren anderen Ländern Buttons auf, die den KI-Assistenten aktivieren. Große Hoffnungen setzt Zuckerberg auch in die gemeinsam mit Ray-Ban entwickelte vernetzte Brille mit Kamera, Mikrofon und Lautsprechern. Die KI kann in der Brille zum Beispiel Fragen dazu beantworten, was ein Nutzer gerade vor sich hat. «Brillen sind das perfekte Gerät für einen KI-Assistenten, weil sie sehen können, was man sieht, und hören können, was man hört», sagte Zuckerberg.

Die Ray-Ban-Brille wurde in der Sparte Reality Labs entwickelt, die auch an der «Metaverse»-Plattform und den Headsets zur Darstellung virtueller Realität (VR) arbeitet. In der Meta-Bilanz sind die Reality Labs ein chronischer Verlustbringer. Allein im vergangenen Quartal verbuchte der Bereich operativ rote Zahlen von 3,85 Milliarden Dollar. Das war unwesentlich besser als der operative Verlust von knapp vier Milliarden Dollar im Vorjahresquartal.

Zuckerberg argumentierte nun, da die Reality Labs zum Beispiel mit der KI in der Ray-Ban-Brille auch andere Dienste bereicherten, müsse man einen besseren Weg finden, ihren Beitrag darzustellen. Unter den Anlegern gab es eine Zeit lang Murren angesichts der seit Jahren hohen Verluste der Sparte. Doch Zuckerberg hielt eisern daran fest - und als das Anzeigengeschäft nach einer kurzen Delle wieder in Schwung kam, verstummten auch die Kritiker.

Im vergangenen Quartal strömten die Werbedollar wieder in die Meta-Kassen. Der Umsatz stieg im Jahresvergleich um 27 Prozent auf 36,45 Milliarden Dollar. Der Gewinn wurde mit rund 12,4 Milliarden Dollar (11,6 Mrd Euro) mehr als verdoppelt.

Anleger fanden jedoch einen Wermutstropfen im Ausblick: Für das laufende Vierteljahr sagte Meta einen Umsatz zwischen 36,5 und 39 Milliarden Dollar voraus. Analysten hatten im Schnitt mit 38,4 Milliarden gerechnet. Zudem kündigen sich bereits die Kosten der KI-Offensive an. So gibt Meta nun die Spanne für die Ausgaben in diesem Jahr mit 96 bis 99 Milliarden Dollar an. Die bisherige Prognose lag bei 94 bis 99 Milliarden Dollar.

© dpa-infocom, dpa:240425-99-799435/3
"
AI,Zeit,2024-04-24,https://www.zeit.de/news/2024-04/24/amnesty-hilferuf-menschenrechte-weltweit-in-gefahr,Freiheit: Amnesty-Hilferuf: Menschenrechte weltweit in Gefahr | ZEIT ONLINE,"Gezielte Angriffe auf Zivilisten, Kriegsverbrechen und Missbrauch von Künstlicher Intelligenz: In einem verheerenden Jahresbericht hat Amnesty International (AI) weltweit schwere Verstöße gegen die Menschenrechte angeprangert. «Rechtsstaatlichkeit und Menschenrechte sind weltweit so bedroht wie seit Jahrzehnten nicht mehr», teilte die Organisation bei der Vorstellung ihres Jahresberichts mit.

Im Gaza-Krieg wirft Amnesty der islamistischen Hamas wie den israelischen Streitkräften Kriegsverbrechen vor. Schwere Vorwürfe der deutschen Amnesty-Sektion gibt es in diesem Zusammenhang gegen die Ampel-Regierung und besonders gegen Außenministerin Annalena Baerbock (Grüne).

Vor allem die Kriege in der Ukraine und im Gazastreifen sowie im Sudan würden die Universalität der Menschenrechte infrage stellen, heißt es im AI-Bericht. Auch wachsende soziale Ungleichheit und die sich zuspitzende Klimakrise seien eine zunehmende Gefahr.

In einer Art Hilferuf für die Menschenrechte warnte Amnesty vor zahlreichen Problemen, die im Schatten der globalen Krisenherde stünden. Dazu zählten Rückschläge für Frauenrechte etwa in Afghanistan oder harte Anti-Abtreibungsgesetze in mehreren US-Bundesstaaten. Gefährliche Tendenzen gebe es auch beim Einsatz Künstlicher Intelligenz. Ein Überblick:

Die Generalsekretärin von Amnesty International in Deutschland, Julia Duchrow, sagte in Berlin, die Hamas und andere bewaffnete Gruppen hätten mit ihrem brutalen Überfall auf Israel am 7. Oktober Kriegsverbrechen begangen. «Das Leid der Opfer ist durch nichts zu relativieren.» Doch der israelische Militäreinsatz in Gaza habe jedes Maß verloren und gehe mit zahlreichen Kriegsverbrechen und Verstößen gegen das humanitäre Völkerrecht einher. Im Umgang mit bewaffneten Konflikten dominierten Doppelstandards.

Auch die Bundesregierung trage zur Erosion der internationalen Ordnung bei, kritisierte Duchrow. «Sie schweigt zu den Kriegsverbrechen der israelischen Armee und verspielt damit ihre Glaubwürdigkeit. Doppelstandards vertragen sich nicht mit der menschenrechtsbasierten Außenpolitik, die Annalena Baerbock angekündigt hat», kritisierte sie. 

Die Bundesregierung weigere sich, «die Kriegsverbrechen der israelischen Armee beim Namen zu nennen». Amnesty fordere alle Staaten und auch die Bundesregierung auf, keine Waffen an Israel oder andere am Konflikt Beteiligte zu liefern, «bei denen die Gefahr besteht, dass damit Kriegsverbrechen oder Menschenrechtsverletzungen begangen werden».  

Auf die Frage, ob AI die Hamas als Terrorgruppe bezeichne, antwortete Duchrow: «Wir bezeichnen keine Organisation als Terrorgruppe.» Amnesty benutze den Begriff nicht, da er nicht legal völkerrechtlich definiert sei.

Positiv vermerkt Amnesty, dass es endlich einen Bundespolizeibeauftragten gebe - dies schaffe mehr Transparenz und stärke die rechtsstaatliche Kontrolle. Insgesamt zeigten sich die globalen Negativ-Trends aber auch in Deutschland. So erkenne Deutschland strukturellen Rassismus nicht ausreichend an und tue zu wenig, um Menschen vor Hasskriminalität zu schützen. Auch die Meinungs- und Versammlungsfreiheit stehe unter Druck. Ein Beispiel seien pauschale Verbote von Protesten, die sich solidarisch mit Palästinensern zeigten. 

«Ganz schweres Geschütz» mit Hausdurchsuchungen, mehrwöchigem Präventivgewahrsam bis hin zu Ermittlungen wegen Bildung einer kriminellen Vereinigung sei gegen die Klimaaktivisten der Letzten Generation aufgefahren worden, beklagte Duchrow: «Das ist ein Angriff auf das Recht auf friedlichen Protest und die Zivilgesellschaft.» Besonders problematisch sei Bayern, wo eine 30-tägige Präventivhaft verhängt werden könne. 

In vielen Staaten habe es Rückschläge im Kampf für Geschlechtergerechtigkeit gegeben. So hätten es Frauen in den USA immer schwerer, eine Schwangerschaft abzubrechen. In Afghanistan sei der Schulbesuch für Mädchen weiter eingeschränkt worden, im Iran gingen die Behörden mit zunehmender Härte gegen Frauen vor, die sich der Zwangsverschleierung widersetzten. Zahlreiche Regierungen schränkten die Rechte von lesbischen, schwulen, bisexuellen sowie von trans- und intergeschlechtlichen Menschen (LGBTI+) ein. In 62 Ländern gebe es Gesetze, die gleichgeschlechtliche Handlungen kriminalisierten.

Dass die auf Menschenrechten basierende internationale Ordnung offensiv infrage gestellt werde, zeige sich am russischen Angriffskrieg in der Ukraine. So greife Russland dicht besiedelte zivile Gebiete, die Energieinfrastruktur und Getreideexporte an, kritisiert AI. Man habe auch den Einsatz von Folter und anderen Misshandlungen gegen Kriegsgefangene dokumentiert.

Im Sudan verüben laut AI beide Konfliktparteien gezielte und wahllose Angriffe gegen Zivilisten. Der Machtkampf zwischen De-facto-Machthaber Abdel Fattah al-Burhan und seinem früheren Stellvertreter Mohamed Hamdan Daglo hat in den vergangenen zwölf Monaten die mittlerweile größte Flüchtlingskrise weltweit ausgelöst. Nach jüngsten Zahlen des UN-Flüchtlingshilfswerks sind mehr als 8,6 Millionen Menschen innerhalb des Sudans und in den Nachbarländern auf der Flucht. Im Sudan herrscht eine der schlimmsten humanitären Krisen weltweit.

Die Expertin für Menschenrechte im digitalen Zeitalter bei Amnesty Deutschland, Lena Rohrbach, warnte, der zunehmende Einsatz neuer Technologien wie Künstliche Intelligenz, Spionage- oder Gesichtserkennungssoftware wirke oft wie ein Verstärker bei der Bedrohung der Menschenrechtslage. Menschen auf der Flucht würden «zum Experimentierfeld neuer Technologien», etwa durch biometrische Überwachung oder algorithmische Entscheidungssysteme wie angebliche Lügendetektoren an der Grenze. Nötig sei eine robuste, zukunftsfeste Regulierung neuer Technologien, verlangte Rohrbach.

Sie kritisierte auch die während der Olympischen Sommerspiele in Paris geplante Videoüberwachung. Es gehe dabei weniger um Gesichtserkennung, als um eine intelligente Videoüberwachung, die bestimmte vorher festgelegte Situationen erkennen solle - etwa wenn eine große Menschenmenge aus Sicht der Standards, die die KI erhalten habe, zu unruhig werde. «Wir halten das für einen Eingriff in die Persönlichkeitsrechte der Menschen, die da aufgenommen werden», sagte Rohrbach.

© dpa-infocom, dpa:240423-99-779489/3
"
Artificial Intelligence,Zeit,2024-04-25,https://www.zeit.de/arbeit/2024-04/chatgpt-remote-work-portale-stellenausschreibungen-einfluss,Künstliche Intelligenz: Welche Jobs ChatGPT bereits ersetzt | ZEIT Arbeit,
KI,Zeit,2024-04-26,https://www.zeit.de/geld/2024-04/aenderungen-mai-2024-steuer-fluege-ki-regeln-meta,"Änderungen im Mai 2024: Höhere Flugsteuern, KI-Regeln für Facebook und Instagram | ZEIT ONLINE","Reisende müssen sich im Mai auf ein paar Änderungen einstellen. So schaltet die Bahn ihre App für Störmeldungen ab. Flugreisen ab Deutschland könnten teurer werden, und Berliner Busse, Trams und U-Bahnen nehmen keine E-Scooter mehr mit.

Autofahrer hingegen können beim Kauf ihres nächsten Autos auf transparentere Angaben zur Energieeffizienz hoffen – und sollten sich künftig auch in der Schweiz an die Verkehrsregeln halten. Außerdem steigt der Mindestlohn für Pflegekräfte in der Altenpflege, und die Meta-Plattformen Facebook und Instagram wollen auf KI-generierte Inhalte hinweisen. Alle Änderungen im Überblick:

Ab Mai müssen sich Flugreisende auf höhere Ticketpreise einstellen. Die Bundesregierung hat die Luftverkehrsabgabe für Passagierflüge von allen deutschen Flughäfen erhöht. Die Airlines müssen dann höhere Steuern zahlen und können die Mehrbelastung über die Ticketpreise an die Kunden weitergeben. Die Fluggesellschaften zahlen bisher für Flugverbindungen von bis zu 2.500 Kilometer 13,03 Euro pro Passagier an den Staat, für Entfernungen von mehr als 6.000 Kilometer 59,43 Euro. Ab Mai steigen die Steuern dann auf 15,53 bis 70,83 Euro an.

Der sogenannte Streckenagent ist eine App, mit der sich Fahrgäste über aktuelle Störungen auf der Zugstrecke informieren lassen können. Die Deutsche Bahn will den Dienst ab dem 2. Mai einstellen und künftig in die DB-Navigator-App integrieren. Damit sollen die Suche nach Zugverbindungen, die Ticketbuchung und die Echtzeitinformation gebündelt werden.

Eine Skala von A bis G, unterlegt mit den Farben von Dunkelgrün bis Dunkelrot: Die Effizienzklassen, die man von Kühlschränken oder Waschmaschinen kennt, gibt es auch schon länger für Autos. Ab dem 1. Mai sollen sie Autokäufern transparentere Informationen über die Energieeffizienz der Modelle geben. Bislang zählten bei der Eingruppierung in die Effizienzklassen nicht die absoluten CO₂-Emissionen, sondern die Emissionen im Verhältnis zum Gewicht des Autos. Ein SUV, der doppelt so schwer ist wie ein Kleinwagen, aber nicht ganz doppelt so viel CO₂ ausstößt, konnte mit einer besseren Energieeffizienz werben als der Kleinwagen. Künftig werden die Effizienzlabels einzig auf Grundlage der absoluten CO₂-Emissionen pro gefahrenem Kilometer berechnet, der Bezug zum Gewicht entfällt.

Autofahrer, die in der Schweiz gegen die Verkehrsregeln verstoßen, können ab dem 1. Mai auch in Deutschland dafür bestraft werden. Grundlage dafür ist die Reform des Polizeivertrags zwischen Deutschland und der Schweiz. Belangt werden sollen Strafen ab 80 Schweizer Franken, was etwa 70 Euro entspricht. Punkte in Flensburg gibt es für Verkehrsverstöße in der Schweiz nicht. Auch in der Schweiz verhängte Fahrverbote gelten nur innerhalb der Schweiz.

Der Mindestlohn in der Altenpflege steigt im Mai. Die Pflegekräfte sollen dann mindestens 15,50 Euro verdienen. Für qualifizierte Pflegekräfte wird der Mindestlohn auf 16,50 Euro und für Fachkräfte auf 19,50 Euro angehoben.

Facebooks und Instagrams Mutterkonzern Meta will ab Mai den Umgang mit KI-generierten Inhalten auf seinen Onlineplattformen neu regulieren. Die von künstlicher Intelligenz geschaffenen oder veränderten Audioaufnahmen, Bilder und Videos sollen künftig nicht mehr gelöscht, sondern mit dem Label ""Made with AI"" gekennzeichnet werden.  

In manchen Großstädten 
wie München und Leipzig ist die Mitnahme von E-Rollern in öffentlichen 
Nahverkehrsmitteln bereits verboten. Ab Mai verbieten nun auch die Berliner Verkehrsbetriebe (BVG) die elektrisch betriebenen Roller in allen U-Bahnen, Trams und Bussen. 
Auch auf Bahnhöfen werden die E-Roller untersagt. Grund für die neue 
Regelung sind Bedenken über die Sicherheit der in den Rollern verbauten 
Lithium-Ionen-Akkus. Zulässig sind aber weiterhin Elektrofahrräder, 
E-Rollstühle und Seniorenscooter.

Mit Material der Nachrichtenagentur AFP
"
KI,Zeit,2024-04-25,https://www.zeit.de/news/2024-04/25/zuckerberg-will-meta-zur-nummer-eins-bei-ki-machen,Facebook-Konzern : Zuckerberg will Meta zur Nummer eins bei KI machen | ZEIT ONLINE,"Mark Zuckerberg hat ein neues Ziel: Er will seinen Facebook-Konzern Meta zur Nummer eins bei Künstlicher Intelligenz machen. Der hauseigene Assistent Meta AI solle zum «weltweit führenden KI-Dienst sowohl bei der Qualität als auch bei der Nutzung werden», verkündete der Facebook-Gründer am Mittwoch. Zugleich stimmte Zuckerberg die Anleger darauf ein, dass die KI-Offensive mit Investitionen in Software und Technik teuer werde - es aber Jahre dauern könnte, bis der Konzern damit Geld verdient. 

Solche KI-Ambitionen vernahm die Wall Street mit Schrecken: Die Aktie rauschte im frühen US-Handel am Donnerstag um mehr als zwölf Prozent nach unten, obwohl das Werbegeschäft im vergangenen Quartal weiter auf Hochtouren lief. Zweieinhalb Jahre nachdem Zuckerberg den Konzern von Facebook in Meta umbenennen ließ, um den Fokus auf die verlustreichen virtuellen «Metaverse»-Welten zu betonen, scheint die Aussicht auf einen weiteren Umbruch Investoren den Angstschweiß auf die Stirn zu treiben.

Dabei wirkte Zuckerbergs Programmrede in einer Telefonkonferenz mit Analysten wie das Werben um einen Vertrauensvorschuss der Börse. Er verwies darauf, dass der Ansatz des Facebook-Konzerns, zunächst bei Nutzern beliebte Dienste aufzubauen und erst dann ans Geldverdienen zu denken, immer wieder funktioniert habe. Geschäftsmöglichkeiten sieht er unter anderem in der Kommunikation zwischen Unternehmen und ihren Kunden auf Metas Plattformen wie Facebook, Instagram und WhatsApp.

Auch stellt sich Zuckerberg «KI-Agenten» vor, die anders als heutige Chatbots nicht nur einzelne Fragen beantworten, sondern für die Nutzer auch komplexere Aufgaben übernehmen könnten, die im Hintergrund eigenes Handeln und Recherche erforderten.

Zuckerbergs große KI-Pläne bringen den Konzern stärker in Wettbewerb mit dem ChatGPT-Entwickler OpenAI sowie anderen Tech-Schwergewichten wie Microsoft, Google und Amazon, die alle eine führende Rolle bei Künstlicher Intelligenz spielen wollen. Zugleich hat Meta als strategischen Vorteil die Basis von 3,24 Milliarden Nutzer, die zuletzt täglich auf mindestens eine App des Konzerns zugriffen. Der Konzern muss Szenarien finden, die KI sinnvoll in ihren Alltag integrieren.

Seit vergangener Woche tauchen in den Meta-Apps in den USA und mehreren anderen Ländern Buttons auf, die den KI-Assistenten aktivieren. Große Hoffnungen setzt Zuckerberg auch in die gemeinsam mit Ray-Ban entwickelte vernetzte Brille mit Kamera, Mikrofon und Lautsprechern. Die KI kann in der Brille zum Beispiel Fragen dazu beantworten, was ein Nutzer gerade vor sich hat. «Brillen sind das perfekte Gerät für einen KI-Assistenten, weil sie sehen können, was man sieht, und hören können, was man hört», sagte Zuckerberg.

Die Ray-Ban-Brille wurde in der Sparte Reality Labs entwickelt, die auch an der «Metaverse»-Plattform und den Headsets zur Darstellung virtueller Realität (VR) arbeitet. In der Meta-Bilanz sind die Reality Labs ein chronischer Verlustbringer. Allein im vergangenen Quartal verbuchte der Bereich operativ rote Zahlen von 3,85 Milliarden Dollar. Das war unwesentlich besser als der operative Verlust von knapp vier Milliarden Dollar im Vorjahresquartal.

Zuckerberg argumentierte nun, da die Reality Labs zum Beispiel mit der KI in der Ray-Ban-Brille auch andere Dienste bereicherten, müsse man einen besseren Weg finden, ihren Beitrag darzustellen. Unter den Anlegern gab es eine Zeit lang Murren angesichts der seit Jahren hohen Verluste der Sparte. Doch Zuckerberg hielt eisern daran fest - und als das Anzeigengeschäft nach einer kurzen Delle wieder in Schwung kam, verstummten auch die Kritiker.

Im vergangenen Quartal strömten die Werbedollar wieder in die Meta-Kassen. Der Umsatz stieg im Jahresvergleich um 27 Prozent auf 36,45 Milliarden Dollar. Der Gewinn wurde mit rund 12,4 Milliarden Dollar (11,6 Mrd Euro) mehr als verdoppelt.

Anleger fanden jedoch einen Wermutstropfen im Ausblick: Für das laufende Vierteljahr sagte Meta einen Umsatz zwischen 36,5 und 39 Milliarden Dollar voraus. Analysten hatten im Schnitt mit 38,4 Milliarden gerechnet. Zudem kündigen sich bereits die Kosten der KI-Offensive an. So gibt Meta nun die Spanne für die Ausgaben in diesem Jahr mit 96 bis 99 Milliarden Dollar an. Die bisherige Prognose lag bei 94 bis 99 Milliarden Dollar.

© dpa-infocom, dpa:240425-99-799435/3
"
KI,Zeit,2024-04-25,https://www.zeit.de/news/2024-04/25/bueroarbeiter-bangen-wegen-ki-kaum-um-ihre-jobs,Arbeit: Büroarbeiter bangen wegen KI kaum um ihre Jobs | ZEIT ONLINE,"Nur relativ wenige Menschen mit Bürojobs in Deutschland haben Angst, ihren Beruf durch Künstliche Intelligenz (KI) zu verlieren. Einer Aussage mit dieser Befürchtung stimmten nur 28 Prozent der Befragten ganz oder eher zu, wie aus einer Umfrage von YouGov im Auftrag des Technologieunternehmens Slack hervorgeht. 62 Prozent lehnten sie ganz oder eher ab. Befragt worden waren gut 2000 Menschen mit Bürojobs.

Dabei macht die KI jüngeren Menschen offenbar mehr Sorgen als älteren: Bei den 25- bis 34-Jährigen waren 35 Prozent besorgt, bei den 18- bis 24-Jährigen sogar 38 Prozent. Die Zahlen zur jüngsten Altersgruppe sind allerdings mit Vorsicht zu betrachten, da die befragte Stichprobe hier sehr klein war. Mit zunehmendem Alter sinkt die Sorge. Bei den Arbeitnehmern ab 55 fürchten nur noch 22 Prozent um ihren Job. Sie haben allerdings auch deutlich weniger verbleibende Berufsjahre, in denen ihnen die KI in die Quere kommen könnte.

Ein Grund für die insgesamt nicht besonders hohe Angst könnte sein, dass die Ergebnisse der KI-Nutzung durchwachsen bewertet werden. So stimmten nur 7 Prozent der Aussage, dass der Einsatz von KI-Tools zufriedenstellende Ergebnisse bringe, voll und ganz zu. Weitere 31 Prozent stimmten eher zu. 18 Prozent lehnten die Aussage dagegen voll und ganz ab, weitere 17 Prozent eher. 

Und auf die Frage, welche Entscheidungen sie künftig KI überlassen würden, sagten 42 Prozent, sie trauten der KI nicht zu, Entscheidungen zu treffen. Noch am ehesten auf die KI als Entscheider setzten die Befragten im Bereich Einhaltung von Vorschriften, die 22 Prozent nannten, gefolgt von Marketing mit 20 Prozent und Finanzplanung und Budgetierung mit 19 Prozent. Hier waren allerdings Mehrfachnennungen erlaubt.

Insgesamt sind die Belegschaften bei der Bedeutung von KI gespalten. So stimmen 43 Prozent der Aussage, dass das Thema für sie nicht wichtig sei und der Informationsfluss dazu an ihnen vorübergehe ganz oder eher zu. 51 Prozent lehnen sie ab. Das Alter spielt dabei kaum eine Rolle.

Täglich genutzt wird KI der Umfrage zufolge von rund 26 Prozent der Büroarbeiter. 44 Prozent nutzen sie gar nicht, weitere 27 weniger als einmal täglich oder unregelmäßig. Hier spielt das Alter eine Rolle. Die Gruppen bis 44 Jahre sind deutlich KI-affiner als die älteren. 

Die Unternehmen selbst befeuern die Nutzung von KI durch ihre Beschäftigten offenbar nur teilweise. Von offiziellen Empfehlungen des Arbeitgebers, KI oder KI-Tools wie ChatGPT zu nutzen, berichten nur 29 Prozent der Befragten. 

Nina Koch von Slack sieht dagegen großes Potenzial für KI in Unternehmen. Sie erwartet - ausreichendes Vertrauen der Unternehmen und Arbeitnehmer vorausgesetzt - starke Zuwächse in der Produktivität. «KI und Automatisierung sollen keine Arbeitskräfte ersetzen, sondern dabei helfen, Teams von repetitiven und organisatorischen Aufgaben zu befreien», betont sie.

Für die Erhebung hatte YouGov zwischen dem 9. und 22. April 2038 Büroarbeiterinnen und -arbeiter online befragt.

© dpa-infocom, dpa:240425-99-799251/2
"
KI,Zeit,2024-04-25,https://www.zeit.de/news/2024-04/25/uebersetzungsdienst-deepl-fuehrt-ki-sprachassistenten-ein,Internet: Übersetzungsdienst DeepL führt KI-Sprachassistenten ein | ZEIT ONLINE,"Der Übersetzungs-Spezialist DeepL wird sein Angebot um einen Sprachassistenten mit Künstlicher Intelligenz erweitern. Das kündigte das Unternehmen am Donnerstag in Köln an. Anders als herkömmliche generative KI‑Tools oder Grammatikprüfungen begleite das neue Produkt DeepL Write Pro den kreativen Schreibprozess mit KI‑gestützten Echtzeit-Optimierungsvorschlägen zu Wortwahl, Formulierung, Stil und Ton. So könnten Nutzer ihre Texte unabhängig von ihren Sprachkenntnissen optimieren und die richtigen Worte für jede Situation und jedes Zielpublikum finden.

Das neue Produkt DeepL Write Pro sei das erste Produkt der Firma, das auf einem eigenen großen KI-Sprachmodell (Large Language Model oder LLM) basiere. LLM sind maschinelle Lernmodelle, die darauf trainiert sind, menschliche Sprache zu verstehen und zu generieren. Bekannte LLM sind GPT von OpenAI, Gemini von Google oder Llama vom Facebook-Konzern Meta.

Der neue KI‑Schreibassistent richtet sich vor allem an gewerbliche Anwender und soll Teams in Firmen beim Verfassen von geschäftlichen Inhalten unterstützen. So könnten Unternehmen eine präzise und sichere Kommunikation auf globaler Ebene gewährleisten – von internen Inhalten bis hin zu externen Kundenmitteilungen und Verträgen, erklärte DeepL.

Das Kölner Start-up tritt damit gegen Chatbots und andere KI-Tools von großen IT-Konzernen wie Microsoft und Google an, aber auch gegen das weltweit führende KI-Start-up OpenAI. In der Vergangenheit musste sich DeepL mit seiner Übersetzungs-App vor allem gegen Google Translate behaupten und erzielte in Testberichten oft bessere Bewertungen als der Dienst des US-Konzerns. Allerdings unterstützt der DeepL Übersetzer nur 30 verschiedene Sprachen, während Google Translate Texte in 133 Sprachen übersetzen kann.

© dpa-infocom, dpa:240425-99-801745/2
"
KI,Zeit,2024-04-24,https://www.zeit.de/digital/2024-04/synthetische-daten-kuenstliche-intelligenz-chatgpt,Synthetische Daten: Kann KI von KI lernen? | ZEIT ONLINE,
Künstliche Intelligenz,Zeit,2024-04-22,https://www.zeit.de/news/2024-04/22/microsoft-ki-reif-fuer-industrielle-produktionsprozesse,Künstliche Intelligenz: Microsoft: KI reif für industrielle Produktionsprozesse | ZEIT ONLINE,"Systeme mit Künstlicher Intelligenz sind nach Einschätzung des Softwarekonzerns Microsoft inzwischen so ausgereift, um in industrielle Produktionsprozesse integriert zu werden. «Es geht nicht mehr darum, staunend zu erleben, was die Technologie so alles kann», sagte Microsoft-Manager Florian Deter zum Auftakt der Hannover Messe der Deutschen Presser-Agentur.  

Inzwischen gehe es darum, mithilfe der KI einen konkreten Wertbeitrag zu leisten. «Um dieses Ziel zu erreichen, muss die KI jetzt die Labore verlassen und Einzug in den Maschinenraum der deutschen Wirtschaft halten», sagte Deter, der in der Geschäftsführung von Microsoft Deutschland das Großkundengeschäft verantwortet.

Beim Einsatz der KI in der Industrie gehe es nicht nur um eine Steigerung der Produktivität, sondern auch um Themen wie Zuverlässigkeit oder Arbeitsschutz, sagte der Manager. Deutschland sei in einer sehr guten Position, um von den kommenden Veränderungen durch KI-Technologien zu profitieren. 

«Das liegt nicht nur an der sehr starken industriellen Basis. Der Standort Deutschland ist sehr stark in der KI-Grundlagenforschung und gehört zu den weltweit führenden KI-Patentanmeldern.» Hierzulande seien mehrere KI-Cluster entstanden, darunter Tübingen, Dresden, Heilbronn und München. Die TU München zähle nach verschiedenen Rankings, beispielsweise von der OECD, zu den forschungsstärksten KI-Instituten weltweit.

Nach Deters Einschätzung kann KI sich in eine Schlüsseltechnologie für ein nachhaltiges Wachstum in Deutschland entwickeln. Dazu müsse jedoch das über Jahrzehnte aufgebaute traditionelle Know-how der deutschen Industrie mit den neuen KI-Technologien verknüpft werden. «Dann kann es auch gelingen, eine starke Grundlagenforschung in marktfähige Produkte zu übertragen. KI kann auch dabei helfen, bestimmte Standortschwächen auszugleichen und beispielsweise den Fachkräftemangel abzufedern.»

Microsoft ist neben Google, Meta und Amazon einer der führenden Anbieter von KI-Systemen weltweit, auch weil Microsoft-CEO Satya Nadella frühzeitig Milliarden in eine umfassende Kooperation mit der kalifornischen KI-Start-up OpenAI und den dort entwickelten Chatroboter ChatGPT investiert hat. 

Auf der Hannover Messe zeigt Microsoft aber auch Integrationen mit Sprachmodellen, die nicht von OpenAI stammen, etwa Mistral aus Frankreich. Man könne Industrieunternehmen in die Lage versetzen, ihre eigenen KI-Co-Piloten zu erstellen. Diese seien auch für den Umgang mit sensiblen Daten geeignet, die nicht öffentlich werden dürfen.

© dpa-infocom, dpa:240422-99-760962/2
"
Künstliche Intelligenz,Zeit,2024-04-21,https://www.zeit.de/campus/2024/03/kuenstliche-intelligenz-studierende-gruender-start-up/komplettansicht,Künstliche Intelligenz: Generation GPT | ZEIT Campus,
Künstliche Intelligenz,Zeit,2024-04-21,https://www.zeit.de/campus/2024/03/kuenstliche-intelligenz-studierende-gruender-start-up,Künstliche Intelligenz: Generation GPT | ZEIT Campus,
Künstliche Intelligenz,Zeit,2024-04-19,https://www.zeit.de/news/2024-04/18/erste-digitalministerkonferenz-wissing-offen-fuer-ki,Künstliche Intelligenz: Wissing bei erster Digitalministerkonferenz: KI fördern | ZEIT ONLINE,"Bundesdigitalminister Volker Wissing (FDP) sieht Deutschland bei der Entwicklung von Technologien der Künstlichen Intelligenz (KI) in einer führenden Rolle weltweit und plädiert für einen offenen Umgang mit digitalen Entwicklungen. Zugleich forderte er, die Länder sollten das Gesetz für eine digitale Verwaltung nicht länger blockieren.

Deutschland sei bei den KI-Patenten die Nummer zwei auf der Welt nach den USA und vor China und Japan, sagte Wissing bei der ersten Digitalministerkonferenz am Freitag in Potsdam. Diese Chance dürfe nicht verspielt werden. «Ohne Künstliche Intelligenz wird keine Volkswirtschaft wettbewerbsfähig bleiben können», so Wissing. Deutschland solle dabei selbst die Technologie in der Hand haben, sagte der Minister. Aus seiner Sicht ist es daher wichtig, KI mit Offenheit zu begegnen und ihren breiten Einsatz zu fördern. Am Freitag gründeten die Länder die Digitalministerkonferenz, um ihre Zusammenarbeit auf diesem Gebiet zu stärken.

Die Bundesregierung will mehr Behördengänge durch einheitliche digitale Verfahren ersetzen, doch das Onlinezugangsgesetz (OZG 2.0) stößt vor allem bei unionsgeführten Ländern auf Kritik. Minister Wissing sagte, bei der Reform dieses Gesetzes, das im Bundesrat gescheitert war, sei nun eine rasche Lösung nötig. Die Länder sollten es nicht auf Dauer blockieren. Das Onlinezugangsgesetz bezieht sich auf Bundesverwaltungen, es soll aber auch auf die Bundesländer und Kommunen ausstrahlen. Es soll auch verhindert werden, dass in den Verwaltungen mehrfach Software für die Lösung des gleichen Problems entwickelt wird.

Der Branchenverband Bitkom kritisierte am Freitag in einer Mitteilung, bei der Digitalisierung herrsche in Deutschland zu oft Kleinstaaterei und Kirchturmpolitik. Ein Flickenteppich unterschiedlicher Regelungen zum Beispiel in den Verwaltungen und im Datenschutz mache es Verbrauchern und Unternehmen unnötig schwer und erzeuge Kosten ohne Nutzen. Der Verband begrüße daher die Einrichtung einer festen Digitalministerkonferenz, die zu einer bundesweiten Beschleunigung und Harmonisierung der Digitalpolitik beitragen solle. Auch die Berliner Staatssekretärin für Digitalisierung und Verwaltungsmodernisierung, Martina Klement, sagte: «Wir müssen sicherstellen, dass nicht jeder sein eigenes Süppchen kocht.»

Bei einem Digital-Ranking des Branchenverbandes Bitkom zeigten sich Unterschiede zwischen den 16 Bundesländern. Hamburg setzte sich vor Berlin und Bayern an die Spitze. Die niedrigsten Indexwerte erreichten Sachsen-Anhalt und Thüringen. Brandenburg landete auf dem elften Platz. Verglichen wurden vier Bereiche: «digitale Wirtschaft», «digitale Infrastruktur», «Governance & digitale Verwaltung» sowie «digitale Gesellschaft». Dabei wurden unter anderem der 5G-Ausbau, die Glasfaserversorgung oder die Ladeinfrastruktur für Elektroautos untersucht, aber auch die Zahl der Informatik-Pflichtstunden an den Schulen, die Nutzung digitaler Behördenleistungen oder die Einstellung der Bevölkerung zur Digitalisierung.

© dpa-infocom, dpa:240418-99-726143/4
"
AI,Zeit,2024-04-22,https://www.zeit.de/news/2024-04/22/hochschulen-fuerchten-um-geld-aus-soeders-hightech-agenda,Spitzenforschung : Hochschulen fürchten um Geld aus Söders Hightech-Agenda | ZEIT ONLINE,"Bayerns Universitäten und Hochschulen fürchten um die milliardenschweren Fördergelder der Staatsregierung zur Hightech-Agenda (HTA) von Ministerpräsident Markus Söder (CSU). In einem dreiseitigen Brief warnen die Universitätskonferenz und die Hochschulen für angewandte Wissenschaften (HaW) das Wissenschaftsministerium vor einer massiven Gefährdung der Ziele der HTA. Das Schreiben liegt der Deutschen Presse-Agentur in München vor, zuerst hatte die «Süddeutsche Zeitung» über den Sachverhalt berichtet.

«Das übergeordnete Ziel der massiven Investitionen im Rahmen der HTA liegt darin, Zukunftstechnologien zu fördern, dafür die besten Köpfe für den Wissenschaftsstandort Bayern zu gewinnen und die Investitionen der HTA in Innovationen umzumünzen, sodass der Freistaat nicht nur bundesweit, sondern auch international eine Vorreiterrolle in der technologischen Entwicklung einnimmt», heißt es im Schreiben. Dieses Ziel sei angesichts des im Raum stehenden Einzugs von sogenannten Restmitteln aber «massiv gefährdet». Es bestehe die Gefahr eines Reputationsverlustes der Staatsregierung als verlässlicher Finanzierer von weltweit führender Wissenschaft in hoch kompetitiven Zukunftsfeldern.

«Zur Frage, in welchem Umfang eine Verwendung von Restmitteln der HTA für gemeinsame und übergreifende Aufgaben möglich ist, befinden wir uns mit den Hochschulen im engen und konstruktiven Austausch», sagte Wissenschaftsminister Markus Blume (CSU) am Montag auf Nachfrage der dpa. Mit der Rahmenvereinbarung bis 2027 und den darauf aufbauenden Hochschulverträgen hätten die Hochschulen in Bayern umfassende Planungssicherheit erhalten. «Die verstetigte Hightech Agenda Bayern und die besonders geschützten Haushaltsansätze der Hochschulen sind bundesweit einmalig. Das gilt auch für die Haushaltssteigerungen der Hochschulen von über 35 Prozent in den letzten fünf Jahren.»

Ministerpräsident Markus Söder (CSU) hatte die Hightech-Agenda 2019 gestartet. Mit einem Investitionsvolumen von anfangs 3,5 Milliarden Euro sollten 13.000 neue Studienplätze und 2500 neue Stellen geschaffen werden, darunter 1000 neue Professuren. Im April 2023 kündigte Blume an, die HTA bis mindestens 2027 fortschreiben und dafür auch weitere zwei Milliarden Euro bereitstellen zu wollen.

Unter sogenannten Restmitteln werden nicht verwendete Gelder verstanden, diese fließen nach einer gewissen Zeit wieder zurück in den Staatshaushalt. Im Schreiben erklären die beiden Absender, dass die «Verausgabung von Sachmitteln zumeist im Rahmen von Investitionen in Infrastruktur» stattfinden würden. «Diese sind jedoch erst dann sinnvoll vorzunehmen, wenn die neuen Professorinnen und Professoren erfolgreich berufen sind, weil diese am besten wissen, was sie für eine erfolgreiche Arbeit benötigen.»

Da die Hochschulen sich in den Berufungszusagen für die Professuren «mit meist 5-jähriger Laufzeit gegenüber den Neuberufenen zur Bereitstellung der Ressourcen verpflichtet» hätten, wäre ein Ausfall der Zusagen «fatal für die Planungssicherheit der Hochschulen und hätte direkte Auswirkungen auf die Umsetzung der HTA und die Reputation des Freistaats als Wissenschaftsstandort».

Generell sei die finanzielle Lage der Universitäten und Hochschulen herausfordernd, heißt es weiter im Schreiben: «Trotz der erreichten Haushaltssteigerungen sinkt die finanzielle Ausstattung der Hochschulen aufgrund der Inflation und steigenden Personalkosten real.» In Verbindung damit sei der Einzug von jeweils 8 Prozent der verplanten HTA-Restmittel in den vergangenen beiden Jahren bereits sehr schmerzlich gewesen. «Wir haben diese Einbußen aber in einer zugegebenermaßen schwierigen politischen und wirtschaftlichen Lage mitgetragen. Weitere Einzüge sind aus unserer Sicht nicht vertretbar, ohne die Ziele der HTA akut zu gefährden.»

© dpa-infocom, dpa:240422-99-766455/4
"
AI,Zeit,2024-04-19,https://www.zeit.de/digital/internet/2024-04/meta-kuenstliche-intelligenz-assistent-whatsapp-facebook-instagram/seite-2,Meta : Llama 3 ist frei verfügbar | ZEIT ONLINE,"Anders als die entsprechenden Modelle von OpenAI und Google ist Llama 3 Open Source und soll sich auf der Seite des Konzerns frei verfügbar und kostenlos herunterladen lassen. Das könnte auch für Unternehmen interessant sein, die sich unabhängiger von geschlossenen Systemen wie von OpenAI machen wollen.

Bisher existiert das Sprachmodell Llama 3 in mindestens zwei unterschiedlich großen Versionen. Die größere Variante fußt auf 70 Milliarden Parametern, die kleinere Llama-3-Variante auf acht Milliarden Parametern. Der Assistent Meta AI nutzt nach eigenen Angaben eine verkleinerte Variante des großen Sprachmodells. Ein drittes größeres Modell soll noch folgen.

Mit dem neuen Sprachmodell will der Konzern zum Konkurrenten OpenAI und dessen Chatbot ChatGPT aufschließen. Yann LeCun, Metas leitender KI-Wissenschaftler, deutete gegenüber dem US-Magazin Wired an, dass bereits ein leistungsfähiger Nachfolger von Llama 3 in Arbeit sei, der die besten geschlossenen KI-Modelle der Welt, darunter GPT-4 von OpenAI und Gemini von Google, in den Schatten stellen könnte. 

Das Unternehmen hat eigene Benchmark-Daten veröffentlicht. Demnach schneide Llama 3 besser in der Kategorie Weltwissen als das Sprachmodell Gemini 1.5 von Google ab, außerdem erzielte es in Mathematik höhere Werte als das Modell 7B Instruct des französischen KI-Start-ups Mistral. Solche Benchmarks darf man aber höchstens als Indikatoren verstehen, weil sie nicht unbedingt etwas über die tatsächliche Nutzungsqualität aussagen.

Im vergangenen Jahr hatte Microsoft ChatGPT von OpenAI bereits in seine Suchmaschine Bing integriert. Google hatte ebenfalls im vergangenen Jahr generative KI in seine Produkte wie Docs, Gmail und Google Search eingebaut und für Nutzer, die ein Abo für Microsofts Office-Software haben, zugänglich gemacht. Künstliche Intelligenz kommt langsam im Alltag an. 
"
AI,Zeit,2024-04-19,https://www.zeit.de/digital/internet/2024-04/meta-kuenstliche-intelligenz-assistent-whatsapp-facebook-instagram,Meta : WhatsApp kann jetzt Restaurants empfehlen | ZEIT ONLINE,"Sich mal schnell die Nachricht bei WhatsApp korrigieren oder im Chat ein Bild erstellen lassen: Das soll bald für Milliarden Menschen möglich werden. Meta, das Unternehmen hinter WhatsApp, Instagram und Facebook, hat angekündigt, seinen smarten Assistenten Meta AI weltweit ausrollen und neue Funktionen für die Apps Instagram, WhatsApp, Facebook, im Messenger, die Ray-Ban Smart Glasses sowie bald auch auf der Quest 3 anbieten zu wollen. 

Gleichzeitig hat der Konzern eine neue Version seines Sprachmodells angekündigt, auf dem Meta AI basiert: Llama 3 soll quelloffen und somit auch für andere Unternehmen, Forscherinnen oder Entwickler nutzbar sein.

Die Firma in Menlo Park bietet den KI-Assistenten Meta AI künftig in 14 Ländern an, darunter etwa in Kanada, Australien oder Malawi. In Deutschland ist er vorerst nicht verfügbar, ein genaues Datum für eine Veröffentlichung ist noch nicht bekannt. Wer hierzulande die Webseite Meta.AI aufruft, über die der Chatbot ebenfalls verfügbar sein soll, bekommt bisher lediglich den Hinweis zu sehen: ""Meta AI ist in Ihrem Land noch nicht verfügbar.""

Meta AI ist bisher zudem nur in englischer Sprache verfügbar. Nutzbar sein soll der KI-Assistent beispielsweise im Feed, in Chats oder in der Suche der Meta-Apps, um Dinge zu erledigen und auf Echtzeitinformationen zuzugreifen, ohne die App verlassen zu müssen.

Auf seiner Seite hat der Konzern veröffentlicht, wie die neuen Funktionen konkret aussehen sollen: Wer sich beispielsweise in einem Gruppenchat mit Freunden darüber abstimmen will, in welchem Park man sich am besten trifft, kann sich dort direkt Vorschläge des KI-Assistenten generieren lassen. Oder man lässt Meta AI nach Restaurants mit veganem Essen oder nach Konzerttipps suchen.

Die Informationen, die Meta AI anzeigt, betont der Konzern, seien Echtzeitinformationen. Meint: Wenn das Restaurant, das man mit seinen Freunden ansteuern will, gerade auf seiner Homepage veröffentlicht hat, dass es die Woche geschlossen hat, dann dürfte Meta AI es nicht empfehlen. Wer beim Organisieren solcher Treffen sonst permanent zwischen App und Onlinesuche hin und her springen muss, der weiß, wie nervig Terminfindung sein kann.

Offen bleibt derweil, ob Chatbots wie von Meta Suchmaschinen wie von Google eines Tages komplett ablösen könnten. Bisher verlinkt der neue Assistent in seinen Antworten teilweise noch auf öffentliche Suchmaschinen.

Ein anderes Feature: Wer zum Beispiel umzieht und neue Möbel sucht, kann sich, während man noch tippt, direkt Vorschläge als Bilder im WhatsApp-Chat generieren lassen, wie die Ästhetik in der Wohnung aussehen könnte.  Das soll ebenfalls in Echtzeit gehen. Eine entsprechende Beta-Version des Imagine-Features werde derzeit in den USA ausgerollt, schreibt Meta dazu. Die Bilder lassen sich auch entsprechend animieren und beispielsweise in ein Gif, also ein Kurzvideo, umwandeln. Und wer durch seinen Facebook-Feed scrollt, kann sich zu einzelnen Beiträgen weitere Informationen generieren lassen.

Mit dem Chatbot Meta AI, der im September vergangenen Jahres veröffentlicht wurde, gibt es bisher allerdings sehr unterschiedliche Erfahrungen. Das US-Portal 404 Media berichtete, der Chatbot würde sich in privaten Facebook-Gruppen mit teils seltsamen Beobachtungen einschalten. In einer Gruppe für Eltern hatte der Bot etwa geschrieben, ein sogenanntes 2e-Kind zu haben. Gemeint sind damit Kinder, die hochbegabt sind und eine Schwäche, etwa beim Lernen oder in der Wahrnehmung, haben. In dem Chat empfahl der Bot auch direkt ein passendes Programm an einer New Yorker Schule, was ein Facebook-Nutzer daraufhin mit den Worten kommentierte: ""Was zum Black Mirror ist das hier?""

Klar ist: Wenn Meta die neue KI-Technologie weiter ausrollt, dürfte der US-Konzern Milliarden Menschen auf einmal mit seinen Tools erreichen. Rund drei Milliarden Nutzer verwenden weltweit Instagram, Facebook oder WhatsApp.
"
Artificial Intelligence,Zeit,2024-04-21,https://www.zeit.de/campus/2024/03/kuenstliche-intelligenz-studierende-gruender-start-up/komplettansicht,Künstliche Intelligenz: Generation GPT | ZEIT Campus,
KI,Zeit,2024-04-22,https://www.zeit.de/news/2024-04/22/ki-reif-fuer-maschinenraum-der-deutschen-wirtschaft,"Microsoft: KI reif für ""Maschinenraum der deutschen Wirtschaft"" | ZEIT ONLINE","Systeme mit Künstlicher Intelligenz sind nach Einschätzung des Softwarekonzerns Microsoft inzwischen so ausgereift, um in industrielle Produktionsprozesse integriert zu werden. «Es geht nicht mehr darum, staunend zu erleben, was die Technologie so alles kann», sagte Microsoft-Manager Florian Deter am Montag zum Auftakt der Hannover Messe der Deutschen Presser-Agentur.

Inzwischen gehe es darum, mithilfe der KI einen konkreten Wertbeitrag zu leisten. «Um dieses Ziel zu erreichen, muss die KI jetzt die Labore verlassen und Einzug in den Maschinenraum der deutschen Wirtschaft halten», sagte Deter, der in der Geschäftsführung von Microsoft Deutschland das Großkundengeschäft verantwortet.

Beim Einsatz der KI in der Industrie gehe es nicht nur um eine Steigerung der Produktivität, sondern auch um Themen wie Zuverlässigkeit oder Arbeitsschutz, sagte der Manager. Deutschland sei in einer sehr guten Position, um von den kommenden Veränderungen durch KI-Technologien zu profitieren. «Das liegt nicht nur an der sehr starken industriellen Basis. Der Standort Deutschland ist sehr stark in der KI-Grundlagenforschung und gehört zu den weltweit führenden KI-Patentanmeldern.» Hierzulande seien mehrere KI-Cluster entstanden, darunter Tübingen, Dresden, Heilbronn und München. Die TU München zähle nach verschiedenen Rankings, beispielsweise von der OECD, zu den forschungsstärksten KI-Instituten weltweit.

Nach Deters Einschätzung kann KI sich in eine Schlüsseltechnologie für ein nachhaltiges Wachstum in Deutschland entwickeln. Dazu müsse jedoch das über Jahrzehnte aufgebaute traditionelle Know-how der deutschen Industrie mit den neuen KI-Technologien verknüpft werden. «Dann kann es auch gelingen, eine starke Grundlagenforschung in marktfähige Produkte zu übertragen. KI kann auch dabei helfen, bestimmte Standortschwächen auszugleichen und beispielsweise den Fachkräftemangel abzufedern.»

Microsoft ist neben Google, Meta und Amazon einer der führenden Anbieter von KI-Systemen weltweit, auch weil Microsoft-CEO Satya Nadella frühzeitig Milliarden in eine umfassende Kooperation mit der kalifornischen KI-Start-up OpenAI und den dort entwickelten Chatroboter ChatGPT investiert hat. Auf der Hannover Messe zeigt Microsoft aber auch Integrationen mit Sprachmodellen, die nicht von OpenAI stammen, etwa Mistral aus Frankreich. Man könne Industrieunternehmen in die Lage versetzen, ihre eigenen KI-Co-Piloten zu erstellen. Diese seien auch für den Umgang mit sensiblen Daten geeignet, die nicht öffentlich werden dürfen.

© dpa-infocom, dpa:240422-99-761313/3
"
KI,Zeit,2024-04-21,https://www.zeit.de/news/2024-04/21/theatertreffen-leiterin-hat-keine-angst-vor-ki,Kultur: Theatertreffen-Leiterin hat keine Angst vor KI | ZEIT ONLINE,"Den Einsatz von Künstlicher Intelligenz (KI) im Theater sieht die neue Leiterin des Berliner Theatertreffens Nora Hertlein-Hull eher gelassen. ""Eine Gefahr im Einsatz von KI sehe ich gar nicht"", sagte Hertlein-Hull der Deutschen Presse-Agentur in Berlin. ""Ich glaube, im Theater besteht weniger die Gefahr, sozusagen das Medium an sich infrage zu stellen als zum Beispiel im Film."" Während des Hollywood-Streiks im vergangenen Jahr hatten Schauspieler unter anderem gefürchtet, KI könne künftig ihre Arbeit übernehmen. ""Das kann man im Theater nicht so leicht"", sagte sie.

Sie könne sich vorstellen, dass Künstliche Intelligenz auf Textebene zum Einsatz kommen wird. ""Beim Thema Autorenschaft wird KI sicherlich eine Rolle spielen, also dass man mit Texten spielen kann, die künstlich erstellt wurden. Ich glaube aber nicht, dass der humane Fingerabdruck aus dem Theater entfernt werden wird"", sagte Hertlein-Hull, die Anfang des Jahres die Leitung des Theatertreffens übernommen hat. Das Gipfeltreffen der deutschsprachigen Bühnen beginnt am 2. Mai.

An ersten Theatern ist das Thema KI schon angekommen - wie zum Beispiel am Schubert Theater in Wien. In der bereits abgespielten Inszenierung ""Projekt Pinocchio"" des Figurentheaters schrieb das Texttool GPT-2 rund 70 Prozent des Stücks, erklärte Regisseur und Theaterleiter Simon Meusburger. Das Sprachmodell habe er mit Originaltexten von Pinocchio gefüttert.

Auch sei ausschließlich Musik einer KI verwendet worden. Im kreativen Bereich stünden auch Produktionen mit kleinen Budgets vielfältige Tools zu Verfügung, so Meusburger. Im Bereich der Bilderstellung oder in der Theatermusik halte er KI für sinnvoll. ""Ich denke, in der Texterstellung gibt es spannende Ansätze, aber gute dramatische Texte scheinen dann doch nicht so einfach von der Maschine machbar zu sein, da gibt es schon Grenzen.""

Am Berliner Ensemble wurde für die Inszenierung von Sibylle Bergs Roman ""RCE #RemoteCodeExecution"" für die Videokunst mit KI-Tools gearbeitet, wie Intendant Oliver Reese sagte. Das Stück soll am 25. April Premiere feiern. ""Ich sehe im Einsatz von KI durchaus Chancen, aber wir müssen schon sehr genau schauen, in welchem Kontext wir die Büchse der Pandora öffnen"", betonte er.

© dpa-infocom, dpa:240421-99-751740/2
"
KI,Zeit,2024-04-20,https://www.zeit.de/news/2024-04/20/mehrheit-hat-noch-keine-ki-genutzt,Umfrage: Mehrheit hat noch keine KI genutzt | ZEIT ONLINE,"Jeder Dritte in Deutschland sieht einer Umfrage zufolge beim Thema Künstliche Intelligenz (KI) vor allem die Risiken. 30 Prozent der gut 2000 Befragten gaben an, dass für sie persönlich die Risiken gegenüber den Chancen überwiegen. Für 26 Prozent stehen die Chancen dagegen stärker im Vordergrund, wie die YouGov-Umfrage für die Deutsche Presse-Agentur ergab. 39 Prozent waren unentschieden oder gaben an, die Frage mangels klarer Vorstellung von KI nicht beurteilen zu können.

Wenn es darum geht, die Technologien selbst auszuprobieren, sind die meisten noch zurückhaltend: 56 Prozent haben bisher KI nicht selbst genutzt. 36 Prozent haben die Technologien hingegen schon einmal ausprobiert. Menschen unter vierzig hatten im Vergleich jedoch schon mehr Berührungspunkte: In dieser Altersgruppe haben mehr als die Hälfte die Technologien bereits getestet.

Von denjenigen, die KI-basierte Technologien schon genutzt haben, hat sich eine deutliche Mehrheit jedenfalls auch im privaten Rahmen damit beschäftigt. 81 Prozent von ihnen gaben das an. Im Beruf kam KI bei 36 Prozent und im Studium oder in der Ausbildung bei 15 Prozent zum Einsatz.

Bei der Frage, wie deutsche Unternehmen bei Zukunftstechnologien wie KI im internationalen Vergleich aufgestellt sind, fällt das Urteil vieler Bundesbürger negativ aus. 39 Prozent halten deutsche Unternehmen hier für «eher schlecht» und 9 Prozent sogar für «sehr schlecht» aufgestellt. Mit «sehr gut» bewerteten das nur 3 Prozent. «Eher gut» schätzen 20 Prozent die Situation ein. Knapp ein Drittel (29 Prozent) antwortete mit «weiß nicht» oder machte keine Angabe.

An der repräsentativen Umfrage hatten zwischen dem 5. April und dem 10. April bundesweit 2013 Erwachsene mit deutscher Staatsangehörigkeit teilgenommen.

© dpa-infocom, dpa:240420-99-743188/3
"
KI,Zeit,2024-04-19,https://www.zeit.de/news/2024-04/18/erste-digitalministerkonferenz-wissing-offen-fuer-ki,Künstliche Intelligenz: Wissing bei erster Digitalministerkonferenz: KI fördern | ZEIT ONLINE,"Bundesdigitalminister Volker Wissing (FDP) sieht Deutschland bei der Entwicklung von Technologien der Künstlichen Intelligenz (KI) in einer führenden Rolle weltweit und plädiert für einen offenen Umgang mit digitalen Entwicklungen. Zugleich forderte er, die Länder sollten das Gesetz für eine digitale Verwaltung nicht länger blockieren.

Deutschland sei bei den KI-Patenten die Nummer zwei auf der Welt nach den USA und vor China und Japan, sagte Wissing bei der ersten Digitalministerkonferenz am Freitag in Potsdam. Diese Chance dürfe nicht verspielt werden. «Ohne Künstliche Intelligenz wird keine Volkswirtschaft wettbewerbsfähig bleiben können», so Wissing. Deutschland solle dabei selbst die Technologie in der Hand haben, sagte der Minister. Aus seiner Sicht ist es daher wichtig, KI mit Offenheit zu begegnen und ihren breiten Einsatz zu fördern. Am Freitag gründeten die Länder die Digitalministerkonferenz, um ihre Zusammenarbeit auf diesem Gebiet zu stärken.

Die Bundesregierung will mehr Behördengänge durch einheitliche digitale Verfahren ersetzen, doch das Onlinezugangsgesetz (OZG 2.0) stößt vor allem bei unionsgeführten Ländern auf Kritik. Minister Wissing sagte, bei der Reform dieses Gesetzes, das im Bundesrat gescheitert war, sei nun eine rasche Lösung nötig. Die Länder sollten es nicht auf Dauer blockieren. Das Onlinezugangsgesetz bezieht sich auf Bundesverwaltungen, es soll aber auch auf die Bundesländer und Kommunen ausstrahlen. Es soll auch verhindert werden, dass in den Verwaltungen mehrfach Software für die Lösung des gleichen Problems entwickelt wird.

Der Branchenverband Bitkom kritisierte am Freitag in einer Mitteilung, bei der Digitalisierung herrsche in Deutschland zu oft Kleinstaaterei und Kirchturmpolitik. Ein Flickenteppich unterschiedlicher Regelungen zum Beispiel in den Verwaltungen und im Datenschutz mache es Verbrauchern und Unternehmen unnötig schwer und erzeuge Kosten ohne Nutzen. Der Verband begrüße daher die Einrichtung einer festen Digitalministerkonferenz, die zu einer bundesweiten Beschleunigung und Harmonisierung der Digitalpolitik beitragen solle. Auch die Berliner Staatssekretärin für Digitalisierung und Verwaltungsmodernisierung, Martina Klement, sagte: «Wir müssen sicherstellen, dass nicht jeder sein eigenes Süppchen kocht.»

Bei einem Digital-Ranking des Branchenverbandes Bitkom zeigten sich Unterschiede zwischen den 16 Bundesländern. Hamburg setzte sich vor Berlin und Bayern an die Spitze. Die niedrigsten Indexwerte erreichten Sachsen-Anhalt und Thüringen. Brandenburg landete auf dem elften Platz. Verglichen wurden vier Bereiche: «digitale Wirtschaft», «digitale Infrastruktur», «Governance & digitale Verwaltung» sowie «digitale Gesellschaft». Dabei wurden unter anderem der 5G-Ausbau, die Glasfaserversorgung oder die Ladeinfrastruktur für Elektroautos untersucht, aber auch die Zahl der Informatik-Pflichtstunden an den Schulen, die Nutzung digitaler Behördenleistungen oder die Einstellung der Bevölkerung zur Digitalisierung.

© dpa-infocom, dpa:240418-99-726143/4
"
KI,Zeit,2024-04-19,https://www.zeit.de/news/2024-04/19/auf-dem-weg-zur-sprechenden-kochplatte-ki-im-fokus,Hannover Messe: Auf dem Weg zur sprechenden Kochplatte - KI im Fokus | ZEIT ONLINE,"Manche Ideen für clevere KI-Lösungen entstehen in der heimischen Küche. Wenn auf dem Ceran-Kochfeld etwas überläuft oder man versehentlich etwas auf dem Bedienfeld ablegt, macht der Sensor bisher einen lauten Piepton. «Mit KI lässt sich das deutlich verbessern», glaubt Gunther Kegel, der mit seiner Firma Pepperl und Fuchs aus Mannheim Sensoren für die Industrie liefert.

«Der Sensor lernt, übergelaufene Feuchtigkeit von einem Finger zu unterscheiden.» Und irgendwann brauche man womöglich nicht einmal mehr den Finger. «Zukünftig können wir das Ceranfeld vielleicht über Sprachsteuerung bedienen», sagt Kegel. Dann müsse man nur noch sagen: «Platte eins, Stufe acht.»

Noch ist die sprechende Herdplatte Zukunftsmusik. Doch vieles ist mit KI schon heute möglich. Und die Entwicklung schreitet rasant voran. «KI wird in Zukunft Handbücher aus vorhandenen Daten zusammenstellen können und fertige Produkte mit der zuvor geplanten Spezifikation abgleichen», sagt Kegel, der im Ehrenamt auch Präsident des Verbands der Elektro- und Digitalindustrie (ZVEI) ist. «Auch das Einlesen und Verarbeiten von Rechnungen, eingehenden Schreiben, Kundenbeschwerden lässt sich per KI automatisieren.» Am Ende werde immer noch ein Mensch einen Blick darauf werfen. «Routinen sowie ermüdende Arbeiten kann die Maschine erledigen.» Das werde die Effizienz deutlich steigern.

Was in der Industrie heute schon möglich ist, wird ab Montag auch auf der Hannover Messe (22. bis 26. April) zu sehen sein: Maschinen, die Fehler automatisch erkennen, Roboter, die eigenständig lernen, Anlagen, die selbst ermitteln, wann der beste Termin für ihre eigene Wartung ist. Siemens zeigt seinen entwickelten «Industrial Copilot», der gemeinsame mit Microsoft entwickelt wurde. Eine Art «ChatGPT für Ingenieure», mit dem sich Industrieroboter per Sprache steuern lassen, wie es Messechef Jochen Köckler in Anspielung an den erfolgreichen Chatbot auf den Punkt bringt.

Und anders als noch vor wenigen Jahren geglaubt, seien solche Lösungen inzwischen einsatzfähig, sagt Köckler, der KI in diesem Jahr zu einem der Themenschwerpunkte der weltgrößten Industrieschau gemacht hat. «Die Geschwindigkeit, mit der KI-Lösungen ihren Weg in die Industrie finden, ist atemberaubend», sagt er. «Das Tempo ist enorm, die Auswirkungen werden gewaltig sein.» Das biete der Industrie auch die Chance, trotz Arbeitskräftemangels weiterzuwachsen. Und gerade ChatGPT zeige, welche Kraft KI entwickeln könne. «Wenn ich das auf die Industrie übertrage, hat das eine Wirkung, die sich heute noch gar nicht abschätzen lässt.»

Vieles davon sei schon heute Realität, berichtet ZVEI-Chef Kegel. In seinem Unternehmen etwa setze er schon seit Jahren KI in Sensoren ein, etwa bei der Bildverarbeitung und bei der Signalauswertung der komplexeren Sensoren. Der Kamerasensor lerne dabei, eine Schraube sicher zu erkennen, ohne dass zuvor alle denkbaren Schraubenköpfe als Vorlage hinterlegt werden müssten. Und auch beim Erfassen eingehender Rechnungen oder im automatischen Bestellwesen komme bei ihm bereits KI zum Einsatz.

«Durch generative KI und sprachgesteuerte Anwendungen wie ChatGPT nimmt der Einsatz von KI noch einmal zusätzlich Fahrt auf», ist Kegel überzeugt. Vor allem der Einsatz in Produkten biete gewaltige Chancen für die deutsche Industrie. «Wenn wir das geschickt machen, kann das zum Alleinstellungsmerkmal werden, mit dem die deutsche und europäische Industrie international punkten kann.»

Laut einer Studie, die das Forschungsinstituts IW Consult im Auftrag von Google erstellt hat, könnte KI der deutschen Industrie einen Schub in Milliardenhöhe verleihen: Um fast acht Prozent könnte die Bruttowertschöpfung dadurch steigen. Das entspräche 56 Milliarden Euro allein im verarbeitenden Gewerbe. Für die gesamte Wirtschaft wurde die mögliche Wertschöpfung mittels KI sogar auf 330 Milliarden Euro geschätzt.

Europa müsse aber aufpassen, diese Chance nicht durch zu strenge Regeln zu verspielen, warnt Kegel. Zwar habe man beim jüngst verabschiedete KI-Gesetz der EU, das in rund zwei Jahren voll greifen soll, noch einige Verbesserungen erzielen können. Es sei aber nach wie vor nicht geklärt, was am Ende wirklich als Risiko-KI einzustufen sei. «Bei strenger Auslegung wird selbst ein einfaches Ceran-Kochfeld, das einen KI-Baustein für den Sensor im Bedienfeld nutzt, zur kritischen Hochrisiko-Anwendung», warnt Kegel. «Das kann nicht im Sinne der Richtlinie sein.» Dies müsse man nun bei der Umsetzung in deutsches Recht verhindern. «Es darf nicht alles in einen Topf geworfen werden. Sonst wird die Regulierung zur massiven Innovationsbremse.»

© dpa-infocom, dpa:240419-99-731115/4
"
KI,Zeit,2024-04-19,https://www.zeit.de/sport/2024-04/olympische-spiele-paris-2024-ki-video-ueberwachung,Olympische Spiele 2024: Paris testet KI-basierte Videoüberwachung an Bahnhöfen | ZEIT ONLINE,"In Paris wird in den nächsten Tagen KI-basierte Videoüberwachung zum Einsatz kommen, die auch in begrenztem Umfang während der Olympischen Spiele genutzt werden soll. Für die von Freitag bis Montag laufende Probephase hat die Pariser Polizei der Bahngesellschaft SNCF und dem Verkehrsbetrieb RATP die Auswertung von Kamerabildern mit intelligenter, algorithmusbasierter Technik erlaubt. 

Anlässlich eines Konzerts der Gruppe Black Eyed Peas in der La Défense Arena und des Fußballspiels von Paris Saint-Germain gegen Olympique Lyon dürfen SNCF und RATP in vier Bahnhöfen die Bilder von über 100 Überwachungskameras auswerten. In den entsprechenden Bahnhöfen weisen Schilder auf die besondere Überwachung hin.

Die Software, die zur Analyse der Bilder dieser Kameras eingesetzt wird, soll versuchen, das Eindringen von Personen in nicht öffentliche oder sensible Bereiche zu erkennen. Zudem soll sie auf eine ungewöhnlich hohe Ansammlung von Menschen und herrenlose Gepäckstücke aufmerksam machen.  

Die Analyse der Bilder soll zwölf Monate lang gespeichert werden, wie aus der Polizeiverfügung hervorgeht. Eine Gesichtserkennung über die Überwachungskameras, die in Frankreich in bestimmten Fällen bereits praktiziert wird, soll es während der Sommerspiele nicht geben.

Eine erste Erprobung der neuen Überwachungstechnik hatte es in Paris bereits Anfang März gegeben, damals bei zwei Konzerten der Gruppe Depeche Mode.    

Kritik an der Überwachung kommt unter anderem von Amnesty International. Die Organisation hatte bereits während der Planung vor einer ""Zukunft mit Überwachungsmaßnahmen in dystopischen Ausmaßen"" gewarnt.  

Nach 
einem vor gut einer Woche vorgelegten Bericht des Senats ist die 
intelligente Videotechnik noch weit von den gesetzten Zielen entfernt. ""Die erweiterte Videoüberwachung wird zum Zeitpunkt der Olympischen 
Spiele nicht optimal sein. Die Olympischen Spiele werden jedoch ein 
großer Spielplatz sein, um damit zu experimentieren"", sagte die Senatorin 
Agnès Canayer. Um die absehbaren Mängel der Technik auszugleichen, 
müssten mehr Sicherheitskräfte für die Spiele mobilisiert werden, sagte Canayer. 

Die Sicherheit während der Olympischen Spiele in Paris ist zu einem großen Diskussionspunkt geworden, seitdem in Frankreich die höchste Terrorwarnstufe verhängt wurde. Die Sicherheitsbehörden verweisen zwar auf eine jahrelange Vorbereitung auf das Großereignis und den Einsatz Zehntausender Beamter und privater Sicherheitskräfte. Dennoch räumte Frankreichs Präsident Emmanuel Macron vor einigen Tagen ein, dass die Eröffnungsfeier der Spiele doch nicht auf der Seine stattfinde, wenn sich die Sicherheitslage verschlechtere. 

Es gebe einen Plan B und C. Im Fall einer terroristischen Bedrohung könnte die Feier zum Beispiel nur am Trocadéro-Platz oder gleich ganz im Stadion stattfinden, sagte Macron. Die Olympischen Spiele in Paris finden vom 26. Juli bis 11. August statt.
"
Künstliche Intelligenz,Zeit,2024-04-18,https://www.zeit.de/news/2024-04/18/facebook-konzern-veroeffentlicht-neues-ki-modell,Künstliche Intelligenz: Facebook-Konzern veröffentlicht neues KI-Modell | ZEIT ONLINE,"Der Facebook-Konzern Meta veröffentlicht eine neue, leistungsstärkere Version seines KI-Modells. Die Software mit dem Namen Llama-3 soll unter anderem neue Funktionen in Apps wie Instagram und WhatsApp bringen sowie im hauseigenen Assistenten Meta AI laufen. 

Llama-3 werde zunächst zwar nur auf Englisch verfügbar sein, sagte Meta-Manager Nick Clegg der Deutschen Presse-Agentur zur Ankündigung des Programms. Allerdings seien mehr als fünf Prozent der Daten, mit denen Llama-3 trainiert worden sei, in anderen Sprachen gewesen.

Seinen KI-Assistenten bringt Meta unter anderem die in die zusammen mit Ray Ban entwickelte vernetzte Brille, die Kamera, Mikrofon und Lautsprecher hat. Man könne damit zum Beispiel beim Skifahren den Assistenten Fragen, wann und wie Kleopatra gestorben sei oder wie das Wetter in Berlin werde, sagte Clegg. Auch werde die Software schneller Bilder aus Text-Vorgaben erzeugen und diese auch als Animation darstellen können.

Meta mache sich derzeit keine Gedanken über Geschäftsmodelle für KI-Software, betonte Clegg, der Politikchef des Konzerns ist. Der Facebook-Konzern wolle erst Technologie entwickeln, die Menschen nützlich oder interessant fänden - «und dann finden wir später heraus, wie man damit Geld verdienen kann».

Anders als zum Beispiel der ChatGPT-Entwickler OpenAI macht Meta seine Llama-Technologie (Large Language Model Meta AI) als Open-Source-Software verfügbar, bei der der Quellcode öffentlich einsehbar ist. Es setzte sich immer mehr die Ansicht durch, dass Open-Source-Modelle sicherer seien, weil viele sie auf den Prüfstand stellen könnten und «man sich nicht darauf verlassen muss, dass ein Unternehmen die Schwachstellen in seiner Software ausbügelt». Zugleich fehle im Moment eine einheitliche Grundlage zur Bewertung von Risiken bei Künstlicher Intelligenz, bemängelte Clegg.

© dpa-infocom, dpa:240418-99-726259/2
"
Künstliche Intelligenz,Zeit,2024-04-16,https://www.zeit.de/wirtschaft/unternehmen/2024-04/generative-kuenstliche-intelligenz-verarbeitendes-gewerbe-milliardensumme-google-studie,Künstliche Intelligenz: IW-Studie sieht in KI großes Potenzial für Deutschlands Industrie | ZEIT ONLINE,"Der Einsatz von generativer künstlicher Intelligenz (KI) könnte dem verarbeitenden Gewerbe in Deutschland hohe Gewinne einbringen. Das geht aus einer Studie des Forschungsinstituts IW Consult im Auftrag von Google hervor. Demnach könnte die Bruttowertschöpfung im verarbeitenden Gewerbe durch generative KI um bis zu 7,8 Prozent erhöht werden, was einer Gesamtsteigerung von 56 Milliarden Euro entspreche. Die Bruttowertschöpfung ist der im Produktionsprozess geschaffene Mehrwert, also der Gesamtwert der Waren und Dienstleistungen unter Abzug der Vorleistungen.

Generative KI ist eine Variante der künstlichen Intelligenz, mit der man neue, originelle Inhalte schaffen kann. Mithilfe der Algorithmen und sogenannter Sprachmodelle können Inhalte wie Texte, Bilder und Videos, aber auch Musik oder Programmcodes erzeugt werden. Die Vorgaben für das KI-System müssen nicht programmiert, sondern können in natürlicher Sprache übermittelt werden. 

Der Studie der Tochtergesellschaft des Instituts der deutschen Wirtschaft (IW) in Köln zufolge müssen sich vor allem Akademikerinnen und Akademiker sowie Büroangestellte auf starke Veränderungen ihrer Arbeit durch KI einstellen. Im verarbeitenden Gewerbe seien dies rund 0,6 Millionen Beschäftigte, bei denen man eine starke Auswirkung von KI auf die Arbeit erwarte. Bei weiteren 4,1 Millionen Beschäftigten könne generative KI die eigene Arbeit unterstützen, etwa bei der Optimierung von Programmcodes oder als Ideenanregung beim Produktdesign.

Das Institut der deutschen Wirtschaft ist ein privates Forschungsinstitut mit Sitz in Köln. Es ist als eingetragener Verein organisiert und wird finanziert durch etwa 100 Wirtschafts- und Arbeitgeberverbände in Deutschland sowie Einzelunternehmen. Es vertritt in der Regel wirtschaftsliberale Positionen und arbeitet unter anderem mit der Initiative Neue Soziale Marktwirtschaft zusammen. Direktor ist der Wirtschaftsforscher Michael Hüther.


Im Alltag klassischer Industriejobs wie Reparatur- oder Wartungsarbeiten werde die KI dagegen deutlich seltener zum Einsatz kommen. Diese rund 3,3 Millionen Stellen sind der Studie zufolge durch KI nicht oder nur schwer automatisierbar. Das betreffe etwa 41 Prozent aller Arbeitsplätze im verarbeitenden Gewerbe.

Laut dem IW-Direktor Michael Hüther ist seit 2018 die reale Arbeitsproduktivität im Maschinenbau und anderen 
Bereichen des verarbeitenden Gewerbes mit lediglich 0,4 Prozent Plus pro
 Jahr nahezu konstant geblieben. Durch generative KI könne die Branche ""eine 
beachtliche KI-Dividende"" erzielen, schätzt er. Damit könnte das verarbeitende Gewerbe auch seine Produktivitätsvorteile 
auf den Weltmärkten sichern, sagte Hüther. Es sei erfreulich, dass viele Unternehmen diese Chance schon erkannt hätten.

Mehr als  50 Prozent der Industriebetriebe in Deutschland setzen KI laut der Studie bereits ein – deutlich mehr als der Durchschnitt der deutschen Wirtschaft (17 Prozent). KI werde in der verarbeitenden Industrie dazu 
verwendet, interne Systeme zu automatisieren (42 Prozent), Dokumente zu 
verfassen (31 Prozent) und Daten zu analysieren (24 Prozent). 

Das verarbeitende Gewerbe ist eines der wichtigsten Wirtschaftszweige in Deutschland und umfasst beispielsweise die Metallbranche. Laut IW beträgt die Wertschöpfung im Bereich des verarbeitenden Gewerbes 781 Milliarden Euro und fast acht Millionen Beschäftigte. Im Vergleich zu anderen Industrieländern ist in Deutschland der Anteil des verarbeitenden Gewerbes an der gesamtwirtschaftlichen Wertschöpfung mit mehr als 20 Prozent deutlich höher und nahezu doppelt so hoch wie in Großbritannien oder den USA. Für die gesamte Wirtschaft werde die mögliche Wertschöpfung mittels KI auf 330 Milliarden Euro geschätzt.  
"
Künstliche Intelligenz,Zeit,2024-04-16,https://www.zeit.de/zeit-magazin/2024/16/kuenstliche-intelligenz-bilder-raetsel-designer-schriftsteller,Künstliche Intelligenz: Bilderrätsel | ZEITmagazin,
Künstliche Intelligenz,Zeit,2024-04-16,https://www.zeit.de/news/2024-04/16/iw-studie-milliarden-schub-fuer-deutsche-industrie-durch-ki,Künstliche Intelligenz: IW-Studie: Milliarden-Schub für deutsche Industrie durch KI | ZEIT ONLINE,"Der Einsatz von Künstlicher Intelligenz (KI) könnte der deutschen Industrie einen Schub in Milliardenhöhe verleihen. Laut einer Studie des Forschungsinstituts IW Consult im Auftrag von Google, die am Dienstag in Berlin vorgestellt wurde, könnte die Bruttowertschöpfung im verarbeitenden Gewerbe durch generative KI um bis zu 7,8 Prozent erhöht werden. Das entspreche einer Gesamtsteigerung von 56 Milliarden Euro. Die Bruttowertschöpfung bezeichnet den Gesamtwert der Waren und Dienstleistungen unter Abzug der Vorleistungen, und damit den im Produktionsprozess geschaffenen Mehrwert.

Generative KI ist eine Variante der Künstlichen Intelligenz, mit der man neue, originelle Inhalte schaffen («generieren») kann. Mithilfe der Algorithmen und sogenannter Sprachmodelle können Inhalte wie Texte, Bilder und Videos, aber auch Musik oder Programmcodes erzeugt werden. Die Vorgaben für das KI-System müssen nicht programmiert, sondern können in natürlicher Sprache übermittelt werden. Ein bedeutender Meilenstein für generative KI war die Veröffentlichung des Chatbots ChatGPT durch das Start-up OpenAI im November 2022. Unter anderem Google bietet mit Gemini ein eigenes Dialogsystem für generative KI an, das mit ChatGPT konkurriert.

Der Studie der Tochtergesellschaft des Instituts der deutschen Wirtschaft (IW) in Köln zufolge müssen sich vor allem Akademiker und Büroangestellte auf starke Veränderungen ihrer Arbeit durch KI einstellen. Im verarbeitenden Gewerbe seien dies rund 0,6 Millionen Beschäftigte, bei denen man eine starke Auswirkung von KI auf die Arbeit erwarte. Bei weiteren 4,1 Millionen Beschäftigten könne generative KI die eigene Arbeit unterstützen, etwa bei der Optimierung von Programmcodes oder als Ideengeber beim Produktdesign.

Im Alltag bei klassischen Industriejobs wie etwa Reparatur- oder Wartungsarbeiten werde die KI dagegen deutlich seltener zum Einsatz kommen. Diese rund 3,3 Millionen Stellen sind der Studie zufolge durch KI nicht oder nur schwer automatisierbar. Das betreffe etwa 41 Prozent aller Arbeitsplätze im verarbeitenden Gewerbe.

Der Direktor des Instituts der deutschen Wirtschaft Köln, Michael Hüther, erklärte, seit 2018 sei die reale Arbeitsproduktivität im Maschinenbau und anderen Bereichen des verarbeitenden Gewerbes mit lediglich 0,4 Prozent Plus pro Jahr nahezu konstant geblieben. «Durch generative künstliche Intelligenz könnte die Branche eine beachtliche KI-Dividende erzielen und damit ihre Produktivitätsvorteile auf den Weltmärkten sichern.» Es sei erfreulich, dass viele Unternehmen diese Chance schon erkannt hätten.

Aus der IW-Studie geht hervor, dass mehr als 50 Prozent der Industriebetriebe in Deutschland schon KI einsetzen. Damit liege die Branche deutlich über dem Durchschnitt der deutschen Wirtschaft (17 Prozent). Die Künstliche Intelligenz werde in der verarbeitenden Industrie dazu verwendet, interne Systeme zu automatisieren (42 Prozent), Dokumente zu verfassen (31 Prozent) und Daten zu analysieren (24 Prozent).

Das verarbeitende Gewerbe ist einer der wichtigsten Wirtschaftszweige in Deutschland. Laut IW beträgt die Wertschöpfung in diesem Bereich 781 Milliarden Euro und es gibt fast acht Millionen Beschäftigte. Im Vergleich zu anderen Industrieländern ist in Deutschland der Anteil des verarbeitenden Gewerbes an der gesamtwirtschaftlichen Wertschöpfung mit mehr als 20 Prozent deutlich höher und nahezu doppelt so hoch wie in Großbritannien oder den USA. Für die gesamte Wirtschaft werde die mögliche Wertschöpfung mittels KI auf 330 Milliarden Euro geschätzt.

© dpa-infocom, dpa:240416-99-689167/2
"
AI,Zeit,2024-04-18,https://www.zeit.de/news/2024-04/18/facebook-konzern-veroeffentlicht-neues-ki-modell,Künstliche Intelligenz: Facebook-Konzern veröffentlicht neues KI-Modell | ZEIT ONLINE,"Der Facebook-Konzern Meta veröffentlicht eine neue, leistungsstärkere Version seines KI-Modells. Die Software mit dem Namen Llama-3 soll unter anderem neue Funktionen in Apps wie Instagram und WhatsApp bringen sowie im hauseigenen Assistenten Meta AI laufen. 

Llama-3 werde zunächst zwar nur auf Englisch verfügbar sein, sagte Meta-Manager Nick Clegg der Deutschen Presse-Agentur zur Ankündigung des Programms. Allerdings seien mehr als fünf Prozent der Daten, mit denen Llama-3 trainiert worden sei, in anderen Sprachen gewesen.

Seinen KI-Assistenten bringt Meta unter anderem die in die zusammen mit Ray Ban entwickelte vernetzte Brille, die Kamera, Mikrofon und Lautsprecher hat. Man könne damit zum Beispiel beim Skifahren den Assistenten Fragen, wann und wie Kleopatra gestorben sei oder wie das Wetter in Berlin werde, sagte Clegg. Auch werde die Software schneller Bilder aus Text-Vorgaben erzeugen und diese auch als Animation darstellen können.

Meta mache sich derzeit keine Gedanken über Geschäftsmodelle für KI-Software, betonte Clegg, der Politikchef des Konzerns ist. Der Facebook-Konzern wolle erst Technologie entwickeln, die Menschen nützlich oder interessant fänden - «und dann finden wir später heraus, wie man damit Geld verdienen kann».

Anders als zum Beispiel der ChatGPT-Entwickler OpenAI macht Meta seine Llama-Technologie (Large Language Model Meta AI) als Open-Source-Software verfügbar, bei der der Quellcode öffentlich einsehbar ist. Es setzte sich immer mehr die Ansicht durch, dass Open-Source-Modelle sicherer seien, weil viele sie auf den Prüfstand stellen könnten und «man sich nicht darauf verlassen muss, dass ein Unternehmen die Schwachstellen in seiner Software ausbügelt». Zugleich fehle im Moment eine einheitliche Grundlage zur Bewertung von Risiken bei Künstlicher Intelligenz, bemängelte Clegg.

© dpa-infocom, dpa:240418-99-726259/2
"
Artificial Intelligence,Zeit,2024-04-17,https://www.zeit.de/zeit-magazin/leben/2024-04/michael-walzer-just-war-israel-gaza-english,Michael Walzer: What Is a Just War? | ZEITmagazin,"Lesen Sie diesen Text auf Deutsch

ZEITmagazin: Professor Walzer, is Israel waging a just war against Hamas? 

Michael Walzer: Yes. Of central importance, however, is the distinction between ius ad bellum, the just reason for going to war, and ius in bello, the just conduct in a war. The massacre of October 7 is a just cause for going to war; Israel obviously has the right to defend itself. However, there is much to criticize about the way Israel is waging this war. 

ZEITmagazin: What characterizes a just war? 

Walzer: The transfer into everyday life helps us to understand. If I am attacked on the street, I can defend myself. And whoever steps in and comes to my aid is acting just as justly. These are the two fundamental principles: self-defense and the defense of others. A third case of just war is intervention to prevent a massacre, such as when the Vietnamese ended the Khmer Rouge's reign of terror in Cambodia. Or what the West should have done it in Rwanda, but didn't. 

ZEITmagazin: How have you experienced the last six months, as an ethicist and as a Jew? 

Walzer: I have been criticizing Israeli governments for several decades now, especially those led by the Likud party, which is now led by Benjamin Netanyahu. His policies were bound to lead to disaster. Nevertheless, the events of October 7th shocked me. My wife Judy and I just moved to New York from Princeton last year; we want to be here in the city with our children and grandchildren. We joined a synagogue for the first time in our lives. I am a secular Jew, on the Princeton campus I regularly attended the student services but wasn’t a member of any community. Since October 7th, it has been important for me to see the same people every week – people who have the same fears as me and who also have family and friends in Israel.

ZEITmagazin: What’s your take on the recent massive Iranian attack on Israel? 

Walzer: I thought the Israeli attack on the Iranian consulate in Damascus in early April was a reckless escalation, even though it doesn't measure up to the attack on the Israeli embassy in Argentina in 1992, which is now officially attributed to Iran by an Argentinian court. The Iranian response last Saturday night was a still greater escalation. How reckless it was depends on whether the Iranians expected that at least some of the missiles would get through and cause death and injury or somehow knew that they wouldn't. 

ZEITmagazin: What do you think was the regime’s calculation? 

Walzer: I assume the first, which would justify a further Israeli response if you believe in the law of tit for tat. I don't, and so I hope that Israel will claim a remarkable defensive victory and celebrate the multi-national coalition that made the victory possible. And then I hope against hope that this awful Israeli government seizes the opportunity and works with the coalition to achieve a political victory over Hamas  since a military victory, if it is possible, would have unacceptable human costs.

ZEITmagazin: In your research, you have looked at wars waged over the past 3,000 years. What is unique about the Israeli war against Hamas? 

Walzer: There has never been a war against an underground city. This isn't talked about enough. The Viet Cong had a system of tunnels, but it was very primitive compared to Hamas. The Tunnel Rats, as the American soldiers were called back then, had to be small and thin so that they could fit into the tunnels. This is not necessary in the Hamas tunnels. What Hamas has built is an engineering marvel, over 700 kilometers of tunnels in such a small area, laid out on three levels one above the other. This completely overwhelmed the Israelis. The fact that they dropped all those 2000-pound bombs on Gaza that caused such destruction also has something to do with this: They wanted to hit the tunnels but had little success. The network is too deep in the ground. In this respect, this war is almost experimental. 

ZEITmagazin: The Gaza Strip is also cut off on all sides, and there is no escape for the civilian population. 

Walzer: The Jewish philosopher Maimonides wrote in the Middle Ages: A city may only be besieged from three sides. The fourth side must remain open to allow civilians to escape and relief supplies to come in. A paradoxical sentence, it ultimately means that a besieging army is not allowed to completely surround the city. Gaza does not have this fourth, open side, because the Egyptians to the south do not want to build tent cities for the Palestinian refugees, for fear that Israel would no longer allow the displaced people to return. But a siege like the one Israel applied at the beginning of the war cannot work for yet another reason: throughout history, armies have always aimed to starve trapped enemies in order to force them to force their rulers to surrender. But Hamas doesn't care in the slightest about the suffering of the civilian population. Therefore, this action by the Israelis was a political and strategic as well as a moral mistake. 

ZEITmagazin: The secretary general of the United Nations recently accused Israel of using ""hunger as a weapon of war,"" the humanitarian situation is devastating, and the population is facing famine. How do you view this development? 

Walzer: Israel started this war without making any provision for the care of the civilian population. The leadership probably thought that Hamas or humanitarian organizations would take care of the people. For many Israelis, after the events of October 7th, it simply wasn't that important. But the humanitarian organizations are dependent on help coming across the border, and  the Israeli side was very hesitant in allowing the deliveries to pass through. Elements of the right-wing extremist camp wanted to actively stop the deliveries. That this government is incompetent and amoral is nothing new. But there should have been people in the army who knew how devastating such an approach would be for the country's external image. Now, Israel is being blamed for the humanitarian situation, and to some extent it is responsible. 

ZEITmagazin: The heavy bombing of the Gaza Strip is being compared to the bombing attacks on Dresden, Aleppo and Grozny. Is that fair? 

Walzer: In Dresden, the Allies' goal was to create a firestorm in which as many people as possible would die. There were hardly any relevant military targets there anymore. In Hamburg and Cologne, the British bombed residential areas to make ordinary people homeless. The Israelis may not always have been careful enough in the weeks after October 7th and have apparently relaxed the criteria for accepting collateral damage relative to previous conflicts, which aimed to minimize civilian casualties. But their goal today is not to kill civilians. Otherwise, the number of victims would be much higher. 

ZEITmagazin: The Israeli publication +972 Magazine recently reported on software controlled by artificial intelligence that pre-sorts targets in the Gaza Strip and recommends them to the army. The software is apparently programmed in such a way that it favors attacks, especially at night when the terrorists are with their families. What is your view of this form of automated warfare? 
"
KI,Zeit,2024-04-18,https://www.zeit.de/news/2024-04/18/facebook-konzern-veroeffentlicht-neues-ki-modell,Künstliche Intelligenz: Facebook-Konzern veröffentlicht neues KI-Modell | ZEIT ONLINE,"Der Facebook-Konzern Meta veröffentlicht eine neue, leistungsstärkere Version seines KI-Modells. Die Software mit dem Namen Llama-3 soll unter anderem neue Funktionen in Apps wie Instagram und WhatsApp bringen sowie im hauseigenen Assistenten Meta AI laufen. 

Llama-3 werde zunächst zwar nur auf Englisch verfügbar sein, sagte Meta-Manager Nick Clegg der Deutschen Presse-Agentur zur Ankündigung des Programms. Allerdings seien mehr als fünf Prozent der Daten, mit denen Llama-3 trainiert worden sei, in anderen Sprachen gewesen.

Seinen KI-Assistenten bringt Meta unter anderem die in die zusammen mit Ray Ban entwickelte vernetzte Brille, die Kamera, Mikrofon und Lautsprecher hat. Man könne damit zum Beispiel beim Skifahren den Assistenten Fragen, wann und wie Kleopatra gestorben sei oder wie das Wetter in Berlin werde, sagte Clegg. Auch werde die Software schneller Bilder aus Text-Vorgaben erzeugen und diese auch als Animation darstellen können.

Meta mache sich derzeit keine Gedanken über Geschäftsmodelle für KI-Software, betonte Clegg, der Politikchef des Konzerns ist. Der Facebook-Konzern wolle erst Technologie entwickeln, die Menschen nützlich oder interessant fänden - «und dann finden wir später heraus, wie man damit Geld verdienen kann».

Anders als zum Beispiel der ChatGPT-Entwickler OpenAI macht Meta seine Llama-Technologie (Large Language Model Meta AI) als Open-Source-Software verfügbar, bei der der Quellcode öffentlich einsehbar ist. Es setzte sich immer mehr die Ansicht durch, dass Open-Source-Modelle sicherer seien, weil viele sie auf den Prüfstand stellen könnten und «man sich nicht darauf verlassen muss, dass ein Unternehmen die Schwachstellen in seiner Software ausbügelt». Zugleich fehle im Moment eine einheitliche Grundlage zur Bewertung von Risiken bei Künstlicher Intelligenz, bemängelte Clegg.

© dpa-infocom, dpa:240418-99-726259/2
"
KI,Zeit,2024-04-16,https://www.zeit.de/news/2024-04/16/fast-200-ki-unternehmen-in-sachsen-dresden-ist-hotspot,Technik: Fast 200 KI-Unternehmen in Sachsen: Dresden ist Hotspot | ZEIT ONLINE,"Sachsen hat bei Unternehmen im Bereich Künstlicher Intelligenz (KI) deutlich zugelegt. Waren es 2019 noch 80, hat sich ihre Zahl mehr als verdoppelt auf 196. Rund drei Viertel von ihnen haben auch den Hauptsitz im Freistaat, wie eine Analyse der Digitalagentur Sachsen im Auftrag des Wirtschaftsministeriums ergab. Sie wurde am Dienstag zum Auftakt des 3. Sächsischen KI-Kongresses in Chemnitz vorgestellt. Im regionalen Vergleich hat Dresden bei dem Zukunftsthema klar die Nase vorn: 43 Prozent der sächsischen KI-Unternehmen sind in der Landeshauptstadt ansässig, aber auch in Leipzig und Chemnitz sind beliebt. Begründet wurde dies vor allem mit der Nähe zu den Universitäten in diesen drei Großstädten.

Eine genaue Angabe zu Umsatz und Arbeitsplätzen liefert die Studie nicht. Allerdings sei der Anteil der Firmen mit einem Jahresumsatz unter 10 Millionen Euro relativ hoch (60 Prozent). Denn viele Unternehmen seien erst in jüngerer Vergangenheit gegründet worden, hieß es.

Die Firmen bieten der Studie zufolge Hardware, Software und Dienstleistungen rund um KI. Dabei gehe es etwa um Automatisierung von industrieller Produktion, autonomes Fahren sowie die Entwicklung und Herstellung von Halbleiterprodukten. Es gebe eine enge Verzahnung mit der Forschung. Mit mehr als 160 Professuren, Zentren, außeruniversitären Instituten und Forschungsverbünden sei die KI-Forschung in Sachsen im Bundesvergleich sehr gut aufgestellt. Fast 60 Prozent dieser Einrichtungen widmeten sich schwerpunktmäßig der angewandten Forschung, insbesondere zu Algorithmen, Daten und KI-Hardware.

""Sachsen ist für das KI-Zeitalter gut aufgestellt"", sagte Wirtschaftsminister Martin Dulig (SPD). Allerdings dürfe der Freistaat nicht stehen bleiben. Für die Zukunft sei es wichtig, sich auf bestimmte Bereiche zu konzentrieren. Es gehe vor allem um Anknüpfungspunkte zu bestehenden Schwerpunktbranchen wie den Maschinen- und Anlagenbau, die Automobilindustrie und die Logistik. Zudem müsse der noch niedrige Frauenanteil in diesem Segment erhöht werden und passgenaue Aus- und Weiterbildungsangebote in den unterschiedlichsten Branchen entstehen. Die KI sei weniger ein Jobmotor, sondern vielmehr eine Jobgarantie, erklärte Dulig. Deswegen müsse Sachsen die sich mit dieser Technologie bietenden Chancen nutzen.

Beim KI-Kongress in Chemnitz wollten am Dienstag und Mittwoch Vertreter der Regierung mit Experten aus Wirtschaft, Wissenschaft, Bildung und Justiz darüber ins Gespräch kommen, wie KI zum alltäglichen Unterstützer werden kann. Die Rede war von rund 300 Gästen und Ausstellern bei dem Kongress.

© dpa-infocom, dpa:240416-99-694934/2
"
KI,Zeit,2024-04-16,https://www.zeit.de/wirtschaft/unternehmen/2024-04/generative-kuenstliche-intelligenz-verarbeitendes-gewerbe-milliardensumme-google-studie,Künstliche Intelligenz: IW-Studie sieht in KI großes Potenzial für Deutschlands Industrie | ZEIT ONLINE,"Der Einsatz von generativer künstlicher Intelligenz (KI) könnte dem verarbeitenden Gewerbe in Deutschland hohe Gewinne einbringen. Das geht aus einer Studie des Forschungsinstituts IW Consult im Auftrag von Google hervor. Demnach könnte die Bruttowertschöpfung im verarbeitenden Gewerbe durch generative KI um bis zu 7,8 Prozent erhöht werden, was einer Gesamtsteigerung von 56 Milliarden Euro entspreche. Die Bruttowertschöpfung ist der im Produktionsprozess geschaffene Mehrwert, also der Gesamtwert der Waren und Dienstleistungen unter Abzug der Vorleistungen.

Generative KI ist eine Variante der künstlichen Intelligenz, mit der man neue, originelle Inhalte schaffen kann. Mithilfe der Algorithmen und sogenannter Sprachmodelle können Inhalte wie Texte, Bilder und Videos, aber auch Musik oder Programmcodes erzeugt werden. Die Vorgaben für das KI-System müssen nicht programmiert, sondern können in natürlicher Sprache übermittelt werden. 

Der Studie der Tochtergesellschaft des Instituts der deutschen Wirtschaft (IW) in Köln zufolge müssen sich vor allem Akademikerinnen und Akademiker sowie Büroangestellte auf starke Veränderungen ihrer Arbeit durch KI einstellen. Im verarbeitenden Gewerbe seien dies rund 0,6 Millionen Beschäftigte, bei denen man eine starke Auswirkung von KI auf die Arbeit erwarte. Bei weiteren 4,1 Millionen Beschäftigten könne generative KI die eigene Arbeit unterstützen, etwa bei der Optimierung von Programmcodes oder als Ideenanregung beim Produktdesign.

Das Institut der deutschen Wirtschaft ist ein privates Forschungsinstitut mit Sitz in Köln. Es ist als eingetragener Verein organisiert und wird finanziert durch etwa 100 Wirtschafts- und Arbeitgeberverbände in Deutschland sowie Einzelunternehmen. Es vertritt in der Regel wirtschaftsliberale Positionen und arbeitet unter anderem mit der Initiative Neue Soziale Marktwirtschaft zusammen. Direktor ist der Wirtschaftsforscher Michael Hüther.


Im Alltag klassischer Industriejobs wie Reparatur- oder Wartungsarbeiten werde die KI dagegen deutlich seltener zum Einsatz kommen. Diese rund 3,3 Millionen Stellen sind der Studie zufolge durch KI nicht oder nur schwer automatisierbar. Das betreffe etwa 41 Prozent aller Arbeitsplätze im verarbeitenden Gewerbe.

Laut dem IW-Direktor Michael Hüther ist seit 2018 die reale Arbeitsproduktivität im Maschinenbau und anderen 
Bereichen des verarbeitenden Gewerbes mit lediglich 0,4 Prozent Plus pro
 Jahr nahezu konstant geblieben. Durch generative KI könne die Branche ""eine 
beachtliche KI-Dividende"" erzielen, schätzt er. Damit könnte das verarbeitende Gewerbe auch seine Produktivitätsvorteile 
auf den Weltmärkten sichern, sagte Hüther. Es sei erfreulich, dass viele Unternehmen diese Chance schon erkannt hätten.

Mehr als  50 Prozent der Industriebetriebe in Deutschland setzen KI laut der Studie bereits ein – deutlich mehr als der Durchschnitt der deutschen Wirtschaft (17 Prozent). KI werde in der verarbeitenden Industrie dazu 
verwendet, interne Systeme zu automatisieren (42 Prozent), Dokumente zu 
verfassen (31 Prozent) und Daten zu analysieren (24 Prozent). 

Das verarbeitende Gewerbe ist eines der wichtigsten Wirtschaftszweige in Deutschland und umfasst beispielsweise die Metallbranche. Laut IW beträgt die Wertschöpfung im Bereich des verarbeitenden Gewerbes 781 Milliarden Euro und fast acht Millionen Beschäftigte. Im Vergleich zu anderen Industrieländern ist in Deutschland der Anteil des verarbeitenden Gewerbes an der gesamtwirtschaftlichen Wertschöpfung mit mehr als 20 Prozent deutlich höher und nahezu doppelt so hoch wie in Großbritannien oder den USA. Für die gesamte Wirtschaft werde die mögliche Wertschöpfung mittels KI auf 330 Milliarden Euro geschätzt.  
"
KI,Zeit,2024-04-16,https://www.zeit.de/news/2024-04/16/iw-studie-milliarden-schub-fuer-deutsche-industrie-durch-ki,Künstliche Intelligenz: IW-Studie: Milliarden-Schub für deutsche Industrie durch KI | ZEIT ONLINE,"Der Einsatz von Künstlicher Intelligenz (KI) könnte der deutschen Industrie einen Schub in Milliardenhöhe verleihen. Laut einer Studie des Forschungsinstituts IW Consult im Auftrag von Google, die am Dienstag in Berlin vorgestellt wurde, könnte die Bruttowertschöpfung im verarbeitenden Gewerbe durch generative KI um bis zu 7,8 Prozent erhöht werden. Das entspreche einer Gesamtsteigerung von 56 Milliarden Euro. Die Bruttowertschöpfung bezeichnet den Gesamtwert der Waren und Dienstleistungen unter Abzug der Vorleistungen, und damit den im Produktionsprozess geschaffenen Mehrwert.

Generative KI ist eine Variante der Künstlichen Intelligenz, mit der man neue, originelle Inhalte schaffen («generieren») kann. Mithilfe der Algorithmen und sogenannter Sprachmodelle können Inhalte wie Texte, Bilder und Videos, aber auch Musik oder Programmcodes erzeugt werden. Die Vorgaben für das KI-System müssen nicht programmiert, sondern können in natürlicher Sprache übermittelt werden. Ein bedeutender Meilenstein für generative KI war die Veröffentlichung des Chatbots ChatGPT durch das Start-up OpenAI im November 2022. Unter anderem Google bietet mit Gemini ein eigenes Dialogsystem für generative KI an, das mit ChatGPT konkurriert.

Der Studie der Tochtergesellschaft des Instituts der deutschen Wirtschaft (IW) in Köln zufolge müssen sich vor allem Akademiker und Büroangestellte auf starke Veränderungen ihrer Arbeit durch KI einstellen. Im verarbeitenden Gewerbe seien dies rund 0,6 Millionen Beschäftigte, bei denen man eine starke Auswirkung von KI auf die Arbeit erwarte. Bei weiteren 4,1 Millionen Beschäftigten könne generative KI die eigene Arbeit unterstützen, etwa bei der Optimierung von Programmcodes oder als Ideengeber beim Produktdesign.

Im Alltag bei klassischen Industriejobs wie etwa Reparatur- oder Wartungsarbeiten werde die KI dagegen deutlich seltener zum Einsatz kommen. Diese rund 3,3 Millionen Stellen sind der Studie zufolge durch KI nicht oder nur schwer automatisierbar. Das betreffe etwa 41 Prozent aller Arbeitsplätze im verarbeitenden Gewerbe.

Der Direktor des Instituts der deutschen Wirtschaft Köln, Michael Hüther, erklärte, seit 2018 sei die reale Arbeitsproduktivität im Maschinenbau und anderen Bereichen des verarbeitenden Gewerbes mit lediglich 0,4 Prozent Plus pro Jahr nahezu konstant geblieben. «Durch generative künstliche Intelligenz könnte die Branche eine beachtliche KI-Dividende erzielen und damit ihre Produktivitätsvorteile auf den Weltmärkten sichern.» Es sei erfreulich, dass viele Unternehmen diese Chance schon erkannt hätten.

Aus der IW-Studie geht hervor, dass mehr als 50 Prozent der Industriebetriebe in Deutschland schon KI einsetzen. Damit liege die Branche deutlich über dem Durchschnitt der deutschen Wirtschaft (17 Prozent). Die Künstliche Intelligenz werde in der verarbeitenden Industrie dazu verwendet, interne Systeme zu automatisieren (42 Prozent), Dokumente zu verfassen (31 Prozent) und Daten zu analysieren (24 Prozent).

Das verarbeitende Gewerbe ist einer der wichtigsten Wirtschaftszweige in Deutschland. Laut IW beträgt die Wertschöpfung in diesem Bereich 781 Milliarden Euro und es gibt fast acht Millionen Beschäftigte. Im Vergleich zu anderen Industrieländern ist in Deutschland der Anteil des verarbeitenden Gewerbes an der gesamtwirtschaftlichen Wertschöpfung mit mehr als 20 Prozent deutlich höher und nahezu doppelt so hoch wie in Großbritannien oder den USA. Für die gesamte Wirtschaft werde die mögliche Wertschöpfung mittels KI auf 330 Milliarden Euro geschätzt.

© dpa-infocom, dpa:240416-99-689167/2
"
Künstliche Intelligenz,Zeit,2024-04-12,https://www.zeit.de/news/2024-04/12/chatgpt-wird-fuer-zahlende-nutzer-aktueller,Künstliche Intelligenz: ChatGPT wird für zahlende Nutzer aktueller | ZEIT ONLINE,"Die neueste Version des Chatbots ChatGPT für zahlende Kunden hat ein deutlich aktuelleres Wissen über die Welt. Das frische KI-Modell «GPT-4-Turbo» sei mit öffentlich zugänglichen Informationen bis Ende 2023 trainiert worden, teilte die Entwicklerfirma OpenAI mit. Bisher war bei April 2023 Schluss. OpenAI hatte in Aussicht gestellt, dass der Chatbot Zugriff auf immer aktuellere Informationen haben werde. Anfangs reichte sein Wissen nur bis Herbst 2021.

ChatGPT löste vor über einem Jahr den Hype um Künstliche Intelligenz aus. Solche KI-Chatbots werden mit gewaltigen Mengen an Informationen angelernt und können Texte auf dem sprachlichen Niveau eines Menschen formulieren, Software-Code schreiben und Informationen zusammenfassen.

Das Prinzip dahinter ist, dass sie Wort für Wort abschätzen, wie ein Satz weitergehen sollte. Ein Nachteil: Die Software gibt manchmal auch völlig falsche Antworten, selbst wenn sie nur korrekte Informationen als Basis hatte. Entwickler arbeiten daran, den Programmen verlässliche Leitplanken zu setzen, um so etwas zu verhindern.

Die neueste ChatGPT-Version werde auch weniger ausschweifende Texte formulieren, die umgangssprachlicher wirken sollen, kündigte OpenAI an. So liest sich eine vom Chatbot vorformulierte SMS-Erinnerung an Freunde, auf die Einladung zu einem Geburtstagsessen zu reagieren, kurz und sachlich - so wie sie auch ein Mensch schreiben würde. Zuvor hätte ChatGPT in dem von OpenAI demonstrierten Vergleich einen etwa drei Mal längeren blumigen Text geschrieben, samt der Feststellung, dass es nicht dasselbe wäre, ohne die Eingeladenen zu feiern.

© dpa-infocom, dpa:240412-99-646362/5
"
AI,Zeit,2024-04-11,https://www.zeit.de/politik/ausland/2024-04/ki-gaza-krieg-israel-lavender-big-data,"KI im Gaza-Krieg: Entscheidet eine Maschine, wer sterben muss? | ZEIT ONLINE",
KI,Zeit,2024-04-14,https://www.zeit.de/digital/2024-04/ki-chips-nvidia-intel-google-konkurrenz,Gaudi 3: Intel robbt sich ran | ZEIT ONLINE,"Ging es um Chips für KI-Anwendungen, dann schien in den
vergangenen Monaten vor
allem ein Unternehmen massiv zu profitieren: Nvidia. Der Marktanteil ihrer
Chips ist hoch, die Nachfrage so groß, dass viele Kunden monatelang
auf Lieferungen warten müssen. 

Klingt so, als könnte da noch Platz sein für eine Alternative – zumindest für eine, die technisch Schritt halten kann. Und genau auf diesem Platz scheint sich Intel nun sein Handtuch
auslegen zu wollen. In der vergangenen Woche stellte das Unternehmen seinen KI-Beschleuniger
Gaudi 3 vor. 

Dieser Chip soll Intels eigenen Angaben zufolge schneller und
energieeffizienter arbeiten können als Nvidias äußerst erfolgreicher KI-Chip
H100. Bestimmte Typen von KI-Modellen sollen damit 1,7-mal schneller trainiert werden können. Damit würde sich der Intel-Chip etwa
auf dem Niveau von Nvidias neuerem H200 bewegen – oder, wie Intel sagte: in
einigen Bereichen sogar noch etwas besser performen. Zur Wahrheit gehört aber auch:
Überprüfen kann diese Ankündigungen derzeit niemand, öffentlich verfügbar sind bislang nur die Angaben, die Intel selbst publiziert hat. Anfangs sollen die Chips in Geräten von Dell, Lenovo oder Supermicro verbaut werden, ab dem dritten
Quartal dieses Jahrs sollen sie generell verfügbar sein. 

Intel, der US-Chipriese, dessen Prozessoren über Jahre
hinweg der Standard für PCs und in vielen Smartphones verbaut waren, dessen Technologie aber
auch Grundlage vieler Cloud-Computing-Services darstellte, kann einen Erfolg gut gebrauchen. Zuletzt lief es für das Unternehmen nicht gut. Selbst Intel-Chef Pat Gelsinger räumte im vergangenen Jahr ein, man
sei ""gestolpert"" und habe ""Momentum verloren"". Den neuen KI-Beschleuniger Gaudi 3 muss man darum auch als einen der Versuche Intels sehen, dieser Entwicklung etwas entgegenzusetzen.  

Intel ist aber nicht das einzige Unternehmen, das diese
Woche mit neuen Chipankündigungen aufwartete. 

Google teilte in dieser Woche zum einen mit, dass seine
neueste Generation hauseigener KI-Chips nun
über die Google Cloud verfügbar ist. Die Chips mit dem sperrigen Namen TPU v5p sollen große Sprachmodelle fast dreimal schneller als ihre
Vorgänger trainieren können, ließ das Unternehmen schon bei der ersten Ankündigung der Spezialchips im vergangenen Dezember wissen. 

Außerdem kündigte Google erstmals
einen Hauptprozessorchip (CPU) an, der in Rechenzentren zum Einsatz kommen
soll. Axion soll in der Lage sein, Aufgaben von Big-Data-Analysen bis hin zur
Organisation von YouTube-Werbung übernehmen zu können. Gegenüber
dem Wall Street Journal betonten Google-Vertreter die Rolle,
die auch dieser Chip für KI spielen könnte – sei es
beim Verarbeiten großer Datenmengen oder auch bei der Anwendung der Dienste
für Milliarden Nutzerinnen. 

Analysten
sehen in Googles Chipbemühungen insgesamt den Versuch des Unternehmens,
seine Abhängigkeit von externen Herstellern zu reduzieren. Denn: An Rechenkapazität hat ein Cloud-Unternehmen, wie Google es ist, generell hohen Bedarf. Spätestens mit dem jüngsten KI-Boom hat sich dieser aber in der gesamten Branche noch einmal intensiviert. Besonders groß ist das Interesse derzeit an Grafikprozessoren (GPUs), die darauf ausgelegt sind, zahlreiche Berechnungen parallel auszuführen, und die sich darum für KI-Anwendungen besonders gut eignen. Darauf fußt der Erfolg von Nvidia. 

Wie Microsoft und Amazon setzt Google nun aber auch darauf, auf Basis von Chipdesigns der britischen Firma Arm Prozessoren für seine Datencenter zu bauen. Das wiederum könnte zulasten der Firmen gehen, die traditionell die großen Spieler auf dem CPU-Markt sind: Intel und AMD.  

Auf der anderen Seite macht Google rund um seine KI-Entwicklungen unmissverständlich klar, dass – zusätzlich zur eigenen Technologie  – Nvidia-Chips
für die KI-Architektur ihrer Cloud weiterhin eine wichtige Rolle spielen und künftig auch spielen werden. 

Google hat sich dagegen entschieden, seine Chips direkt an Kunden
zu verkaufen, damit diese sie in ihren Datencentern verbauen können. Stattdessen will der Konzern Entwicklern lediglich über die Google-Cloud-Plattform Zugang
gewähren. 

Dahinter dürfte die Sorge stehen, den mächtigen Chiplieferanten Nvidia zu vergrätzen, indem man sich zu deutlich als Konkurrenz zu ihm positioniert – dafür scheint man immer noch viel zu stark auf Nvidia angewiesen zu sein. So formuliert Amin Vahdat, Googles Vizepräsident für hausinterne
Chipentwicklungen, demWall
Street Journal gegenüber recht versöhnlich, dass er die Chipanstrengungen seines Unternehmens eher als ""Basis"" sehe, ""um die Größe des Kuchens wachsen zu lassen"".   

All diesen Entwicklungen hat derweil auch der Meta-Konzern nicht untätig zugesehen: Er präsentierte in dieser Woche ebenfalls einen selbst entwickelten
KI-Beschleuniger, der speziell KI-Anwendungen auf seinen Plattformen Facebook,
Instagram und WhatsApp zugeschnitten sei. Leisten soll die neue MTIA-Generation zum Beispiel die effizientere
Bedienung der Ranking- und Empfehlungsmodelle. Wie auch Google bemüht sich Meta aber zu betonen: Nvidia-Technologie
solle nicht ersetzt, sondern ergänzt werden. 
"
KI,Zeit,2024-04-12,https://www.zeit.de/news/2024-04/12/zu-ende-gedacht-film-zeichnet-diktatur-in-deutschland,"KI: ""Zu Ende gedacht"" - Film zeichnet Diktatur in Deutschland | ZEIT ONLINE","Mit einem KI-generierten Video haben Filmschaffende um eine Hamburger Produktionsfirma eine Zukunft in einem von diktatorischen Verhältnissen und Zerfall geprägten Deutschland gezeichnet. Geschildert wird eine katastrophale Situation als Folge der Deportation von Menschen mit Migrationshintergrund. Unter dem Titel ""Oma, was war nochmal dieses Deutschland?"" ist das Video auf der Plattform Youtube abrufbar. Zuvor hatten einige Medien über das Projekt berichtet.

""Wir haben es zu Ende gedacht"", schreiben die Produzenten Andreas Loff, Behzad Karim Khani und Christian Suhr im Begleittext. In dem dreieinhalbminütigen Video berichtet eine von der Schauspielerin Anna Thalbach gesprochene Großmutter ihrer Enkelin (Nellie Thalbach) im Jahr 2060 rückblickend über den Niedergang Deutschlands. Die desaströse Entwicklung beginnt damit, dass eine als ""Die Blauen"" bezeichnete Partei die Macht übernimmt. In der Folge werden Wahlen abgeschafft und Menschen mit Migrationshintergrund deportiert.

Die Oma schildert der Enkelin die katastrophalen Folgen durch einen dramatischen Mangel an Arbeitskräften: ""Der Müll nicht abgeholt, Post nicht zugestellt, Felder nicht geerntet, Lebensmittel nicht geliefert, Fabriken geschlossen. Die Nationalmannschaft verlor noch höher. Alte Menschen vereinsamten ohne Pfleger. Internationale Beziehungen waren dahin. Menschen verarmten. Große Städte wurden leer und auf dem Land fehlte sowieso alles.""

Die Video-Macher wollen nach Angaben von Andreas Loff, Geschäftsführer von Ponywurst Productions, auf die Folgen möglicher rechtsextremer Erfolge bei den anstehenden Wahlen hinweisen. Erreicht werden sollten vor allem Menschen, die mit ihrer Wahlentscheidung vielleicht noch auf der Kippe stünden. Vielen seien die Folgen nicht klar, dies solle mit dem Video bewusst gemacht werden.

© dpa-infocom, dpa:240412-99-650765/3
"
KI,Zeit,2024-04-11,https://www.zeit.de/politik/ausland/2024-04/ki-gaza-krieg-israel-lavender-big-data,"KI im Gaza-Krieg: Entscheidet eine Maschine, wer sterben muss? | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-04-09,https://www.zeit.de/news/2024-04/09/chatgpt-fuer-alle-an-der-universitaet-hamburg,Künstliche Intelligenz: ChatGPT für alle an der Universität Hamburg | ZEIT ONLINE,"Allen Studierenden und Beschäftigten der Universität Hamburg (UHH) steht seit Dienstag über die Hochschule ein Zugang zu ChatGPT zur Verfügung. Betrieben werde es vom Zentrum für nachhaltiges Forschungsdatenmanagement und dem Regionalen Rechenzentrum, das an der Universität die IT-Infrastruktur verantworte, teilte die Hochschule mit. Fast 43.000 Studierende und mehr als 15.000 Beschäftigte könnten die Anwendung nutzen, ohne dass personenbezogene Daten an ChatGPT übertragen oder gespeichert würden. Auch würden die eingegebenen Informationen nicht für die weitere Entwicklung des Chatbots verwendet. Der Zugang zu «UHHGPT» läuft den Angaben zufolge über ein Webtool, das ursprünglich an der Hochschule Hildesheim entwickelt und für die Universität Hamburg angepasst wurde.

«Künstliche Intelligenz (KI) birgt ein enormes Potenzial für Wissenschaft und Gesellschaft, das wir an der Universität Hamburg entschlossen nutzen wollen», erklärte Uni-Präsident Hauke Heekeren. Als Wissensorganisation müssten sich Universitäten in ihren Kernbereichen Forschung und Lehre, aber auch in der Verwaltung mit den Entwicklungen der künstlichen Intelligenz aktiv auseinandersetzen. «Die Einführung von UHHGPT ist ein weiterer Schritt in der Digitalstrategie der UHH.»

Zur Einführung erhalten die Mitglieder der Universität nach Hochschulangaben per Mail und in entsprechenden Fragen und Antworten alle wichtigen Informationen zur Nutzung - etwa mit Blick auf Urheberrecht und Hausarbeiten. ChatGPT löste vor gut einem Jahr den Hype um Künstliche Intelligenz aus. Solche KI-Chatbots werden mit gewaltigen Mengen an Informationen angelernt und können Texte auf dem sprachlichen Niveau eines Menschen formulieren, Software-Codes schreiben und Informationen zusammenfassen.

Das Prinzip dahinter ist, dass sie Wort für Wort abschätzen, wie ein Satz weitergehen sollte. Ein Nachteil: Die Software gibt manchmal auch völlig falsche Antworten, selbst wenn sie nur korrekte Informationen als Basis hatte. Entwickler arbeiten daran, den Programmen verlässliche Leitplanken zu setzen, um so etwas zu verhindern.

© dpa-infocom, dpa:240409-99-614234/3
"
Künstliche Intelligenz,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-filmbranche-special-effects-newsletter-kuenstliche-intelligenz,KI in der Filmbranche: So generiert man keinen Oscar | ZEIT ONLINE,"Sie lesen den KI-Newsletter
""Natürlich intelligent"" vom 04. April 2024. Um den Newsletter jeden
zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Generative AI
goes to Hollywood. Zumindest wünschen sich
das wohl die Chefs von OpenAI, die Medienberichten zufolge gerade auf Werbetour
durch einige große Filmstudios unterwegs waren.

Künstliche Intelligenz spielt in der
Filmbranche schon lange eine Rolle, beispielsweise bei der Umsetzung von
Spezialeffekten wie der Verjüngung von Harrison Ford. In der stark
gewerkschaftlich organisierten Filmindustrie gibt es gegen viele der Einsatzmöglichkeiten
Widerstände. Die typische Sorge: KI könnte Menschen ersetzen.

Mit der neusten Spielart der
Technologie, Text-to-Video, sollen Filme nun wie von allein, basierend nur auf
Textbeschreibungen entstehen. OpenAI hat dazu mit den Democlips aus dem
Sora-Modell die bislang beeindruckendsten Ergebnisse gezeigt. Wer generierte
Clips wie den einer Frau sieht, die durch eine Innenstadt läuft und sich in
einer Pfütze spiegelt, mag davon ausgehen, dass es nicht mehr allzu lange
dauern wird, bis wir die ersten von KI erzeugten Videos im Kino sehen.

Tatsächlich dürfte diese Erwartung
aber etwas voreilig sein. Denn die Limitierungen, die Sora noch hat, sind nicht
die Kleinigkeiten, für die man sie auf den ersten Blick halten kann.

Noch sehen wir in den Videos die für
generative KI typischen Inkonsistenzen. Objekte tauchen aus dem Nichts auf,
Menschen verschwinden oder haben zu viele Gliedmaßen. Derartige Fehler werden
sich durch besseres Training schnell reduzieren lassen, so wie es bei Bild-KI
in den letzten Jahren geschehen ist.

Videos haben dabei sogar noch einen
Vorteil: Was ein konsistentes Verhalten eines bewegten Objekts ist, kann man
einem Computer in der Theorie viel besser beibringen als Faktentreue in einem
Text. Denn die Grundlage ist Physik, und für deren Simulation gibt es schon
seit Jahren Programmbibliotheken aus der Videospielindustrie.

Trotzdem muss viel zusammenkommen, um
schöne, realistische und vor allem in der Praxis nutzbare KI-Videos zu
erzeugen. Die Fülle an möglichen Settings, Bewegungen und Texturen bedeutet
eine enorme Herausforderung für die Ingenieurinnen, die den Systemen beibringen
sollen, Videos nach unseren Erwartungen zu erzeugen.

Effizient lernen bedeutet, sich auf
die wichtigsten Aspekte zu fokussieren. Unsere menschliche Wahrnehmung kann das
gut: Das Gehirn filtert ständig heraus, was es nicht braucht. Aber wie setzt
man das in Maschinen um?

Ein physikalisch realistisches Video
von einer Verfolgungsjagd bringt Hollywood wenig, wenn beim Training die Farben
oder Texturen dafür vernachlässigt werden mussten. Aber ein Modell, das alle
Aspekte gleichzeitig verarbeitet, ist mathematisch anspruchsvoll und vor allem
furchtbar energieaufwendig. Schon die jetzigen Modelle sind riesig und brauchen
ein Vielfaches mehr an Strom und Zeit als ihre Text- oder Bildkollegen. Je länger das
Video sein soll, desto komplizierter wird es. Sora kann Videos erzeugen, die
maximal eine Minute dauern, und ist möglicherweise auch deshalb nicht
öffentlich zugänglich, weil es zu viel Rechenleistung bräuchte, wenn Millionen
Menschen damit herumspielen würden.

Und dann wären
da noch die Details, auf die gerade bei teuren Film- und Serienproduktionen
penibel geachtet wird. Generierte Videos lassen sich, genau wie generierte
Texte und Bilder, nur begrenzt steuern.

Ist der Blick
des Mannes in einer Szene verliebt genug? Greift er genau im richtigen Moment
nach der Hand seines Partners? Sieht sein Bart ein bisschen blonder aus als in
der zuvor generierten Einstellung? Wenn einzelne Dinge nicht stimmen, muss man
noch mal generieren und hoffen – oder man bearbeitet es händisch so lange nach,
bis es passt. Doch wieder Menschenarbeit.
"
Künstliche Intelligenz,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/openai-newsletter-kuenstliche-intelligenz,OpenAI: Liebe KI-Utopisten: Sie müssen jetzt ganz stark sein | ZEIT ONLINE,"Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 30. November 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Innerhalb von wenigen Tagen wurde OpenAI-Chef Sam Altman
erst gefeuert – und dann auf seinen alten Posten zurückgeholt. Der Machtkampf
an der Spitze von OpenAI kam unerwartet, schnell und heftig. Und jetzt kochen
Gerüchte hoch, was dieses ganze Drama eigentlich verursacht hat.

Eine Lieblingstheorie auch in der Medienberichterstattung
ist, dass ein bahnbrechender Durchbruch in der KI-Forschung der Auslöser
gewesen sein soll. Die Nachrichtenagentur Reuters und die Plattform The
Information berichteten unter Berufung auf anonyme interne Quellen, OpenAI habe
ein Modell namens Q* entwickelt, das Mathematikaufgaben auf Grundschulniveau
lösen könne – und möglicherweise ""die Menschheit bedrohen"". Insider
sollen in der Technologie einen entscheidenden Schritt hin zur menschenüberflügelnden
Superintelligenz sehen.

Ob da was dran ist, ist schwer zu sagen. OpenAI hat schon
oft gezeigt, dass Mysterien gutes Marketing sind: Schon vor vier Jahren unkte
man dort, man wolle nur Teile seines Modells GPT-2 öffentlich machen – aus
""Sicherheitsgründen"". Die Firma schweigt nun zu allen Spekulationen
um Q*. Aber Sam Altman dürfte es ganz recht sein, dass in den Medien wieder
mehr über die vermeintliche Macht einer gerüchteumwobenen
Möglicherweise-Super-KI gesprochen wird statt beispielsweise über
zwischenmenschliche Verhakungen im Management von OpenAI.

""Könnte die Menschheit bedrohen"" ist in der Welt von Sam Altman und Co nämlich immer nur einen Schritt entfernt von einer anderen
Vision: Wenn es uns gelingt, dass uns die Super-KI nicht vorher umbringt, soll
sie uns in ein Zeitalter des ungekannten ökonomischen Wohlstands führen. Die
Wirtschaft würde komplett von ihr transformiert. Niemand müsste mehr arbeiten,
allen ginge es gut. Immerhin ist die Frage nach dem exponentiellen
Wirtschaftswachstum dank KI greifbarer und überprüfbarer als die, wann der Durchbruch
kommt, der KI übermenschliche, gefährliche Fähigkeiten verleiht.

Arjun Ramani und Zhengdong Wang, ein Wirtschaftsjournalist
und ein KI-Forscher, haben dazu im Sommer aus vielen Quellen die besten
Anhaltspunkte zusammengetragen. Und sie kommen zu dem Schluss, es werde
""wirklich, wirklich schwierig"", tatsächlich transformative KI zu
erreichen.

Sie entlarven einen Denkfehler in der Hoffnung auf eine
vollautomatisierte Luxuswelt: Selbst exponentielles Wachstum in einem
Teilbereich weitet sich nicht zwangsläufig auf andere Bereiche aus. Im
Gegenteil: Oft begrenzt der Sektor, der am langsamsten wächst, die
Produktivitätssteigerungen insgesamt.

Unsere Wirtschaft beruht zum größten Teil noch auf
physischen Prozessen, die selbst mit tollen KI-Lösungen nur schwer verändert
werden können. Selbst wenn eine Superintelligenz die Architektur
revolutioniert: Irgendjemand muss noch immer für die Mauern die Steine
verspachteln. Denn die Robotik ist noch lange nicht so weit, dass sie Menschen
in allen Konstruktionsaufgaben ersetzen kann.

Maschinen haben uns schon jetzt in vielen Bereichen
überflügelt. Und der technische Fortschritt hat unsere Welt in den letzten
Jahren fundamental verändert, teils verbessert. Auch wirtschaftlich.

Zur Lösung unserer Probleme mangelt es uns oft aber nicht an
Wissen oder Intelligenz. Sondern an Kooperationsfähigkeit.

Beispiel humanitäre Hilfe: Deren Logistik ist ein Problem,
das mathematisch gut lösbar wäre. Schon heute. Menschen in Krisenregionen
hungern trotzdem noch immer. Und wir wissen genau, dass wir Emissionen von
Treibhausgasen verringern müssen, um die Klimakrise zu verlangsamen. Aber wir
scheitern an der Umsetzung.

Die KI-Forschung wird weiter Fortschritte machen. Hilfreiche
und kritikwürdige. Aber keine noch so intelligente Technologie wird je der
Killswitch für alle Probleme der Menschheit sein. Wer alles auf diese Hoffnung
setzt, wird enttäuscht werden. Und wer nur KI als Hoffnung verkauft, wird
enttäuschen.
"
Künstliche Intelligenz,Zeit,2024-04-08,https://www.zeit.de/gesellschaft/schule/2024-04/kuenstliche-intelligenz-schule-bildung-gpt,Künstliche Intelligenz: Die Vertretungsstunde macht heute die KI | ZEIT ONLINE,"Der Hamburger Grundschullehrer Alex Tscheulin ist ein IT-Nerd. Er probiert jedes digitale Tool aus. ChatGPT hat ihn bisher enttäuscht, viel zu allgemein seien die Antworten. ""Und immer das Risiko eines Fehlers"", sagt er, das ärgert ihn. Aber nun hat er seinen eigenen Tscheulin-Bot, den er selbst mit Daten trainieren kann. Tscheulin lud die neuen Bildungspläne hoch und fragte seinen Co-Lehrer nach einem Einstieg in eine Stunde. ""Bring doch eine Schatzkiste mit"", riet ihm Tscheulin-GPT, ""und dann gehst Du die Schätze Deiner Schüler suchen."" 

Tscheulin ist Beauftragter für Digitalität der privaten katholischen Schulen im Erzbistum Hamburg. Als er seinen Schulleitern das neue Fachlehrer-GPT vorstellte, waren sie erfreut: endlich ein ChatBot, der nicht plappert und halluziniert, sondern lehrergerechte Antworten gibt. Solche mit eigenen Texten aufgeladenen und mit System-Prompts fokussierten KI-Assistenten heißen Custom-GPTs. GPT bedeutet Generative Pretrained Transformer – und hier setzt die neue Variante an: Die Lehrkräfte trainieren ab sofort selbst. 

Das Schlimmste, was Sprach-KI-Systeme in den Augen deutscher Studienräte nämlich tun: Sie lügen, genauer, sie halluzinieren. Damit soll jetzt Schluss sein, jedenfalls soll die Schwindelei mithilfe von Custom-GPTs nahe null gedrückt werden. Die beiden KI-Anbieter Fobizz und Schulverwalter, die einige Bundesländer mit ChatGPT versorgen, stellen personalisierte Lernbots zur Verfügung. Zwei Drittel der Schüler und Studierenden nutzen laut Umfragen die Sprach-KI bereits. Ziehen die Lehrkräfte jetzt mit selbst angelernten KI-Assistenten nach? 

""Bei uns kann ab sofort jede Lehrkraft fünf Dateien mit je zehn Megabyte als Hintergrundwissen in ihre eigene KI hochladen"", berichtet Diana Knodel von Fobizz. Der digitale Weiterbildungsanbieter hat sich auf pädagogische Anwendungen künstlicher Intelligenz spezialisiert. Eine Biologielehrerin kann nun den wichtigen Aufsatz über den Zitronensäurezyklus in ihr GPT einspeisen, der Geschichtslehrer Texte zu Wallenstein hochladen. Rund 100 Seiten Zusatzwissen sind derzeit möglich. 

Auch der Leipziger KI-Anbieter Schulverwalter sagte zu ZEIT ONLINE, dass dort auf Fächer und Schulen fokussierte Sprachmodelle buchbar seien. ""Das ist im Prinzip immer noch dieselbe Sprach-KI, nur dass die Lehrkraft ihr erklärt, wo sie Zusatzinformationen herbekommt, damit die Antworten endlich besser werden"", sagt Julian Dorn, der Gründer der Schulverwalter, der als Informatiklehrer arbeitet. 

Sowohl die Schulverwalter als auch Fobizz wollen das Angebot zunächst als kostenfreie Ergänzung für Lizenznehmer bereitstellen. Fobizz bietet ChatGPT flächendeckend in den Schulen von Mecklenburg-Vorpommern und Rheinland-Pfalz an. Die Sprach-KI der Schulverwalter wird gerade in Niedersachsen und Bayern in Pilotprojekten getestet. 
"
Künstliche Intelligenz,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-hype-newsletter-kuenstliche-intelligenz,"KI-Hype: Vielleicht wird KI doch nicht so schnell schlauer, wie alle denken | ZEIT ONLINE","Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 02. November 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Es ist Quartalszahlen-Zeit und das heißt auch, Konzerne
erklären ihren Anlegern und der Öffentlichkeit, wie die Geschäfte laufen. Im
KI-Bereich waren die Erwartungen groß, immerhin redet alle Welt davon, dass
KI-Modelle wie GPT-4 mindestens das neue Internet sein könnten, wenn nicht
gleich die Neuerfindung des Rades. Die Firmen, die KI-Services anbieten oder
die dafür nötige Rechenleistung vermieten, die Amazons, die Microsofts, die
Alphabets, müssten doch eigentlich wachsen wie wild. Oder?

Tatsächlich hat sich das Wachstum der Cloud-Sparten von
Amazon und Alphabet verlangsamt. Einzig Microsoft gewinnt durch KI stärker als
erwartet; die Einnahmen durch Cloud-Services stiegen im Vergleich zum Vorjahr
um 24 Prozent. Das mehrere Milliarden Dollar schwere Investment in OpenAI
scheint sich auszuzahlen.

Trotzdem wirft das ruckelige Wachstum die Frage auf, wie es
mit dem KI-Hype weitergeht. In die begeisterten Stimmen mischt sich langsam
etwas Skepsis.

Microsoft-Gründer Bill Gates zum Beispiel sagte vor einigen
Tagen in einem Interview mit dem Handelsblatt, dass er ein Plateau erwarte:
GPT-5 könnte nur ein bisschen besser werden als GPT-4. Aus dem Interview geht
nicht hervor, worauf er diese Annahme stützt, aber er könnte auf etwas
anspielen, das diesen Sommer bereits von einigen KI-Expertinnen und -experten
diskutiert wurde: Der KI gehen die Daten aus.

Um eine leistungsstarke KI zu bauen, braucht man grob gesagt
zwei Dinge: Daten und Rechenleistung. Daten sind der Rohstoff und die
Rechenleistung muss aufgebracht werden, um daraus ein Netzwerk zu schmieden.

Einerseits sind die leistungsstarken Chips knapp, mit denen
die Rechenleistung für KI aufgebracht wird. Aber das ist ein Problem, das
lösbar scheint. Neue Technologien starten meistens mit großem Energieaufwand
und mit der Zeit finden sich Wege, sie effizienter zu machen. Viele
KI-Anwendungen werden in nächster Zeit günstiger im Betrieb werden.

Auf der Seite der Daten sieht es anders aus, vor allem, was
Texte angeht. Es gibt einfach nicht unbegrenzt verfügbare und vor allem
qualitativ hochwertige Texte, mit denen KIs trainiert werden können. Die
Menschheit kann gar nicht so schnell schreiben, wie KIs lesen können.

Unternehmer wie OpenAI-Chef Sam Altman sagen, sie machen
sich deshalb keine Sorgen. Tatsächlich suchen Forschende nach Wegen, bestehende
Datensätze zu verbessern. Sie können auch KI nutzen, um beispielsweise Texte
aus Videos zu transkribieren oder (in begrenztem Maße) neue Trainingsdaten zu
generieren. Und Unternehmen werden Wege suchen, noch mehr Daten aus unseren
Alltagskonversationen und Privatnachrichten abzugrasen. Aber egal, wie viel sie
aus den letzten Ecken herauskehren: Die einfach verfügbaren riesigen
Datenmengen (früher sagte man: Bibliotheken) aus den vergangenen Jahrhunderten
sind schon verbraucht. Kleine Verbesserungen bedeuten nun großen Mehraufwand.

Daher kommt die Sorge vor einem Plateau: Die beeindruckenden
Fortschritte, etwa von GPT-3 zu GPT-4, entstanden vor allem durch mehr Daten.
Das war es, was OpenAI so erfolgreich gemacht hat und in dessen Folge Microsoft
nun besser dasteht als die Konkurrenz. Das Unternehmen setzte schon früh alles
auf die Karte ""größer ist besser"".

Auch deshalb war die Annahme in den vergangenen Monaten: Wer
nicht mehrere Milliarden Dollar in KI-Entwicklungen stecken kann, muss gar
nicht erst anfangen. Im Kampf um Marktanteile nehmen Amazon, Microsoft und
Alphabet sogar in Kauf, erst mal ordentlich Geld zu verlieren. Laut einem
Bericht des Wall Street Journal subventioniert Microsofts Coding-Plattform
GitHub seinen KI-Assistenten jeden Monat mit durchschnittlich 20 Dollar pro
Nutzendem.

Die Kosten für den Betrieb und auch für das Training von KI
werden günstiger werden. Und wenn gleichzeitig die KI-Fähigkeiten, wie Bill
Gates vermutet, ein Plateau erreichen, sind das vor allem gute Nachrichten für
kleine Unternehmen und Start-ups. Der Vorsprung, den sich die Tech-Giganten mit
ihrem vielen Geld erkauft haben, ist nicht groß. Das OpenAI, ja sogar das
Microsoft der 2030er-Jahre könnte eine Firma sein, deren Name wir heute noch
gar nicht kennen.
"
Künstliche Intelligenz,Zeit,2024-04-07,https://www.zeit.de/2024/15/kuenstliche-intelligenz-anguilla-internet-domain-ai,Künstliche Intelligenz: Endung gut | ZEIT ONLINE,
AI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-urheberrecht-new-york-times-newsletter-kuenstliche-intelligenz,"KI und Urheberrecht: Darf OpenAI sich bei der ""New York Times"" bedienen? | ZEIT ONLINE","Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 11. Januar 2024. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Die New York Times verklagt OpenAI und Microsoft. Für mich
ist das eine der spannendsten Entwicklungen der letzten Monate. Denn die Klage
könnte endlich Antworten auf zwei grundlegende Fragen erzwingen: was Large
Language Models wirklich sind. Und wer mit Inhalten, die im Netz stehen, Geld
verdienen darf.

Die New York Times will belegen können, dass ins Training
von OpenAIs Produkten Millionen von ihren Artikeln eingeflossen sind – ohne,
dass die Zeitung zugestimmt hätte oder kompensiert worden wäre. Es ist nicht
die erste Klage dieser Art: Auch die Techunternehmen OpenAI, Meta, Google,
Github, Midjourney und StabilityAI wurden von Kreativschaffenden verklagt, weil
diese ihre Werke unzulässig verwendet sahen. In den meisten Fällen stehen die
Urteile noch aus.

Das Verfahren, das die New York Times nun anstößt, sticht
aber heraus. Die Zeitung ist eine bedeutende und reichweitenstarke Marke,
sodass ihr Fall nicht nur vor Gericht, sondern auch in der Öffentlichkeit
diskutiert werden wird. Die zentrale Frage ist, ob große Sprachmodelle den
Journalismus kaputtverdauen – selbst den eines der mächtigsten Medienhäuser der
Welt. Denn: Wenn ChatGPT New-York-Times-Artikel so gut wieder ausspucken
könnte, dass es keinen Anreiz mehr gibt, deren Homepage zu besuchen, geschweige
denn ein Abo abzuschließen, dann hätte die Zeitung ein Problem.

Aber ob große Sprachmodelle wirklich derartig mit dem
Journalismus in Konkurrenz treten können, ist noch nicht klar – unter anderem,
weil nicht mal Einigkeit darüber besteht, wie ihre Informationsverarbeitung
genau zu verstehen ist. Sind sie kluge Contentproduktionsmaschinen, nur ein
paar zusätzliche Grafikkarten davon entfernt, eigenständig zu denken? Oder sind
sie doch eher so etwas wie Kompressionsalgorithmen – nichts mehr als ein
""verschwommenes JPEG des Internets"", wie es der Autor Ted Chiang in
einem bekannten Essay formulierte?

KI-Firmen wie OpenAI schwimmen bislang zwischen diesen
beiden Narrativen. Einerseits betonen sie immer wieder das Ziel, dass lernende
Maschinen das verarbeitete Wissen auch auf neue Konzepte anwenden können.
Anders gesagt: Mit Artikeln trainiert gibt die generative KI neue Antworten.
OpenAI weist die Vorwürfe der New York Times zurück: Wenn ChatGPT ganze
Textpassagen reproduziere, wie die Zeitung unter anderem argumentiert, handle
es sich um ""seltene Fehlfunktionen"", die nur durch Trickserei gelängen.
OpenAI vergleicht seine Modelle mit Menschen: Auch die dürfen aus dem Internet
lernen.

Andererseits arbeitet die Firma hart daran, sogenannte
Halluzinationen zu verringern, also Sätze, die zu stark von den Informationen
in den Trainingsdaten abweichen. Das wäre wichtig, je mehr man seine Produkte
als Suchmaschinenalternative vermarkten will. Es würde aber auch bedeuten, dass
die Sprachmodelle eigentlich die Kompression von antrainiertem Wissen leisten.

Es könnte spannend werden, wie dies nun vor Gericht
verhandelt wird. Egal aus welcher dieser beiden Richtungen man argumentiert,
kann die New York Times meiner Meinung nach gut antworten. Wenn OpenAIs
Sprachmodelle New-York-Times-Artikel unkompensiert reproduzierten, verstoße das
gegen das Urheberrecht, heißt es in der Anklageschrift. Aber selbst, wenn das
Gericht das Reproduktionsargument nicht sehen sollte, dann würde ChatGPT
zufällig – und dabei oft falsch – aus New-York-Times-Artikeln zitieren. Letzteres
könnte man als rufschädigend sehen.

Das zweite Problem, das mitverhandelt wird, ist, dass das
Urheberrecht an sich einigermaßen veraltet ist. Erst digitale Technologien,
dann das Internet haben verändert, was wir unter ""Kopien"" verstehen –
und bei ""geistigem Eigentum"" wird es noch komplizierter. Darum hakt
es immer wieder bei der Frage, wie man Informationen im Digitalen schützt und
honoriert.

Für Medien ist die Situation online besonders kniffelig. Auf
der einen Seite steht Wissen als gesellschaftliches Gut, das so breit
zugänglich sein sollte wie möglich. Nachrichten, der Journalismus der New York Times, haben einen Wert für demokratische Gesellschaften, darauf können sich
die meisten Menschen einigen.

Auf der anderen Seite steht die Entlohnung derer, die diese
Informationen bereitstellen. Themen zu recherchieren, Inhalte aufzubereiten,
kostet Geld – darum gibt es Paywalls, Werbeanzeigen. Doch ein großer Teil der
Werbeeinnahmen bleibt schon jetzt bei Google und Meta hängen. Und für
Onlinejournalismus direkt zu bezahlen, ist nie selbstverständlich geworden.
Viele Nutzerinnen und Nutzer tun das nicht – und große KI-Unternehmen bedienen
sich offenbar auch freimütig. 

Informationen zugänglich machen und doch die Urheberinnen
vergüten: Dieses Dilemma aufzulösen, wird mit generativer KI nicht einfacher.
Sie kann den Trend fortführen, dass einige wenige Geld verdienen mit den
Leistungen vieler. Teure Produktionsmittel führen oft dazu, dass sich die Macht
darüber an der Spitze konzentriert.

Aber ein Large Language Model ist nichts wert ohne die
Daten, die es speisen. Wir alle füttern es – vom Artikel in der New York Times
bis hin zum Tweet, den keiner liest. Insofern ist es richtig, gut, dass ein
Großunternehmen wie die New York Times sich nicht einfach von OpenAI in Deals
verstricken lässt, sondern Grundsatzfragen vor Gericht stellt. Und vielleicht
wird 2024 ja das Jahr, in dem wir bessere Wege finden, das Teilen von Wissen zu
belohnen.
"
AI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-arbeitswelt-newsletter-kuenstliche-intelligenz,KI in der Arbeitswelt: Wann macht die KI denn nun meine Arbeit? | ZEIT ONLINE,"Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 21. März 2024. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Der KI-Bot Devin soll selbstständig
programmieren können. Er ist ein sogenannter Agent, also
ein System, das durch die Verknüpfung verschiedener Automatisierungsmethoden
komplexe Aufgaben lösen können soll. ChatGPT gibt eine Antwort auf eine
Eingabe, ein Agent nimmt eine Anfrage, zerlegt sie in Teile und spricht so
lange mit sich selbst und dem Internet, bis er sie erfüllt hat. Das ist
jedenfalls die Idee.

Als vor etwa
einem Jahr die ersten auf GPT basierenden Agenten auftauchten, waren sie sehr
primitiv und voller Fehler. Nun hat Devin unter Programmiererinnen und
Programmierern aber einen ziemlichen Schock ausgelöst: Der KI-Bot soll einfach
die ganze Arbeit machen können, konzipieren, programmieren, Fehler suchen.
Viele Programmierer verwenden schon lange KI-Helfer, zum Beispiel GitHub
Copilot. Aber, wie der Name sagt, ist der ein Assistent, er macht Vorschläge,
Mensch und KI machen die Arbeit gemeinsam. 

Wie so oft,
wenn Demovideos die Timelines elektrisieren, ist die Aufregung vielleicht ein
bisschen zu groß und deutlich zu voreilig. Große Sprachmodelle und ihre
Anwendung in Programmen wie dem KI-Agent Devin können im weitesten Sinne genau
eine Sache: Informationsverarbeitung. Sie können anhand gelernter Regeln Texte
zusammensetzen. Und je mehr Daten, je strukturierter die Sprache und je
geringer die Varianz, desto leichter ist es für Maschinen, gute Ergebnisse zu
erzielen. Deshalb sind Programmiersprachen für die Systeme recht leicht zu
beherrschen.

Ein großer
Teil der Arbeit einer Programmiererin fällt genau in die Bereiche, in denen
Maschinen gerade exzellent werden: Sie muss in der Lage sein, konkrete
Anforderungen in Programmiersprachen zu übertragen, und wenn sie es nicht
auswendig weiß, Informationen dazu zu suchen, wie das geht. Es ist ein gängiger
Witz unter Programmierenden, dass sie eigentlich mehr googeln als selbst
Aufgaben lösen. 

Das sind alles
Dinge, die ein KI-Agent können kann. Die Vision mancher KI-Hersteller ist es
deshalb, dass man schon bald keine Softwareentwicklerinnen mehr braucht. Oder
zumindest viel weniger, weil ein einzelner Mensch viele Agenten beaufsichtigen
könnte, so wie heute ein Senior Developer viele Juniors anleitet.

Nur: Maschinen
liefern im wahrsten Sinne des Wortes durchschnittliche Ergebnisse. Viele
Aufgaben in der Programmierung sind vorhersehbar, da sie oft da gewesen sind.
Bei diesen kleinen, einfachen Problemen, zum Beispiel die Programmierung einer
Website, wird das reichen, was die Maschinen können. 

Aber dort, wo
nicht die erwartbarste Lösung die richtige ist, wird es schnell richtig
ineffizient, wenn man Agenten ans Werk lässt. Denn Informationsverarbeitung
funktioniert dann gut, wenn auch Informationen da sind. Aber das Internet ist
ein schlecht strukturiertes Netzwerk. Wenn der Agent nicht zufällig das eine
Forum findet, in dem exakt die Lösung steht, dann wird er sich schwertun. 

KI-Agenten
sollen zwar auch komplexe Probleme automatisieren können. Aber sie machen das
mit einer Art Brute-Force-Methode. Sie suchen einigermaßen wahllos im Internet,
sie stellen Hunderte Abfragen in kurzer Zeit an das im Hintergrund arbeitende
Sprachmodell. Vielleicht finden sie zufällig dabei auch mal eine Lösung, aber
wenn es schlecht läuft, fressen sie enorm viel Strom und Zeit, während sie
komplett in die falsche Richtung laufen.

Menschen
hingegen sind gut darin, auch mit lückenhaften Informationen und komplexen
Problemlagen umzugehen. Zumindest die besten von uns. Deshalb sind
Softwareentwicklerinnen auch so teuer.

John von
Neumann, ein Pionier der künstlichen Intelligenz (und der Atombombe), sagte
einmal: ""Wenn Sie mir konkret sagen, was genau eine Maschine nicht kann,
kann ich immer noch eine bauen, die genau das leistet."" Von Neumann war
Mathematiker und genau deswegen übersieht er vielleicht die physischen Grenzen
seiner Behauptung. In der Theorie kann man alles Mögliche automatisieren, aber
in der Praxis braucht Arbeit Zeit und Energie. Eine Maschine, die rät, anstatt
zu logisch zu denken, ist nicht effizient. 

Die Realität
ist oft chaotisch, die Probleme, die wir Menschen lösen müssen, sind komplex
und vielseitig. Das gilt in der Softwareentwicklung und in vielen anderen
Berufen. Für viele Aufgaben wird es wohl noch eine ganze Zeit lang notwendig
sein, die Probleme selbst gründlich zu durchdringen, statt sie an Agenten zu
delegieren.
"
AI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-filmbranche-special-effects-newsletter-kuenstliche-intelligenz,KI in der Filmbranche: So generiert man keinen Oscar | ZEIT ONLINE,"Sie lesen den KI-Newsletter
""Natürlich intelligent"" vom 04. April 2024. Um den Newsletter jeden
zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Generative AI
goes to Hollywood. Zumindest wünschen sich
das wohl die Chefs von OpenAI, die Medienberichten zufolge gerade auf Werbetour
durch einige große Filmstudios unterwegs waren.

Künstliche Intelligenz spielt in der
Filmbranche schon lange eine Rolle, beispielsweise bei der Umsetzung von
Spezialeffekten wie der Verjüngung von Harrison Ford. In der stark
gewerkschaftlich organisierten Filmindustrie gibt es gegen viele der Einsatzmöglichkeiten
Widerstände. Die typische Sorge: KI könnte Menschen ersetzen.

Mit der neusten Spielart der
Technologie, Text-to-Video, sollen Filme nun wie von allein, basierend nur auf
Textbeschreibungen entstehen. OpenAI hat dazu mit den Democlips aus dem
Sora-Modell die bislang beeindruckendsten Ergebnisse gezeigt. Wer generierte
Clips wie den einer Frau sieht, die durch eine Innenstadt läuft und sich in
einer Pfütze spiegelt, mag davon ausgehen, dass es nicht mehr allzu lange
dauern wird, bis wir die ersten von KI erzeugten Videos im Kino sehen.

Tatsächlich dürfte diese Erwartung
aber etwas voreilig sein. Denn die Limitierungen, die Sora noch hat, sind nicht
die Kleinigkeiten, für die man sie auf den ersten Blick halten kann.

Noch sehen wir in den Videos die für
generative KI typischen Inkonsistenzen. Objekte tauchen aus dem Nichts auf,
Menschen verschwinden oder haben zu viele Gliedmaßen. Derartige Fehler werden
sich durch besseres Training schnell reduzieren lassen, so wie es bei Bild-KI
in den letzten Jahren geschehen ist.

Videos haben dabei sogar noch einen
Vorteil: Was ein konsistentes Verhalten eines bewegten Objekts ist, kann man
einem Computer in der Theorie viel besser beibringen als Faktentreue in einem
Text. Denn die Grundlage ist Physik, und für deren Simulation gibt es schon
seit Jahren Programmbibliotheken aus der Videospielindustrie.

Trotzdem muss viel zusammenkommen, um
schöne, realistische und vor allem in der Praxis nutzbare KI-Videos zu
erzeugen. Die Fülle an möglichen Settings, Bewegungen und Texturen bedeutet
eine enorme Herausforderung für die Ingenieurinnen, die den Systemen beibringen
sollen, Videos nach unseren Erwartungen zu erzeugen.

Effizient lernen bedeutet, sich auf
die wichtigsten Aspekte zu fokussieren. Unsere menschliche Wahrnehmung kann das
gut: Das Gehirn filtert ständig heraus, was es nicht braucht. Aber wie setzt
man das in Maschinen um?

Ein physikalisch realistisches Video
von einer Verfolgungsjagd bringt Hollywood wenig, wenn beim Training die Farben
oder Texturen dafür vernachlässigt werden mussten. Aber ein Modell, das alle
Aspekte gleichzeitig verarbeitet, ist mathematisch anspruchsvoll und vor allem
furchtbar energieaufwendig. Schon die jetzigen Modelle sind riesig und brauchen
ein Vielfaches mehr an Strom und Zeit als ihre Text- oder Bildkollegen. Je länger das
Video sein soll, desto komplizierter wird es. Sora kann Videos erzeugen, die
maximal eine Minute dauern, und ist möglicherweise auch deshalb nicht
öffentlich zugänglich, weil es zu viel Rechenleistung bräuchte, wenn Millionen
Menschen damit herumspielen würden.

Und dann wären
da noch die Details, auf die gerade bei teuren Film- und Serienproduktionen
penibel geachtet wird. Generierte Videos lassen sich, genau wie generierte
Texte und Bilder, nur begrenzt steuern.

Ist der Blick
des Mannes in einer Szene verliebt genug? Greift er genau im richtigen Moment
nach der Hand seines Partners? Sieht sein Bart ein bisschen blonder aus als in
der zuvor generierten Einstellung? Wenn einzelne Dinge nicht stimmen, muss man
noch mal generieren und hoffen – oder man bearbeitet es händisch so lange nach,
bis es passt. Doch wieder Menschenarbeit.
"
AI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/openai-newsletter-kuenstliche-intelligenz,OpenAI: Liebe KI-Utopisten: Sie müssen jetzt ganz stark sein | ZEIT ONLINE,"Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 30. November 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Innerhalb von wenigen Tagen wurde OpenAI-Chef Sam Altman
erst gefeuert – und dann auf seinen alten Posten zurückgeholt. Der Machtkampf
an der Spitze von OpenAI kam unerwartet, schnell und heftig. Und jetzt kochen
Gerüchte hoch, was dieses ganze Drama eigentlich verursacht hat.

Eine Lieblingstheorie auch in der Medienberichterstattung
ist, dass ein bahnbrechender Durchbruch in der KI-Forschung der Auslöser
gewesen sein soll. Die Nachrichtenagentur Reuters und die Plattform The
Information berichteten unter Berufung auf anonyme interne Quellen, OpenAI habe
ein Modell namens Q* entwickelt, das Mathematikaufgaben auf Grundschulniveau
lösen könne – und möglicherweise ""die Menschheit bedrohen"". Insider
sollen in der Technologie einen entscheidenden Schritt hin zur menschenüberflügelnden
Superintelligenz sehen.

Ob da was dran ist, ist schwer zu sagen. OpenAI hat schon
oft gezeigt, dass Mysterien gutes Marketing sind: Schon vor vier Jahren unkte
man dort, man wolle nur Teile seines Modells GPT-2 öffentlich machen – aus
""Sicherheitsgründen"". Die Firma schweigt nun zu allen Spekulationen
um Q*. Aber Sam Altman dürfte es ganz recht sein, dass in den Medien wieder
mehr über die vermeintliche Macht einer gerüchteumwobenen
Möglicherweise-Super-KI gesprochen wird statt beispielsweise über
zwischenmenschliche Verhakungen im Management von OpenAI.

""Könnte die Menschheit bedrohen"" ist in der Welt von Sam Altman und Co nämlich immer nur einen Schritt entfernt von einer anderen
Vision: Wenn es uns gelingt, dass uns die Super-KI nicht vorher umbringt, soll
sie uns in ein Zeitalter des ungekannten ökonomischen Wohlstands führen. Die
Wirtschaft würde komplett von ihr transformiert. Niemand müsste mehr arbeiten,
allen ginge es gut. Immerhin ist die Frage nach dem exponentiellen
Wirtschaftswachstum dank KI greifbarer und überprüfbarer als die, wann der Durchbruch
kommt, der KI übermenschliche, gefährliche Fähigkeiten verleiht.

Arjun Ramani und Zhengdong Wang, ein Wirtschaftsjournalist
und ein KI-Forscher, haben dazu im Sommer aus vielen Quellen die besten
Anhaltspunkte zusammengetragen. Und sie kommen zu dem Schluss, es werde
""wirklich, wirklich schwierig"", tatsächlich transformative KI zu
erreichen.

Sie entlarven einen Denkfehler in der Hoffnung auf eine
vollautomatisierte Luxuswelt: Selbst exponentielles Wachstum in einem
Teilbereich weitet sich nicht zwangsläufig auf andere Bereiche aus. Im
Gegenteil: Oft begrenzt der Sektor, der am langsamsten wächst, die
Produktivitätssteigerungen insgesamt.

Unsere Wirtschaft beruht zum größten Teil noch auf
physischen Prozessen, die selbst mit tollen KI-Lösungen nur schwer verändert
werden können. Selbst wenn eine Superintelligenz die Architektur
revolutioniert: Irgendjemand muss noch immer für die Mauern die Steine
verspachteln. Denn die Robotik ist noch lange nicht so weit, dass sie Menschen
in allen Konstruktionsaufgaben ersetzen kann.

Maschinen haben uns schon jetzt in vielen Bereichen
überflügelt. Und der technische Fortschritt hat unsere Welt in den letzten
Jahren fundamental verändert, teils verbessert. Auch wirtschaftlich.

Zur Lösung unserer Probleme mangelt es uns oft aber nicht an
Wissen oder Intelligenz. Sondern an Kooperationsfähigkeit.

Beispiel humanitäre Hilfe: Deren Logistik ist ein Problem,
das mathematisch gut lösbar wäre. Schon heute. Menschen in Krisenregionen
hungern trotzdem noch immer. Und wir wissen genau, dass wir Emissionen von
Treibhausgasen verringern müssen, um die Klimakrise zu verlangsamen. Aber wir
scheitern an der Umsetzung.

Die KI-Forschung wird weiter Fortschritte machen. Hilfreiche
und kritikwürdige. Aber keine noch so intelligente Technologie wird je der
Killswitch für alle Probleme der Menschheit sein. Wer alles auf diese Hoffnung
setzt, wird enttäuscht werden. Und wer nur KI als Hoffnung verkauft, wird
enttäuschen.
"
AI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/autonome-agenten-newsletter-kuenstliche-intelligenz,"Autonome Agenten: KI-Agenten, die auf sich selbst starren | ZEIT ONLINE","Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 19. Oktober 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Vergangene Woche haben sich in San Francisco Menschen aus
aller Welt getroffen, um an einer der spannendsten Anwendungen von generativer
KI zu basteln, die es derzeit gibt: autonome Agenten.

Agenten sind (mehr oder weniger) selbstständig handelnde
KI-Systeme. Man muss, wie so oft, vorsichtig mit dem Begriff sein, denn
""Agenten"" klingt schon sehr nach Bewusstsein, und das haben sie
gewiss nicht. Aber mehr agency, also Handlungsfähigkeit, als ChatGPT haben sie
auf jeden Fall.

Bei dem Hackathon in San Francisco, zu dem das Start-up
AutoGPT eingeladen hatte, entwickelte ein Team zum Beispiel einen Prototyp, der
selbstständig die einzelnen Teile einer Reise recherchieren und planen können
soll. An solchen Beispielen lässt sich gut erklären, wie KI-Agenten arbeiten.

Sagen wir, ich will ein neues Fahrrad kaufen. Ich habe ein
definiertes Budget und möchte das am besten bewertete Rad, das für dieses
Budget auf dem Markt zu haben ist. Ich könnte mir dafür ein Programm schreiben,
das im Internet sucht, Bewertungen ausliest und Preise vergleicht. Aber alles,
was mir wichtig ist – zum Beispiel, dass die Rahmengröße zu meinen kurzen
Beinen passt –, müsste ich mühsam und exakt einprogrammieren. Alternativ, und
so ist es in meinem echten Leben gelaufen, kann ich meinen Freund fragen. Der
denkt mit, macht sich einen Plan, informiert sich und kommt mit Rückfragen
wieder auf mich zu.

Autonome Agenten kombinieren – so die Theorie – das Beste
aus beiden Welten. Wie ein Computerprogramm sind sie schnell, rechnen gut,
können viele Daten speichern und werden nicht müde. Aber wie mein Freund sind
sie nicht abhängig von einer detaillierten Schritt-für-Schritt-Anleitung. Sie
denken mit. Und sie können das, weil wir ihnen Zugang zur besten
Mitdenk-Technologie geben, die wir Menschen bislang entwickelt haben: unserer
Sprache.

Agentensysteme erweitern Sprachmodelle wie GPT-4 um die
Ebene der Handlungsfähigkeit. Die bekanntesten Programme heißen AutoGPT oder
BabyAGI. Man gibt ihnen ein Ziel und Zugang zu verschiedenen Werkzeugen:
Suchmaschinen, eine Möglichkeit, zu programmieren, und einen Ordner, in dem sie
Dateien abspeichern und auslesen können.

Verknüpft wird alles durch Zugriff auf ein großes
Sprachmodell. Das ist spannend, denn es zeigt, was neu ist in der aktuellen
KI-Welle. Es ist schon lange möglich, verschiedene Computerprogramme zu
verketten. Aber der Zugang zu Sprache ermöglicht den Maschinen einen neuen
Arbeitsmodus, der menschlichem Denken tatsächlich näherkommt als alles, was
Computer bisher getan haben.

Das System unterteilt einen nicht exakt formulierten Auftrag
in Teilaufgaben, hinterfragt die eigene Herangehensweise kritisch und passt den
Plan an, je nachdem wie die ersten Zwischenergebnisse aussehen. Die Ansätze für
die Lösungen des Problems, man könnte von ""Ideen"" sprechen, kommen
aus dem Wissensschatz, der in den Sprachmodellen verschlüsselt ist. Wer kurze
Beine hat, braucht ein kleines Fahrrad. Das muss man einem Sprachmodell nicht
mehr extra dazu sagen.

Das kann uns etwas darüber zeigen, wie wir selbst
funktionieren. Ist es Selbstreflexion, die uns erst handlungsfähig macht? Wie
viel von unserem Bewusstsein ist Sprache?

Ich möchte nicht falsch verstanden werden: Ich sehe aktuell
keine Anhaltspunkte, dass Agenten bald zu wirklich autonomer KI werden, die wir
aus Science-Fiction kennen. Eine Motivation, einen inneren Antrieb hat auch
AutoGPT nicht und die Handlungsfähigkeit ist noch weit davon entfernt, uns in
moralische Dilemmata oder in existenzielle Gefahr zu bringen.

Dass Agenten unsere Sprache als Vehikel benutzen, um zu
planen, sollte uns nicht dazu verleiten, ihnen Menschlichkeit zuzuschreiben.
Aber ich glaube, die Prinzipien der Selbstbeobachtung und des ständigen
Rückbesinnens auf Ziele und bisherige Schritte werden es KI-Systemen erst
ermöglichen, wirklich nützlich für uns zu sein.

Bis dahin ist es allerdings noch ein weiter Weg. Die
Technologie ist neu und voller Bugs. Die Programme machen oft Fehler oder
bleiben hängen. So geschehen zum Glück auch bei der skrupellosesten Anwendung:
ChaosGPT, der ein paar Scherzkekse unter anderem das Ziel gaben, ""die
Menschheit auszulöschen"". Die Bilanz blieb harmlos: Das System legte sich
einen Twitter-Account zu, setzte zwei Tweets ab und stellte einige
Google-Suchanfragen nach der Zar-Bombe an. Weiter kam es nicht.
"
AI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-hype-newsletter-kuenstliche-intelligenz,"KI-Hype: Vielleicht wird KI doch nicht so schnell schlauer, wie alle denken | ZEIT ONLINE","Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 02. November 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Es ist Quartalszahlen-Zeit und das heißt auch, Konzerne
erklären ihren Anlegern und der Öffentlichkeit, wie die Geschäfte laufen. Im
KI-Bereich waren die Erwartungen groß, immerhin redet alle Welt davon, dass
KI-Modelle wie GPT-4 mindestens das neue Internet sein könnten, wenn nicht
gleich die Neuerfindung des Rades. Die Firmen, die KI-Services anbieten oder
die dafür nötige Rechenleistung vermieten, die Amazons, die Microsofts, die
Alphabets, müssten doch eigentlich wachsen wie wild. Oder?

Tatsächlich hat sich das Wachstum der Cloud-Sparten von
Amazon und Alphabet verlangsamt. Einzig Microsoft gewinnt durch KI stärker als
erwartet; die Einnahmen durch Cloud-Services stiegen im Vergleich zum Vorjahr
um 24 Prozent. Das mehrere Milliarden Dollar schwere Investment in OpenAI
scheint sich auszuzahlen.

Trotzdem wirft das ruckelige Wachstum die Frage auf, wie es
mit dem KI-Hype weitergeht. In die begeisterten Stimmen mischt sich langsam
etwas Skepsis.

Microsoft-Gründer Bill Gates zum Beispiel sagte vor einigen
Tagen in einem Interview mit dem Handelsblatt, dass er ein Plateau erwarte:
GPT-5 könnte nur ein bisschen besser werden als GPT-4. Aus dem Interview geht
nicht hervor, worauf er diese Annahme stützt, aber er könnte auf etwas
anspielen, das diesen Sommer bereits von einigen KI-Expertinnen und -experten
diskutiert wurde: Der KI gehen die Daten aus.

Um eine leistungsstarke KI zu bauen, braucht man grob gesagt
zwei Dinge: Daten und Rechenleistung. Daten sind der Rohstoff und die
Rechenleistung muss aufgebracht werden, um daraus ein Netzwerk zu schmieden.

Einerseits sind die leistungsstarken Chips knapp, mit denen
die Rechenleistung für KI aufgebracht wird. Aber das ist ein Problem, das
lösbar scheint. Neue Technologien starten meistens mit großem Energieaufwand
und mit der Zeit finden sich Wege, sie effizienter zu machen. Viele
KI-Anwendungen werden in nächster Zeit günstiger im Betrieb werden.

Auf der Seite der Daten sieht es anders aus, vor allem, was
Texte angeht. Es gibt einfach nicht unbegrenzt verfügbare und vor allem
qualitativ hochwertige Texte, mit denen KIs trainiert werden können. Die
Menschheit kann gar nicht so schnell schreiben, wie KIs lesen können.

Unternehmer wie OpenAI-Chef Sam Altman sagen, sie machen
sich deshalb keine Sorgen. Tatsächlich suchen Forschende nach Wegen, bestehende
Datensätze zu verbessern. Sie können auch KI nutzen, um beispielsweise Texte
aus Videos zu transkribieren oder (in begrenztem Maße) neue Trainingsdaten zu
generieren. Und Unternehmen werden Wege suchen, noch mehr Daten aus unseren
Alltagskonversationen und Privatnachrichten abzugrasen. Aber egal, wie viel sie
aus den letzten Ecken herauskehren: Die einfach verfügbaren riesigen
Datenmengen (früher sagte man: Bibliotheken) aus den vergangenen Jahrhunderten
sind schon verbraucht. Kleine Verbesserungen bedeuten nun großen Mehraufwand.

Daher kommt die Sorge vor einem Plateau: Die beeindruckenden
Fortschritte, etwa von GPT-3 zu GPT-4, entstanden vor allem durch mehr Daten.
Das war es, was OpenAI so erfolgreich gemacht hat und in dessen Folge Microsoft
nun besser dasteht als die Konkurrenz. Das Unternehmen setzte schon früh alles
auf die Karte ""größer ist besser"".

Auch deshalb war die Annahme in den vergangenen Monaten: Wer
nicht mehrere Milliarden Dollar in KI-Entwicklungen stecken kann, muss gar
nicht erst anfangen. Im Kampf um Marktanteile nehmen Amazon, Microsoft und
Alphabet sogar in Kauf, erst mal ordentlich Geld zu verlieren. Laut einem
Bericht des Wall Street Journal subventioniert Microsofts Coding-Plattform
GitHub seinen KI-Assistenten jeden Monat mit durchschnittlich 20 Dollar pro
Nutzendem.

Die Kosten für den Betrieb und auch für das Training von KI
werden günstiger werden. Und wenn gleichzeitig die KI-Fähigkeiten, wie Bill
Gates vermutet, ein Plateau erreichen, sind das vor allem gute Nachrichten für
kleine Unternehmen und Start-ups. Der Vorsprung, den sich die Tech-Giganten mit
ihrem vielen Geld erkauft haben, ist nicht groß. Das OpenAI, ja sogar das
Microsoft der 2030er-Jahre könnte eine Firma sein, deren Name wir heute noch
gar nicht kennen.
"
AI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/klage-openai-elon-musk-newsletter-kuenstliche-intelligenz,Klage gegen OpenAI: Warum OpenAI nie offen sein konnte | ZEIT ONLINE,"Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 07. März 2024. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Die KI-Prominenz zeigt sich aktuell
mal wieder von ihrer überraschenden Seite: Elon Musk hat eine Klage gegen
OpenAI eingereicht, in der er die mangelnde Transparenz und die
Profitinteressen der Firma anprangert. Der zweitreichste Mensch der Welt
spricht sich damit also plötzlich gegen das Geldverdienen aus. Und die
Bemühungen von OpenAI, sich offen zu zeigen, bringen E-Mails ans Licht, die
Musks Vorwürfen widersprechen – aber auch einen alten Mythos der Firma
ankratzen.

Was ist passiert? OpenAI war 2015 als
Non-Profit-Organisation gegründet worden. Musk und die anderen Gründer waren
sich damals noch einig, dass man die Entwicklung von künstlicher Intelligenz
nicht Großunternehmen wie Google überlassen sollte. Das open im Namen stand für
den Grundsatz, KI möglichst transparent zu entwickeln, zum Wohle der
Öffentlichkeit. Musk überwarf sich einige Zeit später mit den anderen Gründern
und setzte mit xAI seine eigene Konkurrenz-KI-Klitsche auf.

Rechtlich gesehen ist der Fall
ziemlich dünn, sagen Expertinnen. Viele der Absprachen wurden vertraglich nicht
eindeutig festgehalten. Noch dazu hat OpenAI am Dienstag E-Mails
veröffentlicht, aus denen hervorgeht, dass die Gründer, Musk inklusive, schon früh
sowohl Profitinteressen als auch das Zurückhalten von Informationen vor der
Öffentlichkeit durchaus als Teil ihres Plans betrachteten.

Hier zerfällt also gerade vor unseren
Augen eine Erzählung, an der OpenAI in den letzten Jahren sehr fleißig
gebastelt hat. Die lautet grob vereinfacht ungefähr so: ""Wir bauen diese
mächtigen KI-Systeme für euch alle. Weil nur wir das korrekt und transparent
machen. Wenn jemand anderes sie baut, wird es gefährlich.""

Ich möchte gar nicht anzweifeln, dass
OpenAI ­– und auch Musk – wirklich glauben, dass sie als Entwickler einer
Superintelligenz zu Rettern der Welt werden können. Aber sie haben sich von
Anfang an in die eigene Tasche gelogen mit der Annahme, OpenAI könne offen
sein.

Denn: Eine herbeifantasierte Über-KI
dürfte natürlich nur in einem geschlossenen System entwickelt werden – wenn man
überzeugt ist, dass das Wissen über diese potenziell gefährlichen Systeme nicht
in falsche Hände fallen darf. Aber wer das Wohl der Menschheit im Blick hat,
braucht überhaupt keine Über-KI. Die Über-KI braucht nur der, der Profite
generieren will.

Auch in Musks OpenAI-Klage ist der
Angelpunkt die Entwicklung von Artificial General Intelligence (AGI), also von
KI-Systemen, die menschenähnliche oder übermenschliche Leistungen auf einem
breiten Spektrum von Tätigkeiten erbringen können soll. Als Beleg dafür, dass
OpenAI schon eine AGI gebaut haben soll, wird unter anderem ein
Forschungsbericht von Microsoft zum OpenAI-Modell GPT-4 zitiert.

Dieser Bericht hat aber seine
Schwächen: Die Autoren des Papers verwenden bewusst Testverfahren abseits der
im Machine-Learning etablierten Standards. Sie geben teils subjektive Eindrücke
wieder. Und sie arbeiteten mit einer frühen Version des Modells, die bis heute
nicht öffentlich zugänglich ist. Darum können die Ergebnisse der Studie auch
nicht unter gleichen Bedingungen von Dritten repliziert werden.

Kein Einzelfall: Auch OpenAI selbst
und Google veröffentlichen immer wieder Forschungsberichte, die den Namen nicht
verdient haben. Denn die KI-Unternehmen halten Datengrundlagen und wichtige
Spezifikationen ihrer Modelle zurück. Sie geben lange Autorinnenlisten an und
spicken die Texte mit Fachbegriffen, um darüber hinwegzutäuschen, dass es
eigentlich vor allem PR-Mitteilungen sind.

Dass es kein Peer-Review gibt und
geben kann, wird dann damit begründet, dass eine vollständige Veröffentlichung
aller Details zu den eigenen Modellen zu gefährlich wäre. So wird es Fachleuten
unmöglich gemacht, die von den Unternehmen getroffenen Aussagen kritisch zu
analysieren. Inklusive der Behauptung, KI sei überhaupt übermenschlich oder
gefährlich. Das ist ein absurder Zirkelschluss. Aber der unwissenschaftliche
Hype um die AGI ist gute Werbung, um Investorengelder einzusammeln.

Sowohl für schnelle als auch für
sichere KI-Forschung wirklich von Vorteil wäre Offenheit. Die Offenheit, die
sich die OpenAI-Gründer in den Namen geschrieben haben. Obwohl sie sie nie
erfüllen konnten oder wollten.
"
AI,Zeit,2024-04-07,https://www.zeit.de/2024/15/kuenstliche-intelligenz-anguilla-internet-domain-ai,Künstliche Intelligenz: Endung gut | ZEIT ONLINE,
Artificial Intelligence,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-filmbranche-special-effects-newsletter-kuenstliche-intelligenz,KI in der Filmbranche: So generiert man keinen Oscar | ZEIT ONLINE,"Sie lesen den KI-Newsletter
""Natürlich intelligent"" vom 04. April 2024. Um den Newsletter jeden
zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Generative AI
goes to Hollywood. Zumindest wünschen sich
das wohl die Chefs von OpenAI, die Medienberichten zufolge gerade auf Werbetour
durch einige große Filmstudios unterwegs waren.

Künstliche Intelligenz spielt in der
Filmbranche schon lange eine Rolle, beispielsweise bei der Umsetzung von
Spezialeffekten wie der Verjüngung von Harrison Ford. In der stark
gewerkschaftlich organisierten Filmindustrie gibt es gegen viele der Einsatzmöglichkeiten
Widerstände. Die typische Sorge: KI könnte Menschen ersetzen.

Mit der neusten Spielart der
Technologie, Text-to-Video, sollen Filme nun wie von allein, basierend nur auf
Textbeschreibungen entstehen. OpenAI hat dazu mit den Democlips aus dem
Sora-Modell die bislang beeindruckendsten Ergebnisse gezeigt. Wer generierte
Clips wie den einer Frau sieht, die durch eine Innenstadt läuft und sich in
einer Pfütze spiegelt, mag davon ausgehen, dass es nicht mehr allzu lange
dauern wird, bis wir die ersten von KI erzeugten Videos im Kino sehen.

Tatsächlich dürfte diese Erwartung
aber etwas voreilig sein. Denn die Limitierungen, die Sora noch hat, sind nicht
die Kleinigkeiten, für die man sie auf den ersten Blick halten kann.

Noch sehen wir in den Videos die für
generative KI typischen Inkonsistenzen. Objekte tauchen aus dem Nichts auf,
Menschen verschwinden oder haben zu viele Gliedmaßen. Derartige Fehler werden
sich durch besseres Training schnell reduzieren lassen, so wie es bei Bild-KI
in den letzten Jahren geschehen ist.

Videos haben dabei sogar noch einen
Vorteil: Was ein konsistentes Verhalten eines bewegten Objekts ist, kann man
einem Computer in der Theorie viel besser beibringen als Faktentreue in einem
Text. Denn die Grundlage ist Physik, und für deren Simulation gibt es schon
seit Jahren Programmbibliotheken aus der Videospielindustrie.

Trotzdem muss viel zusammenkommen, um
schöne, realistische und vor allem in der Praxis nutzbare KI-Videos zu
erzeugen. Die Fülle an möglichen Settings, Bewegungen und Texturen bedeutet
eine enorme Herausforderung für die Ingenieurinnen, die den Systemen beibringen
sollen, Videos nach unseren Erwartungen zu erzeugen.

Effizient lernen bedeutet, sich auf
die wichtigsten Aspekte zu fokussieren. Unsere menschliche Wahrnehmung kann das
gut: Das Gehirn filtert ständig heraus, was es nicht braucht. Aber wie setzt
man das in Maschinen um?

Ein physikalisch realistisches Video
von einer Verfolgungsjagd bringt Hollywood wenig, wenn beim Training die Farben
oder Texturen dafür vernachlässigt werden mussten. Aber ein Modell, das alle
Aspekte gleichzeitig verarbeitet, ist mathematisch anspruchsvoll und vor allem
furchtbar energieaufwendig. Schon die jetzigen Modelle sind riesig und brauchen
ein Vielfaches mehr an Strom und Zeit als ihre Text- oder Bildkollegen. Je länger das
Video sein soll, desto komplizierter wird es. Sora kann Videos erzeugen, die
maximal eine Minute dauern, und ist möglicherweise auch deshalb nicht
öffentlich zugänglich, weil es zu viel Rechenleistung bräuchte, wenn Millionen
Menschen damit herumspielen würden.

Und dann wären
da noch die Details, auf die gerade bei teuren Film- und Serienproduktionen
penibel geachtet wird. Generierte Videos lassen sich, genau wie generierte
Texte und Bilder, nur begrenzt steuern.

Ist der Blick
des Mannes in einer Szene verliebt genug? Greift er genau im richtigen Moment
nach der Hand seines Partners? Sieht sein Bart ein bisschen blonder aus als in
der zuvor generierten Einstellung? Wenn einzelne Dinge nicht stimmen, muss man
noch mal generieren und hoffen – oder man bearbeitet es händisch so lange nach,
bis es passt. Doch wieder Menschenarbeit.
"
Artificial Intelligence,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/openai-newsletter-kuenstliche-intelligenz,OpenAI: Liebe KI-Utopisten: Sie müssen jetzt ganz stark sein | ZEIT ONLINE,"Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 30. November 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Innerhalb von wenigen Tagen wurde OpenAI-Chef Sam Altman
erst gefeuert – und dann auf seinen alten Posten zurückgeholt. Der Machtkampf
an der Spitze von OpenAI kam unerwartet, schnell und heftig. Und jetzt kochen
Gerüchte hoch, was dieses ganze Drama eigentlich verursacht hat.

Eine Lieblingstheorie auch in der Medienberichterstattung
ist, dass ein bahnbrechender Durchbruch in der KI-Forschung der Auslöser
gewesen sein soll. Die Nachrichtenagentur Reuters und die Plattform The
Information berichteten unter Berufung auf anonyme interne Quellen, OpenAI habe
ein Modell namens Q* entwickelt, das Mathematikaufgaben auf Grundschulniveau
lösen könne – und möglicherweise ""die Menschheit bedrohen"". Insider
sollen in der Technologie einen entscheidenden Schritt hin zur menschenüberflügelnden
Superintelligenz sehen.

Ob da was dran ist, ist schwer zu sagen. OpenAI hat schon
oft gezeigt, dass Mysterien gutes Marketing sind: Schon vor vier Jahren unkte
man dort, man wolle nur Teile seines Modells GPT-2 öffentlich machen – aus
""Sicherheitsgründen"". Die Firma schweigt nun zu allen Spekulationen
um Q*. Aber Sam Altman dürfte es ganz recht sein, dass in den Medien wieder
mehr über die vermeintliche Macht einer gerüchteumwobenen
Möglicherweise-Super-KI gesprochen wird statt beispielsweise über
zwischenmenschliche Verhakungen im Management von OpenAI.

""Könnte die Menschheit bedrohen"" ist in der Welt von Sam Altman und Co nämlich immer nur einen Schritt entfernt von einer anderen
Vision: Wenn es uns gelingt, dass uns die Super-KI nicht vorher umbringt, soll
sie uns in ein Zeitalter des ungekannten ökonomischen Wohlstands führen. Die
Wirtschaft würde komplett von ihr transformiert. Niemand müsste mehr arbeiten,
allen ginge es gut. Immerhin ist die Frage nach dem exponentiellen
Wirtschaftswachstum dank KI greifbarer und überprüfbarer als die, wann der Durchbruch
kommt, der KI übermenschliche, gefährliche Fähigkeiten verleiht.

Arjun Ramani und Zhengdong Wang, ein Wirtschaftsjournalist
und ein KI-Forscher, haben dazu im Sommer aus vielen Quellen die besten
Anhaltspunkte zusammengetragen. Und sie kommen zu dem Schluss, es werde
""wirklich, wirklich schwierig"", tatsächlich transformative KI zu
erreichen.

Sie entlarven einen Denkfehler in der Hoffnung auf eine
vollautomatisierte Luxuswelt: Selbst exponentielles Wachstum in einem
Teilbereich weitet sich nicht zwangsläufig auf andere Bereiche aus. Im
Gegenteil: Oft begrenzt der Sektor, der am langsamsten wächst, die
Produktivitätssteigerungen insgesamt.

Unsere Wirtschaft beruht zum größten Teil noch auf
physischen Prozessen, die selbst mit tollen KI-Lösungen nur schwer verändert
werden können. Selbst wenn eine Superintelligenz die Architektur
revolutioniert: Irgendjemand muss noch immer für die Mauern die Steine
verspachteln. Denn die Robotik ist noch lange nicht so weit, dass sie Menschen
in allen Konstruktionsaufgaben ersetzen kann.

Maschinen haben uns schon jetzt in vielen Bereichen
überflügelt. Und der technische Fortschritt hat unsere Welt in den letzten
Jahren fundamental verändert, teils verbessert. Auch wirtschaftlich.

Zur Lösung unserer Probleme mangelt es uns oft aber nicht an
Wissen oder Intelligenz. Sondern an Kooperationsfähigkeit.

Beispiel humanitäre Hilfe: Deren Logistik ist ein Problem,
das mathematisch gut lösbar wäre. Schon heute. Menschen in Krisenregionen
hungern trotzdem noch immer. Und wir wissen genau, dass wir Emissionen von
Treibhausgasen verringern müssen, um die Klimakrise zu verlangsamen. Aber wir
scheitern an der Umsetzung.

Die KI-Forschung wird weiter Fortschritte machen. Hilfreiche
und kritikwürdige. Aber keine noch so intelligente Technologie wird je der
Killswitch für alle Probleme der Menschheit sein. Wer alles auf diese Hoffnung
setzt, wird enttäuscht werden. Und wer nur KI als Hoffnung verkauft, wird
enttäuschen.
"
Artificial Intelligence,Zeit,2024-04-08,https://www.zeit.de/mobilitaet/2024-04/elon-musk-tesla-robotaxi-august-aktionaere/seite-2,Elon Musk: Bis zum Marktstart des Tesla Robotaxis dürften Jahre vergehen | ZEIT ONLINE,"Denn Elon Musk hat schon viele Dinge versprochen, die er
nicht oder erst Jahre später halten konnte. Manche sprechen gar von einer
eigenen Zeitrechnung, der Elon Time, für die es sogar Rechner gibt. Der Cybertruck
etwa kam vier Jahre nach seiner Vorstellung und zwei Jahre später als
angekündigt auf den Markt.

Dasselbe könnte dem Model 2 passieren, das ab 25.000 Dollar
zu haben sein soll. Schon im ""Masterplan"" von 2006 kündigte
Musk an, erst teure Modelle und irgendwann dann ein bezahlbares Familienauto
zu bauen. Zuletzt hieß es von Tesla, dass es ab der zweiten Jahreshälfte 2025 produziert
werden soll.

Das als Robotaxi angekündigte Fahrzeug soll im August zunächst
nur vorgestellt werden. Der Verkauf wird also ebenfalls wohl noch Jahre auf sich warten lassen. Zumal in Deutschland, wo strengere Vorgaben für das autonome Fahren gelten als in den USA.
Und es ist völlig unklar, wie autonom Teslas Robotaxi wirklich fahren wird. Musk hatte zwar schon
2016 gesagt, dass Teslas die Ausstattung hätten, um vollständig
eigenverantwortlich zu fahren. Aber das autonome Fahren hat sich nicht nur für Tesla als eine
wesentlich größere Herausforderung entpuppt, als man noch vor ein paar Jahren
dachte. 

Zwar können heute Fahrzeuge selbstständig lenken, bremsen,
Spuren wechseln, überholen, sich
einen Parkplatz suchen. Die große Schwierigkeit ist, sie auf jede mögliche
Situation im Straßenverkehr vorzubereiten. Ein Kind, das auf die Straße läuft,
erkennt ein autonomes Auto und bremst oft schneller als ein Mensch. Ein Kind, das als Alien
verkleidet auf die Straße läuft, ist schon viel schwieriger zu erkennen. Die Bandbreite
der möglichen Szenarien ist nahezu unendlich.

Außerdem gehen Hersteller ein großes rechtliches Risiko ein,
wenn sie die Verantwortung für Unfälle den Kundinnen und Kunden abnehmen. Tesla
steht bereits wegen Unfällen, in die Teslas teilautomatisiertes Fahrsystem verwickelt
war, vor Gericht. Das Full Self Driving genannte System wird immer wieder dafür
kritisiert, dass es den Fahrerinnen und Fahrern das
Gefühl gibt, nicht mehr verantwortlich zu sein.

Nachdem Robotaxis der General-Motors-Tochter Cruise bei mehreren
Unfällen in San Francisco eine Rolle gespielt und Einsätze von
Rettungskräften gestört hatten, hat das Unternehmen Ende 2023 den
Robotaxi-Betrieb in den gesamten Vereinigten Staaten wieder eingestellt. Von einem
Auto ganz ohne Lenkrad und Pedale nimmt
Cruise jetzt Abstand. Apple arbeitete zehn Jahre lang an einem
selbstfahrenden Auto, Ende
Februar wurde das Projekt gestoppt. Allein Googles Schwesterfirma Waymo konnte zuletzt Erfolge vorweisen.

Wenn Musk angesichts dieser Entwicklungen ein Robotaxi ankündigt, ist
das mindestens kühn. Zumal Tesla beim autonomen Fahren teils auf sehr simple Technik
setzt. Um die Umgebung zu erfassen, würden Kameras ausreichen, glaubt Musk. Mit dieser Haltung ist er in der Automobilwelt ziemlich
allein. Andere Hersteller setzen auf Kombinationen von Kameras, Radar und Laser.

Nun interessieren sich Kunden und die Börse meist wenig für
technische Feinheiten. Die absehbare Vorstellung eines Robotaxis könnte die
Aktionäre kurzfristig beruhigen. Vielleicht so lange, bis Tesla endlich das
Model 2 verkaufen kann. Wenn jedoch der Verkaufsstart mal wieder nicht gehalten
wird, muss Musk bald die nächste Ankündigung folgen lassen.
"
Artificial Intelligence,Zeit,2024-04-08,https://www.zeit.de/mobilitaet/2024-04/elon-musk-tesla-robotaxi-august-aktionaere,Elon Musk: Tesla kämpft um seinen Ruf | ZEIT ONLINE,"Die Ankündigung war typisch für Elon Musk: ""Robotaxi Enthüllung
am 8/8"", schrieb
Musk auf seiner Plattform X am Freitagnachmittag US-Ostküstenzeit. Ein
autonom fahrendes Auto also – aber keine Erklärung, keine Informationen darüber,
warum ausgerechnet der 8. August, nichts. Trotzdem: 93.000 Likes, fast 12.000
Kommentare. Und ein Nutzer schreibt: ""WELCHES JAHR ELON?""

Daran lässt sich gut ablesen, was derzeit
los ist bei Tesla: Seit dem Jahreswechsel ist die Aktie des E-Autoherstellers um
ein Drittel gefallen, sie befindet sich mittlerweile auf dem Niveau von vor drei
Jahren. Die Verkaufszahlen waren im ersten Quartal dieses Jahres schlecht – rund
40.000 ausgelieferte Autos weniger als vergangenes Jahr. Das ist der erste
Rückgang der Verkaufszahlen seit Beginn der Corona-Pandemie.

Das liegt unter anderem daran, dass die Konkurrenz wächst.
Andere Unternehmen können mittlerweile ebenfalls ordentliche Elektroautos bauen, die
chinesischen sogar noch günstiger als Tesla. Zuletzt kündigte Smartphonehersteller
Xiaomi an, ins E-Auto-Geschäft einzusteigen. Zudem erlebt die Elektroautobranche
allgemein gerade einen Dämpfer – oder eigentlich andersherum: Der Verbrenner
erlebt ein kleines Comeback. Dazu kommt die ohnehin weltweit geringere
Nachfrage aufgrund der schlechten Wirtschaftslage.

Tesla war einst Aufrührer und Anführer der noch jungen Branche – mit keinem geringeren Ziel, als der größte Autohersteller der Welt zu werden. Doch viel Strahlkraft ist verloren gegangen. Es
fehlen schon länger wirkliche Innovationen. Ende 2023 hat Tesla zwar den Cybertruck, einen
martialischen E-Pick-up, auf den Markt gebracht, doch der richtet sich an
eine sehr kleine Zielgruppe. Die Überarbeitung des weltweit meistverkauften
Model Y wurde immer wieder verschoben.

Derweil leidet
in den USA das Image des Konzerns offenbar unter Elon Musk und seinen politischen Beiträgen auf seiner Plattform X. Das Bild des Superman-Investors
hält nicht stand, dadurch büßen auch seine Marken ein. Musk selbst wiederum
sucht die Gründe für die Flaute im Kleinen – in Grünheide bei Berlin zum Beispiel:
Dort stocken die Pläne, die Fabrik zu erweitern, der Brandanschlag auf die
Stromversorgung kostete Tesla nach eigenen Angaben mehrere Hundert Millionen
Euro, zudem sorgte die Betriebsratswahl
vergangene Woche für Wirbel. Gegenwind, wo sonst nur Rückenwind wehte.

Am Freitag nun führte ein
Bericht über das vermeintliche Ende des geplanten Billigautos Model 2 dazu,
dass der Aktienkurs um fast vier Prozent einbrach. Musk dementierte – und schob
die Ankündigung zum Robotaxi hinterher. 

Vor diesem Hintergrund liegt auf der Hand, was Musk damit
bezwecken möchte: Hoffnung wecken, vor allem bei Aktionärinnen und Aktionären. Dass
da etwas Neues kommt, dass Tesla einen neuen Markt erschließt, dass die Marke weiter
innovativ bleibt. 

Kurzfristig scheint das funktioniert zu haben. Nachbörslich
stieg der Aktienkurs bereits wieder in selbem Ausmaß, in dem er vorher gefallen
war. Nur: Die neu geschürte Hoffnung könnte schnell der Realität weichen.
"
Artificial Intelligence,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/klage-openai-elon-musk-newsletter-kuenstliche-intelligenz,Klage gegen OpenAI: Warum OpenAI nie offen sein konnte | ZEIT ONLINE,"Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 07. März 2024. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Die KI-Prominenz zeigt sich aktuell
mal wieder von ihrer überraschenden Seite: Elon Musk hat eine Klage gegen
OpenAI eingereicht, in der er die mangelnde Transparenz und die
Profitinteressen der Firma anprangert. Der zweitreichste Mensch der Welt
spricht sich damit also plötzlich gegen das Geldverdienen aus. Und die
Bemühungen von OpenAI, sich offen zu zeigen, bringen E-Mails ans Licht, die
Musks Vorwürfen widersprechen – aber auch einen alten Mythos der Firma
ankratzen.

Was ist passiert? OpenAI war 2015 als
Non-Profit-Organisation gegründet worden. Musk und die anderen Gründer waren
sich damals noch einig, dass man die Entwicklung von künstlicher Intelligenz
nicht Großunternehmen wie Google überlassen sollte. Das open im Namen stand für
den Grundsatz, KI möglichst transparent zu entwickeln, zum Wohle der
Öffentlichkeit. Musk überwarf sich einige Zeit später mit den anderen Gründern
und setzte mit xAI seine eigene Konkurrenz-KI-Klitsche auf.

Rechtlich gesehen ist der Fall
ziemlich dünn, sagen Expertinnen. Viele der Absprachen wurden vertraglich nicht
eindeutig festgehalten. Noch dazu hat OpenAI am Dienstag E-Mails
veröffentlicht, aus denen hervorgeht, dass die Gründer, Musk inklusive, schon früh
sowohl Profitinteressen als auch das Zurückhalten von Informationen vor der
Öffentlichkeit durchaus als Teil ihres Plans betrachteten.

Hier zerfällt also gerade vor unseren
Augen eine Erzählung, an der OpenAI in den letzten Jahren sehr fleißig
gebastelt hat. Die lautet grob vereinfacht ungefähr so: ""Wir bauen diese
mächtigen KI-Systeme für euch alle. Weil nur wir das korrekt und transparent
machen. Wenn jemand anderes sie baut, wird es gefährlich.""

Ich möchte gar nicht anzweifeln, dass
OpenAI ­– und auch Musk – wirklich glauben, dass sie als Entwickler einer
Superintelligenz zu Rettern der Welt werden können. Aber sie haben sich von
Anfang an in die eigene Tasche gelogen mit der Annahme, OpenAI könne offen
sein.

Denn: Eine herbeifantasierte Über-KI
dürfte natürlich nur in einem geschlossenen System entwickelt werden – wenn man
überzeugt ist, dass das Wissen über diese potenziell gefährlichen Systeme nicht
in falsche Hände fallen darf. Aber wer das Wohl der Menschheit im Blick hat,
braucht überhaupt keine Über-KI. Die Über-KI braucht nur der, der Profite
generieren will.

Auch in Musks OpenAI-Klage ist der
Angelpunkt die Entwicklung von Artificial General Intelligence (AGI), also von
KI-Systemen, die menschenähnliche oder übermenschliche Leistungen auf einem
breiten Spektrum von Tätigkeiten erbringen können soll. Als Beleg dafür, dass
OpenAI schon eine AGI gebaut haben soll, wird unter anderem ein
Forschungsbericht von Microsoft zum OpenAI-Modell GPT-4 zitiert.

Dieser Bericht hat aber seine
Schwächen: Die Autoren des Papers verwenden bewusst Testverfahren abseits der
im Machine-Learning etablierten Standards. Sie geben teils subjektive Eindrücke
wieder. Und sie arbeiteten mit einer frühen Version des Modells, die bis heute
nicht öffentlich zugänglich ist. Darum können die Ergebnisse der Studie auch
nicht unter gleichen Bedingungen von Dritten repliziert werden.

Kein Einzelfall: Auch OpenAI selbst
und Google veröffentlichen immer wieder Forschungsberichte, die den Namen nicht
verdient haben. Denn die KI-Unternehmen halten Datengrundlagen und wichtige
Spezifikationen ihrer Modelle zurück. Sie geben lange Autorinnenlisten an und
spicken die Texte mit Fachbegriffen, um darüber hinwegzutäuschen, dass es
eigentlich vor allem PR-Mitteilungen sind.

Dass es kein Peer-Review gibt und
geben kann, wird dann damit begründet, dass eine vollständige Veröffentlichung
aller Details zu den eigenen Modellen zu gefährlich wäre. So wird es Fachleuten
unmöglich gemacht, die von den Unternehmen getroffenen Aussagen kritisch zu
analysieren. Inklusive der Behauptung, KI sei überhaupt übermenschlich oder
gefährlich. Das ist ein absurder Zirkelschluss. Aber der unwissenschaftliche
Hype um die AGI ist gute Werbung, um Investorengelder einzusammeln.

Sowohl für schnelle als auch für
sichere KI-Forschung wirklich von Vorteil wäre Offenheit. Die Offenheit, die
sich die OpenAI-Gründer in den Namen geschrieben haben. Obwohl sie sie nie
erfüllen konnten oder wollten.
"
Artificial Intelligence,Zeit,2024-04-07,https://www.zeit.de/2024/15/kuenstliche-intelligenz-anguilla-internet-domain-ai,Künstliche Intelligenz: Endung gut | ZEIT ONLINE,
KI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-urheberrecht-new-york-times-newsletter-kuenstliche-intelligenz,"KI und Urheberrecht: Darf OpenAI sich bei der ""New York Times"" bedienen? | ZEIT ONLINE","Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 11. Januar 2024. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Die New York Times verklagt OpenAI und Microsoft. Für mich
ist das eine der spannendsten Entwicklungen der letzten Monate. Denn die Klage
könnte endlich Antworten auf zwei grundlegende Fragen erzwingen: was Large
Language Models wirklich sind. Und wer mit Inhalten, die im Netz stehen, Geld
verdienen darf.

Die New York Times will belegen können, dass ins Training
von OpenAIs Produkten Millionen von ihren Artikeln eingeflossen sind – ohne,
dass die Zeitung zugestimmt hätte oder kompensiert worden wäre. Es ist nicht
die erste Klage dieser Art: Auch die Techunternehmen OpenAI, Meta, Google,
Github, Midjourney und StabilityAI wurden von Kreativschaffenden verklagt, weil
diese ihre Werke unzulässig verwendet sahen. In den meisten Fällen stehen die
Urteile noch aus.

Das Verfahren, das die New York Times nun anstößt, sticht
aber heraus. Die Zeitung ist eine bedeutende und reichweitenstarke Marke,
sodass ihr Fall nicht nur vor Gericht, sondern auch in der Öffentlichkeit
diskutiert werden wird. Die zentrale Frage ist, ob große Sprachmodelle den
Journalismus kaputtverdauen – selbst den eines der mächtigsten Medienhäuser der
Welt. Denn: Wenn ChatGPT New-York-Times-Artikel so gut wieder ausspucken
könnte, dass es keinen Anreiz mehr gibt, deren Homepage zu besuchen, geschweige
denn ein Abo abzuschließen, dann hätte die Zeitung ein Problem.

Aber ob große Sprachmodelle wirklich derartig mit dem
Journalismus in Konkurrenz treten können, ist noch nicht klar – unter anderem,
weil nicht mal Einigkeit darüber besteht, wie ihre Informationsverarbeitung
genau zu verstehen ist. Sind sie kluge Contentproduktionsmaschinen, nur ein
paar zusätzliche Grafikkarten davon entfernt, eigenständig zu denken? Oder sind
sie doch eher so etwas wie Kompressionsalgorithmen – nichts mehr als ein
""verschwommenes JPEG des Internets"", wie es der Autor Ted Chiang in
einem bekannten Essay formulierte?

KI-Firmen wie OpenAI schwimmen bislang zwischen diesen
beiden Narrativen. Einerseits betonen sie immer wieder das Ziel, dass lernende
Maschinen das verarbeitete Wissen auch auf neue Konzepte anwenden können.
Anders gesagt: Mit Artikeln trainiert gibt die generative KI neue Antworten.
OpenAI weist die Vorwürfe der New York Times zurück: Wenn ChatGPT ganze
Textpassagen reproduziere, wie die Zeitung unter anderem argumentiert, handle
es sich um ""seltene Fehlfunktionen"", die nur durch Trickserei gelängen.
OpenAI vergleicht seine Modelle mit Menschen: Auch die dürfen aus dem Internet
lernen.

Andererseits arbeitet die Firma hart daran, sogenannte
Halluzinationen zu verringern, also Sätze, die zu stark von den Informationen
in den Trainingsdaten abweichen. Das wäre wichtig, je mehr man seine Produkte
als Suchmaschinenalternative vermarkten will. Es würde aber auch bedeuten, dass
die Sprachmodelle eigentlich die Kompression von antrainiertem Wissen leisten.

Es könnte spannend werden, wie dies nun vor Gericht
verhandelt wird. Egal aus welcher dieser beiden Richtungen man argumentiert,
kann die New York Times meiner Meinung nach gut antworten. Wenn OpenAIs
Sprachmodelle New-York-Times-Artikel unkompensiert reproduzierten, verstoße das
gegen das Urheberrecht, heißt es in der Anklageschrift. Aber selbst, wenn das
Gericht das Reproduktionsargument nicht sehen sollte, dann würde ChatGPT
zufällig – und dabei oft falsch – aus New-York-Times-Artikeln zitieren. Letzteres
könnte man als rufschädigend sehen.

Das zweite Problem, das mitverhandelt wird, ist, dass das
Urheberrecht an sich einigermaßen veraltet ist. Erst digitale Technologien,
dann das Internet haben verändert, was wir unter ""Kopien"" verstehen –
und bei ""geistigem Eigentum"" wird es noch komplizierter. Darum hakt
es immer wieder bei der Frage, wie man Informationen im Digitalen schützt und
honoriert.

Für Medien ist die Situation online besonders kniffelig. Auf
der einen Seite steht Wissen als gesellschaftliches Gut, das so breit
zugänglich sein sollte wie möglich. Nachrichten, der Journalismus der New York Times, haben einen Wert für demokratische Gesellschaften, darauf können sich
die meisten Menschen einigen.

Auf der anderen Seite steht die Entlohnung derer, die diese
Informationen bereitstellen. Themen zu recherchieren, Inhalte aufzubereiten,
kostet Geld – darum gibt es Paywalls, Werbeanzeigen. Doch ein großer Teil der
Werbeeinnahmen bleibt schon jetzt bei Google und Meta hängen. Und für
Onlinejournalismus direkt zu bezahlen, ist nie selbstverständlich geworden.
Viele Nutzerinnen und Nutzer tun das nicht – und große KI-Unternehmen bedienen
sich offenbar auch freimütig. 

Informationen zugänglich machen und doch die Urheberinnen
vergüten: Dieses Dilemma aufzulösen, wird mit generativer KI nicht einfacher.
Sie kann den Trend fortführen, dass einige wenige Geld verdienen mit den
Leistungen vieler. Teure Produktionsmittel führen oft dazu, dass sich die Macht
darüber an der Spitze konzentriert.

Aber ein Large Language Model ist nichts wert ohne die
Daten, die es speisen. Wir alle füttern es – vom Artikel in der New York Times
bis hin zum Tweet, den keiner liest. Insofern ist es richtig, gut, dass ein
Großunternehmen wie die New York Times sich nicht einfach von OpenAI in Deals
verstricken lässt, sondern Grundsatzfragen vor Gericht stellt. Und vielleicht
wird 2024 ja das Jahr, in dem wir bessere Wege finden, das Teilen von Wissen zu
belohnen.
"
KI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-arbeitswelt-newsletter-kuenstliche-intelligenz,KI in der Arbeitswelt: Wann macht die KI denn nun meine Arbeit? | ZEIT ONLINE,"Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 21. März 2024. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Der KI-Bot Devin soll selbstständig
programmieren können. Er ist ein sogenannter Agent, also
ein System, das durch die Verknüpfung verschiedener Automatisierungsmethoden
komplexe Aufgaben lösen können soll. ChatGPT gibt eine Antwort auf eine
Eingabe, ein Agent nimmt eine Anfrage, zerlegt sie in Teile und spricht so
lange mit sich selbst und dem Internet, bis er sie erfüllt hat. Das ist
jedenfalls die Idee.

Als vor etwa
einem Jahr die ersten auf GPT basierenden Agenten auftauchten, waren sie sehr
primitiv und voller Fehler. Nun hat Devin unter Programmiererinnen und
Programmierern aber einen ziemlichen Schock ausgelöst: Der KI-Bot soll einfach
die ganze Arbeit machen können, konzipieren, programmieren, Fehler suchen.
Viele Programmierer verwenden schon lange KI-Helfer, zum Beispiel GitHub
Copilot. Aber, wie der Name sagt, ist der ein Assistent, er macht Vorschläge,
Mensch und KI machen die Arbeit gemeinsam. 

Wie so oft,
wenn Demovideos die Timelines elektrisieren, ist die Aufregung vielleicht ein
bisschen zu groß und deutlich zu voreilig. Große Sprachmodelle und ihre
Anwendung in Programmen wie dem KI-Agent Devin können im weitesten Sinne genau
eine Sache: Informationsverarbeitung. Sie können anhand gelernter Regeln Texte
zusammensetzen. Und je mehr Daten, je strukturierter die Sprache und je
geringer die Varianz, desto leichter ist es für Maschinen, gute Ergebnisse zu
erzielen. Deshalb sind Programmiersprachen für die Systeme recht leicht zu
beherrschen.

Ein großer
Teil der Arbeit einer Programmiererin fällt genau in die Bereiche, in denen
Maschinen gerade exzellent werden: Sie muss in der Lage sein, konkrete
Anforderungen in Programmiersprachen zu übertragen, und wenn sie es nicht
auswendig weiß, Informationen dazu zu suchen, wie das geht. Es ist ein gängiger
Witz unter Programmierenden, dass sie eigentlich mehr googeln als selbst
Aufgaben lösen. 

Das sind alles
Dinge, die ein KI-Agent können kann. Die Vision mancher KI-Hersteller ist es
deshalb, dass man schon bald keine Softwareentwicklerinnen mehr braucht. Oder
zumindest viel weniger, weil ein einzelner Mensch viele Agenten beaufsichtigen
könnte, so wie heute ein Senior Developer viele Juniors anleitet.

Nur: Maschinen
liefern im wahrsten Sinne des Wortes durchschnittliche Ergebnisse. Viele
Aufgaben in der Programmierung sind vorhersehbar, da sie oft da gewesen sind.
Bei diesen kleinen, einfachen Problemen, zum Beispiel die Programmierung einer
Website, wird das reichen, was die Maschinen können. 

Aber dort, wo
nicht die erwartbarste Lösung die richtige ist, wird es schnell richtig
ineffizient, wenn man Agenten ans Werk lässt. Denn Informationsverarbeitung
funktioniert dann gut, wenn auch Informationen da sind. Aber das Internet ist
ein schlecht strukturiertes Netzwerk. Wenn der Agent nicht zufällig das eine
Forum findet, in dem exakt die Lösung steht, dann wird er sich schwertun. 

KI-Agenten
sollen zwar auch komplexe Probleme automatisieren können. Aber sie machen das
mit einer Art Brute-Force-Methode. Sie suchen einigermaßen wahllos im Internet,
sie stellen Hunderte Abfragen in kurzer Zeit an das im Hintergrund arbeitende
Sprachmodell. Vielleicht finden sie zufällig dabei auch mal eine Lösung, aber
wenn es schlecht läuft, fressen sie enorm viel Strom und Zeit, während sie
komplett in die falsche Richtung laufen.

Menschen
hingegen sind gut darin, auch mit lückenhaften Informationen und komplexen
Problemlagen umzugehen. Zumindest die besten von uns. Deshalb sind
Softwareentwicklerinnen auch so teuer.

John von
Neumann, ein Pionier der künstlichen Intelligenz (und der Atombombe), sagte
einmal: ""Wenn Sie mir konkret sagen, was genau eine Maschine nicht kann,
kann ich immer noch eine bauen, die genau das leistet."" Von Neumann war
Mathematiker und genau deswegen übersieht er vielleicht die physischen Grenzen
seiner Behauptung. In der Theorie kann man alles Mögliche automatisieren, aber
in der Praxis braucht Arbeit Zeit und Energie. Eine Maschine, die rät, anstatt
zu logisch zu denken, ist nicht effizient. 

Die Realität
ist oft chaotisch, die Probleme, die wir Menschen lösen müssen, sind komplex
und vielseitig. Das gilt in der Softwareentwicklung und in vielen anderen
Berufen. Für viele Aufgaben wird es wohl noch eine ganze Zeit lang notwendig
sein, die Probleme selbst gründlich zu durchdringen, statt sie an Agenten zu
delegieren.
"
KI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-filmbranche-special-effects-newsletter-kuenstliche-intelligenz,KI in der Filmbranche: So generiert man keinen Oscar | ZEIT ONLINE,"Sie lesen den KI-Newsletter
""Natürlich intelligent"" vom 04. April 2024. Um den Newsletter jeden
zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Generative AI
goes to Hollywood. Zumindest wünschen sich
das wohl die Chefs von OpenAI, die Medienberichten zufolge gerade auf Werbetour
durch einige große Filmstudios unterwegs waren.

Künstliche Intelligenz spielt in der
Filmbranche schon lange eine Rolle, beispielsweise bei der Umsetzung von
Spezialeffekten wie der Verjüngung von Harrison Ford. In der stark
gewerkschaftlich organisierten Filmindustrie gibt es gegen viele der Einsatzmöglichkeiten
Widerstände. Die typische Sorge: KI könnte Menschen ersetzen.

Mit der neusten Spielart der
Technologie, Text-to-Video, sollen Filme nun wie von allein, basierend nur auf
Textbeschreibungen entstehen. OpenAI hat dazu mit den Democlips aus dem
Sora-Modell die bislang beeindruckendsten Ergebnisse gezeigt. Wer generierte
Clips wie den einer Frau sieht, die durch eine Innenstadt läuft und sich in
einer Pfütze spiegelt, mag davon ausgehen, dass es nicht mehr allzu lange
dauern wird, bis wir die ersten von KI erzeugten Videos im Kino sehen.

Tatsächlich dürfte diese Erwartung
aber etwas voreilig sein. Denn die Limitierungen, die Sora noch hat, sind nicht
die Kleinigkeiten, für die man sie auf den ersten Blick halten kann.

Noch sehen wir in den Videos die für
generative KI typischen Inkonsistenzen. Objekte tauchen aus dem Nichts auf,
Menschen verschwinden oder haben zu viele Gliedmaßen. Derartige Fehler werden
sich durch besseres Training schnell reduzieren lassen, so wie es bei Bild-KI
in den letzten Jahren geschehen ist.

Videos haben dabei sogar noch einen
Vorteil: Was ein konsistentes Verhalten eines bewegten Objekts ist, kann man
einem Computer in der Theorie viel besser beibringen als Faktentreue in einem
Text. Denn die Grundlage ist Physik, und für deren Simulation gibt es schon
seit Jahren Programmbibliotheken aus der Videospielindustrie.

Trotzdem muss viel zusammenkommen, um
schöne, realistische und vor allem in der Praxis nutzbare KI-Videos zu
erzeugen. Die Fülle an möglichen Settings, Bewegungen und Texturen bedeutet
eine enorme Herausforderung für die Ingenieurinnen, die den Systemen beibringen
sollen, Videos nach unseren Erwartungen zu erzeugen.

Effizient lernen bedeutet, sich auf
die wichtigsten Aspekte zu fokussieren. Unsere menschliche Wahrnehmung kann das
gut: Das Gehirn filtert ständig heraus, was es nicht braucht. Aber wie setzt
man das in Maschinen um?

Ein physikalisch realistisches Video
von einer Verfolgungsjagd bringt Hollywood wenig, wenn beim Training die Farben
oder Texturen dafür vernachlässigt werden mussten. Aber ein Modell, das alle
Aspekte gleichzeitig verarbeitet, ist mathematisch anspruchsvoll und vor allem
furchtbar energieaufwendig. Schon die jetzigen Modelle sind riesig und brauchen
ein Vielfaches mehr an Strom und Zeit als ihre Text- oder Bildkollegen. Je länger das
Video sein soll, desto komplizierter wird es. Sora kann Videos erzeugen, die
maximal eine Minute dauern, und ist möglicherweise auch deshalb nicht
öffentlich zugänglich, weil es zu viel Rechenleistung bräuchte, wenn Millionen
Menschen damit herumspielen würden.

Und dann wären
da noch die Details, auf die gerade bei teuren Film- und Serienproduktionen
penibel geachtet wird. Generierte Videos lassen sich, genau wie generierte
Texte und Bilder, nur begrenzt steuern.

Ist der Blick
des Mannes in einer Szene verliebt genug? Greift er genau im richtigen Moment
nach der Hand seines Partners? Sieht sein Bart ein bisschen blonder aus als in
der zuvor generierten Einstellung? Wenn einzelne Dinge nicht stimmen, muss man
noch mal generieren und hoffen – oder man bearbeitet es händisch so lange nach,
bis es passt. Doch wieder Menschenarbeit.
"
KI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/google-ki-newsletter-kuenstliche-intelligenz,Google: Wie generative KI Google entmachten könnte | ZEIT ONLINE,"Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 14. Dezember 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Google wirkte lange unantastbar. Aber obwohl die Firma viele
der Grundsteine legte, auf denen der heutige KI-Boom beruht, wirkt sie heute
unsouverän und nervös. Vergangene Woche musste sich der Konzern wenige Stunden
nach der Ankündigung des neusten KI-Modells Gemini für ein vielbeachtetes
Demovideo entschuldigen.

Die beeindruckenden Reaktionen des Sprachmodells, unter
anderem auf eine in die Kamera gehaltene blaue Ente und verschiedene
Zeichnungen, waren gestellt und suggerierten Fähigkeiten, über die das Modell
gar nicht verfügt. Es ist nicht der erste PR-Schnitzer in diesem Jahr. Und
Insider berichten: Beim einst so übermächtigen Google wächst die Sorge,
abgehängt zu werden. 

Gründe dafür gibt es genügend. Einer ist: Die Flut von
generativen Inhalten verschlechtert schon jetzt nachweislich die Qualität von
Suchergebnissen. Ein weiterer, aus Googles Sicht noch bedrohlicherer: Seit
ChatGPT auf viele Fragen einigermaßen richtige Antworten geben kann und sogar
Bing eine Chatfunktion in seine Suche eingebaut hat, wird diskutiert, ob im
Internet der Zukunft Chatbots die Suchmaschinen, wie wir sie bisher kennen und
nutzen, nicht komplett ablösen.

Diese Aussicht erfreut längst nicht alle: Manche sehen das
Internet durch den Aufstieg generativer KI auf dem Weg in eine dystopische
Zukunft, in der wir nichts mehr glauben können und an jeder Ecke mit Werbung,
Lügen und Spam konfrontiert sind. Denn mit generativer KI lassen sich natürlich
jede Menge irreführende Informationen erzeugen und streuen – und
zusammenhalluzinierte Inhalte mit wenig Wahrheitsgehalt produzieren sie auch. 

Ich halte es aber für unrealistisch, dass ein neues Internet
voller falscher und halbrichtiger Informationen lange überleben würde. Denn:
Der ökonomische Wert von Kommunikation im Internet entsteht erst durch sozialen
Wert, den der digitale Austausch für Menschen hat. 

Ob ich im Internet Informationen suche (ein Kochrezept, eine
Adresse, einen Nachrichtenartikel) oder ob ich mich auf Plattformen mit anderen
vernetze: Hier treten Menschen miteinander in Transaktion. Und erst durch diese
Transaktionen sind große Internetfirmen so wertvoll geworden. 

Google oder Meta haben von den Transaktionen aber nicht nur
ökonomisch profitiert, sondern auch die Metriken verändert, wie Wert im
Digitalen gemessen wird. Dank ihnen sind die Maßeinheiten Klicks oder
Werbeeinnahmen. Das Problem ist: Wenn die Metriken irgendwann nicht mehr den
zugrundeliegenden Wert repräsentieren – weil Botarmeen Anzeigenklicks
generieren oder SEO-optimierte Misinformation in den Suchergebnissen aufsteigt
–, dann bekommt irgendwann niemand mehr, was er sucht. Und dann kann auch
niemand mehr Geld damit verdienen.

Anders gesagt: Nicht die KI-Sprachmodelle drohen das
Internet zu zerstören, sondern die Unternehmenspolitik von Google und Meta. 

Die utopische Perspektive darauf, was als Nächstes passiert,
ist die, dass die großen Umbrüche durch KI endlich den Weg freimachen für eine
neue Logik des Internets. Wir sind an einem Punkt, wo es noch einmal zu
überdenken gilt, wie Informationen im Internet bestmöglich zugänglich gemacht
werden können. Die KI-Forschung und die Informationswissenschaft kennen viele
smarte Methoden, Informationen mit Maschinenhilfe aufzubereiten – auch abseits
von unzuverlässigen Chatbots. So kann zum Beispiel Retrieval Augmented
Generation (RAG) die Outputs von Sprachmodellen mit externen Fakten abgleichen.

Vielleicht findet Google den besten Weg dafür. Das muss aber
nicht sein. Der KI-Boom, der ohne Google selbst kaum möglich gewesen wäre, hat
den Riesen angekratzt. Und gleichzeitig weltumspannend Gelder und Motivationen
für die Konkurrenz freigemacht.

Eine Technologieverschiebung wie die, die wir gerade sehen,
ermöglicht immer auch die Verschiebung von Macht.
"
KI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/openai-newsletter-kuenstliche-intelligenz,OpenAI: Liebe KI-Utopisten: Sie müssen jetzt ganz stark sein | ZEIT ONLINE,"Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 30. November 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Innerhalb von wenigen Tagen wurde OpenAI-Chef Sam Altman
erst gefeuert – und dann auf seinen alten Posten zurückgeholt. Der Machtkampf
an der Spitze von OpenAI kam unerwartet, schnell und heftig. Und jetzt kochen
Gerüchte hoch, was dieses ganze Drama eigentlich verursacht hat.

Eine Lieblingstheorie auch in der Medienberichterstattung
ist, dass ein bahnbrechender Durchbruch in der KI-Forschung der Auslöser
gewesen sein soll. Die Nachrichtenagentur Reuters und die Plattform The
Information berichteten unter Berufung auf anonyme interne Quellen, OpenAI habe
ein Modell namens Q* entwickelt, das Mathematikaufgaben auf Grundschulniveau
lösen könne – und möglicherweise ""die Menschheit bedrohen"". Insider
sollen in der Technologie einen entscheidenden Schritt hin zur menschenüberflügelnden
Superintelligenz sehen.

Ob da was dran ist, ist schwer zu sagen. OpenAI hat schon
oft gezeigt, dass Mysterien gutes Marketing sind: Schon vor vier Jahren unkte
man dort, man wolle nur Teile seines Modells GPT-2 öffentlich machen – aus
""Sicherheitsgründen"". Die Firma schweigt nun zu allen Spekulationen
um Q*. Aber Sam Altman dürfte es ganz recht sein, dass in den Medien wieder
mehr über die vermeintliche Macht einer gerüchteumwobenen
Möglicherweise-Super-KI gesprochen wird statt beispielsweise über
zwischenmenschliche Verhakungen im Management von OpenAI.

""Könnte die Menschheit bedrohen"" ist in der Welt von Sam Altman und Co nämlich immer nur einen Schritt entfernt von einer anderen
Vision: Wenn es uns gelingt, dass uns die Super-KI nicht vorher umbringt, soll
sie uns in ein Zeitalter des ungekannten ökonomischen Wohlstands führen. Die
Wirtschaft würde komplett von ihr transformiert. Niemand müsste mehr arbeiten,
allen ginge es gut. Immerhin ist die Frage nach dem exponentiellen
Wirtschaftswachstum dank KI greifbarer und überprüfbarer als die, wann der Durchbruch
kommt, der KI übermenschliche, gefährliche Fähigkeiten verleiht.

Arjun Ramani und Zhengdong Wang, ein Wirtschaftsjournalist
und ein KI-Forscher, haben dazu im Sommer aus vielen Quellen die besten
Anhaltspunkte zusammengetragen. Und sie kommen zu dem Schluss, es werde
""wirklich, wirklich schwierig"", tatsächlich transformative KI zu
erreichen.

Sie entlarven einen Denkfehler in der Hoffnung auf eine
vollautomatisierte Luxuswelt: Selbst exponentielles Wachstum in einem
Teilbereich weitet sich nicht zwangsläufig auf andere Bereiche aus. Im
Gegenteil: Oft begrenzt der Sektor, der am langsamsten wächst, die
Produktivitätssteigerungen insgesamt.

Unsere Wirtschaft beruht zum größten Teil noch auf
physischen Prozessen, die selbst mit tollen KI-Lösungen nur schwer verändert
werden können. Selbst wenn eine Superintelligenz die Architektur
revolutioniert: Irgendjemand muss noch immer für die Mauern die Steine
verspachteln. Denn die Robotik ist noch lange nicht so weit, dass sie Menschen
in allen Konstruktionsaufgaben ersetzen kann.

Maschinen haben uns schon jetzt in vielen Bereichen
überflügelt. Und der technische Fortschritt hat unsere Welt in den letzten
Jahren fundamental verändert, teils verbessert. Auch wirtschaftlich.

Zur Lösung unserer Probleme mangelt es uns oft aber nicht an
Wissen oder Intelligenz. Sondern an Kooperationsfähigkeit.

Beispiel humanitäre Hilfe: Deren Logistik ist ein Problem,
das mathematisch gut lösbar wäre. Schon heute. Menschen in Krisenregionen
hungern trotzdem noch immer. Und wir wissen genau, dass wir Emissionen von
Treibhausgasen verringern müssen, um die Klimakrise zu verlangsamen. Aber wir
scheitern an der Umsetzung.

Die KI-Forschung wird weiter Fortschritte machen. Hilfreiche
und kritikwürdige. Aber keine noch so intelligente Technologie wird je der
Killswitch für alle Probleme der Menschheit sein. Wer alles auf diese Hoffnung
setzt, wird enttäuscht werden. Und wer nur KI als Hoffnung verkauft, wird
enttäuschen.
"
KI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/autonome-agenten-newsletter-kuenstliche-intelligenz,"Autonome Agenten: KI-Agenten, die auf sich selbst starren | ZEIT ONLINE","Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 19. Oktober 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Vergangene Woche haben sich in San Francisco Menschen aus
aller Welt getroffen, um an einer der spannendsten Anwendungen von generativer
KI zu basteln, die es derzeit gibt: autonome Agenten.

Agenten sind (mehr oder weniger) selbstständig handelnde
KI-Systeme. Man muss, wie so oft, vorsichtig mit dem Begriff sein, denn
""Agenten"" klingt schon sehr nach Bewusstsein, und das haben sie
gewiss nicht. Aber mehr agency, also Handlungsfähigkeit, als ChatGPT haben sie
auf jeden Fall.

Bei dem Hackathon in San Francisco, zu dem das Start-up
AutoGPT eingeladen hatte, entwickelte ein Team zum Beispiel einen Prototyp, der
selbstständig die einzelnen Teile einer Reise recherchieren und planen können
soll. An solchen Beispielen lässt sich gut erklären, wie KI-Agenten arbeiten.

Sagen wir, ich will ein neues Fahrrad kaufen. Ich habe ein
definiertes Budget und möchte das am besten bewertete Rad, das für dieses
Budget auf dem Markt zu haben ist. Ich könnte mir dafür ein Programm schreiben,
das im Internet sucht, Bewertungen ausliest und Preise vergleicht. Aber alles,
was mir wichtig ist – zum Beispiel, dass die Rahmengröße zu meinen kurzen
Beinen passt –, müsste ich mühsam und exakt einprogrammieren. Alternativ, und
so ist es in meinem echten Leben gelaufen, kann ich meinen Freund fragen. Der
denkt mit, macht sich einen Plan, informiert sich und kommt mit Rückfragen
wieder auf mich zu.

Autonome Agenten kombinieren – so die Theorie – das Beste
aus beiden Welten. Wie ein Computerprogramm sind sie schnell, rechnen gut,
können viele Daten speichern und werden nicht müde. Aber wie mein Freund sind
sie nicht abhängig von einer detaillierten Schritt-für-Schritt-Anleitung. Sie
denken mit. Und sie können das, weil wir ihnen Zugang zur besten
Mitdenk-Technologie geben, die wir Menschen bislang entwickelt haben: unserer
Sprache.

Agentensysteme erweitern Sprachmodelle wie GPT-4 um die
Ebene der Handlungsfähigkeit. Die bekanntesten Programme heißen AutoGPT oder
BabyAGI. Man gibt ihnen ein Ziel und Zugang zu verschiedenen Werkzeugen:
Suchmaschinen, eine Möglichkeit, zu programmieren, und einen Ordner, in dem sie
Dateien abspeichern und auslesen können.

Verknüpft wird alles durch Zugriff auf ein großes
Sprachmodell. Das ist spannend, denn es zeigt, was neu ist in der aktuellen
KI-Welle. Es ist schon lange möglich, verschiedene Computerprogramme zu
verketten. Aber der Zugang zu Sprache ermöglicht den Maschinen einen neuen
Arbeitsmodus, der menschlichem Denken tatsächlich näherkommt als alles, was
Computer bisher getan haben.

Das System unterteilt einen nicht exakt formulierten Auftrag
in Teilaufgaben, hinterfragt die eigene Herangehensweise kritisch und passt den
Plan an, je nachdem wie die ersten Zwischenergebnisse aussehen. Die Ansätze für
die Lösungen des Problems, man könnte von ""Ideen"" sprechen, kommen
aus dem Wissensschatz, der in den Sprachmodellen verschlüsselt ist. Wer kurze
Beine hat, braucht ein kleines Fahrrad. Das muss man einem Sprachmodell nicht
mehr extra dazu sagen.

Das kann uns etwas darüber zeigen, wie wir selbst
funktionieren. Ist es Selbstreflexion, die uns erst handlungsfähig macht? Wie
viel von unserem Bewusstsein ist Sprache?

Ich möchte nicht falsch verstanden werden: Ich sehe aktuell
keine Anhaltspunkte, dass Agenten bald zu wirklich autonomer KI werden, die wir
aus Science-Fiction kennen. Eine Motivation, einen inneren Antrieb hat auch
AutoGPT nicht und die Handlungsfähigkeit ist noch weit davon entfernt, uns in
moralische Dilemmata oder in existenzielle Gefahr zu bringen.

Dass Agenten unsere Sprache als Vehikel benutzen, um zu
planen, sollte uns nicht dazu verleiten, ihnen Menschlichkeit zuzuschreiben.
Aber ich glaube, die Prinzipien der Selbstbeobachtung und des ständigen
Rückbesinnens auf Ziele und bisherige Schritte werden es KI-Systemen erst
ermöglichen, wirklich nützlich für uns zu sein.

Bis dahin ist es allerdings noch ein weiter Weg. Die
Technologie ist neu und voller Bugs. Die Programme machen oft Fehler oder
bleiben hängen. So geschehen zum Glück auch bei der skrupellosesten Anwendung:
ChaosGPT, der ein paar Scherzkekse unter anderem das Ziel gaben, ""die
Menschheit auszulöschen"". Die Bilanz blieb harmlos: Das System legte sich
einen Twitter-Account zu, setzte zwei Tweets ab und stellte einige
Google-Suchanfragen nach der Zar-Bombe an. Weiter kam es nicht.
"
KI,Zeit,2024-04-08,https://www.zeit.de/gesellschaft/schule/2024-04/kuenstliche-intelligenz-schule-bildung-gpt,Künstliche Intelligenz: Die Vertretungsstunde macht heute die KI | ZEIT ONLINE,"Der Hamburger Grundschullehrer Alex Tscheulin ist ein IT-Nerd. Er probiert jedes digitale Tool aus. ChatGPT hat ihn bisher enttäuscht, viel zu allgemein seien die Antworten. ""Und immer das Risiko eines Fehlers"", sagt er, das ärgert ihn. Aber nun hat er seinen eigenen Tscheulin-Bot, den er selbst mit Daten trainieren kann. Tscheulin lud die neuen Bildungspläne hoch und fragte seinen Co-Lehrer nach einem Einstieg in eine Stunde. ""Bring doch eine Schatzkiste mit"", riet ihm Tscheulin-GPT, ""und dann gehst Du die Schätze Deiner Schüler suchen."" 

Tscheulin ist Beauftragter für Digitalität der privaten katholischen Schulen im Erzbistum Hamburg. Als er seinen Schulleitern das neue Fachlehrer-GPT vorstellte, waren sie erfreut: endlich ein ChatBot, der nicht plappert und halluziniert, sondern lehrergerechte Antworten gibt. Solche mit eigenen Texten aufgeladenen und mit System-Prompts fokussierten KI-Assistenten heißen Custom-GPTs. GPT bedeutet Generative Pretrained Transformer – und hier setzt die neue Variante an: Die Lehrkräfte trainieren ab sofort selbst. 

Das Schlimmste, was Sprach-KI-Systeme in den Augen deutscher Studienräte nämlich tun: Sie lügen, genauer, sie halluzinieren. Damit soll jetzt Schluss sein, jedenfalls soll die Schwindelei mithilfe von Custom-GPTs nahe null gedrückt werden. Die beiden KI-Anbieter Fobizz und Schulverwalter, die einige Bundesländer mit ChatGPT versorgen, stellen personalisierte Lernbots zur Verfügung. Zwei Drittel der Schüler und Studierenden nutzen laut Umfragen die Sprach-KI bereits. Ziehen die Lehrkräfte jetzt mit selbst angelernten KI-Assistenten nach? 

""Bei uns kann ab sofort jede Lehrkraft fünf Dateien mit je zehn Megabyte als Hintergrundwissen in ihre eigene KI hochladen"", berichtet Diana Knodel von Fobizz. Der digitale Weiterbildungsanbieter hat sich auf pädagogische Anwendungen künstlicher Intelligenz spezialisiert. Eine Biologielehrerin kann nun den wichtigen Aufsatz über den Zitronensäurezyklus in ihr GPT einspeisen, der Geschichtslehrer Texte zu Wallenstein hochladen. Rund 100 Seiten Zusatzwissen sind derzeit möglich. 

Auch der Leipziger KI-Anbieter Schulverwalter sagte zu ZEIT ONLINE, dass dort auf Fächer und Schulen fokussierte Sprachmodelle buchbar seien. ""Das ist im Prinzip immer noch dieselbe Sprach-KI, nur dass die Lehrkraft ihr erklärt, wo sie Zusatzinformationen herbekommt, damit die Antworten endlich besser werden"", sagt Julian Dorn, der Gründer der Schulverwalter, der als Informatiklehrer arbeitet. 

Sowohl die Schulverwalter als auch Fobizz wollen das Angebot zunächst als kostenfreie Ergänzung für Lizenznehmer bereitstellen. Fobizz bietet ChatGPT flächendeckend in den Schulen von Mecklenburg-Vorpommern und Rheinland-Pfalz an. Die Sprach-KI der Schulverwalter wird gerade in Niedersachsen und Bayern in Pilotprojekten getestet. 
"
KI,Zeit,2024-04-08,https://www.zeit.de/digital/internet/2024-04/ki-hype-newsletter-kuenstliche-intelligenz,"KI-Hype: Vielleicht wird KI doch nicht so schnell schlauer, wie alle denken | ZEIT ONLINE","Sie lesen den KI-Newsletter ""Natürlich intelligent"" vom 02. November 2023. Um den Newsletter jeden zweiten Donnerstag per Mail zu erhalten, melden Sie sich hier an.

Es ist Quartalszahlen-Zeit und das heißt auch, Konzerne
erklären ihren Anlegern und der Öffentlichkeit, wie die Geschäfte laufen. Im
KI-Bereich waren die Erwartungen groß, immerhin redet alle Welt davon, dass
KI-Modelle wie GPT-4 mindestens das neue Internet sein könnten, wenn nicht
gleich die Neuerfindung des Rades. Die Firmen, die KI-Services anbieten oder
die dafür nötige Rechenleistung vermieten, die Amazons, die Microsofts, die
Alphabets, müssten doch eigentlich wachsen wie wild. Oder?

Tatsächlich hat sich das Wachstum der Cloud-Sparten von
Amazon und Alphabet verlangsamt. Einzig Microsoft gewinnt durch KI stärker als
erwartet; die Einnahmen durch Cloud-Services stiegen im Vergleich zum Vorjahr
um 24 Prozent. Das mehrere Milliarden Dollar schwere Investment in OpenAI
scheint sich auszuzahlen.

Trotzdem wirft das ruckelige Wachstum die Frage auf, wie es
mit dem KI-Hype weitergeht. In die begeisterten Stimmen mischt sich langsam
etwas Skepsis.

Microsoft-Gründer Bill Gates zum Beispiel sagte vor einigen
Tagen in einem Interview mit dem Handelsblatt, dass er ein Plateau erwarte:
GPT-5 könnte nur ein bisschen besser werden als GPT-4. Aus dem Interview geht
nicht hervor, worauf er diese Annahme stützt, aber er könnte auf etwas
anspielen, das diesen Sommer bereits von einigen KI-Expertinnen und -experten
diskutiert wurde: Der KI gehen die Daten aus.

Um eine leistungsstarke KI zu bauen, braucht man grob gesagt
zwei Dinge: Daten und Rechenleistung. Daten sind der Rohstoff und die
Rechenleistung muss aufgebracht werden, um daraus ein Netzwerk zu schmieden.

Einerseits sind die leistungsstarken Chips knapp, mit denen
die Rechenleistung für KI aufgebracht wird. Aber das ist ein Problem, das
lösbar scheint. Neue Technologien starten meistens mit großem Energieaufwand
und mit der Zeit finden sich Wege, sie effizienter zu machen. Viele
KI-Anwendungen werden in nächster Zeit günstiger im Betrieb werden.

Auf der Seite der Daten sieht es anders aus, vor allem, was
Texte angeht. Es gibt einfach nicht unbegrenzt verfügbare und vor allem
qualitativ hochwertige Texte, mit denen KIs trainiert werden können. Die
Menschheit kann gar nicht so schnell schreiben, wie KIs lesen können.

Unternehmer wie OpenAI-Chef Sam Altman sagen, sie machen
sich deshalb keine Sorgen. Tatsächlich suchen Forschende nach Wegen, bestehende
Datensätze zu verbessern. Sie können auch KI nutzen, um beispielsweise Texte
aus Videos zu transkribieren oder (in begrenztem Maße) neue Trainingsdaten zu
generieren. Und Unternehmen werden Wege suchen, noch mehr Daten aus unseren
Alltagskonversationen und Privatnachrichten abzugrasen. Aber egal, wie viel sie
aus den letzten Ecken herauskehren: Die einfach verfügbaren riesigen
Datenmengen (früher sagte man: Bibliotheken) aus den vergangenen Jahrhunderten
sind schon verbraucht. Kleine Verbesserungen bedeuten nun großen Mehraufwand.

Daher kommt die Sorge vor einem Plateau: Die beeindruckenden
Fortschritte, etwa von GPT-3 zu GPT-4, entstanden vor allem durch mehr Daten.
Das war es, was OpenAI so erfolgreich gemacht hat und in dessen Folge Microsoft
nun besser dasteht als die Konkurrenz. Das Unternehmen setzte schon früh alles
auf die Karte ""größer ist besser"".

Auch deshalb war die Annahme in den vergangenen Monaten: Wer
nicht mehrere Milliarden Dollar in KI-Entwicklungen stecken kann, muss gar
nicht erst anfangen. Im Kampf um Marktanteile nehmen Amazon, Microsoft und
Alphabet sogar in Kauf, erst mal ordentlich Geld zu verlieren. Laut einem
Bericht des Wall Street Journal subventioniert Microsofts Coding-Plattform
GitHub seinen KI-Assistenten jeden Monat mit durchschnittlich 20 Dollar pro
Nutzendem.

Die Kosten für den Betrieb und auch für das Training von KI
werden günstiger werden. Und wenn gleichzeitig die KI-Fähigkeiten, wie Bill
Gates vermutet, ein Plateau erreichen, sind das vor allem gute Nachrichten für
kleine Unternehmen und Start-ups. Der Vorsprung, den sich die Tech-Giganten mit
ihrem vielen Geld erkauft haben, ist nicht groß. Das OpenAI, ja sogar das
Microsoft der 2030er-Jahre könnte eine Firma sein, deren Name wir heute noch
gar nicht kennen.
"
Künstliche Intelligenz,Zeit,2024-04-06,https://www.zeit.de/digital/2024-04/meta-facebook-ki-inhalte-warnhinweis,Künstliche Intelligenz: Meta will mehr KI-Inhalte mit Warnhinweisen auf Plattformen lassen | ZEIT ONLINE,"Anstatt mit künstlicher Intelligenz erstellte oder manipulierte Fotos und Videos zu löschen, will der Meta-Konzern künftig mehr solcher Inhalte mit Warnhinweisen versehen. Im Gegenzug sollen sie dann auf Meta-Plattformen wie Facebook stehen bleiben dürfen. Damit folgt Meta einer Empfehlung seines unabhängigen Aufsichtsgremiums. Dieses hatte eine Einschränkung der Redefreiheit befürchtet. 

Inhalte sollen anhand von Metadaten in deren Dateien unter anderem mit der Kennzeichnung ""Made with AI"" markiert werden. Die Lockerung gelte jedoch nicht für Inhalte, die Hassrede, Mobbing oder Falschmeldungen zu Wahlen beinhalten, teilte der Konzern am Freitag mit.  

Mithilfe von künstlicher Intelligenz und fortschreitender Technologie lassen sich immer leichter auch Falschmeldungen produzieren und so die öffentliche Meinung manipulieren. Meta will daher in Fällen, in denen diese Gefahr besonders groß ist, entsprechende Warnhinweise besonders deutlich sichtbar platzieren.

Zudem soll eine interne Technologie genutzt werden, um KI-Inhalte automatisch zu erkennen. Die neuen Regeln sollen bei Facebook, Instagram und Threads, dem Kurznachrichtendienst von Meta, angewendet werden. 

Bereits ab Mai will der Facebook-Konzern die neuen Markierungen einführen. Meta kündigte an, den bisher restriktiveren Umgang mit von KI erstellten Beiträgen bis Juli aufgeben zu wollen.  
"
Künstliche Intelligenz,Zeit,2024-04-04,https://www.zeit.de/news/2024-04/04/schulbehoerde-legt-leitlinien-fuer-kuenstliche-intelligenz-vor,Schulen: Schulbehörde legt Leitlinien für Künstliche Intelligenz vor | ZEIT ONLINE,"Hamburgs Schulbehörde hat Leitlinien für den Einsatz Künstlicher Intelligenz (KI) an den Schulen der Hansestadt vorgelegt. Entwickelt worden seien diese von der Kompetenzstelle KI des Landesinstituts für Lehrerbildung und Schulentwicklung Hamburg, von der Behörde und dem Artificial Intelligence Center Hamburg, teilte die Behörde am Donnerstag mit. Die «Leitlinien für den Einsatz von KI-Systemen in Schule und Unterricht» seien ausschließlich online unter www.li.hamburg.de/ki abrufbar und drehten sich um den lernförderlichen Einsatz von Künstlicher Intelligenz im Unterricht, rechtliche Aspekte sowie um Fragen des Datenschutzes.

«Ich möchte alle Lehrkräfte dazu ermutigen, KI-Anwendungen im Unterricht zu erproben und zu reflektieren, um die Schülerinnen und Schüler zu einem kritischen und kompetenten Umgang mit KI zu befähigen», sagte Schulsenatorin Ksenija Bekeris (SPD). Sie hoffe, dass die Leitlinien als hilfreiches Instrument für die pädagogische Arbeit betrachtet würden. Dort werden laut Behörde unter anderem generative KI-Modelle hinsichtlich ihrer Funktionen und Leistungen erörtert und der praktische Einsatz in den Schulen diskutiert. Dabei gehe es etwa um die Frage, wie Aufgaben gestaltet werden müssen, damit die Eigenleistung der Schülerinnen und Schüler sichtbar und von den von der KI erbrachten Inhalten unterscheidbar bleibe.

Der rechtliche Teil befasse sich wiederum unter anderem mit der Frage, wie Leistungen beim Einsatz von KI bewertet werden können und wie etwa bei Prüfungen mit einem nicht erlaubten Einsatz von KI-Anwendungen umgegangen werden sollte. Bekeris betonte: «Die Leitlinien wurden aufgrund der dynamischen Entwicklungen bewusst nicht als PDF oder Druckerzeugnis veröffentlicht, sondern ausschließlich digital.» So könnten Änderungen rasch aktualisiert und die Leitlinien sukzessive um Praxisbeispiele, Anwendungsmöglichkeiten und weitere Materialien für die Lehrkräfte ergänzt werden.

© dpa-infocom, dpa:240404-99-562886/2
"
Künstliche Intelligenz,Zeit,2024-04-03,https://www.zeit.de/zeit-fuer-unternehmer/2024/01/kuenstliche-intelligenz-unternehmen-effizienz-chef,"Künstliche Intelligenz in Unternehmen: Mach mal Platz, Boss! | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-04-03,https://www.zeit.de/2024/12/alles-ueberall-auf-einmal-kuenstliche-intelligenz-buch,"""Alles überall auf einmal"": Wird Gott plötzlich wieder schöpferisch? | ZEIT ONLINE",
AI,Zeit,2024-04-06,https://www.zeit.de/digital/2024-04/meta-facebook-ki-inhalte-warnhinweis,Künstliche Intelligenz: Meta will mehr KI-Inhalte mit Warnhinweisen auf Plattformen lassen | ZEIT ONLINE,"Anstatt mit künstlicher Intelligenz erstellte oder manipulierte Fotos und Videos zu löschen, will der Meta-Konzern künftig mehr solcher Inhalte mit Warnhinweisen versehen. Im Gegenzug sollen sie dann auf Meta-Plattformen wie Facebook stehen bleiben dürfen. Damit folgt Meta einer Empfehlung seines unabhängigen Aufsichtsgremiums. Dieses hatte eine Einschränkung der Redefreiheit befürchtet. 

Inhalte sollen anhand von Metadaten in deren Dateien unter anderem mit der Kennzeichnung ""Made with AI"" markiert werden. Die Lockerung gelte jedoch nicht für Inhalte, die Hassrede, Mobbing oder Falschmeldungen zu Wahlen beinhalten, teilte der Konzern am Freitag mit.  

Mithilfe von künstlicher Intelligenz und fortschreitender Technologie lassen sich immer leichter auch Falschmeldungen produzieren und so die öffentliche Meinung manipulieren. Meta will daher in Fällen, in denen diese Gefahr besonders groß ist, entsprechende Warnhinweise besonders deutlich sichtbar platzieren.

Zudem soll eine interne Technologie genutzt werden, um KI-Inhalte automatisch zu erkennen. Die neuen Regeln sollen bei Facebook, Instagram und Threads, dem Kurznachrichtendienst von Meta, angewendet werden. 

Bereits ab Mai will der Facebook-Konzern die neuen Markierungen einführen. Meta kündigte an, den bisher restriktiveren Umgang mit von KI erstellten Beiträgen bis Juli aufgeben zu wollen.  
"
AI,Zeit,2024-04-06,https://www.zeit.de/wirtschaft/2024-04/sap-stellenabbau-deutschland-software-betriebsrat-kritik,Softwarekonzern: SAP soll in Deutschland 2.600 Stellen streichen wollen | ZEIT ONLINE,"Beim angekündigten Umbau des baden-württembergischen Softwareherstellers SAP könnten in Deutschland rund 2.600 Stellen entfallen. Diese Zahl soll aus einer internen E-Mail des europäischen Betriebsrats stammen, über die das Handelsblatt berichtet. Damit wäre Deutschland ein Schwerpunkt des geplanten Personalabbaus. Bislang war lediglich bekannt, dass von einer Umstrukturierung bei SAP weltweit 8.000 Stellen betroffen sein sollen. Wie viele davon nach SAP-Plänen komplett entfallen sollen, ist noch nicht klar.

Ein Konzernsprecher äußerte sich nicht über die konkrete Zahl. ""SAP hat im Januar ein Restrukturierungsprogramm für das ganze Unternehmen gestartet, um sich auf strategische Wachstumsfelder wie Business AI zu konzentrieren"", hieß es. AI steht für Artificial Intelligence, also künstliche Intelligenz. Die Restrukturierung betreffe 8.000 Arbeitsplätze, bekräftigte der Sprecher. Die betroffenen Mitarbeiter würden ""mit größter Sorgfalt und Einfühlungsvermögen"" behandelt, indem ihnen SAP etwa interne Stellenalternativen oder Freiwilligenprogramme anbiete.

Dem Handelsblatt-Bericht zufolge kritisiert der europäische Betriebsrat den geplanten Großumbau als Maßnahme, die in erster Linie Kosten senken solle. In der E-Mail heiße es, ""dass das Management die geschäftliche Logik nicht ausreichend begründet und keine präzisen Informationen über Ineffizienzen vorgelegt habe"". Der Programmname Next Level Transformation sei eine beschönigende Umschreibung für Personalabbau.

Demnach sollen auch in anderen europäischen Ländern Stellen gestrichen werden, insgesamt seien in Europa mehr als 4.000 Angestellte betroffen. Wie der SAP-Sprecher mitteilte, will Europas größter Softwarehersteller den gesamten Restrukturierungsprozess bis Ende des ersten Quartals 2025 abschließen.
"
AI,Zeit,2024-04-06,https://www.zeit.de/news/2024-04/06/bericht-sap-plant-abbau-von-2600-stellen-in-deutschland,Software: Bericht: SAP plant Abbau von 2600 Stellen in Deutschland | ZEIT ONLINE,"Der Softwarehersteller SAP will nach einem Zeitungsbericht als Teil seines angekündigten Großumbaus voraussichtlich 2600 Stellen in Deutschland streichen. Ein Sprecher des Konzerns äußerte sich am Samstag nicht zu der konkreten Zahl, die aus einem Bericht des «Handelsblatts» hervorgeht. Bisher war lediglich von insgesamt 8000 Stellen die Rede, die von der Umstrukturierung betroffen seien.

«SAP hat im Januar ein Restrukturierungsprogramm für das ganze Unternehmen gestartet, um sich auf strategische Wachstumsfelder wie Business AI zu konzentrieren», teilte der Sprecher mit. AI steht für Artificial Intelligence (Künstliche Intelligenz). «Diese Restrukturierung betrifft 8000 Arbeitsplätze. Wir behandeln die betroffenen Mitarbeiter mit größter Sorgfalt und Einfühlungsvermögen, indem wir ihnen unter anderem interne Stellenalternativen oder Freiwilligenprogramme anbieten.» Dabei gebe es enge Kooperationen mit den Sozialpartnern in den jeweiligen Regionen. Die meisten betroffenen Mitarbeiter werden demnach in den nächsten Wochen benachrichtigt.

Das «Handelsblatt» bezog sich auf eine interne E-Mail des europäischen Betriebsrats von SAP. Demnach kritisiert das Gremium den geplanten Großumbau als Maßnahme, die in erster Linie zur Kostensenkung gedacht sei.

In der E-Mail heiße es, «dass das Management die geschäftliche Logik nicht ausreichend begründet und keine präzisen Informationen über Ineffizienzen vorgelegt habe», schreibt die Zeitung. Der Programmname «Next Level Transformation» sei eine beschönigende Umschreibung für Personalabbau.

Laut dem Bericht sollen auch in anderen europäischen Ländern Stellen gekürzt werden. Im Verantwortungsbereich des europäischen Betriebsrats sollten rund 4100 Stellen entfallen.

Wie der SAP-Sprecher mitteilte, plant Europas größter Softwarehersteller, den gesamten Restrukturierungsprozess bis Ende des ersten Quartals 2025 weltweit abzuschließen. «Gleichzeitig werden wir weiterhin in wichtige Wachstumsbereiche investieren und erwarten, dass wir das Jahr 2024 mit stabilen Mitarbeiterzahlen beenden.»

Der Hype um Künstliche Intelligenz in der Softwarebranche hatte sich im vorvergangenen Jahr an der Veröffentlichung des Dialogsystems ChatGPT entzündet. Seither möchten alle Softwarekonzerne ein Stück vom erhofft großen Kuchen abhaben und stecken viel Geld in die Technologie.

© dpa-infocom, dpa:240406-99-581424/3
"
AI,Zeit,2024-04-04,https://www.zeit.de/digital/2024-04/ki-musik-billie-eilish-suno-generator,KI-Musik: So klingen KI-Lieder | ZEIT ONLINE,
AI,Zeit,2024-04-03,https://www.zeit.de/2024/12/alles-ueberall-auf-einmal-kuenstliche-intelligenz-buch,"""Alles überall auf einmal"": Wird Gott plötzlich wieder schöpferisch? | ZEIT ONLINE",
Artificial Intelligence,Zeit,2024-04-06,https://www.zeit.de/wirtschaft/2024-04/sap-stellenabbau-deutschland-software-betriebsrat-kritik,Softwarekonzern: SAP soll in Deutschland 2.600 Stellen streichen wollen | ZEIT ONLINE,"Beim angekündigten Umbau des baden-württembergischen Softwareherstellers SAP könnten in Deutschland rund 2.600 Stellen entfallen. Diese Zahl soll aus einer internen E-Mail des europäischen Betriebsrats stammen, über die das Handelsblatt berichtet. Damit wäre Deutschland ein Schwerpunkt des geplanten Personalabbaus. Bislang war lediglich bekannt, dass von einer Umstrukturierung bei SAP weltweit 8.000 Stellen betroffen sein sollen. Wie viele davon nach SAP-Plänen komplett entfallen sollen, ist noch nicht klar.

Ein Konzernsprecher äußerte sich nicht über die konkrete Zahl. ""SAP hat im Januar ein Restrukturierungsprogramm für das ganze Unternehmen gestartet, um sich auf strategische Wachstumsfelder wie Business AI zu konzentrieren"", hieß es. AI steht für Artificial Intelligence, also künstliche Intelligenz. Die Restrukturierung betreffe 8.000 Arbeitsplätze, bekräftigte der Sprecher. Die betroffenen Mitarbeiter würden ""mit größter Sorgfalt und Einfühlungsvermögen"" behandelt, indem ihnen SAP etwa interne Stellenalternativen oder Freiwilligenprogramme anbiete.

Dem Handelsblatt-Bericht zufolge kritisiert der europäische Betriebsrat den geplanten Großumbau als Maßnahme, die in erster Linie Kosten senken solle. In der E-Mail heiße es, ""dass das Management die geschäftliche Logik nicht ausreichend begründet und keine präzisen Informationen über Ineffizienzen vorgelegt habe"". Der Programmname Next Level Transformation sei eine beschönigende Umschreibung für Personalabbau.

Demnach sollen auch in anderen europäischen Ländern Stellen gestrichen werden, insgesamt seien in Europa mehr als 4.000 Angestellte betroffen. Wie der SAP-Sprecher mitteilte, will Europas größter Softwarehersteller den gesamten Restrukturierungsprozess bis Ende des ersten Quartals 2025 abschließen.
"
Artificial Intelligence,Zeit,2024-04-06,https://www.zeit.de/news/2024-04/06/bericht-sap-plant-abbau-von-2600-stellen-in-deutschland,Software: Bericht: SAP plant Abbau von 2600 Stellen in Deutschland | ZEIT ONLINE,"Der Softwarehersteller SAP will nach einem Zeitungsbericht als Teil seines angekündigten Großumbaus voraussichtlich 2600 Stellen in Deutschland streichen. Ein Sprecher des Konzerns äußerte sich am Samstag nicht zu der konkreten Zahl, die aus einem Bericht des «Handelsblatts» hervorgeht. Bisher war lediglich von insgesamt 8000 Stellen die Rede, die von der Umstrukturierung betroffen seien.

«SAP hat im Januar ein Restrukturierungsprogramm für das ganze Unternehmen gestartet, um sich auf strategische Wachstumsfelder wie Business AI zu konzentrieren», teilte der Sprecher mit. AI steht für Artificial Intelligence (Künstliche Intelligenz). «Diese Restrukturierung betrifft 8000 Arbeitsplätze. Wir behandeln die betroffenen Mitarbeiter mit größter Sorgfalt und Einfühlungsvermögen, indem wir ihnen unter anderem interne Stellenalternativen oder Freiwilligenprogramme anbieten.» Dabei gebe es enge Kooperationen mit den Sozialpartnern in den jeweiligen Regionen. Die meisten betroffenen Mitarbeiter werden demnach in den nächsten Wochen benachrichtigt.

Das «Handelsblatt» bezog sich auf eine interne E-Mail des europäischen Betriebsrats von SAP. Demnach kritisiert das Gremium den geplanten Großumbau als Maßnahme, die in erster Linie zur Kostensenkung gedacht sei.

In der E-Mail heiße es, «dass das Management die geschäftliche Logik nicht ausreichend begründet und keine präzisen Informationen über Ineffizienzen vorgelegt habe», schreibt die Zeitung. Der Programmname «Next Level Transformation» sei eine beschönigende Umschreibung für Personalabbau.

Laut dem Bericht sollen auch in anderen europäischen Ländern Stellen gekürzt werden. Im Verantwortungsbereich des europäischen Betriebsrats sollten rund 4100 Stellen entfallen.

Wie der SAP-Sprecher mitteilte, plant Europas größter Softwarehersteller, den gesamten Restrukturierungsprozess bis Ende des ersten Quartals 2025 weltweit abzuschließen. «Gleichzeitig werden wir weiterhin in wichtige Wachstumsbereiche investieren und erwarten, dass wir das Jahr 2024 mit stabilen Mitarbeiterzahlen beenden.»

Der Hype um Künstliche Intelligenz in der Softwarebranche hatte sich im vorvergangenen Jahr an der Veröffentlichung des Dialogsystems ChatGPT entzündet. Seither möchten alle Softwarekonzerne ein Stück vom erhofft großen Kuchen abhaben und stecken viel Geld in die Technologie.

© dpa-infocom, dpa:240406-99-581424/3
"
Artificial Intelligence,Zeit,2024-04-05,https://www.zeit.de/hamburg/2024-04/elbvertiefung-05-04-2024,Frustabbau: Wo man in Hamburg Dampf ablassen kann | ZEIT ONLINE,"heute um 9 Uhr werde
ich für eine Recherche auf der Fähre Richtung Helgoland sitzen. Bis zum letzten
Frühjahr wusste ich über die Insel ziemlich wenig. Ich stellte mir einen windigen
Fleck in der Nordsee vor, mit Geschäften, die Alkohol und Zigaretten in
Vorratspackungen verkaufen, und ein paar Kegelrobben, die ab und an ungefragt
Badegästen an den Füßen knabbern.  

Bei meinem ersten
Besuch auf der Insel vor einem Jahr merkte ich, dass es doch ein bisschen mehr
gibt: noch mehr Läden mit Schnaps und Kippen, als ich dachte, aber vor allem
auch mehr Angebote als Shoppen und Robben.  

Helgolands
Veranstaltungskalender listet regelmäßige ungewöhnliche Events. 

Ende April werden Hunderte
schwarz gekleidete Punk- und Metalfans auf der Insel anlanden, zum achten Mal
finden die Rock-’n’-Roll-Butterfahrten statt. Auf dem Line-up des Festivals
stehen dieses Jahr verheißungsvolle Bandnamen wie: Sondaschule,
Betriebsausfall, Eisenpimmel, Kotzreiz, Massendefekt und die Helgoländer
Kalkfinken.  

Falls
Helgoland diese musikalische Apokalypse übersteht, startet im Mai der
alljährliche Marathon, bei dem die Teilnehmer fünfmal die Insel umrunden. Im August ist wieder ein Rennen mit selbst gebastelten
Pappbooten geplant. Wer zuletzt versinkt, gewinnt. Es wird in diesem Jahr
außerdem eine Abba-Aktionswoche geben, das Helgoländer Oktoberfest und
natürlich immer mittendrin: Vogelfans mit riesigen Kameraobjektiven. 

Anfang der
Woche kursierte im Netz die Info, dass auf der Insel ein Thai-Box-Kampf
ausgetragen werden soll. Das stellte sich als Aprilscherz heraus, klang aber
total naheliegend bei Helgolands gewagter Angebotsmischung. Beim Einlauf der
Kämpfer hätten sie Musik von Rolf
Zuckowski spielen können, der hat voriges Jahr ein Lied über Helgoland
beziehungsweise die Reise mit dem ""Halunder Jet""
geschrieben, das sich musikalisch doch deutlich von Sondaschule und Co.
unterscheidet. 

Auf der Fähre heute erwarte ich eher ""Landungsbrücken raus"" und ""Balu"", denn es
handelt sich um eine Shuttle-Fahrt für Kettcar-Fans. Ich habe die Band für die
vorige ZEIT:Hamburg-Ausgabe zu Konzerten gegen die AfD, zu ihrer
Musical-Abneigung und zum neuen Album befragt. Heute
findet das Release-Konzert auf Helgoland stand. Davon erzähl ich ein
andermal.  

Schönen
Start ins Wochenende! 

Ihre
Viola Diem 

Wollen
Sie uns Ihre Meinung sagen, wissen Sie etwas, worüber wir berichten sollten?
Schreiben Sie uns eine E-Mail an hamburg@zeit.de. 

Laut
Hochbahn sind 87 der 92 U-Bahnhöfe in der Stadt inzwischen barrierefrei. Der Umbau der Stationen Meßberg und Saarlandstraße soll im
Sommer 2025 abgeschlossen sein. Die Haltestelle Sierichstraße folgt 2028, für die
Sternschanze gibt es noch keinen Termin. Der Stopp Kiekut soll wegen des
geringen Passagieraufkommens nicht umgebaut werden. 

Im Zusammenhang mit dem Amoklauf bei einem Treffen der Zeugen Jehovas vor über einem Jahr hat die Staatsanwaltschaft die Ermittlungen
gegen einen ehemaligen Mitarbeiter der Hamburger Waffenbehörde eingestellt.
Der Beamte war vor der Tat durch ein anonymes Schreiben auf psychische Probleme des späteren Attentäters
hingewiesen worden, hatte dies jedoch weder weitergeleitet noch
dokumentiert. Wir
berichteten dazu ausführlich. Die
Ermittlungen der Staatsanwalt haben ergeben, dass selbst bei ordnungsgemäßem
Handeln des Beschuldigten nicht zwingend die Waffe des späteren Attentäters
hätte eingezogen werden müssen. Zwar habe der Beamte gegen seine
Dienstpflichten verstoßen, hieß es in der Erklärung, der Vorwurf fahrlässiger
Tötung und fahrlässiger Körperverletzung im Amt ließ sich jedoch nicht mit
hinreichender Sicherheit erhärten.  

Die Schulbehörde, das Artificial
Intelligence Center Hamburg und die Kompetenzstelle KI des Landesinstituts für
Lehrerbildung und Schulentwicklung haben Leitlinien für den Einsatz künstlicher
Intelligenz an Schulen entwickelt. Der Fokus liegt auf Datenschutz,
rechtlichen Aspekten und der Frage, wie KI im Unterricht lernfördernd
eingesetzt werden kann. Interessant ist dabei etwa, wie Aufgaben gestaltet
werden müssen, damit die Leistung der Schülerinnen und Schüler deutlich von der
einer KI unterscheidbar bleibe.

• Von
heute 22 Uhr bis Montagfrüh, 5 Uhr, wird die A7 Richtung Norden zwischen
Hamburg-Heimfeld und Bahrenfeld gesperrt. Von Samstagabend bis
Sonntagmorgen ist außerdem der Elbtunnel zwischen Volkspark und Heimfeld in
südlicher Richtung dicht • Ein brennendes Auto in einer Tiefgarage
verursachte gestern eine größere Rauchwolke in Poppenbüttel. In einem
Supermarkt mussten mehrere Menschen in Sicherheit gebracht werden • Die
Preise für Ticket-Abos von Elbphilharmonie und Leiszhalle werden angehoben.
Deutliche Unterschiede gibt es etwa beim Jugendabo, bei dem die Tickets bislang
pauschal 12 Euro kosteten, ab der Saison 2024/25 werden es 60 Prozent des
jeweiligen Sitzplatzes sein

Manchmal
weiß man nicht, wohin mit seinen Gefühlen. Dann sollte man nicht lange
überlegen müssen. Die ZEIT:Hamburg-Redaktion hat vier Angebote zum schnellen
Frustabbau getestet: Lesen Sie hier den Tipp von Annika Lasarzik: Autos
zertrümmern. 

""Am Tag der Zerstörung
werde ich von Lärm geweckt: Auf der Baustelle neben meiner Wohnung fräst sich
eine Kreissäge durch mein Trommelfell. In der Küche zerspringt mir ein Glas,
der Kaffee ist alle, ein Interviewpartner sagt ab. Normalerweise würde ich
jetzt ins Büro fahren und dort ein Lächeln aufsetzen. Aber nicht heute! Die Wut
muss raus! Nur gut, dass sie nicht meine Kollegen trifft, sondern einen roten
VW Lupo auf einem Schrottplatz in Norderstedt. Für 189 Euro kann man hier zum
""Car Crasher"" werden und eines der vielen ausrangierten Autos zerstören. 

Als ich meine Freundin
Sandra und mich in der Werkstatt anmelde, legt der Mann am Tresen mehrere
Hämmer, Äxte und Arbeitshandschuhe in einen Einkaufswagen. Schutzbrillen gibt
es zum Preis von je drei Euro. Dann schieben wir unser Waffenarsenal vorbei an langen
Reihen aufeinandergestapelter Autos. Als wir ""unser"" Auto in einer Ecke
entdecken, greife ich zum größten Hammer und fühle mich wie die
Kriegerprinzessin Xena – aber das Gefühl ist schnell verflogen, ich kann den
Hammer kaum heben, Sandra lacht. Jetzt erst recht: Ich klettere aufs Autodach,
hole mit aller Kraft aus und lasse den Hammer auf die Karosserie plumpsen. Eine
winzige Delle im Lack. Mehr nicht? Kann doch nicht sein! Ich schlage noch mal
auf das Dach ein und noch mal. Sandra kümmert sich derweil um die
Seitenscheiben, das Glas zerbirst, ein Splitter trifft ihren Finger –
Handschuhe vergessen. Blut tropft auf den Asphalt. Ich nehme die Axt und
malträtiere mit dem dumpfen Ende die Motorhaube. Jetzt komme ich in Fahrt, dong,
dong, dong, eine Delle nach der anderen schlage ich in den roten Lack. Meine
Muskeln brennen, meine Arme werden schwerer, doch ich bin wie ferngesteuert.
Diese Schrottlaube hat keine Höflichkeit verdient! Als mein Werk vollendet ist,
bin ich völlig erschöpft und gestärkt zugleich. Heute kann mich nichts mehr
stressen. Und wenn doch, schlage ich eben noch ein Auto kaputt."" 

Über die Wirksamkeit von
Action-Painting, Axt werfen oder Thaiboxen lesen Sie weiter in der ungekürzten
Fassung auf ZEIT ONLINE. 

Zum vollständigen
Artikel

""Jonathan
Tah hat das Format zu einem der besten acht Verteidiger der
Welt, dümpelte aber zehn Jahre so rum. Es ist auch eine Geschichte über
verschenkte Zeit."" 

Oliver
Fritsch, Sportredakteur bei ZEIT ONLINE 

In der neuen Folge des
Fußballpodcasts ""Kicken kann er"" geht es um den Innenverteidiger
Jonathan Tah. Der Nationalspieler ist in Hamburg geboren, Altona 93 gehörte zu
seinen ersten Vereinen, ehe er zum HSV kam. Heute steht Tah mit Bayer 04
Leverkusen kurz davor, die deutsche Meisterschaft und den DFB-Pokal zu holen.
In dem Podcast mit den Hosts Oliver Fritsch und Fabian Scheler geht es alle
zwei Wochen um die größten Fußballer und Fußballerinnen unserer Zeit. Dass in
dem Zusammenhang über Jonathan Tah gesprochen wird, ist nicht
selbstverständlich. Bereits vor zehn Jahren hatte man ihm eine große Karriere
vorausgesagt, aber so richtig geklappt hat es erst jetzt. Darum geht es viel um
die Frage: Wie lange kann man als
hochtalentierter Fußballer im Spitzenfußball auf den Durchbruch warten?    

Wie
Tah fußballerisch herausragt, und warum diese Zeit für ihn so erfolgreich ist, hören
Sie hier:  

Zum Podcast   

Er hat ganz klein angefangen – und auch wieder nicht. Als André
Stolle vor drei Jahren seine Cantine Papa Lisbeth in der Deichstraße aufmachte,
gab es bei ihm vor allem veredelte Fischbrötchen. Doch schon damals ließ er
sich bereitwillig als Sternekoch bezeichnen, in Erinnerung an Meriten in teils
berühmten Häusern. Nun schließt sich der Kreis: Der Guide Michelin verlieh
seinem Restaurant (wie dem Atlantic und dem Petit Amour) zum ersten Mal einen
Stern. 

The Lisbeth, wie es
inzwischen heißt, hat noch immer den Charme des Provisorischen. Man will hier
keine Show abliefern, sondern ehrliche Küche und Gastlichkeit für jedermann –
jedermann, der 112 Euro oder mehr ausgeben mag fürs Abendmenü. 

Stolle kocht norddeutsch, wie sich das für das denkmalgeschützte
Setting im alten Kaufmannshaus gehört. Modernisiert natürlich: Das Labskaus
heißt ""Dont’t call it Labskaus"" und ist
in viele Elemente zerlegt, darunter leicht gebeizter Hering, Rote-Bete-Jus,
Eigelbbrösel und, sehr spannend, Tataki vom Rinderherz. Nimmt man alles auf die
Gabel, entsteht der bekannte Geschmack. 

Doch anders als man es zum Beispiel aus dem inzwischen
geschlossenen Vlet kennt, bemüht sich Stolle gar nicht um eine Verfeinerung. Im
Gegenteil, er zelebriert die Deftigkeit. Eine Ahnung gibt schon das Amuse-Bouche,
eine Crème vom Erbseneintopf, die auch genauso schmeckt. Beim Hauptgang kommt
es dann richtig dicke, er heißt nicht umsonst Sauerei. Der erste Teil besteht
aus hauchdünnen krossen Schweinebauchscheiben, die in einem essigsauren Brei
von Grünkohl stecken. Dann kommen auch noch Kasseler und Eisbein in
Gourmetqualität samt süßer Schweinesoße, frittiertem Grünkohl und knusprig
ausgebratener Schwarte. Das fordert einen durch seine Wucht und ruft zugleich
Kindheitserinnerungen wach. Man kommt hier beim Essen ins Grübeln; und so etwas
schaffen nur richtig gute Köche. 

The Lisbeth, Deichstraße 32, Neustadt. Tel. 36096767

Michael Allmaier 

Begleitend zur
Ausstellung der Künstlerin Claudia Andujar, ""The End of the World"" im Phoxxi,
Deichtorhallen, zeigt das Metropolis im April und Mai einige Filme von Luiz
Bolognesi und Heide Specogna.
Zum
Beispiel ""The Last Forest"" des brasilianischen Filmemachers und Anthropologen
Luiz Bolognesi über die indigene Gemeinschaft der Yanomami. 

""The
Last Forest"", 8.4., 19.30 Uhr, OV mit englischen
Untertiteln; Metropolis, Kleine Theaterstraße 10 

Vor
Kurzem fuhr ich mit meinem fünfjährigen Neffen durch die Straßen unserer
schönen Stadt. Nach einer Weile sagt er nachdenklich zu mir: ""Mein Papa hat
immer Pech beim Autofahren. Er trifft immer auf die doofen Autofahrer; er muss
immer hupen."" Nach einer kurzen sinnierenden Pause: ""Mama hat immer Glück. Sie
muss nie hupen.""   

Gehört
von Barbara Grieschat 

""Wir werden von der
Realität eingeholt"" (Z+) – Das neue Album der Band Kettcar
besteht zum Großteil aus politischen Songs. Ein Gespräch über die verrückte
Welt, Hamburg und Saufen im Tourbus

Wütend? Ich? (Z+) – Manchmal weiß
man nicht, wohin mit seinen Gefühlen. Dann sollte man nicht lange überlegen
müssen. Vier Angebote zum schnellen Frustabbau 

Warum erst jetzt,
Jonathan Tah? – Jonathan Tah wurde vor mehr als einem Jahrzehnt eine
große Karriere vorausgesagt, doch richtig begonnen hat sie erst jetzt. Was ist
nur passiert? Der Fußballpodcast 
"
Artificial Intelligence,Zeit,2024-04-04,https://www.zeit.de/news/2024-04/04/schulbehoerde-legt-leitlinien-fuer-kuenstliche-intelligenz-vor,Schulen: Schulbehörde legt Leitlinien für Künstliche Intelligenz vor | ZEIT ONLINE,"Hamburgs Schulbehörde hat Leitlinien für den Einsatz Künstlicher Intelligenz (KI) an den Schulen der Hansestadt vorgelegt. Entwickelt worden seien diese von der Kompetenzstelle KI des Landesinstituts für Lehrerbildung und Schulentwicklung Hamburg, von der Behörde und dem Artificial Intelligence Center Hamburg, teilte die Behörde am Donnerstag mit. Die «Leitlinien für den Einsatz von KI-Systemen in Schule und Unterricht» seien ausschließlich online unter www.li.hamburg.de/ki abrufbar und drehten sich um den lernförderlichen Einsatz von Künstlicher Intelligenz im Unterricht, rechtliche Aspekte sowie um Fragen des Datenschutzes.

«Ich möchte alle Lehrkräfte dazu ermutigen, KI-Anwendungen im Unterricht zu erproben und zu reflektieren, um die Schülerinnen und Schüler zu einem kritischen und kompetenten Umgang mit KI zu befähigen», sagte Schulsenatorin Ksenija Bekeris (SPD). Sie hoffe, dass die Leitlinien als hilfreiches Instrument für die pädagogische Arbeit betrachtet würden. Dort werden laut Behörde unter anderem generative KI-Modelle hinsichtlich ihrer Funktionen und Leistungen erörtert und der praktische Einsatz in den Schulen diskutiert. Dabei gehe es etwa um die Frage, wie Aufgaben gestaltet werden müssen, damit die Eigenleistung der Schülerinnen und Schüler sichtbar und von den von der KI erbrachten Inhalten unterscheidbar bleibe.

Der rechtliche Teil befasse sich wiederum unter anderem mit der Frage, wie Leistungen beim Einsatz von KI bewertet werden können und wie etwa bei Prüfungen mit einem nicht erlaubten Einsatz von KI-Anwendungen umgegangen werden sollte. Bekeris betonte: «Die Leitlinien wurden aufgrund der dynamischen Entwicklungen bewusst nicht als PDF oder Druckerzeugnis veröffentlicht, sondern ausschließlich digital.» So könnten Änderungen rasch aktualisiert und die Leitlinien sukzessive um Praxisbeispiele, Anwendungsmöglichkeiten und weitere Materialien für die Lehrkräfte ergänzt werden.

© dpa-infocom, dpa:240404-99-562886/2
"
KI,Zeit,2024-04-06,https://www.zeit.de/digital/2024-04/meta-facebook-ki-inhalte-warnhinweis,Künstliche Intelligenz: Meta will mehr KI-Inhalte mit Warnhinweisen auf Plattformen lassen | ZEIT ONLINE,"Anstatt mit künstlicher Intelligenz erstellte oder manipulierte Fotos und Videos zu löschen, will der Meta-Konzern künftig mehr solcher Inhalte mit Warnhinweisen versehen. Im Gegenzug sollen sie dann auf Meta-Plattformen wie Facebook stehen bleiben dürfen. Damit folgt Meta einer Empfehlung seines unabhängigen Aufsichtsgremiums. Dieses hatte eine Einschränkung der Redefreiheit befürchtet. 

Inhalte sollen anhand von Metadaten in deren Dateien unter anderem mit der Kennzeichnung ""Made with AI"" markiert werden. Die Lockerung gelte jedoch nicht für Inhalte, die Hassrede, Mobbing oder Falschmeldungen zu Wahlen beinhalten, teilte der Konzern am Freitag mit.  

Mithilfe von künstlicher Intelligenz und fortschreitender Technologie lassen sich immer leichter auch Falschmeldungen produzieren und so die öffentliche Meinung manipulieren. Meta will daher in Fällen, in denen diese Gefahr besonders groß ist, entsprechende Warnhinweise besonders deutlich sichtbar platzieren.

Zudem soll eine interne Technologie genutzt werden, um KI-Inhalte automatisch zu erkennen. Die neuen Regeln sollen bei Facebook, Instagram und Threads, dem Kurznachrichtendienst von Meta, angewendet werden. 

Bereits ab Mai will der Facebook-Konzern die neuen Markierungen einführen. Meta kündigte an, den bisher restriktiveren Umgang mit von KI erstellten Beiträgen bis Juli aufgeben zu wollen.  
"
KI,Zeit,2024-04-05,https://www.zeit.de/politik/2024-04/ki-gaza-krieg-israel-lavender-nachrichtenpodcast,"KI im Gaza-Krieg: Entscheidet eine KI, wo in Gaza Bomben fallen? | ZEIT ONLINE","Israel soll eine künstliche Intelligenz nutzen, um über Bombenangriffe im Gazastreifen zu entscheiden. Dabei sollen viele zivile Opfer in Kauf genommen worden seien. Das berichten der Guardian und das +972 Magazine mit Berufung auf israelische Geheimdienstoffiziere. Das auf den Namen Lavender getaufte KI-System soll große Datenmengen verarbeiten, um vor allem Hamas-Kämpfer zu identifizieren. Zwischenzeitlich soll die KI so 37.000 Palästinenser mit vermeintlichen Hamas-Verbindungen identifiziert haben. Zwischen 15 und 20 zivile Opfer seien in Kauf genommen worden, um einen Terroristen zu töten, und die Empfehlungen der KI seien leichtfertig angenommen worden, heißt es in den Berichten. Die israelische Armee widerspricht den Berichten teilweise: Lavender sei lediglich eine Datenbank für Querverweise zu Geheimdienstquellen. Wird KI tatsächlich genutzt, um Terroristen zu identifizieren? Und welche Rolle spielt künstliche Intelligenz generell in Kriegen? Diese und weitere Fragen beantwortet Ulrike Franke. Sie arbeitet beim Council on Foreign Relations und ist Expertin für Militärtechnologien.

Etwa ein Drittel des Deutschlands ist von Wald bedeckt. Aber der Klimawandel gefährdet die Wälder. Forscherinnen und Forscher der Technischen Universität München haben nun erstmals Daten gesammelt, die zeigen, wie sich die Erderhitzung hierzulande auf die Bäume auswirken wird. Besonders gefährdet ist demnach die schnell wachsende Fichte, die ein Viertel des Baumbestandes ausmacht. Bereits bei zwei Grad Erwärmung gäbe es Ende des Jahrhunderts kaum mehr Lebensräume für sie. Generell dürften besonders Nadelbäume aus den Wäldern verdrängt werden, Laubbäume kommen hingegen besser mit Trockenheit zurecht. Welche Baumarten Alternativen sein könnten und wie sich der Wald dadurch verändern wird, weiß Claudia Vallentin, Wissensredakteurin bei ZEIT ONLINE.

Und sonst so? 30. Todestag von Kurt Cobain

Moderation und Produktion: Elise Landschek

Redaktion: Jannis Carmesin

Mitarbeit: Anne Schwedt, Helena Schmidt und Benjamin Probst

Alle Folgen unseres Podcasts finden Sie hier. Fragen, Kritik, Anregungen? Sie erreichen uns unter wasjetzt@zeit.de.
"
KI,Zeit,2024-04-04,https://www.zeit.de/news/2024-04/04/verkehrszaehlung-mit-kamera-und-ki-statt-mit-klemmbrettern,Wiesbaden: Verkehrszählung mit Kamera und KI statt mit Klemmbrettern | ZEIT ONLINE,"Hessen setzt bei der neuen bundesweiten Verkehrszählung erstmals generell auf Videokameras und Künstliche Intelligenz (KI) statt auf zählende Menschen auf Campingstühlen mit Klemmbrettern am Straßenrand. Zusammen mit Rheinland-Pfalz und Nordrhein-Westfalen übernehme das Bundesland mit dieser flächendeckenden Aktion eine Führungsrolle im Auftrag des Bundesministeriums für Digitales und Verkehr, teilte die Landesbehörde Hessen Mobil in Wiesbaden am Donnerstag mit.

An insgesamt rund 3000 Zählstellen an Bundes-, Landes- und Kreisstraßen im Bundesland sollen zeitweise mit Kameras etwa auf Brücken Fahrzeuge statistisch registriert werden. Diese erfassen weder Insassen noch Kennzeichen, sondern nur Fahrzeugtypen. Die Zählungen sollen noch in diesem Monat beginnen und im Herbst 2025 abgeschlossen sein.

Laut einem Sprecher von Hessen Mobil ersetzen die Kameras in Hessen eine dreistellige Zahl Mitarbeiter. Die Videotechnik könne Tag und Nacht aktiv sein - dies und der ungewöhnlich lange Aktionszeitraum bis 2025 erhöhten die Datenqualität.

Die ermittelten Verkehrsbelastungen dienen Hessen Mobil zufolge als Datengrundlage für bundes- und landesweite Straßenplanungen, Straßenbau und Verkehrsmanagement. Überdies können die von KI ausgewerteten Daten von  Forschungsinstitutionen, Verbänden und Bürgerinitiativen für Lärm- und Emissionsberechnungen sowie für Mobilitätsstudien genutzt werden.

Vorsichtshalber weist Hessen Mobil darauf hin, dass für die Montage der Kameras Mitarbeiter auf Brücken aktiv seien, «die keine Steinewerfer sind, sondern im Auftrag des Landes Hessen agieren».

© dpa-infocom, dpa:240404-99-559655/2
"
KI,Zeit,2024-04-04,https://www.zeit.de/digital/2024-04/ki-musik-billie-eilish-suno-generator,KI-Musik: So klingen KI-Lieder | ZEIT ONLINE,
KI,Zeit,2024-04-03,https://www.zeit.de/2024/12/alles-ueberall-auf-einmal-kuenstliche-intelligenz-buch,"""Alles überall auf einmal"": Wird Gott plötzlich wieder schöpferisch? | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-04-02,https://www.zeit.de/digital/internet/2024-03/ki-videos-instagram-deepfake-schlantologie-satire,"Künstliche Intelligenz: ""Deepfakes kommen, ob wir wollen oder nicht"" | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-04-02,https://www.zeit.de/kultur/musik/2024-04/kuenstliche-intelligenz-musik-einsatz-offener-brief-billie-eilish-stevie-wonder,Künstliche Intelligenz: 200 Stars kritisieren zunehmenden Einsatz von KI in der Musikbranche | ZEIT ONLINE,"Mehr als 200 Musikstars sehen im zunehmenden Einsatz künstlicher Intelligenz in ihrer Branche eine Attacke auf die menschliche Kreativität. Techfirmen, Entwickler, Onlineplattformen und digitale Musikdienste müssten aufhören, KI zu nutzen und damit ""die Rechte von menschlichen Künstlern zu verletzen und diese zu entwerten"", hieß es in einem offenen Brief. 

Das Schreiben wurde bei den Unternehmen von der Artist Rights Alliance eingereicht, einer von Künstlern geführten Organisation, die sich für die Rechte von Musiker im digitalen Raum einsetzt. Auf X teilte die Organisation eine Liste mit allen Unterstützerinnen und Unterstützern. Darin heißt es: ""KI hat ein enormes Potenzial als Werkzeug für menschliche Kreativität – aber wenn sie verantwortungslos eingesetzt wird, stellt sie eine existenzielle Bedrohung für unsere Kunst dar.""

Zu den Unterzeichnenden gehören unter anderem Stevie Wonder und Billie Eilish. Aber auch Künstlerinnen wie Nicki Minaj, Camila Cabello, R.E.M., Jon Bon Jovi, Katy Perry, Smokey Robinson und J Balvin unterzeichneten den Brief. 

Zwar wurden in dem Schreiben auch die kreativen Möglichkeiten anerkannt, die neue KI-Technologien bieten. Doch gebe es einige Bedrohungen für die menschengemachte Kunst: etwa KI-Modelle, die – ohne Erlaubnis – an bestehenden Werken geschult werden, um Künstler zu ersetzen. In der Folge würden ""Tantiementöpfe, aus denen Künstler bezahlt würden, erheblich verwässert"". 

""Dieser Angriff auf die menschliche Kreativität muss aufhören"", schreiben die Künstlerinnen und Künstler. Der ""raubtierhafte Einsatz"" von künstlicher Intelligenz stehle die Stimmen professioneller Künstler, verletze die Rechte von Schöpferinnen und zerstöre das ""Musikökosystem"".
"
Künstliche Intelligenz,Zeit,2024-03-30,https://www.zeit.de/kultur/literatur/2024-03/amazon-biografien-kuenstliche-intelligenz-helen-b-joplin/seite-2,"Künstliche Intelligenz: ""Ein Problem mit diesem Produkt melden"" | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-03-30,https://www.zeit.de/kultur/literatur/2024-03/amazon-biografien-kuenstliche-intelligenz-helen-b-joplin,Künstliche Intelligenz: Die kurze Karriere der Meisterbiografin Helen B. Joplin | ZEIT ONLINE,
Künstliche Intelligenz,Zeit,2024-03-30,https://www.zeit.de/digital/2024-03/ki-openai-tool-voice-engine-stimme-klonen,Künstliche Intelligenz: OpenAI stellt Technologie zum Klonen von Stimmen vor | ZEIT ONLINE,"Das US-Unternehmen OpenAI hat ein neues Programm zum Klonen echter Stimmen
vorgestellt. OpenAI präsentiert auf
seiner Website mehrere Sprachaufnahmen, die mithilfe der Software Voice Engine entstanden
seien. Demnach konnte Voice Engine nach Hochladen eines 15-sekündigen Audios einer
menschlichen Stimme diese imitieren, einen Text vorlesen und ihn auch
übersetzen. Somit könnten Nutzer unter anderem eine Tonaufnahme ihrer Stimme auf
Chinesisch oder Französisch erstellen. 

Im September hatte das
Unternehmen, das auch den Chatbot ChatGPT entwickelt hat, mit HeyGen ein Tool vorgestellt, das Videos mithilfe von künstlicher
Intelligenz übersetzen und Lippenbewegungen anpassen kann. Laut OpenAI
greift HeyGen auf die Technologie von Voice Engine zurück. 

Angesichts der Missbrauchsgefahr soll Voice Engine nach Unternehmensangaben zunächst nicht frei zugänglich sein. OpenAI will das Produkt
mit Testern prüfen. 

""Wir verstehen, dass die
Erzeugung von Sprache, die den Stimmen von Menschen ähnelt, schwerwiegende
Risiken hat, die insbesondere in einem Wahljahr im Fokus stehen"", teilte das Unternehmen mit. In den USA wird am 5. November ein neuer Präsident gewählt.
Auch in zahlreichen anderen Ländern finden in diesem Jahr wichtige Wahlen statt.  

Im US-Staat New Hampshire
ermitteln die Behörden derzeit zu Anrufen, bei denen die Stimme von Präsident Joe Biden
mithilfe einer KI imitiert wurde, um Menschen von der Stimmabgabe bei der
dortigen Vorwahl im Januar abzuhalten.
"
AI,Zeit,2024-04-02,https://www.zeit.de/kultur/musik/2024-04/kuenstliche-intelligenz-musik-einsatz-offener-brief-billie-eilish-stevie-wonder,Künstliche Intelligenz: 200 Stars kritisieren zunehmenden Einsatz von KI in der Musikbranche | ZEIT ONLINE,"Mehr als 200 Musikstars sehen im zunehmenden Einsatz künstlicher Intelligenz in ihrer Branche eine Attacke auf die menschliche Kreativität. Techfirmen, Entwickler, Onlineplattformen und digitale Musikdienste müssten aufhören, KI zu nutzen und damit ""die Rechte von menschlichen Künstlern zu verletzen und diese zu entwerten"", hieß es in einem offenen Brief. 

Das Schreiben wurde bei den Unternehmen von der Artist Rights Alliance eingereicht, einer von Künstlern geführten Organisation, die sich für die Rechte von Musiker im digitalen Raum einsetzt. Auf X teilte die Organisation eine Liste mit allen Unterstützerinnen und Unterstützern. Darin heißt es: ""KI hat ein enormes Potenzial als Werkzeug für menschliche Kreativität – aber wenn sie verantwortungslos eingesetzt wird, stellt sie eine existenzielle Bedrohung für unsere Kunst dar.""

Zu den Unterzeichnenden gehören unter anderem Stevie Wonder und Billie Eilish. Aber auch Künstlerinnen wie Nicki Minaj, Camila Cabello, R.E.M., Jon Bon Jovi, Katy Perry, Smokey Robinson und J Balvin unterzeichneten den Brief. 

Zwar wurden in dem Schreiben auch die kreativen Möglichkeiten anerkannt, die neue KI-Technologien bieten. Doch gebe es einige Bedrohungen für die menschengemachte Kunst: etwa KI-Modelle, die – ohne Erlaubnis – an bestehenden Werken geschult werden, um Künstler zu ersetzen. In der Folge würden ""Tantiementöpfe, aus denen Künstler bezahlt würden, erheblich verwässert"". 

""Dieser Angriff auf die menschliche Kreativität muss aufhören"", schreiben die Künstlerinnen und Künstler. Der ""raubtierhafte Einsatz"" von künstlicher Intelligenz stehle die Stimmen professioneller Künstler, verletze die Rechte von Schöpferinnen und zerstöre das ""Musikökosystem"".
"
AI,Zeit,2024-03-30,https://www.zeit.de/kultur/literatur/2024-03/amazon-biografien-kuenstliche-intelligenz-helen-b-joplin/seite-2,"Künstliche Intelligenz: ""Ein Problem mit diesem Produkt melden"" | ZEIT ONLINE",
AI,Zeit,2024-03-30,https://www.zeit.de/digital/2024-03/ki-openai-tool-voice-engine-stimme-klonen,Künstliche Intelligenz: OpenAI stellt Technologie zum Klonen von Stimmen vor | ZEIT ONLINE,"Das US-Unternehmen OpenAI hat ein neues Programm zum Klonen echter Stimmen
vorgestellt. OpenAI präsentiert auf
seiner Website mehrere Sprachaufnahmen, die mithilfe der Software Voice Engine entstanden
seien. Demnach konnte Voice Engine nach Hochladen eines 15-sekündigen Audios einer
menschlichen Stimme diese imitieren, einen Text vorlesen und ihn auch
übersetzen. Somit könnten Nutzer unter anderem eine Tonaufnahme ihrer Stimme auf
Chinesisch oder Französisch erstellen. 

Im September hatte das
Unternehmen, das auch den Chatbot ChatGPT entwickelt hat, mit HeyGen ein Tool vorgestellt, das Videos mithilfe von künstlicher
Intelligenz übersetzen und Lippenbewegungen anpassen kann. Laut OpenAI
greift HeyGen auf die Technologie von Voice Engine zurück. 

Angesichts der Missbrauchsgefahr soll Voice Engine nach Unternehmensangaben zunächst nicht frei zugänglich sein. OpenAI will das Produkt
mit Testern prüfen. 

""Wir verstehen, dass die
Erzeugung von Sprache, die den Stimmen von Menschen ähnelt, schwerwiegende
Risiken hat, die insbesondere in einem Wahljahr im Fokus stehen"", teilte das Unternehmen mit. In den USA wird am 5. November ein neuer Präsident gewählt.
Auch in zahlreichen anderen Ländern finden in diesem Jahr wichtige Wahlen statt.  

Im US-Staat New Hampshire
ermitteln die Behörden derzeit zu Anrufen, bei denen die Stimme von Präsident Joe Biden
mithilfe einer KI imitiert wurde, um Menschen von der Stimmabgabe bei der
dortigen Vorwahl im Januar abzuhalten.
"
KI,Zeit,2024-04-02,https://www.zeit.de/digital/internet/2024-03/ki-videos-instagram-deepfake-schlantologie-satire,"Künstliche Intelligenz: ""Deepfakes kommen, ob wir wollen oder nicht"" | ZEIT ONLINE",
KI,Zeit,2024-04-02,https://www.zeit.de/news/2024-04/02/ki-experte-sieht-grosse-chance-in-neuem-gesetz,Forschung: KI: Experte sieht große Chance in neuem Gesetz | ZEIT ONLINE,"Der Greifswalder Bioinformatiker Lars Kaderali setzt für die Entwicklung Künstlicher Intelligenz (KI) für die Medizin große Hoffnung in ein neues Landesgesetz. Bei dem kürzlich in erster Lesung im Landtag behandelten Gesetz zur Stärkung der Gesundheitsforschung handele es sich um einen Paradigmenwechsel, sagte er. Damit könne man absehbar viele Daten etwa zur Entwicklung von KI-Anwendungen nutzen.

Das Gesetz sehe für den Zugriff auf Patientendaten für die Forschung statt der bisherigen Einwilligungs- eine Widerspruchslösung vor. Derzeit sei die Einholung der Einwilligung so aufwendig, dass dies praktisch nicht passiere. Die neue Lösung sehe eine weniger aufwendige Aufklärung und gegebenenfalls einen aktiven Widerspruch vor.

«Das heißt, für uns ist das ein ganz wichtiger Gesetzentwurf.» Bislang kämen KI-Modelle vor allem aus den USA und China, passten dann aber mitunter nicht ideal auf die hiesige Bevölkerung. Kaderali spricht auch von einem möglichen Wirtschaftsfaktor für das Land. «Ich könnte mir sogar vorstellen, dass das auch bundesweit Interesse auf sich zieht und sich Unternehmen gezielt dann hier in Mecklenburg-Vorpommern ansiedeln, wenn es die Möglichkeit gibt, dort mit solchen Daten zu forschen.»

© dpa-infocom, dpa:240402-99-533765/2
"
KI,Zeit,2024-04-02,https://www.zeit.de/news/2024-04/02/buschmann-kennzeichnungspflicht-fuer-ki-bilder,Internet: Buschmann: Kennzeichnungspflicht für KI-Bilder | ZEIT ONLINE,"Bundesjustizminister Marco Buschmann hat sich für eine Kennzeichnungspflicht für mit Künstlicher Intelligenz (KI) erzeugte Bilder und Filme ausgesprochen. «Bildmaterial, das durch Künstliche Intelligenz hergestellt wurde, sollte meiner Meinung nach als solches kenntlich gemacht werden müssen. Denn Bilder vermitteln das Gefühl von Authentizität. Das kann missbraucht werden», sagte der FDP-Politiker dem Redaktionsnetzwerk Deutschland (RND).

Eine Kennzeichnung stehe kreativer Arbeit nicht im Weg. «Aber es würde einfacher, zu erkennen, ob es sich bei einem Bild eher um ein Kunstwerk oder um eine Abbildung der Wirklichkeit handelt.»

Auf die Frage, ob er juristischen Regelungsbedarf gegen Propaganda, Hetze und Falschmeldungen in sozialen Netzwerken wie Tiktok sehe, sagte Buschmann: «Wir wollen sicher nicht verbieten, dass in diesen Netzwerken Meinungen geäußert werden - und die können auch abseitig oder unvernünftig scheinen.» Die Meinungsfreiheit sei ein hohes Gut und gelte auch in den Netzwerken.

«Aber es gibt eben Grenzen: Beleidigung, Bedrohung, Aufruf zur Hatz gegen Menschen, die Veröffentlichung sogenannter Feindeslisten, verfassungsfeindliche Propaganda sind auch im Internet Straftaten. Da müssen wir geltendes Recht durchsetzen.» Mit rechtsstaatlichem Druck sei es gelungen, dass zum Beispiel der Anbieter Telegram strafbare Inhalte entferne. «Telegram hat Kanäle gelöscht und Gruppen geschlossen, in denen solche Inhalte geteilt wurden.»

© dpa-infocom, dpa:240402-99-531744/2
"
KI,Zeit,2024-04-02,https://www.zeit.de/kultur/musik/2024-04/kuenstliche-intelligenz-musik-einsatz-offener-brief-billie-eilish-stevie-wonder,Künstliche Intelligenz: 200 Stars kritisieren zunehmenden Einsatz von KI in der Musikbranche | ZEIT ONLINE,"Mehr als 200 Musikstars sehen im zunehmenden Einsatz künstlicher Intelligenz in ihrer Branche eine Attacke auf die menschliche Kreativität. Techfirmen, Entwickler, Onlineplattformen und digitale Musikdienste müssten aufhören, KI zu nutzen und damit ""die Rechte von menschlichen Künstlern zu verletzen und diese zu entwerten"", hieß es in einem offenen Brief. 

Das Schreiben wurde bei den Unternehmen von der Artist Rights Alliance eingereicht, einer von Künstlern geführten Organisation, die sich für die Rechte von Musiker im digitalen Raum einsetzt. Auf X teilte die Organisation eine Liste mit allen Unterstützerinnen und Unterstützern. Darin heißt es: ""KI hat ein enormes Potenzial als Werkzeug für menschliche Kreativität – aber wenn sie verantwortungslos eingesetzt wird, stellt sie eine existenzielle Bedrohung für unsere Kunst dar.""

Zu den Unterzeichnenden gehören unter anderem Stevie Wonder und Billie Eilish. Aber auch Künstlerinnen wie Nicki Minaj, Camila Cabello, R.E.M., Jon Bon Jovi, Katy Perry, Smokey Robinson und J Balvin unterzeichneten den Brief. 

Zwar wurden in dem Schreiben auch die kreativen Möglichkeiten anerkannt, die neue KI-Technologien bieten. Doch gebe es einige Bedrohungen für die menschengemachte Kunst: etwa KI-Modelle, die – ohne Erlaubnis – an bestehenden Werken geschult werden, um Künstler zu ersetzen. In der Folge würden ""Tantiementöpfe, aus denen Künstler bezahlt würden, erheblich verwässert"". 

""Dieser Angriff auf die menschliche Kreativität muss aufhören"", schreiben die Künstlerinnen und Künstler. Der ""raubtierhafte Einsatz"" von künstlicher Intelligenz stehle die Stimmen professioneller Künstler, verletze die Rechte von Schöpferinnen und zerstöre das ""Musikökosystem"".
"
KI,Zeit,2024-03-30,https://www.zeit.de/digital/2024-03/ki-openai-tool-voice-engine-stimme-klonen,Künstliche Intelligenz: OpenAI stellt Technologie zum Klonen von Stimmen vor | ZEIT ONLINE,"Das US-Unternehmen OpenAI hat ein neues Programm zum Klonen echter Stimmen
vorgestellt. OpenAI präsentiert auf
seiner Website mehrere Sprachaufnahmen, die mithilfe der Software Voice Engine entstanden
seien. Demnach konnte Voice Engine nach Hochladen eines 15-sekündigen Audios einer
menschlichen Stimme diese imitieren, einen Text vorlesen und ihn auch
übersetzen. Somit könnten Nutzer unter anderem eine Tonaufnahme ihrer Stimme auf
Chinesisch oder Französisch erstellen. 

Im September hatte das
Unternehmen, das auch den Chatbot ChatGPT entwickelt hat, mit HeyGen ein Tool vorgestellt, das Videos mithilfe von künstlicher
Intelligenz übersetzen und Lippenbewegungen anpassen kann. Laut OpenAI
greift HeyGen auf die Technologie von Voice Engine zurück. 

Angesichts der Missbrauchsgefahr soll Voice Engine nach Unternehmensangaben zunächst nicht frei zugänglich sein. OpenAI will das Produkt
mit Testern prüfen. 

""Wir verstehen, dass die
Erzeugung von Sprache, die den Stimmen von Menschen ähnelt, schwerwiegende
Risiken hat, die insbesondere in einem Wahljahr im Fokus stehen"", teilte das Unternehmen mit. In den USA wird am 5. November ein neuer Präsident gewählt.
Auch in zahlreichen anderen Ländern finden in diesem Jahr wichtige Wahlen statt.  

Im US-Staat New Hampshire
ermitteln die Behörden derzeit zu Anrufen, bei denen die Stimme von Präsident Joe Biden
mithilfe einer KI imitiert wurde, um Menschen von der Stimmabgabe bei der
dortigen Vorwahl im Januar abzuhalten.
"
Künstliche Intelligenz,Zeit,2024-03-28,https://www.zeit.de/politik/ausland/2024-03/usa-kuenstliche-intelligenz-behoerden-regelung,Künstliche Intelligenz: US-Regierung beschließt KI-Regeln für Behörden | ZEIT ONLINE,"Die US-Regierung hat den Bundesbehörden neue Regeln für die Anwendung von künstlicher Intelligenz (KI) auferlegt. ""Wenn Regierungsbehörden KI-Anwendungen nutzen, verlangen wir jetzt den Nachweis, dass diese Anwendungen die Rechte und die Sicherheit des amerikanischen Volkes nicht gefährden"", sagte Vizepräsidentin Kamala Harris. Könne dieser Nachweis nicht geliefert werden, dürfe diese Anwendung nicht mehr genutzt werden.

Konkret geht es bei der KI-Regulierung um die Gesichtserkennung an Flughäfen, aber auch um KI-Anwendungen, die bei der Kontrolle des Stromnetzes oder bei der Festlegung von Hypotheken und Versicherungen helfen. Als weiteres Beispiel nannte Harris KI-Diagnosehelfer in Krankenhäusern der Behörde für Militärveteranen. Dabei müsse erst sichergestellt werden, dass die KI nicht auf Grundlage der Hautfarbe unterschiedliche Diagnosen liefere.

Die Abteilungsleiter der Behörden müssen diese Direktive laut Harris bis Dezember umsetzen. Sie ist Teil des Exekutiverlasses von Präsident Joe Biden, den er im vergangenen Herbst unterzeichnet hatte. Der Erlass zielt auch auf den Privatsektor ab.

Die Vizepräsidentin sagte, es gebe mit der Direktive zwei weitere Anforderungen: Einerseits müssten die Behörden einen KI-Verantwortlichen einstellen. Außerdem müssten die verwendeten KI-Programme jährlich samt einer Risikoanalyse veröffentlicht werden.
"
Künstliche Intelligenz,Zeit,2024-03-28,https://www.zeit.de/kultur/2024-03/kuenstliche-intelligenz-auswirkungen-technologie-branche-unternehmen,Künstliche Intelligenz: Von Maschinenstürmern lernen | ZEIT ONLINE,
AI,Zeit,2024-03-28,https://www.zeit.de/politik/ausland/2024-03/usa-kuenstliche-intelligenz-behoerden-regelung,Künstliche Intelligenz: US-Regierung beschließt KI-Regeln für Behörden | ZEIT ONLINE,"Die US-Regierung hat den Bundesbehörden neue Regeln für die Anwendung von künstlicher Intelligenz (KI) auferlegt. ""Wenn Regierungsbehörden KI-Anwendungen nutzen, verlangen wir jetzt den Nachweis, dass diese Anwendungen die Rechte und die Sicherheit des amerikanischen Volkes nicht gefährden"", sagte Vizepräsidentin Kamala Harris. Könne dieser Nachweis nicht geliefert werden, dürfe diese Anwendung nicht mehr genutzt werden.

Konkret geht es bei der KI-Regulierung um die Gesichtserkennung an Flughäfen, aber auch um KI-Anwendungen, die bei der Kontrolle des Stromnetzes oder bei der Festlegung von Hypotheken und Versicherungen helfen. Als weiteres Beispiel nannte Harris KI-Diagnosehelfer in Krankenhäusern der Behörde für Militärveteranen. Dabei müsse erst sichergestellt werden, dass die KI nicht auf Grundlage der Hautfarbe unterschiedliche Diagnosen liefere.

Die Abteilungsleiter der Behörden müssen diese Direktive laut Harris bis Dezember umsetzen. Sie ist Teil des Exekutiverlasses von Präsident Joe Biden, den er im vergangenen Herbst unterzeichnet hatte. Der Erlass zielt auch auf den Privatsektor ab.

Die Vizepräsidentin sagte, es gebe mit der Direktive zwei weitere Anforderungen: Einerseits müssten die Behörden einen KI-Verantwortlichen einstellen. Außerdem müssten die verwendeten KI-Programme jährlich samt einer Risikoanalyse veröffentlicht werden.
"
AI,Zeit,2024-03-28,https://www.zeit.de/kultur/2024-03/kuenstliche-intelligenz-auswirkungen-technologie-branche-unternehmen,Künstliche Intelligenz: Von Maschinenstürmern lernen | ZEIT ONLINE,
AI,Zeit,2024-03-26,https://www.zeit.de/arbeit/2024-03/ki-arbeitswelt-beratung-ausbildung-unternehmen/seite-2,KI in der Arbeitswelt: Eher Dünnbrettbohrer? | ZEIT Arbeit,
AI,Zeit,2024-03-26,https://www.zeit.de/arbeit/2024-03/ki-arbeitswelt-beratung-ausbildung-unternehmen,KI in der Arbeitswelt: 100.000 Euro im Jahr für Vollzeit sind keine Seltenheit | ZEIT Arbeit,
Artificial Intelligence,Zeit,2024-03-28,https://www.zeit.de/politik/ausland/2024-03/usa-kuenstliche-intelligenz-behoerden-regelung,Künstliche Intelligenz: US-Regierung beschließt KI-Regeln für Behörden | ZEIT ONLINE,"Die US-Regierung hat den Bundesbehörden neue Regeln für die Anwendung von künstlicher Intelligenz (KI) auferlegt. ""Wenn Regierungsbehörden KI-Anwendungen nutzen, verlangen wir jetzt den Nachweis, dass diese Anwendungen die Rechte und die Sicherheit des amerikanischen Volkes nicht gefährden"", sagte Vizepräsidentin Kamala Harris. Könne dieser Nachweis nicht geliefert werden, dürfe diese Anwendung nicht mehr genutzt werden.

Konkret geht es bei der KI-Regulierung um die Gesichtserkennung an Flughäfen, aber auch um KI-Anwendungen, die bei der Kontrolle des Stromnetzes oder bei der Festlegung von Hypotheken und Versicherungen helfen. Als weiteres Beispiel nannte Harris KI-Diagnosehelfer in Krankenhäusern der Behörde für Militärveteranen. Dabei müsse erst sichergestellt werden, dass die KI nicht auf Grundlage der Hautfarbe unterschiedliche Diagnosen liefere.

Die Abteilungsleiter der Behörden müssen diese Direktive laut Harris bis Dezember umsetzen. Sie ist Teil des Exekutiverlasses von Präsident Joe Biden, den er im vergangenen Herbst unterzeichnet hatte. Der Erlass zielt auch auf den Privatsektor ab.

Die Vizepräsidentin sagte, es gebe mit der Direktive zwei weitere Anforderungen: Einerseits müssten die Behörden einen KI-Verantwortlichen einstellen. Außerdem müssten die verwendeten KI-Programme jährlich samt einer Risikoanalyse veröffentlicht werden.
"
Artificial Intelligence,Zeit,2024-03-26,https://www.zeit.de/arbeit/2024-03/ki-arbeitswelt-beratung-ausbildung-unternehmen/seite-2,KI in der Arbeitswelt: Eher Dünnbrettbohrer? | ZEIT Arbeit,
Artificial Intelligence,Zeit,2024-03-26,https://www.zeit.de/arbeit/2024-03/ki-arbeitswelt-beratung-ausbildung-unternehmen,KI in der Arbeitswelt: 100.000 Euro im Jahr für Vollzeit sind keine Seltenheit | ZEIT Arbeit,
KI,Zeit,2024-03-28,https://www.zeit.de/politik/ausland/2024-03/usa-kuenstliche-intelligenz-behoerden-regelung,Künstliche Intelligenz: US-Regierung beschließt KI-Regeln für Behörden | ZEIT ONLINE,"Die US-Regierung hat den Bundesbehörden neue Regeln für die Anwendung von künstlicher Intelligenz (KI) auferlegt. ""Wenn Regierungsbehörden KI-Anwendungen nutzen, verlangen wir jetzt den Nachweis, dass diese Anwendungen die Rechte und die Sicherheit des amerikanischen Volkes nicht gefährden"", sagte Vizepräsidentin Kamala Harris. Könne dieser Nachweis nicht geliefert werden, dürfe diese Anwendung nicht mehr genutzt werden.

Konkret geht es bei der KI-Regulierung um die Gesichtserkennung an Flughäfen, aber auch um KI-Anwendungen, die bei der Kontrolle des Stromnetzes oder bei der Festlegung von Hypotheken und Versicherungen helfen. Als weiteres Beispiel nannte Harris KI-Diagnosehelfer in Krankenhäusern der Behörde für Militärveteranen. Dabei müsse erst sichergestellt werden, dass die KI nicht auf Grundlage der Hautfarbe unterschiedliche Diagnosen liefere.

Die Abteilungsleiter der Behörden müssen diese Direktive laut Harris bis Dezember umsetzen. Sie ist Teil des Exekutiverlasses von Präsident Joe Biden, den er im vergangenen Herbst unterzeichnet hatte. Der Erlass zielt auch auf den Privatsektor ab.

Die Vizepräsidentin sagte, es gebe mit der Direktive zwei weitere Anforderungen: Einerseits müssten die Behörden einen KI-Verantwortlichen einstellen. Außerdem müssten die verwendeten KI-Programme jährlich samt einer Risikoanalyse veröffentlicht werden.
"
KI,Zeit,2024-03-26,https://www.zeit.de/arbeit/2024-03/ki-arbeitswelt-beratung-ausbildung-unternehmen/seite-2,KI in der Arbeitswelt: Eher Dünnbrettbohrer? | ZEIT Arbeit,
KI,Zeit,2024-03-26,https://www.zeit.de/arbeit/2024-03/ki-arbeitswelt-beratung-ausbildung-unternehmen,KI in der Arbeitswelt: 100.000 Euro im Jahr für Vollzeit sind keine Seltenheit | ZEIT Arbeit,
Künstliche Intelligenz,Zeit,2024-03-25,https://www.zeit.de/2024/13/diversitaet-google-ki-gemini-bild-generator-papst,Diversität bei Google-KI: Ist das der Papst? | ZEIT ONLINE,"Wenn Sie einen Papst malen sollten, wie sähe er aus? Vielleicht denken Sie an eine goldbestickte, spitz zulaufende Kopfbedeckung oder an ein schlichtes, kreisrundes Käppchen. Aber ziemlich sicher säße beides auf dem Haupt eines älteren weißen Herrn. Würden Sie stattdessen eine junge schwarze Frau zeichnen, wäre das ziemlich ungewöhnlich. Man könnte sagen: ein Fehler. 

Doch genau das hat eine künstliche Intelligenz der Firma Google kürzlich getan. Auf die Bitte hin, Bilder von Päpsten zu generieren, zeigte das System eine schwarze Frau und einen schwarzen Mann. Eine erstaunliche Reaktion – vor allem, weil man von KI-Systemen bisher eher das umgekehrte Verhalten kennt: Sie reproduzieren gesellschaftliche Vorurteile. Erfolgreiche Manager werden als weiße Männer dargestellt, Drogendealer als Schwarze. 

Die KI mit dem Namen Gemini generierte hingegen sogar Wehrmachtsoldaten mit nicht weißer Hautfarbe. 

All das sorgte für deutlich mehr Aufregung als die bisherigen Kreationen, die rassistische Vorurteile fortschrieben. Schließlich entschied Google, die Funktion, Bilder von Menschen zu generieren, vorübergehend abzuschalten. Der Konzern entschuldigte sich und versprach: ""Wir werden es besser machen."" 

Nur wie? Und überhaupt: Warum kriegen Google und Co. ihre KIs nicht in den Griff? 

Es ist nicht das erste Mal seit dem Ausbruch des KI-Hypes vor eineinhalb Jahren, dass ein solches System Probleme macht. Auch Microsoft zum Beispiel hat gerade Ärger mit seinem Bildgenerator, weil das Programm gewaltverherrlichende und sexualisierte Bilder erstellt. Und wenn Chatbots plötzlich davon fabulieren, sich in die Nutzerin verliebt zu haben, mag das niedlich sein. Andere Fehler dagegen sind demokratiegefährdend, so wie der von Microsofts KI-Suchmaschine Bing. Als Wissenschaftler untersuchten, wie korrekt Bing Auskunft zu bevorstehenden Landtagswahlen geben kann, beantwortete die KI ein Drittel der Fragen falsch. So waren etwa Umfragewerte nicht korrekt. 

Es gibt in der KI-Branche eine eigene Sparte, die sich damit beschäftigt, die KI auf Linie zu bringen. Alignment, auf Deutsch etwa Ausrichtung, nennt sich diese Disziplin, und offenbar steckt sie noch in den Kinderschuhen. So jedenfalls argumentierte John Schulman. Er ist einer der Gründer von OpenAI, der Firma hinter ChatGPT, und dort für genau dieses Alignment zuständig. Als Google für seine absurden Bilder kritisiert wurde, sprang Schulman der Konkurrenz in einem Post auf der Plattform X zur Seite. Alignment sei eben eine ziemlich junge Disziplin, und ""Hyper-Wokeness"" sei einfach ein Bug, so nennen Softwareentwickler Fehler. 

Wie man diesen behebt, weiß womöglich Jakob Foerster. Er hat schon für Google, für OpenAI und für Meta an künstlicher Intelligenz gearbeitet, heute ist er Professor an der Universität Oxford und erforscht, wie KI und Mensch zusammenarbeiten können. Foerster ist kein Mann der langen Vorrede, im Videocall sitzt er vor einem mit Graphen bekritzelten Whiteboard, bindet sich die Haare zum Zopf und regt sich erst mal auf. ""Die Methoden, die da benutzt werden, sind offensichtlich relativ schwachsinnig"", sagt er. 

Tatsächlich stellte sich im Lauf der Debatte um Gemini heraus, dass Google eine Art unangekündigtes Experiment mit den Nutzern seines Chatbots durchgeführt hat. Im Hintergrund wurde offenbar jede der Eingaben, sogenannte Prompts, um einen unsichtbaren Hinweis ergänzt. Google äußerte sich zu den genauen Methoden nicht, aber mit Tricks entlockten manche Nutzer dem Bot den geheimen Zusatz. Die Aufforderung ""Zeige mir einen Papst"" wurde demnach an das System weitergegeben mit der Aufforderung, ""explizit verschiedene Geschlechter und Ethnien"" einzubeziehen. 
"
Künstliche Intelligenz,Zeit,2024-03-24,https://www.zeit.de/2024/13/reiseplanung-ki-urlaub-buchen-chatgpt,"Reiseplanung mit KI: Liebe KI, buch mir mal schönen Urlaub. Meerblick, guter Fisch, bisschen Kunst. Danke | ZEIT ONLINE",
AI,Zeit,2024-03-25,https://www.zeit.de/2024/13/diversitaet-google-ki-gemini-bild-generator-papst,Diversität bei Google-KI: Ist das der Papst? | ZEIT ONLINE,"Wenn Sie einen Papst malen sollten, wie sähe er aus? Vielleicht denken Sie an eine goldbestickte, spitz zulaufende Kopfbedeckung oder an ein schlichtes, kreisrundes Käppchen. Aber ziemlich sicher säße beides auf dem Haupt eines älteren weißen Herrn. Würden Sie stattdessen eine junge schwarze Frau zeichnen, wäre das ziemlich ungewöhnlich. Man könnte sagen: ein Fehler. 

Doch genau das hat eine künstliche Intelligenz der Firma Google kürzlich getan. Auf die Bitte hin, Bilder von Päpsten zu generieren, zeigte das System eine schwarze Frau und einen schwarzen Mann. Eine erstaunliche Reaktion – vor allem, weil man von KI-Systemen bisher eher das umgekehrte Verhalten kennt: Sie reproduzieren gesellschaftliche Vorurteile. Erfolgreiche Manager werden als weiße Männer dargestellt, Drogendealer als Schwarze. 

Die KI mit dem Namen Gemini generierte hingegen sogar Wehrmachtsoldaten mit nicht weißer Hautfarbe. 

All das sorgte für deutlich mehr Aufregung als die bisherigen Kreationen, die rassistische Vorurteile fortschrieben. Schließlich entschied Google, die Funktion, Bilder von Menschen zu generieren, vorübergehend abzuschalten. Der Konzern entschuldigte sich und versprach: ""Wir werden es besser machen."" 

Nur wie? Und überhaupt: Warum kriegen Google und Co. ihre KIs nicht in den Griff? 

Es ist nicht das erste Mal seit dem Ausbruch des KI-Hypes vor eineinhalb Jahren, dass ein solches System Probleme macht. Auch Microsoft zum Beispiel hat gerade Ärger mit seinem Bildgenerator, weil das Programm gewaltverherrlichende und sexualisierte Bilder erstellt. Und wenn Chatbots plötzlich davon fabulieren, sich in die Nutzerin verliebt zu haben, mag das niedlich sein. Andere Fehler dagegen sind demokratiegefährdend, so wie der von Microsofts KI-Suchmaschine Bing. Als Wissenschaftler untersuchten, wie korrekt Bing Auskunft zu bevorstehenden Landtagswahlen geben kann, beantwortete die KI ein Drittel der Fragen falsch. So waren etwa Umfragewerte nicht korrekt. 

Es gibt in der KI-Branche eine eigene Sparte, die sich damit beschäftigt, die KI auf Linie zu bringen. Alignment, auf Deutsch etwa Ausrichtung, nennt sich diese Disziplin, und offenbar steckt sie noch in den Kinderschuhen. So jedenfalls argumentierte John Schulman. Er ist einer der Gründer von OpenAI, der Firma hinter ChatGPT, und dort für genau dieses Alignment zuständig. Als Google für seine absurden Bilder kritisiert wurde, sprang Schulman der Konkurrenz in einem Post auf der Plattform X zur Seite. Alignment sei eben eine ziemlich junge Disziplin, und ""Hyper-Wokeness"" sei einfach ein Bug, so nennen Softwareentwickler Fehler. 

Wie man diesen behebt, weiß womöglich Jakob Foerster. Er hat schon für Google, für OpenAI und für Meta an künstlicher Intelligenz gearbeitet, heute ist er Professor an der Universität Oxford und erforscht, wie KI und Mensch zusammenarbeiten können. Foerster ist kein Mann der langen Vorrede, im Videocall sitzt er vor einem mit Graphen bekritzelten Whiteboard, bindet sich die Haare zum Zopf und regt sich erst mal auf. ""Die Methoden, die da benutzt werden, sind offensichtlich relativ schwachsinnig"", sagt er. 

Tatsächlich stellte sich im Lauf der Debatte um Gemini heraus, dass Google eine Art unangekündigtes Experiment mit den Nutzern seines Chatbots durchgeführt hat. Im Hintergrund wurde offenbar jede der Eingaben, sogenannte Prompts, um einen unsichtbaren Hinweis ergänzt. Google äußerte sich zu den genauen Methoden nicht, aber mit Tricks entlockten manche Nutzer dem Bot den geheimen Zusatz. Die Aufforderung ""Zeige mir einen Papst"" wurde demnach an das System weitergegeben mit der Aufforderung, ""explizit verschiedene Geschlechter und Ethnien"" einzubeziehen. 
"
Artificial Intelligence,Zeit,2024-03-24,https://www.zeit.de/news/2024-03/24/giffey-nach-usa-reise-guter-ruf-als-innovationsstandort,Wirtschaftspolitik: Giffey nach USA-Reise: Guter Ruf als Innovationsstandort | ZEIT ONLINE,"Berlins Wirtschaftssenatorin Franziska Giffey (SPD) hat nach ihrer USA-Reise ein positives Fazit gezogen. In Gesprächen mit potenziellen Investoren und Ansiedlungen sowie neuen Kooperationen sei deutlich geworden, das Berlin «einen hervorragenden Ruf als Innovationsstandort» genieße, teilte Giffey am Sonntag mit. Zudem stehe die Stadt für «Weltoffenheit, Toleranz und Vielfalt». Es sei beeindruckend gewesen, zu erleben, mit welcher Begeisterung Gesprächspartner in New York, Berkeley oder im Silicon Valley von Berlin gesprochen hätten. «Es wird dort genau gesehen, wie viel sich in den vergangenen Jahren in Berlin als Ort für Spitzenforschung, Start-ups und eine starke Wirtschaft getan hat, und mehr noch, wie viel Zukunftspotenzial in Berlin steckt, meinte sie.

Bei der fünftägigen Reise Giffeys wurde unter anderem eine Zusammenarbeit zwischen dem Land Berlin und der University of California in Berkeley vereinbart. Beide unterschrieben eine entsprechende Erklärung (Memorandum of Understanding) für eine Zusammenarbeit beim Thema digitale Innovationen für gesundes Altern. Ein erstes gemeinsames Projekt soll sich dem Thema digitale Teilhabe für ältere Erwachsene widmen.

Giffey war am vergangenen Montag zusammen mit einer Delegation in die USA gereist und hatte dabei die Metropolen New York und San Francisco besucht. Nach ihrer Rückkehr in die Hauptstadt betonte die Wirtschaftssenatorin, sie sehe einen großen Bedarf, Berlins Kapazitäten und Kompetenzen im Bereich der Erforschung und Anwendung von Künstlicher Intelligenz zu stärken. «Das Thema Artificial Intelligence zog sich wie ein roter Faden nicht nur durch alle Gespräche und Begegnungen während unserer Reise, so die SPD-Politikerin.


© dpa-infocom, dpa:240324-99-451151/2
"
KI,Zeit,2024-03-25,https://www.zeit.de/2024/13/diversitaet-google-ki-gemini-bild-generator-papst,Diversität bei Google-KI: Ist das der Papst? | ZEIT ONLINE,"Wenn Sie einen Papst malen sollten, wie sähe er aus? Vielleicht denken Sie an eine goldbestickte, spitz zulaufende Kopfbedeckung oder an ein schlichtes, kreisrundes Käppchen. Aber ziemlich sicher säße beides auf dem Haupt eines älteren weißen Herrn. Würden Sie stattdessen eine junge schwarze Frau zeichnen, wäre das ziemlich ungewöhnlich. Man könnte sagen: ein Fehler. 

Doch genau das hat eine künstliche Intelligenz der Firma Google kürzlich getan. Auf die Bitte hin, Bilder von Päpsten zu generieren, zeigte das System eine schwarze Frau und einen schwarzen Mann. Eine erstaunliche Reaktion – vor allem, weil man von KI-Systemen bisher eher das umgekehrte Verhalten kennt: Sie reproduzieren gesellschaftliche Vorurteile. Erfolgreiche Manager werden als weiße Männer dargestellt, Drogendealer als Schwarze. 

Die KI mit dem Namen Gemini generierte hingegen sogar Wehrmachtsoldaten mit nicht weißer Hautfarbe. 

All das sorgte für deutlich mehr Aufregung als die bisherigen Kreationen, die rassistische Vorurteile fortschrieben. Schließlich entschied Google, die Funktion, Bilder von Menschen zu generieren, vorübergehend abzuschalten. Der Konzern entschuldigte sich und versprach: ""Wir werden es besser machen."" 

Nur wie? Und überhaupt: Warum kriegen Google und Co. ihre KIs nicht in den Griff? 

Es ist nicht das erste Mal seit dem Ausbruch des KI-Hypes vor eineinhalb Jahren, dass ein solches System Probleme macht. Auch Microsoft zum Beispiel hat gerade Ärger mit seinem Bildgenerator, weil das Programm gewaltverherrlichende und sexualisierte Bilder erstellt. Und wenn Chatbots plötzlich davon fabulieren, sich in die Nutzerin verliebt zu haben, mag das niedlich sein. Andere Fehler dagegen sind demokratiegefährdend, so wie der von Microsofts KI-Suchmaschine Bing. Als Wissenschaftler untersuchten, wie korrekt Bing Auskunft zu bevorstehenden Landtagswahlen geben kann, beantwortete die KI ein Drittel der Fragen falsch. So waren etwa Umfragewerte nicht korrekt. 

Es gibt in der KI-Branche eine eigene Sparte, die sich damit beschäftigt, die KI auf Linie zu bringen. Alignment, auf Deutsch etwa Ausrichtung, nennt sich diese Disziplin, und offenbar steckt sie noch in den Kinderschuhen. So jedenfalls argumentierte John Schulman. Er ist einer der Gründer von OpenAI, der Firma hinter ChatGPT, und dort für genau dieses Alignment zuständig. Als Google für seine absurden Bilder kritisiert wurde, sprang Schulman der Konkurrenz in einem Post auf der Plattform X zur Seite. Alignment sei eben eine ziemlich junge Disziplin, und ""Hyper-Wokeness"" sei einfach ein Bug, so nennen Softwareentwickler Fehler. 

Wie man diesen behebt, weiß womöglich Jakob Foerster. Er hat schon für Google, für OpenAI und für Meta an künstlicher Intelligenz gearbeitet, heute ist er Professor an der Universität Oxford und erforscht, wie KI und Mensch zusammenarbeiten können. Foerster ist kein Mann der langen Vorrede, im Videocall sitzt er vor einem mit Graphen bekritzelten Whiteboard, bindet sich die Haare zum Zopf und regt sich erst mal auf. ""Die Methoden, die da benutzt werden, sind offensichtlich relativ schwachsinnig"", sagt er. 

Tatsächlich stellte sich im Lauf der Debatte um Gemini heraus, dass Google eine Art unangekündigtes Experiment mit den Nutzern seines Chatbots durchgeführt hat. Im Hintergrund wurde offenbar jede der Eingaben, sogenannte Prompts, um einen unsichtbaren Hinweis ergänzt. Google äußerte sich zu den genauen Methoden nicht, aber mit Tricks entlockten manche Nutzer dem Bot den geheimen Zusatz. Die Aufforderung ""Zeige mir einen Papst"" wurde demnach an das System weitergegeben mit der Aufforderung, ""explizit verschiedene Geschlechter und Ethnien"" einzubeziehen. 
"
KI,Zeit,2024-03-24,https://www.zeit.de/2024/13/reiseplanung-ki-urlaub-buchen-chatgpt,"Reiseplanung mit KI: Liebe KI, buch mir mal schönen Urlaub. Meerblick, guter Fisch, bisschen Kunst. Danke | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-03-21,https://www.zeit.de/digital/2024-03/deutschland-hochschule-ki-assistent-studierende,Künstliche Intelligenz: Zwei Drittel der Studierenden in Deutschland nutzen KI-Systeme | ZEIT ONLINE,"An deutschen Hochschulen werden digitale Assistenten mit künstlicher Intelligenz (KI) von rund zwei Dritteln der Studierenden (65 Prozent) eingesetzt. Das geht aus einer Umfrage des Digitalverbands Bitkom hervor. Demnach sagten weitere 22 Prozent, sie könnten sich die Verwendung von ChatGPT und ähnlichen Systemen vorstellen. Lediglich neun Prozent der Befragten kannten die KI-Assistenten zwar, konnten sich eine Nutzung aber nicht vorstellen. Nur vier Prozent hatten noch nie von Chatbots wie ChatGPT oder Google Gemini gehört oder gelesen.

An vielen Hochschulen gibt es laut den Umfrageergebnissen keine festen Regeln für die Verwendung von KI-Systemen. Nur 17 Prozent der Befragten sagten, dass es zentrale Regeln gebe, 20 Prozent berichteten von Regeln, die vereinzelt vom Lehrpersonal festgelegt würden. Ein Drittel der Befragten (33 Prozent) berichtete, es gebe keine Regeln für den Einsatz von generativer KI wie etwa ChatGPT. 30 Prozent wussten nichts von entsprechenden Regeln oder antworteten nicht.

Für die Studie wurden 506 Studierende aller Hochschulformen in Deutschland ab 18 Jahren Anfang Januar online befragt. Die Umfrage ist zwar nicht repräsentativ, ergibt aber ein Stimmungsbild.

Die Studierenden verwenden demnach KI vor allem, um sich in ein Thema einzuarbeiten. 68 Prozent sagten, sie nutzten ChatGPT oder vergleichbare Systeme für die Recherche. 40 Prozent ließen sich von der KI Zusammenfassungen erstellen. 37 Prozent bereiteten mit der KI eine Präsentation vor. Jeweils gut ein Drittel korrigierte mithilfe der Technologie die eigenen Texte (37 Prozent) oder ließ Texte übersetzen (35 Prozent). Ein Drittel verwendete die KI, um sich auf Prüfungen vorzubereiten (33 Prozent), gut ein Viertel zum Schreiben von Hausarbeiten (26 Prozent). Gleichzeitig sprach sich knapp die Hälfte der Befragten (44 Prozent) dafür aus, die Nutzung von Chatbots für Hausarbeiten oder Abschlussarbeiten zu verbieten.

Bei der Auswertung der Antworten machte der Bitkom ein ""Spannungsfeld Künstliche Intelligenz"" aus Zustimmung und Ablehnung aus. Zum einen verlangen die Studierenden mit großer Mehrheit (74 Prozent), dass man an der Hochschule lernen sollte, wie man KI richtig nutzt. Knapp die Hälfte meint, der KI-Einsatz sollte an allen Hochschulen Standard sein. Zum anderen meinen 60 Prozent, dies führe dazu, dass Studierende weniger selbstständig denken und lernen. Jeder zweite Befragte (51 Prozent) glaubt, durch ChatGPT beschäftigten sich Studierende weniger mit den Studieninhalten.
"
Künstliche Intelligenz,Zeit,2024-03-21,https://www.zeit.de/news/2024-03/21/un-vollversammlung-billigt-erste-resolution-zu-ki,Künstliche Intelligenz: UN-Vollversammlung billigt erste Resolution zu KI | ZEIT ONLINE,"Die Vollversammlung der Vereinten Nationen hat erstmals eine Resolution zu Künstlicher Intelligenz verabschiedet. Das von den Vereinigten Staaten eingebrachte Papier wurde in New York angenommen. 

In dem Text heißt es, die schnelle Entwicklung von KI brauche internationale Regeln und ermögliche eine Zusammenarbeit besonders mit ärmeren Ländern. Dringend erreicht werden müsse ein globaler Konsens über «sichere und vertrauenswürdige Systeme der Künstlichen Intelligenz». Verurteilt wird in dem Papier der Missbrauch von KI. 

Der Entwurf war nach monatelangen Verhandlungen unter den Diplomatinnen und Diplomaten bereits zuvor im größten UN-Gremium ohne weitere Änderungsvorschläge akzeptiert worden. Dies machte eine einstimmige Annahme in der Vollversammlung mit ihren 193 Mitgliedern im Konsensverfahren ohne formale Abstimmung möglich. Resolutionen der Vollversammlung sind nicht rechtlich bindend.

Künstliche Intelligenz bezeichnet meist Anwendungen auf Basis maschinellen Lernens. Die rasante Entwicklung immer potenterer Software bietet dabei riesige Chancen und schürt Hoffnung etwa auf große Fortschritte im Kampf gegen Krankheiten wie Krebs oder bei Problemen wie der Klimakrise. Gleichzeitig gibt es aber Sorgen, dass mächtige Anwendungen großen Schaden anrichten könnten, wenn sie nicht reguliert werden. 

UN-Generalsekretär António Guterres hatte schon mehrfach vor den Gefahren der Technologie gewarnt und eine Expertenrunde einberufen, die die Vereinten Nationen bei dem Thema berät. Auch der UN-Sicherheitsrat hat sich schon mit dem Thema befasst. 

Mitte des Monats hatte das EU-Parlament grünes Licht für schärfere Regeln für Künstliche Intelligenz in der Europäischen Union gegeben. Nach Angaben des Parlaments handelt es sich um das weltweit erste KI-Gesetz. Demnach sollen KI-Systeme künftig in verschiedene Risikogruppen eingeteilt werden. Je höher die potenziellen Gefahren einer Anwendung sind, desto höher sollen die Anforderungen sein.

© dpa-infocom, dpa:240321-99-420445/2
"
Künstliche Intelligenz,Zeit,2024-03-20,https://www.zeit.de/digital/mobil/2024-03/apple-google-ki-iphone-gemini-abkommen,Apple und Google: Vielleicht der schnellste Verfolger | ZEIT ONLINE,"Selten gab es eine
Technologie, auf die so
schnell alle großen Techunternehmen aufgesprungen sind, wie künstliche Intelligenz: Microsoft finanziert OpenAI und damit ChatGPT, Google will seine
Suche durch das KI-Modell Gemini verbessern, Meta hat sein Modell Llama als Open
Source veröffentlicht, auf Amazon sollen Händler ihre Produktseiten mit KI
erstellen lassen, und Chiphersteller Nvidia verdient sich mit dem ganzen Hype einen
goldenen Chip. 

Fehlt Ihnen ein
Unternehmen in dieser Aufzählung? Richtig, Apple. Von einer KI aus Cupertino
hat man bisher wenig vernommen. 

In den
vergangenen Tagen sickerte dann aber etwas durch: Wie zuerst Bloomberg berichtete, soll
sich Apple in Gesprächen mit Google befinden, um die KI von Google Gemini zu lizenzieren und in künftige iPhone-Software einzubauen. Ja, das Google, das selbst
Smartphones mit KI herstellt, das mit seinem Betriebssystem Android gar ein direkter
Konkurrent von Apple ist. 

Wenn die Berichte
stimmen, wäre die naheliegendste Lesart also: Das Unternehmen aus Cupertino, das lange als technologischer
Innovationstreiber galt, lange den Titel des wertvollsten Unternehmens der Welt
hielt, gibt das Rennen um die größte Innovation der Gegenwart auf. Gute Nacht, Apple.

So einfach ist es
aber nicht. Im Gegenteil: Ein Pakt mit Google könnte gerade das Klügste sein,
was Apple machen kann.

Apple ist bekannt
dafür, dass es nicht gerne künstliche Intelligenz sagt (so
wie es ja auch nicht so gerne virtuelle Realität sagt). Während Google-Chef
Sundar Pichai im
vergangenen Jahr auf der eigenen
Entwicklerkonferenz künstliche Intelligenz beinahe massenhaft (oder jedenfalls mit Meme-Potenzial)
erwähnte, sagte drei Wochen später Apple-Chef Tim
Cook auf Apples Entwicklerkonferenz das Wort nicht einmal. Apple spricht
lieber von ""maschinellem Lernen"" – nicht unbedingt, um sich abzugrenzen, es scheint
schlicht der Unternehmenssprech zu sein. Der stammt vermutlich noch aus der Prä-ChatGPT-Zeit,
in der sich die Techszene stärker als aktuell bemühte, zwischen verschiedenen
Formen von künstlicher Intelligenz sprachlich zu differenzieren. 

Denn Apple baut
schon seit einiger Zeit auf künstliche Intelligenz: Seit den 2017 vorgestellten
iPhone 8 und iPhone X haben iPhones auf ihrem Chip eine für KI-Anwendungen
(oder eben: maschinelles Lernen) optimierte Neural Engine. Seitdem Apple auch
die Chips für Macs selbst entwickelt, ist diese auch dort eingebaut, also
seit 2020. Die Geräte nutzen diese Engine etwa für Videobearbeitung oder beim
Fotografieren, um den
Himmel blauer und die Haut von Menschen lebendiger abzubilden. Sie ist der wesentliche
Grund dafür, warum iPhone-Fotos oft besser
aussehen als die Realität.

Apple nutzt KI
also längst – nur nicht im Scheinwerferlicht. Das ist aber das, was seit der Veröffentlichung
von ChatGPT im Herbst 2022 viele Kundinnen (und vermutlich noch mehr Investorinnen)
erwarten: Text- und Bildgeneratoren, die aus kurzen prompts, also
Aufforderungen, Gedichte schreiben, Kunstwerke zeichnen oder Songs komponieren.


Diese Erwartung wird
zum Problem für den Konzern: Seit 2011 war Apple die meiste Zeit das
wertvollste Unternehmen der Welt, nur zeitweise von Microsoft und Google-Mutter
Alphabet überholt. Seit Anfang 2024 ist Microsoft wieder auf Platz eins – der
wesentliche Grund dafür dürfte das Engagement des Konzerns im Bereich
generativer künstlicher Intelligenz sein. Microsoft investierte Milliarden in
das Unternehmen hinter ChatGPT, OpenAI, und sicherte sich so die Möglichkeiten
zu, die Technologie in Windows und sein Office-Paket zu integrieren. Microsoft beginnt
gerade erst, damit wirklich Geld zu verdienen, aber allein das ökonomische
Potenzial von KI treibt den Wert des Unternehmens in die Höhe.

Auch wenn Apple
oft versucht, sich unabhängig von Trends zu machen: An dem KI-Trend kommt auch
das Unternehmen aus Cupertino nicht vorbei. Das zeigte sich etwa bei der jährlichen
Investorenversammlung des Unternehmens Ende Februar: Da versprach Tim Cook seinen
Investoren, Apple werde in diesem Jahr neue
Wege in der generativen KI beschreiten – diesmal sprach er nicht von
maschinellem Lernen. Das dürfte bisher die deutlichste Ankündigung des überaus
verschwiegenen Konzerns sein, dass es größere KI-Engagements plant. Ein
weiteres Signal: Seit vergangenem Jahr sucht Apple verstärkt
Mitarbeiter mit Expertise in generativer KI, auch die ehemaligen Mitarbeiterinnen
des dem Vernehmen nach eingestellten
Elektroauto-Projektes sollen angeblich künftig an künstlicher Intelligenz
arbeiten.

In der Ankündigung
seines neuen MacBook Airs mit M3-Chip lässt sich die neue Stoßrichtung
ablesen: Darin schreibt Apple, das MacBook Air sei ""nach wie vor der weltweit
beste Consumer Laptop für KI"". Bei den Pressevorführungen des Gerätes zeigte
Apple die KI-Funktionen der Grafiksoftware Blender, die Copilot-Funktionen von (Microsofts!)
Powerpoint und die App FreeChat, mit der ein durch Metas Modell Llama
angetriebener Chatbot ohne Cloud-Anbindung, auf dem Gerät selbst, lief. Der antwortete
fast in Echtzeit.

Die Botschaft: Wir
können auch KI. Und das noch mindestens so gut wie Microsoft.
"
Künstliche Intelligenz,Zeit,2024-03-20,https://www.zeit.de/digital/datenschutz/2024-03/ki-kuenstliche-intelligenz-grundrechte-datenschutz-privatsphaere,Künstliche Intelligenz: Datenschutzbeauftragter mahnt zur Einhaltung der Grundrechte bei KI | ZEIT ONLINE,"Der Datenschutzbeauftragte der Bundesregierung, Ulrich Kelber, hat verbindliche
Rahmenbedingungen für den Einsatz von künstlicher Intelligenz (KI) gefordert.
""Datenschutz und Privatsphäre sind Kernelemente, ohne die der sichere
Einsatz von KI nicht denkbar ist"", schreibt Kelber in seinem Tätigkeitsbericht für das Jahr 2023. Bei der Forschung, Anwendung, Auswertung und Regulierung im KI-Bereich
müssten Datenschutz und Privatsphäre geschützt werden. Je nachdem, wie künstliche Intelligenz
eingesetzt werde, berge sie ""das Potenzial für Grundrechtseinschränkungen
und Diskriminierungen"".

Kelber übergab seinen Bericht Bundestagspräsidentin
Bärbel Bas (SPD). Darin mahnt er die Bundesregierung an, die Datenschutz-Grundverordnung bei der
Umsetzung der kürzlich auf europäischer Ebene beschlossenen KI-Verordnung zu
berücksichtigen. ""Ich empfehle dem Gesetzgeber, die sich aus der
KI-Verordnung der EU ergebende nationale KI-Aufsichtsstruktur zeitnah
festzulegen"", schreibt Kelber. Die KI-Verordnung des Europaparlaments beinhaltet
strengere Auflagen für Gesichtserkennungssysteme und andere Anwendungen.

Der Datenschutzbeauftragte sieht zudem die geplante EU-Verordnung zur
Chatkontrolle kritisch. Bei dieser geht es um das Ausspähen verschlüsselter
Privatchats. Kelber empfiehlt dem Bundestag in seinem Bericht, gegenüber der
Bundesregierung und dem EU-Gesetzgeber ""auf eine erhebliche,
grundrechtskonforme Überarbeitung"" des Verordnungsentwurfs zur
Chatkontrolle zu drängen.

Der Entwurf müsse eine ""durchgehende
Ende-zu-Ende-Verschlüsselung"" gewährleisten, die deutsche und europäische
Grundrechte wahre und ""ein flächendeckendes und anlassloses Auslesen
privater Kommunikation verbietet"". Andernfalls sei ein Entwurf
""insgesamt abzulehnen"". 

In seinem Bericht schildert Kelber auch, dass sich Bürgerinnen und
Bürger an seine Behörde gewandt hätten, da sie Sorge um die Sicherheit ihrer
Daten im Gesundheitsbereich hätten. ""Die Digitalisierung des
Gesundheitswesens und der Pflege begrüße ich"", schrieb Kelber.
""Allerdings muss die Digitalisierung datenschutzkonform erfolgen.""
Die geplante Widerspruchslösung bei der elektronischen Patientenakte greife
""erheblich in das Grundrecht auf die informationelle Selbstbestimmung
ein"".

Der Bundesrat hatte im Februar einen Gesetzentwurf von
Gesundheitsminister Karl Lauterbach (SPD) und damit die Ausweitung der
E-Patientenakte gebilligt. Deren Nutzung soll ab 2025 für alle gesetzlich
Versicherten zum Normalfall werden. Patientinnen und Patienten können dem aber widersprechen.
"
Künstliche Intelligenz,Zeit,2024-03-20,https://www.zeit.de/news/2024-03/20/nvidia-chef-problem-von-ki-halluzinationen-loesbar,"Künstliche Intelligenz: Nvidia-Chef: Problem von KI-""Halluzinationen"" lösbar | ZEIT ONLINE","Ein zentrales Problem der Verlässlichkeit von KI-Software wird sich nach Einschätzung von Nvidia-Chef Jensen Huang durchaus beheben lassen. Ein Weg sei, die Software die Informationen überprüfen zu lassen. «Halluzinationen sind sehr lösbar», sagte Huang am Rande der Nvidia-Entwicklerkonferenz GTC. Als KI-Halluzinationen werden Fälle bezeichnet, in denen Software Dinge einfach erfindet.

Das Problem entsteht, weil Programme wie der Chatbot ChatGPT beim Formulieren von Texten Wort für Wort schätzen, wie ein Satz weitergehen könnte. Deswegen können sie manchmal völlig falsche Antworten geben, selbst wenn sie nur mit korrekten Informationen trainiert wurden.

Der Halbleiter-Spezialist Nvidia spielt mit seinen Chip-Systemen eine Schlüsselrolle im aktuellen Boom bei Künstlicher Intelligenz. Bei der GTC stellte Huang die nächste Generation der Computer-Plattform mit dem Namen Blackwell vor, auf der KI-Anwendungen effizienter laufen sollen.

Huang zeigte sich überzeugt, dass Künstliche Intelligenz den Menschen weitgehend das Schreiben von Software-Code abnehmen werde. Eine Zeit lang habe es geheißen, dass alle Programmieren lernen sollten. «Ich denke, das ist falsch», sagte der Nvidia-Chef. Es sei nicht die Aufgabe der Menschen gewesen, eine Programmiersprache zu lernen. Stattdessen sollten Computer nützlich sein, auch wenn man keine Programmiersprache beherrsche. Man werde allerdings lernen müssen, von KI-Software mithilfe richtig formulierter Anfragen - sogenannter «Prompts» - das gewünschte Ergebnis zu bekommen. Das sei aber grundsätzlich vergleichbar mit Fähigkeiten der Menschen im Umgang untereinander: «Wenn meine Frau mit mir spricht, ist das auch Prompt-Gestaltung - und das funktioniert perfekt.»

Huang hält es für möglich, dass Künstliche Intelligenz in wenigen Jahren in verschiedenen Bereichen die Fähigkeiten von Menschen übertreffen werde - zum Beispiel bei Mathematik- und Anwalts-Prüfungen oder beim Verständnis von Texten. Daran orientiere er sich auch in der aktuellen Debatte darüber, ob Software die Stufe der sogenannten allgemeinen Künstlichen Intelligenz erreichen werde - womit meist gemeint ist, dass sie verallgemeinern könne und den Menschen überlegen sei. Allerdings gebe es keine Definition dafür, über die sich alle einig seien, betonte er.

© dpa-infocom, dpa:240320-99-399374/2
"
Künstliche Intelligenz,Zeit,2024-03-19,https://www.zeit.de/news/2024-03/19/nvidia-kuendigt-neues-computersystem-fuer-ki-infrastruktur-an,Künstliche Intelligenz: Nvidia kündigt neues Computersystem für KI-Infrastruktur an | ZEIT ONLINE,"Der Chipkonzern Nvidia will seine führende Rolle bei Technik für Künstliche Intelligenz ausbauen. Firmenchef Jensen Huang stellte dafür eine neue Generation von Nvidias Computerplattform vor. Nvidia sieht das System mit dem Namen Blackwell als «Antrieb einer neuen industriellen Revolution» durch KI. Das System sei beim Anlernen von Künstlicher Intelligenz vier Mal leistungsstärker als die aktuelle Generation Grace Hopper.

Nvidias Computersysteme dominieren in Rechenzentren beim KI-Training. Der Konzern will auch seine Rolle bei der Erzeugung von Inhalten mit Hilfe Künstlicher Intelligenz ausbauen. Das «Blackwell»-System sei darin 30 Mal besser als «Hopper», betonte Huang am Montag auf der hauseigenen Entwicklerkonferenz GTC in San Jose. Zudem gibt es von Nvidia dafür neue Software, die über Schnittstellen auch via Cloud genutzt werden kann. 

Huang zeigte sich überzeugt, dass in der Zukunft die meisten Inhalte nicht vorgefertigt aus Speichern abgerufen werden, sondern dass KI-Software sie ausgehend aus der aktuellen Situation frisch erzeugen werde. Nvidia habe das Computersystem für diese Zukunft entwickelt. So werde man sich zum Beispiel mit Gebäuden per Chatbot unterhalten können, anstelle irgendwo Daten einzusehen.

Mit Grace Hopper hätte man zum Beispiel den Chatbot ChatGPT innerhalb von drei Monaten mit 8000 Nvidia-Chips und einem Stromverbrauch von 15 Megawatt trainieren können, sagte Huang. Mit Blackwell schaffe man das in derselben Zeit mit 2000 Chips und 4 Megawatt Strom.

ChatGPT ist der Chatbot der Entwicklerfirma OpenAI, der vor über einem Jahr den aktuellen Hype rund um Künstliche Intelligenz auslöste. Solche Software wird mit gewaltigen Mengen an Informationen trainiert und kann auf dieser Grundlage zum Beispiel Sätze auf dem sprachlichen Niveau eines Menschen und Bilder aus Text-Vorgaben generieren. Nvidia wolle die Technologie auch nutzen, um Wettervorhersagen zu erzeugen, sagte Huang. Dafür entwickelt der Konzern eine Welt-Simulation mit dem Namen «Earth 2».

Nvidia bündele seine Chips für Blackwell in Computersysteme mit rund 600 000 Bauteilen und einem Gewicht von mehr als 1,3 Tonnen, betonte Huang. Die ursprünglich für Grafikkarten entwickelten Nvidia-Technologien bewähren sich schon seit Langem bei der Rechenarbeit für Anwendungen mit Künstlicher Intelligenz. Konkurrenten wie Intel und AMD konnten bisher keinen Anschluss finden. Das lässt das Geschäft - und den Börsenwert - von Nvidia rasant wachsen. Große KI-Unternehmen wie Microsoft, Google und Amazon planen bereits den Einsatz von Blackwell. 

Nvidias neuer «KI-Superchip» ist nach dem amerikanischen Mathematiker David Blackwell benannt. Nvidia will damit auch den Einsatz sogenannter «digitaler Zwillinge» vorantreiben, in denen Unternehmen ihr gesamtes Geschäft im Computer simulieren können. Bevor man etwas in der realen Welt baut, werde man es künftig zunächst digital simulieren, betonte Huang.

Mit diesem Konzept macht Nvidia auch Apples Computerbrille Vision Pro nützlicher für den Einsatz in Unternehmen. Nvidias 3D-Umgebung Omniverse, mit der Firmen «digitale Zwillinge» ihrer Produkte und Fabriken erstellen können, kommt auf das Apple-Gerät. Die Technologie wird oft bei der Gestaltung von Objekten verwendet. Huang demonstrierte sie am Montag bei der hauseigenen Entwicklerkonferenz GTC unter anderem am Beispiel des südkoreanischen Autobauers Hyundai. So konnte ein Designer verschiedene Farbvarianten eines Automodells in unterschiedlichen Umgebungen ansehen - und sich auch virtuell ans Lenkrad setzen. Apple will mit der 3499 Dollar (rund 3200 Euro) teuren Vision Pro einen sogenannten «räumlichen Computer» etablieren, mit dem man sich digitale Inhalte auch innerhalb der realen Umgebung anzeigen lassen kann. 

Nvidia setzt auch auf Roboter. «Alles, was sich bewegt, wird robotisch sein», sagte der Firmenchef. Das Ziel sei, dass Roboter allein schon dadurch lernen können, dass sie Menschen beobachten. «Der ChatGPT-Moment für Robotik könnte unmittelbar bevorstehen», sagte Huang. Eine zentrale Rolle sollen dabei Simulationen spielen, in denen Roboter für die reale Welt lernen.

© dpa-infocom, dpa:240319-99-385354/9
"
Künstliche Intelligenz,Zeit,2024-03-18,https://www.zeit.de/news/2024-03/18/patentamt-deutsche-industrie-verstaerkt-ki-entwicklung,Künstliche Intelligenz: Patentamt: Deutsche Industrie verstärkt KI-Entwicklung | ZEIT ONLINE,"Industrie und Wissenschaft in Deutschland haben Forschung und Entwicklung bei Künstlicher Intelligenz verstärkt, liegen jedoch weit hinter den USA zurück. Das geht aus einer am Montag veröffentlichten Analyse des Deutschen Patent- und Markenamts (DPMA) hervor. Demnach war die Zahl der veröffentlichten Patentanmeldungen mit KI-Bezug beim Deutschen und beim Europäischen Patentamt im vergangenen Jahr 40 Prozent höher als 2019, wie die Münchner Bundesbehörde am Montag mitteilte.

Es dominierten Unternehmen und Forschungseinrichtungen aus den USA, die allein fast ein Drittel der Neuentwicklungen in Technologien mit KI-Bezug angemeldet hatten. Auf Rang zwei folgte Deutschland mit einem Anteil von 17,1 Prozent vor Japan mit 12,4 Prozent. Die Ränge 4 und 5 belegten China (10,1 Prozent) und Südkorea (4,7 Prozent). Am schnellsten gestiegen ist die Zahl chinesischer Entwicklungen, die 2023 bereits 15 Prozent höher war als 2022. Deutsche Anmeldungen legten im gleichen Zeitraum demnach um 5,2 Prozent zu.

«Die starke und breite Innovationsdynamik verdeutlicht, dass KI zukünftig in vielen Lebensbereichen eine große Rolle spielen wird», sagte DPMA-Präsidentin Eva Schewior. Die KI-Analyse ist an diesem Dienstag und Mittwoch Thema des alljährlichen DPMA-Nutzerforums.

Da Unternehmen Patente für neue Erfindungen vor allem in ihren Heimatländern anmelden, liegen in der deutschen Patentstatistik insgesamt üblicherweise einheimische Unternehmen auf den ersten Plätzen.

Bei den Technologiefeldern mit KI-Bezug ist dies anders: Zwar belegte mit Bosch ein deutsches Unternehmen Platz ein, doch auf den weiteren Plätzen folgten der chinesische Huawei-Konzern, die US-Unternehmen Google und Microsoft sowie Samsung aus Südkorea.

Bundesbildungsministerin Bettina Stark-Watzinger (FDP) wertete die Analyse als Beleg, dass Deutschland in einer sehr guten Ausgangsposition sei. «Aber darauf dürfen wir uns nicht ausruhen.» KI sei eine riesige Chance und habe enormes Potenzial etwa in Bildung, Forschung und Wirtschaft. «Wir dürfen sie nicht verpassen.»

Patente werden nach einem international einheitlichen Standard der Patentklassifikation angemeldet. Da es für KI keine eigenen Patentklassen gibt, werteten die Fachleute des DPMA die Klassen aus den Technologiefeldern aus, in denen KI zum Einsatz kommt.

Dazu zählen beispielsweise Computer-, Medizin- und Fahrzeugtechnik, Robotik sowie Sprach- und Bildanalyse. In die Auswertung flossen Patentanmeldungen mit Wirkung für Deutschland ein, die beim Deutschen oder beim ebenfalls in München ansässigen Europäischen Patentamt eingereicht wurden, Doppelanmeldungen bei beiden Behörden herausgerechnet.

Die Behörde betonte, dass diese Methodik eine Annäherung darstelle, aber die verschiedenen Trends gut aufzeige. Die Analyse erhebt laut DPMA aber keinen Anspruch auf Vollständigkeit für alle KI-bezogenen Erfindungen, außerdem könnten auch Erfindungen ohne KI-Bezug enthalten sein. Da Patentanmeldungen erst nach einer Frist von 18 Monaten veröffentlicht werden, sind in der Statistik noch keine Erfindungen des vergangenen Jahres enthalten.

© dpa-infocom, dpa:240318-99-376238/3
"
AI,Zeit,2024-03-20,https://www.zeit.de/digital/mobil/2024-03/apple-google-ki-iphone-gemini-abkommen/seite-2,Apple und Google: Von einem Deal würden sowohl Apple als auch Google profitieren | ZEIT ONLINE,"Will Apple zumindest
in der öffentlichen Wahrnehmung aufholen, braucht es zunächst die Basis dafür: ein oder mehrere große Sprachmodelle (LLMS, Large Language Models),
bestenfalls multimodaler Art, also solche, die Text und Bilder verarbeiten
können. Bisher hielt sich Apple auch in dieser Hinsicht bedeckt. Laut The
Information soll das Unternehmen mehrere
Millionen Dollar am Tag ins Training solcher Modelle stecken, die Bilder
generieren, den Support-Chat von AppleCare übernehmen oder die oft gescholtene Sprachassistentin
Siri verbessern sollen. Angeblich soll das ""Ajax GPT"" genannte Modell mit ChatGPTs
Ursprungsmodell GPT-3.5 locker mithalten können. Nur: Das ist nun auch schon
anderthalb Jahre alt.

ChatGPT
allerdings braucht eine Internetverbindung – die Antworten kommen von den Servern
von OpenAI. Apple, das stets betont, wie wichtig ihm Datenschutz sei, wird vermutlich
eher versuchen, einen Chatbot auf iPhones zu bringen, der nur auf dem Gerät,
also offline, läuft. Dafür spricht ein wissenschaftliches Paper, das Apple
Forscher Anfang des Jahres veröffentlicht haben (PDF). Darin führen sie aus, wie
ein Sprachmodell auf einem Gerät mit begrenztem Speicher laufen kann. Es folgten
weitere Paper zu Animationsdesign mit KI (PDF) und, erst in dieser Woche,
ein Paper (PDF), in dem die
Autoren MM1 vorstellen: eine ""Familie"" von multimodalen Sprachmodellen, die demnach
besonders gut mit Kombinationen aus Bild- und Textdaten umgehen können.

Es ist davon
auszugehen, dass das nächste iPhone-Betriebssystem iOS 18 im Herbst mit
Offline-KI-Funktionen kommen wird, denkbar wären KI-Hilfen beim E-Mail-Schreiben,
bei der Terminverwaltung im Kalender oder beim Übersetzer. Das allerdings wird
nicht reichen, wenn Apple mithalten will: Müssen große Mengen an Daten
verarbeitet werden, zum Beispiel bei der Generierung von Bildern, wie sie etwa
OpenAIs Dall-E anbietet, wird das für die kleinen Chips eines iPhones
schwierig. Auch wenn die Neural Engine, wie Gerüchte sagen, im
kommenden iPhone 16 wirklich wesentlich ausgebaut werden soll. Dann braucht
Apple Server, auf denen KIs laufen – und die können ihnen vermutlich derzeit nur Amazon, Microsoft
und Google bieten.

So wie
unterschiedliche Browser und Suchmaschinen auf dem iPhone nutzbar sind, könnte Apple
auch mehrere KIs, also etwa Gemini und Dall-E, nebeneinander anbieten. Auch mit OpenAI soll Apple laut Bloomberg Gespräche geführt haben. Dass
Apple jedoch lieber mit Google kooperiert als mit dem im KI-Rennen
augenscheinlich vorauseilenden Microsoft, könnte ein Deal sein, von dem beide
Unternehmen profitieren: Apple würde Googles KI-Kapazitäten nutzen können (und
zudem den Rechtfertigungsdruck von sich lenken, wenn die KI mal wieder
halluziniert), Google würde dafür mit einem Schlag eine riesige Anzahl Kunden
gewinnen. Immerhin gibt es mehr
als 2,2 Milliarden aktive Apple-Geräte weltweit. Durch die Kooperation würden sich zwei der ärgsten Microsoft-Konkurrenten gegenseitig stärken. 

So unklar wie die
Existenz dieser Verhandlungen ist auch ihr Inhalt: Beispielsweise wäre es
möglich, dass Apple Google bezahlt, um die KI-Funktionen als eigene Features
verkaufen zu können. Es wäre aber auch möglich, dass iPhone-Nutzer zu Google und zu Gemini
geführt werden – und wie schon jetzt für die bessere Version Gemini Pro zusätzlich
zahlen müssen. Dafür könnte Apple wiederum eine Provision von Google
einstreichen. 

Solche Deals sind
für die beiden Unternehmen nichts Neues: So zahlt Google einem Bericht
der New York Times nach jährlich 18 Milliarden US-Dollar an Apple,
um die Standard-Suchmaschine auf iPhones zu bleiben. Würde Google auch derartige Beträge zahlen,
um auf iPhones die Standard-KI zu sein? Bei den für KI nötigen Serverkapazitäten
und daraus folgenden -kosten wohl eher nicht.

Wie eine KI im
iPhone konkret aussehen könnte, sieht man bei Konkurrent Samsung: In der
Anfang des Jahres vorgestellten S24-Reihe
gibt es etwa einen Live-Übersetzer für Telefonate oder eine Transkriptionsfunktion,
die lokal auf dem Gerät mit einer Samsung-KI laufen (nicht
immer vollkommen zufriedenstellend, zeigte der Test von ZEIT ONLINE). Für
andere KI-Funktionen wie etwa eine Bilderkennung (""Wie heißt das Gebäude vor
mir?"") nutzt aber auch Samsung die Google-KI, ebenso für das Bearbeiten von
Fotos. 

Würde auch Apple
dafür auf Googles Algorithmen zurückgreifen, gäbe es die kuriose Situation,
dass mit Google, Samsung und Apple drei der wichtigsten Smartphonehersteller
die gleichen oder ähnliche KI-Features implementieren würden. Deshalb wäre ein
Deal auch nicht ohne Risiko: Gegen das Suchmaschinenabkommen klagt bereits das
US-Justizministerium, und die Europäische
Union bewies zuletzt mit ihrem Digital Markets Act (DMA), dass die
Techgiganten auch auf dieser Seite des Atlantiks unter neuerlicher strenger
Beobachtung stehen. 

Apple könnte wie
schon bei anderen Technologien zuvor wieder einmal beweisen, dass es nicht
immer gut ist, der Erste zu sein. Andere können den Weg ebnen, die Probleme wegräumen,
damit Apple es dann besser machen kann: Schon bei iPhone, Apple Watch und bei
der Vision Pro war das Unternehmen lieber Fast Follower als First
Mover, lieber schneller Verfolger als Innovationsbringer. Oder beim Thema Chips:
Lange liefen Mac-Computer mit Intel-Chips, im Hintergrund werkelte Apple
derweil an eigenen. Genauso könnte es auch beim Thema KI laufen. 

Sollte Apple im
Juni, zur Entwicklerkonferenz WWDC, neue KI-Funktionen ankündigen, feiert sich
Apple sicher trotzdem als Innovator. Im Marketing schließlich ist Apple
weiterhin die Nummer eins. Wenn Apple allerdings mit
Google-KI-Funktionen auftrumpfen will, gibt es bis dahin noch mindestens ein Problem zu
lösen: Derzeit generiert Gemini keine Bilder von Personen, weil es unter anderem schwarze
Nazis produzierte. Damit der Deal also zustande kommt, muss sich vielleicht
Google mehr ranhalten als Apple.
"
AI,Zeit,2024-03-21,https://www.zeit.de/digital/internet/2024-03/nvidia-blackwell-ki-gpu-chip,"Nvidia Blackwell: ""Viel kleiner geht nicht mehr"" | ZEIT ONLINE",
AI,Zeit,2024-03-20,https://www.zeit.de/digital/mobil/2024-03/apple-google-ki-iphone-gemini-abkommen,Apple und Google: Vielleicht der schnellste Verfolger | ZEIT ONLINE,"Selten gab es eine
Technologie, auf die so
schnell alle großen Techunternehmen aufgesprungen sind, wie künstliche Intelligenz: Microsoft finanziert OpenAI und damit ChatGPT, Google will seine
Suche durch das KI-Modell Gemini verbessern, Meta hat sein Modell Llama als Open
Source veröffentlicht, auf Amazon sollen Händler ihre Produktseiten mit KI
erstellen lassen, und Chiphersteller Nvidia verdient sich mit dem ganzen Hype einen
goldenen Chip. 

Fehlt Ihnen ein
Unternehmen in dieser Aufzählung? Richtig, Apple. Von einer KI aus Cupertino
hat man bisher wenig vernommen. 

In den
vergangenen Tagen sickerte dann aber etwas durch: Wie zuerst Bloomberg berichtete, soll
sich Apple in Gesprächen mit Google befinden, um die KI von Google Gemini zu lizenzieren und in künftige iPhone-Software einzubauen. Ja, das Google, das selbst
Smartphones mit KI herstellt, das mit seinem Betriebssystem Android gar ein direkter
Konkurrent von Apple ist. 

Wenn die Berichte
stimmen, wäre die naheliegendste Lesart also: Das Unternehmen aus Cupertino, das lange als technologischer
Innovationstreiber galt, lange den Titel des wertvollsten Unternehmens der Welt
hielt, gibt das Rennen um die größte Innovation der Gegenwart auf. Gute Nacht, Apple.

So einfach ist es
aber nicht. Im Gegenteil: Ein Pakt mit Google könnte gerade das Klügste sein,
was Apple machen kann.

Apple ist bekannt
dafür, dass es nicht gerne künstliche Intelligenz sagt (so
wie es ja auch nicht so gerne virtuelle Realität sagt). Während Google-Chef
Sundar Pichai im
vergangenen Jahr auf der eigenen
Entwicklerkonferenz künstliche Intelligenz beinahe massenhaft (oder jedenfalls mit Meme-Potenzial)
erwähnte, sagte drei Wochen später Apple-Chef Tim
Cook auf Apples Entwicklerkonferenz das Wort nicht einmal. Apple spricht
lieber von ""maschinellem Lernen"" – nicht unbedingt, um sich abzugrenzen, es scheint
schlicht der Unternehmenssprech zu sein. Der stammt vermutlich noch aus der Prä-ChatGPT-Zeit,
in der sich die Techszene stärker als aktuell bemühte, zwischen verschiedenen
Formen von künstlicher Intelligenz sprachlich zu differenzieren. 

Denn Apple baut
schon seit einiger Zeit auf künstliche Intelligenz: Seit den 2017 vorgestellten
iPhone 8 und iPhone X haben iPhones auf ihrem Chip eine für KI-Anwendungen
(oder eben: maschinelles Lernen) optimierte Neural Engine. Seitdem Apple auch
die Chips für Macs selbst entwickelt, ist diese auch dort eingebaut, also
seit 2020. Die Geräte nutzen diese Engine etwa für Videobearbeitung oder beim
Fotografieren, um den
Himmel blauer und die Haut von Menschen lebendiger abzubilden. Sie ist der wesentliche
Grund dafür, warum iPhone-Fotos oft besser
aussehen als die Realität.

Apple nutzt KI
also längst – nur nicht im Scheinwerferlicht. Das ist aber das, was seit der Veröffentlichung
von ChatGPT im Herbst 2022 viele Kundinnen (und vermutlich noch mehr Investorinnen)
erwarten: Text- und Bildgeneratoren, die aus kurzen prompts, also
Aufforderungen, Gedichte schreiben, Kunstwerke zeichnen oder Songs komponieren.


Diese Erwartung wird
zum Problem für den Konzern: Seit 2011 war Apple die meiste Zeit das
wertvollste Unternehmen der Welt, nur zeitweise von Microsoft und Google-Mutter
Alphabet überholt. Seit Anfang 2024 ist Microsoft wieder auf Platz eins – der
wesentliche Grund dafür dürfte das Engagement des Konzerns im Bereich
generativer künstlicher Intelligenz sein. Microsoft investierte Milliarden in
das Unternehmen hinter ChatGPT, OpenAI, und sicherte sich so die Möglichkeiten
zu, die Technologie in Windows und sein Office-Paket zu integrieren. Microsoft beginnt
gerade erst, damit wirklich Geld zu verdienen, aber allein das ökonomische
Potenzial von KI treibt den Wert des Unternehmens in die Höhe.

Auch wenn Apple
oft versucht, sich unabhängig von Trends zu machen: An dem KI-Trend kommt auch
das Unternehmen aus Cupertino nicht vorbei. Das zeigte sich etwa bei der jährlichen
Investorenversammlung des Unternehmens Ende Februar: Da versprach Tim Cook seinen
Investoren, Apple werde in diesem Jahr neue
Wege in der generativen KI beschreiten – diesmal sprach er nicht von
maschinellem Lernen. Das dürfte bisher die deutlichste Ankündigung des überaus
verschwiegenen Konzerns sein, dass es größere KI-Engagements plant. Ein
weiteres Signal: Seit vergangenem Jahr sucht Apple verstärkt
Mitarbeiter mit Expertise in generativer KI, auch die ehemaligen Mitarbeiterinnen
des dem Vernehmen nach eingestellten
Elektroauto-Projektes sollen angeblich künftig an künstlicher Intelligenz
arbeiten.

In der Ankündigung
seines neuen MacBook Airs mit M3-Chip lässt sich die neue Stoßrichtung
ablesen: Darin schreibt Apple, das MacBook Air sei ""nach wie vor der weltweit
beste Consumer Laptop für KI"". Bei den Pressevorführungen des Gerätes zeigte
Apple die KI-Funktionen der Grafiksoftware Blender, die Copilot-Funktionen von (Microsofts!)
Powerpoint und die App FreeChat, mit der ein durch Metas Modell Llama
angetriebener Chatbot ohne Cloud-Anbindung, auf dem Gerät selbst, lief. Der antwortete
fast in Echtzeit.

Die Botschaft: Wir
können auch KI. Und das noch mindestens so gut wie Microsoft.
"
Artificial Intelligence,Zeit,2024-03-21,https://www.zeit.de/wissen/2024-03/neuralink-implantat-patient-spielt-schach-gedanken,Gehirnimplantat: Patient mit Neuralink-Chip spielt Schach mit seinen Gedanken | ZEIT ONLINE,"Erstmals hat ein Mensch mit einem Gehirnimplantat der Firma Neuralink offenbar nur mithilfe seiner Gedanken digital Schach gespielt. Der 29-Jährige, der nach einem Tauchunfall unterhalb der 
Schultern gelähmt ist, war in einem Livestream dabei zu sehen, wie er Onlineschach am Laptop spielte und dabei
mit seinen Gedanken den Mauszeiger bewegte.

Zuvor hatte der Patient einen Computerchip eingepflanzt bekommen, der vom Start-up-Unternehmen Neuralink des Milliardärs Elon Musk entwickelt wurde. Musk teilte den Livestream auf seinem X-Account.

Mit Neuralink sollen unmittelbare Schnittstellen zwischen dem menschlichen Körper und digitalen Anwendungen erforscht und ermöglicht werden. Der 29-jährige Patient war im vergangenen Februar der erste, der ein entsprechendes Implantat erhalten hatte. Kurz darauf hatte Musk bekannt gegeben, dass der Patient nach erfolgreicher Operation  in der Lage sei, einen Mauscursor durch bloßes Denken zu bewegen.

Er sei bereits einen Tag nach der Operation aus dem Krankenhaus entlassen worden und habe keine kognitiven Beeinträchtigungen, sagte der Patient nun in einem Video. Er wolle zwar nicht, dass Menschen denken würden, dies sei das Ende der Reise. ""Aber es hat bereits mein Leben verändert.""

Kurz  vor der Zulassung der Hirnimplantate für die Erprobung am Menschen in den USA hatte es Bedenken bei den zuständigen Behörden gegeben. Inspektoren der US-Arzneimittelbehörde FDA hatten bei Neuralink Probleme mit den Aufzeichnungen und Qualitätskontrollen von Tierversuchen festgestellt. Das Unternehmen  reagierte damals nicht auf Nachfragen zu der Inspektion der FDA.

Dennoch sei es kein ""Durchbruch"", was Neuralink gezeigt habe, sagte der ehemalige Programmdirektor für Neurotechnologie am US-amerikanischen National Institute of Health, Kip Ludwig. ""Wir befinden uns noch in einem sehr frühen Stadium nach der Implantation."" Sowohl Neuralink als auch die Probanden müssten noch viel lernen, um die Steuerung zu optimieren. Dennoch sei die neuartige Kommunikation des Patienten mit einem Computer ein guter Ausgangspunkt und eine positive Entwicklung.

Bis vor Kurzem waren die etwa münzgroßen Implantate nur bei Tieren eingepflanzt worden. Bei der Präsentation von Neuralink wurden Affen gezeigt, die mithilfe des Gehirnchips einfache Videospiele ""spielten"" oder einen Cursor auf einem Bildschirm bewegten. Musk sagte, er wolle mit der Forschung helfen, neurologische und andere Erkrankungen zu heilen.

Neuralink entwickelt derzeit weitere Implantate, die in das Rückenmark oder die Augen eingesetzt werden sollen, um die Mobilität oder das Sehvermögen wiederherzustellen. Das Start-up  ist nicht das einzige Unternehmen, das an Hirn-Computer-Schnittstellen arbeitet.
"
Artificial Intelligence,Zeit,2024-03-21,https://www.zeit.de/digital/2024-03/neuralink-elon-musk-chip-gehirnimplantat-marketing,Neuralink von Elon Musk: Wir wissen heute nicht mehr über Gehirnchips als gestern | ZEIT ONLINE,"Es ist nicht einfach, nach diesem Video einen kritischen Kommentar zu schreiben. In Selfie-Optik ist zu sehen, wie der querschnittsgelähmte Noland Arbaugh einen Computer nur mit seinen Gedanken steuert, wie er die Schachfiguren auf dem Bildschirm bewegt und wie er sich freut, dass er dank des Chips in seinem Gehirn die ganze Nacht das Computerspiel Civilization 6 zocken konnte, obwohl er seine Hände nicht bewegen kann. Das ist so wholesome, dass auch dem schlecht gelauntesten Techniknörgler die Tränen in die Augen schießen.

Aber genau das ist dann eben doch auch ein Problem: Vor lauter Rührung könnte man nämlich fast übersehen, dass man nach den neun Minuten praktisch nicht mehr weiß als vorher. Nicht darüber, wie der Chip in Arbaughs Gehirn funktioniert, nicht, was der Chip eigentlich genau ermöglicht, und auch nicht, wie zuverlässig das funktioniert. 

Der Clip soll vor allem eine Demonstration sein. Aber Demovideos sind keine Wissenschaft. Das scheint Elon Musk, der Chef der Firma, die den Chip in Arbaughs Kopf implantiert hat, zu vergessen oder aktiv zu ignorieren. Und mit ihm eine ganze Branche von Techunternehmern.

Das Video, das die Firma Neuralink in der Nacht zu Donnerstag auf der Plattform X streamte, wurde dort mittlerweile mehr zwölf Millionen Mal angesehen, es verbreitete sich rasend schnell durch die sozialen und klassischen Medien. Das ist verständlich, denn es ist beeindruckend.

Man sieht den 29-jährigen Arbaugh sitzend, festgeschnallt in einem speziellen Stuhl, an dem ein Laptop befestigt ist. Ein sympathischer dude mit Fusselbart und einer Schar Hunden, die am Anfang des Videos kurz zu sehen sind. Arbaugh ist seit einem Tauchunfall unterhalb der Schultern gelähmt und der erste (und bis heute wohl einzige) Mensch, der den Gehirnchip von Neuralink implantiert bekommen hat. Den Erfolg der Operation gab der Unternehmenschef Elon Musk im Januar per Post auf X bekannt. 

Neben Arbaugh sitzt im Video ein zweiter junger Mann, der sich als Ingenieur bei Neuralink vorstellt und das Ganze mit seinem Smartphone filmt und Arbaugh Fragen stellt. Es wirkt alles sehr spontan und locker, aber es ist vermutlich sorgfältig inszeniert, worauf schon die Ankündigung des Livestreams einige Stunden vorher hindeutet. 

Am Anfang läuft Musik im Hintergrund. Der Neuralink-Mitarbeiter fragt, ob Arbaugh sie ausschalten könnte, und als die Musik endet, fragt er: ""Hast du das auch mit deinem Gehirn gemacht?"" Arbaugh sagt: ""Jap, das ist alles Gehirn-Power.""

In dem Video fallen die Worte ""telekinetisch"" und ""Zauberer"", Musk behauptet auf X, es zeige ""Telepathie"". Arbaugh erzählt, dass er erst üben musste, den Cursor auf seinem Computer zu bewegen, in dem er sich die entsprechende Handbewegung vorstellte, und wie es dann irgendwann klappte. Er vergleicht das mit der Macht aus Star Wars.

Auch das ist nicht nur schön anzusehen, sondern auch absolut verständlich. Es ist schwer vorstellbar, welch ein Moment das für Arbaugh persönlich gewesen sein muss. Und auch beim Zuschauer flackern sofort Zukunftsvisionen auf, von einer Welt, in der Menschen mit Querschnittslähmung wieder gehen und Blinde sehen können, in der die Menschheit zu sich findet, weil wir uns ohne das umständliche Hilfsmittel Sprache verständigen können. In der wir buchstäblich Gedanken lesen können. Wer das nicht erhebend findet, hat Science-Fiction nie geliebt. 
"
Artificial Intelligence,Zeit,2024-03-19,https://www.zeit.de/news/2024-03/19/wechsel-an-der-spitze-der-technischen-universitaet-nuernberg,Hochschule: Wechsel an der Spitze der Technischen Universität Nürnberg | ZEIT ONLINE,"Wechsel an der Spitze der Technischen Universität Nürnberg: Gründungspräsident Hans Jürgen Prömel wird das Amt des Präsidenten an den neu berufenen Michael Huth übergeben. Dies teilte das Wissenschaftsministerium in München am Dienstag mit. Zuvor hatten die «Nürnberger Nachrichten» berichtet. 

Der Wechsel erfolge im Zuge der Fokussierung der Hochschule auf das Thema der Künstlichen Intelligenz (KI), sagte eine Sprecherin. Ministerpräsident Markus Söder (CSU) hat das Ziel ausgegeben, die TU Nürnberg solle zur ersten rein auf KI spezialisierten Universität Deutschlands werden. Als Präsident sei Michael Huth dafür die Idealbesetzung, teilte Söder nun mit. Huth sei eine Koryphäe im Bereich der Künstlichen Intelligenz. «Damit setzen wir ein starkes Signal für ideale Forschungsbedingungen.» Wissenschaftsminister Markus Blume (CSU) fügte hinzu: «Fragen der Sicherheit und Ethik Künstlicher Intelligenz gewinnen weltweit immer größere Bedeutung. Genau in diesem Bereich forscht der weltweit anerkannte Wissenschaftler.»

Der im unterfränkischen Aschaffenburg geborene Huth ist den Angaben zufolge seit mehr als 30 Jahren für renommierte Universitäten in Großbritannien und in den USA tätig. Derzeit ist er Professor of Computer Science und Leiter des Departments of Computing am Imperial College London. Die Schwerpunkte seiner Arbeit liegen demnach auf den Themen Künstliche Intelligenz und Cyber Security. 

Minister Blume dankte zugleich Gründungspräsident Prömel «für seine außerordentliche Aufbauarbeit». Prömel habe im Bereich der Berufungen Maßstäbe gesetzt, so seien etwa beide Department Chairs Leibniz-Preisträger.

Hans Jürgen Prömel war im März 2021 für zunächst fünf Jahre berufen worden. Sein Amt ende einvernehmlich zum 31. März, hieß es. Anschließend wird Prömel nach Angaben der Hochschule in den Ruhestand gehen. Michael Huth solle das Amt des Präsidenten zum 1. Oktober übernehmen, bis dahin nehme die Aufgabe Gründungsvizepräsident Alexander Martin wahr.

Die TU Nürnberg wurde 2021 neu gegründet  - es war die erste Neugründung einer staatlichen Universität in Bayern seit 1978. Im vergangenen Herbst begannen die ersten acht Studierenden dort mit ihrem Masterstudium «Artificial Intelligence and Robotics». Die Hochschule setzt auf ein Lehrkonzept ohne Hörsäle und klassische Klausuren. Gelehrt wird zunächst in einem Interimsgebäude im Süden der Stadt. Im Laufe des Jahres sollen erste Kurse auf dem noch im Bau befindlichen Campus stattfinden.

© dpa-infocom, dpa:240319-99-393698/2
"
Artificial Intelligence,Zeit,2024-03-18,https://www.zeit.de/news/2024-03/18/patentamt-deutsche-industrie-verstaerkt-ki-entwicklung,Künstliche Intelligenz: Patentamt: Deutsche Industrie verstärkt KI-Entwicklung | ZEIT ONLINE,"Industrie und Wissenschaft in Deutschland haben Forschung und Entwicklung bei Künstlicher Intelligenz verstärkt, liegen jedoch weit hinter den USA zurück. Das geht aus einer am Montag veröffentlichten Analyse des Deutschen Patent- und Markenamts (DPMA) hervor. Demnach war die Zahl der veröffentlichten Patentanmeldungen mit KI-Bezug beim Deutschen und beim Europäischen Patentamt im vergangenen Jahr 40 Prozent höher als 2019, wie die Münchner Bundesbehörde am Montag mitteilte.

Es dominierten Unternehmen und Forschungseinrichtungen aus den USA, die allein fast ein Drittel der Neuentwicklungen in Technologien mit KI-Bezug angemeldet hatten. Auf Rang zwei folgte Deutschland mit einem Anteil von 17,1 Prozent vor Japan mit 12,4 Prozent. Die Ränge 4 und 5 belegten China (10,1 Prozent) und Südkorea (4,7 Prozent). Am schnellsten gestiegen ist die Zahl chinesischer Entwicklungen, die 2023 bereits 15 Prozent höher war als 2022. Deutsche Anmeldungen legten im gleichen Zeitraum demnach um 5,2 Prozent zu.

«Die starke und breite Innovationsdynamik verdeutlicht, dass KI zukünftig in vielen Lebensbereichen eine große Rolle spielen wird», sagte DPMA-Präsidentin Eva Schewior. Die KI-Analyse ist an diesem Dienstag und Mittwoch Thema des alljährlichen DPMA-Nutzerforums.

Da Unternehmen Patente für neue Erfindungen vor allem in ihren Heimatländern anmelden, liegen in der deutschen Patentstatistik insgesamt üblicherweise einheimische Unternehmen auf den ersten Plätzen.

Bei den Technologiefeldern mit KI-Bezug ist dies anders: Zwar belegte mit Bosch ein deutsches Unternehmen Platz ein, doch auf den weiteren Plätzen folgten der chinesische Huawei-Konzern, die US-Unternehmen Google und Microsoft sowie Samsung aus Südkorea.

Bundesbildungsministerin Bettina Stark-Watzinger (FDP) wertete die Analyse als Beleg, dass Deutschland in einer sehr guten Ausgangsposition sei. «Aber darauf dürfen wir uns nicht ausruhen.» KI sei eine riesige Chance und habe enormes Potenzial etwa in Bildung, Forschung und Wirtschaft. «Wir dürfen sie nicht verpassen.»

Patente werden nach einem international einheitlichen Standard der Patentklassifikation angemeldet. Da es für KI keine eigenen Patentklassen gibt, werteten die Fachleute des DPMA die Klassen aus den Technologiefeldern aus, in denen KI zum Einsatz kommt.

Dazu zählen beispielsweise Computer-, Medizin- und Fahrzeugtechnik, Robotik sowie Sprach- und Bildanalyse. In die Auswertung flossen Patentanmeldungen mit Wirkung für Deutschland ein, die beim Deutschen oder beim ebenfalls in München ansässigen Europäischen Patentamt eingereicht wurden, Doppelanmeldungen bei beiden Behörden herausgerechnet.

Die Behörde betonte, dass diese Methodik eine Annäherung darstelle, aber die verschiedenen Trends gut aufzeige. Die Analyse erhebt laut DPMA aber keinen Anspruch auf Vollständigkeit für alle KI-bezogenen Erfindungen, außerdem könnten auch Erfindungen ohne KI-Bezug enthalten sein. Da Patentanmeldungen erst nach einer Frist von 18 Monaten veröffentlicht werden, sind in der Statistik noch keine Erfindungen des vergangenen Jahres enthalten.

© dpa-infocom, dpa:240318-99-376238/3
"
KI,Zeit,2024-03-20,https://www.zeit.de/digital/mobil/2024-03/apple-google-ki-iphone-gemini-abkommen/seite-2,Apple und Google: Von einem Deal würden sowohl Apple als auch Google profitieren | ZEIT ONLINE,"Will Apple zumindest
in der öffentlichen Wahrnehmung aufholen, braucht es zunächst die Basis dafür: ein oder mehrere große Sprachmodelle (LLMS, Large Language Models),
bestenfalls multimodaler Art, also solche, die Text und Bilder verarbeiten
können. Bisher hielt sich Apple auch in dieser Hinsicht bedeckt. Laut The
Information soll das Unternehmen mehrere
Millionen Dollar am Tag ins Training solcher Modelle stecken, die Bilder
generieren, den Support-Chat von AppleCare übernehmen oder die oft gescholtene Sprachassistentin
Siri verbessern sollen. Angeblich soll das ""Ajax GPT"" genannte Modell mit ChatGPTs
Ursprungsmodell GPT-3.5 locker mithalten können. Nur: Das ist nun auch schon
anderthalb Jahre alt.

ChatGPT
allerdings braucht eine Internetverbindung – die Antworten kommen von den Servern
von OpenAI. Apple, das stets betont, wie wichtig ihm Datenschutz sei, wird vermutlich
eher versuchen, einen Chatbot auf iPhones zu bringen, der nur auf dem Gerät,
also offline, läuft. Dafür spricht ein wissenschaftliches Paper, das Apple
Forscher Anfang des Jahres veröffentlicht haben (PDF). Darin führen sie aus, wie
ein Sprachmodell auf einem Gerät mit begrenztem Speicher laufen kann. Es folgten
weitere Paper zu Animationsdesign mit KI (PDF) und, erst in dieser Woche,
ein Paper (PDF), in dem die
Autoren MM1 vorstellen: eine ""Familie"" von multimodalen Sprachmodellen, die demnach
besonders gut mit Kombinationen aus Bild- und Textdaten umgehen können.

Es ist davon
auszugehen, dass das nächste iPhone-Betriebssystem iOS 18 im Herbst mit
Offline-KI-Funktionen kommen wird, denkbar wären KI-Hilfen beim E-Mail-Schreiben,
bei der Terminverwaltung im Kalender oder beim Übersetzer. Das allerdings wird
nicht reichen, wenn Apple mithalten will: Müssen große Mengen an Daten
verarbeitet werden, zum Beispiel bei der Generierung von Bildern, wie sie etwa
OpenAIs Dall-E anbietet, wird das für die kleinen Chips eines iPhones
schwierig. Auch wenn die Neural Engine, wie Gerüchte sagen, im
kommenden iPhone 16 wirklich wesentlich ausgebaut werden soll. Dann braucht
Apple Server, auf denen KIs laufen – und die können ihnen vermutlich derzeit nur Amazon, Microsoft
und Google bieten.

So wie
unterschiedliche Browser und Suchmaschinen auf dem iPhone nutzbar sind, könnte Apple
auch mehrere KIs, also etwa Gemini und Dall-E, nebeneinander anbieten. Auch mit OpenAI soll Apple laut Bloomberg Gespräche geführt haben. Dass
Apple jedoch lieber mit Google kooperiert als mit dem im KI-Rennen
augenscheinlich vorauseilenden Microsoft, könnte ein Deal sein, von dem beide
Unternehmen profitieren: Apple würde Googles KI-Kapazitäten nutzen können (und
zudem den Rechtfertigungsdruck von sich lenken, wenn die KI mal wieder
halluziniert), Google würde dafür mit einem Schlag eine riesige Anzahl Kunden
gewinnen. Immerhin gibt es mehr
als 2,2 Milliarden aktive Apple-Geräte weltweit. Durch die Kooperation würden sich zwei der ärgsten Microsoft-Konkurrenten gegenseitig stärken. 

So unklar wie die
Existenz dieser Verhandlungen ist auch ihr Inhalt: Beispielsweise wäre es
möglich, dass Apple Google bezahlt, um die KI-Funktionen als eigene Features
verkaufen zu können. Es wäre aber auch möglich, dass iPhone-Nutzer zu Google und zu Gemini
geführt werden – und wie schon jetzt für die bessere Version Gemini Pro zusätzlich
zahlen müssen. Dafür könnte Apple wiederum eine Provision von Google
einstreichen. 

Solche Deals sind
für die beiden Unternehmen nichts Neues: So zahlt Google einem Bericht
der New York Times nach jährlich 18 Milliarden US-Dollar an Apple,
um die Standard-Suchmaschine auf iPhones zu bleiben. Würde Google auch derartige Beträge zahlen,
um auf iPhones die Standard-KI zu sein? Bei den für KI nötigen Serverkapazitäten
und daraus folgenden -kosten wohl eher nicht.

Wie eine KI im
iPhone konkret aussehen könnte, sieht man bei Konkurrent Samsung: In der
Anfang des Jahres vorgestellten S24-Reihe
gibt es etwa einen Live-Übersetzer für Telefonate oder eine Transkriptionsfunktion,
die lokal auf dem Gerät mit einer Samsung-KI laufen (nicht
immer vollkommen zufriedenstellend, zeigte der Test von ZEIT ONLINE). Für
andere KI-Funktionen wie etwa eine Bilderkennung (""Wie heißt das Gebäude vor
mir?"") nutzt aber auch Samsung die Google-KI, ebenso für das Bearbeiten von
Fotos. 

Würde auch Apple
dafür auf Googles Algorithmen zurückgreifen, gäbe es die kuriose Situation,
dass mit Google, Samsung und Apple drei der wichtigsten Smartphonehersteller
die gleichen oder ähnliche KI-Features implementieren würden. Deshalb wäre ein
Deal auch nicht ohne Risiko: Gegen das Suchmaschinenabkommen klagt bereits das
US-Justizministerium, und die Europäische
Union bewies zuletzt mit ihrem Digital Markets Act (DMA), dass die
Techgiganten auch auf dieser Seite des Atlantiks unter neuerlicher strenger
Beobachtung stehen. 

Apple könnte wie
schon bei anderen Technologien zuvor wieder einmal beweisen, dass es nicht
immer gut ist, der Erste zu sein. Andere können den Weg ebnen, die Probleme wegräumen,
damit Apple es dann besser machen kann: Schon bei iPhone, Apple Watch und bei
der Vision Pro war das Unternehmen lieber Fast Follower als First
Mover, lieber schneller Verfolger als Innovationsbringer. Oder beim Thema Chips:
Lange liefen Mac-Computer mit Intel-Chips, im Hintergrund werkelte Apple
derweil an eigenen. Genauso könnte es auch beim Thema KI laufen. 

Sollte Apple im
Juni, zur Entwicklerkonferenz WWDC, neue KI-Funktionen ankündigen, feiert sich
Apple sicher trotzdem als Innovator. Im Marketing schließlich ist Apple
weiterhin die Nummer eins. Wenn Apple allerdings mit
Google-KI-Funktionen auftrumpfen will, gibt es bis dahin noch mindestens ein Problem zu
lösen: Derzeit generiert Gemini keine Bilder von Personen, weil es unter anderem schwarze
Nazis produzierte. Damit der Deal also zustande kommt, muss sich vielleicht
Google mehr ranhalten als Apple.
"
KI,Zeit,2024-03-21,https://www.zeit.de/news/2024-03/21/wahlkampf-mit-fake-fotos-afd-wirbt-mit-ki-bild-fuer-sich,Kreis Göppingen: Wahlkampf mit Fake-Fotos? - AfD wirbt mit KI-Bild für sich | ZEIT ONLINE,"Der Kreisverband der AfD Göppingen wirbt auf seiner Instagram-Seite mit einem durch eine Künstliche Intelligenz (KI) erstellten Foto einer Frau. Dem Bild wird der falsche Name Dr. Stefanie Müller zugeordnet. Daneben steht die Motivation zum Parteieintritt der vermeintlichen Frau auf dem Bild. Der Post wirft Fragen auf zum Umgang mit KI in Zeiten von Fake News und im Vorfeld der Kommunal- und Europawahl. Zuvor hatte der SWR berichtet.

Der AfD-Kreisvorsitzende Sandro Scheer bestätigte am Donnerstag, dass er das Bild mit einer Künstlichen Intelligenz erstellt habe, auch der Name der Frau sei nicht korrekt. Es sei aber eine Dr. Stefanie in den Kreisverband eingetreten und habe ihre Motivation dazu ähnlich geschildert, wie im Post beschrieben. Um die Frau zu schützen, wollte er demnach nicht ihren echten Namen nennen. Mit Fake News hat der Post laut Scheer nichts zu tun, es sei klar erkennbar, dass das Foto keine echte Person zeige.

Ähnlich sieht es Markus Frohnmaier, Co-Vorsitzender des AfD-Landesverbands. Er sieht bei dem durch eine KI generierten Bild keinen Unterschied zu solchen aus Foto-Datenbanken. Problematisch wäre es aus seiner Sicht nur, wenn nicht offenkundig wäre, dass es sich um KI-generierte Inhalte handele. Bei dem Post des Göppinger Kreisverbands sehe er keine Verwechslungsgefahr. Welche Rolle Künstliche Intelligenz im Wahlkampf spielen wird, sei noch zu diskutieren.

© dpa-infocom, dpa:240321-99-416827/2
"
KI,Zeit,2024-03-21,https://www.zeit.de/digital/2024-03/deutschland-hochschule-ki-assistent-studierende,Künstliche Intelligenz: Zwei Drittel der Studierenden in Deutschland nutzen KI-Systeme | ZEIT ONLINE,"An deutschen Hochschulen werden digitale Assistenten mit künstlicher Intelligenz (KI) von rund zwei Dritteln der Studierenden (65 Prozent) eingesetzt. Das geht aus einer Umfrage des Digitalverbands Bitkom hervor. Demnach sagten weitere 22 Prozent, sie könnten sich die Verwendung von ChatGPT und ähnlichen Systemen vorstellen. Lediglich neun Prozent der Befragten kannten die KI-Assistenten zwar, konnten sich eine Nutzung aber nicht vorstellen. Nur vier Prozent hatten noch nie von Chatbots wie ChatGPT oder Google Gemini gehört oder gelesen.

An vielen Hochschulen gibt es laut den Umfrageergebnissen keine festen Regeln für die Verwendung von KI-Systemen. Nur 17 Prozent der Befragten sagten, dass es zentrale Regeln gebe, 20 Prozent berichteten von Regeln, die vereinzelt vom Lehrpersonal festgelegt würden. Ein Drittel der Befragten (33 Prozent) berichtete, es gebe keine Regeln für den Einsatz von generativer KI wie etwa ChatGPT. 30 Prozent wussten nichts von entsprechenden Regeln oder antworteten nicht.

Für die Studie wurden 506 Studierende aller Hochschulformen in Deutschland ab 18 Jahren Anfang Januar online befragt. Die Umfrage ist zwar nicht repräsentativ, ergibt aber ein Stimmungsbild.

Die Studierenden verwenden demnach KI vor allem, um sich in ein Thema einzuarbeiten. 68 Prozent sagten, sie nutzten ChatGPT oder vergleichbare Systeme für die Recherche. 40 Prozent ließen sich von der KI Zusammenfassungen erstellen. 37 Prozent bereiteten mit der KI eine Präsentation vor. Jeweils gut ein Drittel korrigierte mithilfe der Technologie die eigenen Texte (37 Prozent) oder ließ Texte übersetzen (35 Prozent). Ein Drittel verwendete die KI, um sich auf Prüfungen vorzubereiten (33 Prozent), gut ein Viertel zum Schreiben von Hausarbeiten (26 Prozent). Gleichzeitig sprach sich knapp die Hälfte der Befragten (44 Prozent) dafür aus, die Nutzung von Chatbots für Hausarbeiten oder Abschlussarbeiten zu verbieten.

Bei der Auswertung der Antworten machte der Bitkom ein ""Spannungsfeld Künstliche Intelligenz"" aus Zustimmung und Ablehnung aus. Zum einen verlangen die Studierenden mit großer Mehrheit (74 Prozent), dass man an der Hochschule lernen sollte, wie man KI richtig nutzt. Knapp die Hälfte meint, der KI-Einsatz sollte an allen Hochschulen Standard sein. Zum anderen meinen 60 Prozent, dies führe dazu, dass Studierende weniger selbstständig denken und lernen. Jeder zweite Befragte (51 Prozent) glaubt, durch ChatGPT beschäftigten sich Studierende weniger mit den Studieninhalten.
"
KI,Zeit,2024-03-21,https://www.zeit.de/news/2024-03/21/un-vollversammlung-billigt-erste-resolution-zu-ki,Künstliche Intelligenz: UN-Vollversammlung billigt erste Resolution zu KI | ZEIT ONLINE,"Die Vollversammlung der Vereinten Nationen hat erstmals eine Resolution zu Künstlicher Intelligenz verabschiedet. Das von den Vereinigten Staaten eingebrachte Papier wurde in New York angenommen. 

In dem Text heißt es, die schnelle Entwicklung von KI brauche internationale Regeln und ermögliche eine Zusammenarbeit besonders mit ärmeren Ländern. Dringend erreicht werden müsse ein globaler Konsens über «sichere und vertrauenswürdige Systeme der Künstlichen Intelligenz». Verurteilt wird in dem Papier der Missbrauch von KI. 

Der Entwurf war nach monatelangen Verhandlungen unter den Diplomatinnen und Diplomaten bereits zuvor im größten UN-Gremium ohne weitere Änderungsvorschläge akzeptiert worden. Dies machte eine einstimmige Annahme in der Vollversammlung mit ihren 193 Mitgliedern im Konsensverfahren ohne formale Abstimmung möglich. Resolutionen der Vollversammlung sind nicht rechtlich bindend.

Künstliche Intelligenz bezeichnet meist Anwendungen auf Basis maschinellen Lernens. Die rasante Entwicklung immer potenterer Software bietet dabei riesige Chancen und schürt Hoffnung etwa auf große Fortschritte im Kampf gegen Krankheiten wie Krebs oder bei Problemen wie der Klimakrise. Gleichzeitig gibt es aber Sorgen, dass mächtige Anwendungen großen Schaden anrichten könnten, wenn sie nicht reguliert werden. 

UN-Generalsekretär António Guterres hatte schon mehrfach vor den Gefahren der Technologie gewarnt und eine Expertenrunde einberufen, die die Vereinten Nationen bei dem Thema berät. Auch der UN-Sicherheitsrat hat sich schon mit dem Thema befasst. 

Mitte des Monats hatte das EU-Parlament grünes Licht für schärfere Regeln für Künstliche Intelligenz in der Europäischen Union gegeben. Nach Angaben des Parlaments handelt es sich um das weltweit erste KI-Gesetz. Demnach sollen KI-Systeme künftig in verschiedene Risikogruppen eingeteilt werden. Je höher die potenziellen Gefahren einer Anwendung sind, desto höher sollen die Anforderungen sein.

© dpa-infocom, dpa:240321-99-420445/2
"
KI,Zeit,2024-03-20,https://www.zeit.de/digital/mobil/2024-03/apple-google-ki-iphone-gemini-abkommen,Apple und Google: Vielleicht der schnellste Verfolger | ZEIT ONLINE,"Selten gab es eine
Technologie, auf die so
schnell alle großen Techunternehmen aufgesprungen sind, wie künstliche Intelligenz: Microsoft finanziert OpenAI und damit ChatGPT, Google will seine
Suche durch das KI-Modell Gemini verbessern, Meta hat sein Modell Llama als Open
Source veröffentlicht, auf Amazon sollen Händler ihre Produktseiten mit KI
erstellen lassen, und Chiphersteller Nvidia verdient sich mit dem ganzen Hype einen
goldenen Chip. 

Fehlt Ihnen ein
Unternehmen in dieser Aufzählung? Richtig, Apple. Von einer KI aus Cupertino
hat man bisher wenig vernommen. 

In den
vergangenen Tagen sickerte dann aber etwas durch: Wie zuerst Bloomberg berichtete, soll
sich Apple in Gesprächen mit Google befinden, um die KI von Google Gemini zu lizenzieren und in künftige iPhone-Software einzubauen. Ja, das Google, das selbst
Smartphones mit KI herstellt, das mit seinem Betriebssystem Android gar ein direkter
Konkurrent von Apple ist. 

Wenn die Berichte
stimmen, wäre die naheliegendste Lesart also: Das Unternehmen aus Cupertino, das lange als technologischer
Innovationstreiber galt, lange den Titel des wertvollsten Unternehmens der Welt
hielt, gibt das Rennen um die größte Innovation der Gegenwart auf. Gute Nacht, Apple.

So einfach ist es
aber nicht. Im Gegenteil: Ein Pakt mit Google könnte gerade das Klügste sein,
was Apple machen kann.

Apple ist bekannt
dafür, dass es nicht gerne künstliche Intelligenz sagt (so
wie es ja auch nicht so gerne virtuelle Realität sagt). Während Google-Chef
Sundar Pichai im
vergangenen Jahr auf der eigenen
Entwicklerkonferenz künstliche Intelligenz beinahe massenhaft (oder jedenfalls mit Meme-Potenzial)
erwähnte, sagte drei Wochen später Apple-Chef Tim
Cook auf Apples Entwicklerkonferenz das Wort nicht einmal. Apple spricht
lieber von ""maschinellem Lernen"" – nicht unbedingt, um sich abzugrenzen, es scheint
schlicht der Unternehmenssprech zu sein. Der stammt vermutlich noch aus der Prä-ChatGPT-Zeit,
in der sich die Techszene stärker als aktuell bemühte, zwischen verschiedenen
Formen von künstlicher Intelligenz sprachlich zu differenzieren. 

Denn Apple baut
schon seit einiger Zeit auf künstliche Intelligenz: Seit den 2017 vorgestellten
iPhone 8 und iPhone X haben iPhones auf ihrem Chip eine für KI-Anwendungen
(oder eben: maschinelles Lernen) optimierte Neural Engine. Seitdem Apple auch
die Chips für Macs selbst entwickelt, ist diese auch dort eingebaut, also
seit 2020. Die Geräte nutzen diese Engine etwa für Videobearbeitung oder beim
Fotografieren, um den
Himmel blauer und die Haut von Menschen lebendiger abzubilden. Sie ist der wesentliche
Grund dafür, warum iPhone-Fotos oft besser
aussehen als die Realität.

Apple nutzt KI
also längst – nur nicht im Scheinwerferlicht. Das ist aber das, was seit der Veröffentlichung
von ChatGPT im Herbst 2022 viele Kundinnen (und vermutlich noch mehr Investorinnen)
erwarten: Text- und Bildgeneratoren, die aus kurzen prompts, also
Aufforderungen, Gedichte schreiben, Kunstwerke zeichnen oder Songs komponieren.


Diese Erwartung wird
zum Problem für den Konzern: Seit 2011 war Apple die meiste Zeit das
wertvollste Unternehmen der Welt, nur zeitweise von Microsoft und Google-Mutter
Alphabet überholt. Seit Anfang 2024 ist Microsoft wieder auf Platz eins – der
wesentliche Grund dafür dürfte das Engagement des Konzerns im Bereich
generativer künstlicher Intelligenz sein. Microsoft investierte Milliarden in
das Unternehmen hinter ChatGPT, OpenAI, und sicherte sich so die Möglichkeiten
zu, die Technologie in Windows und sein Office-Paket zu integrieren. Microsoft beginnt
gerade erst, damit wirklich Geld zu verdienen, aber allein das ökonomische
Potenzial von KI treibt den Wert des Unternehmens in die Höhe.

Auch wenn Apple
oft versucht, sich unabhängig von Trends zu machen: An dem KI-Trend kommt auch
das Unternehmen aus Cupertino nicht vorbei. Das zeigte sich etwa bei der jährlichen
Investorenversammlung des Unternehmens Ende Februar: Da versprach Tim Cook seinen
Investoren, Apple werde in diesem Jahr neue
Wege in der generativen KI beschreiten – diesmal sprach er nicht von
maschinellem Lernen. Das dürfte bisher die deutlichste Ankündigung des überaus
verschwiegenen Konzerns sein, dass es größere KI-Engagements plant. Ein
weiteres Signal: Seit vergangenem Jahr sucht Apple verstärkt
Mitarbeiter mit Expertise in generativer KI, auch die ehemaligen Mitarbeiterinnen
des dem Vernehmen nach eingestellten
Elektroauto-Projektes sollen angeblich künftig an künstlicher Intelligenz
arbeiten.

In der Ankündigung
seines neuen MacBook Airs mit M3-Chip lässt sich die neue Stoßrichtung
ablesen: Darin schreibt Apple, das MacBook Air sei ""nach wie vor der weltweit
beste Consumer Laptop für KI"". Bei den Pressevorführungen des Gerätes zeigte
Apple die KI-Funktionen der Grafiksoftware Blender, die Copilot-Funktionen von (Microsofts!)
Powerpoint und die App FreeChat, mit der ein durch Metas Modell Llama
angetriebener Chatbot ohne Cloud-Anbindung, auf dem Gerät selbst, lief. Der antwortete
fast in Echtzeit.

Die Botschaft: Wir
können auch KI. Und das noch mindestens so gut wie Microsoft.
"
KI,Zeit,2024-03-20,https://www.zeit.de/digital/datenschutz/2024-03/ki-kuenstliche-intelligenz-grundrechte-datenschutz-privatsphaere,Künstliche Intelligenz: Datenschutzbeauftragter mahnt zur Einhaltung der Grundrechte bei KI | ZEIT ONLINE,"Der Datenschutzbeauftragte der Bundesregierung, Ulrich Kelber, hat verbindliche
Rahmenbedingungen für den Einsatz von künstlicher Intelligenz (KI) gefordert.
""Datenschutz und Privatsphäre sind Kernelemente, ohne die der sichere
Einsatz von KI nicht denkbar ist"", schreibt Kelber in seinem Tätigkeitsbericht für das Jahr 2023. Bei der Forschung, Anwendung, Auswertung und Regulierung im KI-Bereich
müssten Datenschutz und Privatsphäre geschützt werden. Je nachdem, wie künstliche Intelligenz
eingesetzt werde, berge sie ""das Potenzial für Grundrechtseinschränkungen
und Diskriminierungen"".

Kelber übergab seinen Bericht Bundestagspräsidentin
Bärbel Bas (SPD). Darin mahnt er die Bundesregierung an, die Datenschutz-Grundverordnung bei der
Umsetzung der kürzlich auf europäischer Ebene beschlossenen KI-Verordnung zu
berücksichtigen. ""Ich empfehle dem Gesetzgeber, die sich aus der
KI-Verordnung der EU ergebende nationale KI-Aufsichtsstruktur zeitnah
festzulegen"", schreibt Kelber. Die KI-Verordnung des Europaparlaments beinhaltet
strengere Auflagen für Gesichtserkennungssysteme und andere Anwendungen.

Der Datenschutzbeauftragte sieht zudem die geplante EU-Verordnung zur
Chatkontrolle kritisch. Bei dieser geht es um das Ausspähen verschlüsselter
Privatchats. Kelber empfiehlt dem Bundestag in seinem Bericht, gegenüber der
Bundesregierung und dem EU-Gesetzgeber ""auf eine erhebliche,
grundrechtskonforme Überarbeitung"" des Verordnungsentwurfs zur
Chatkontrolle zu drängen.

Der Entwurf müsse eine ""durchgehende
Ende-zu-Ende-Verschlüsselung"" gewährleisten, die deutsche und europäische
Grundrechte wahre und ""ein flächendeckendes und anlassloses Auslesen
privater Kommunikation verbietet"". Andernfalls sei ein Entwurf
""insgesamt abzulehnen"". 

In seinem Bericht schildert Kelber auch, dass sich Bürgerinnen und
Bürger an seine Behörde gewandt hätten, da sie Sorge um die Sicherheit ihrer
Daten im Gesundheitsbereich hätten. ""Die Digitalisierung des
Gesundheitswesens und der Pflege begrüße ich"", schrieb Kelber.
""Allerdings muss die Digitalisierung datenschutzkonform erfolgen.""
Die geplante Widerspruchslösung bei der elektronischen Patientenakte greife
""erheblich in das Grundrecht auf die informationelle Selbstbestimmung
ein"".

Der Bundesrat hatte im Februar einen Gesetzentwurf von
Gesundheitsminister Karl Lauterbach (SPD) und damit die Ausweitung der
E-Patientenakte gebilligt. Deren Nutzung soll ab 2025 für alle gesetzlich
Versicherten zum Normalfall werden. Patientinnen und Patienten können dem aber widersprechen.
"
KI,Zeit,2024-03-20,https://www.zeit.de/news/2024-03/20/nvidia-chef-problem-von-ki-halluzinationen-loesbar,"Künstliche Intelligenz: Nvidia-Chef: Problem von KI-""Halluzinationen"" lösbar | ZEIT ONLINE","Ein zentrales Problem der Verlässlichkeit von KI-Software wird sich nach Einschätzung von Nvidia-Chef Jensen Huang durchaus beheben lassen. Ein Weg sei, die Software die Informationen überprüfen zu lassen. «Halluzinationen sind sehr lösbar», sagte Huang am Rande der Nvidia-Entwicklerkonferenz GTC. Als KI-Halluzinationen werden Fälle bezeichnet, in denen Software Dinge einfach erfindet.

Das Problem entsteht, weil Programme wie der Chatbot ChatGPT beim Formulieren von Texten Wort für Wort schätzen, wie ein Satz weitergehen könnte. Deswegen können sie manchmal völlig falsche Antworten geben, selbst wenn sie nur mit korrekten Informationen trainiert wurden.

Der Halbleiter-Spezialist Nvidia spielt mit seinen Chip-Systemen eine Schlüsselrolle im aktuellen Boom bei Künstlicher Intelligenz. Bei der GTC stellte Huang die nächste Generation der Computer-Plattform mit dem Namen Blackwell vor, auf der KI-Anwendungen effizienter laufen sollen.

Huang zeigte sich überzeugt, dass Künstliche Intelligenz den Menschen weitgehend das Schreiben von Software-Code abnehmen werde. Eine Zeit lang habe es geheißen, dass alle Programmieren lernen sollten. «Ich denke, das ist falsch», sagte der Nvidia-Chef. Es sei nicht die Aufgabe der Menschen gewesen, eine Programmiersprache zu lernen. Stattdessen sollten Computer nützlich sein, auch wenn man keine Programmiersprache beherrsche. Man werde allerdings lernen müssen, von KI-Software mithilfe richtig formulierter Anfragen - sogenannter «Prompts» - das gewünschte Ergebnis zu bekommen. Das sei aber grundsätzlich vergleichbar mit Fähigkeiten der Menschen im Umgang untereinander: «Wenn meine Frau mit mir spricht, ist das auch Prompt-Gestaltung - und das funktioniert perfekt.»

Huang hält es für möglich, dass Künstliche Intelligenz in wenigen Jahren in verschiedenen Bereichen die Fähigkeiten von Menschen übertreffen werde - zum Beispiel bei Mathematik- und Anwalts-Prüfungen oder beim Verständnis von Texten. Daran orientiere er sich auch in der aktuellen Debatte darüber, ob Software die Stufe der sogenannten allgemeinen Künstlichen Intelligenz erreichen werde - womit meist gemeint ist, dass sie verallgemeinern könne und den Menschen überlegen sei. Allerdings gebe es keine Definition dafür, über die sich alle einig seien, betonte er.

© dpa-infocom, dpa:240320-99-399374/2
"
KI,Zeit,2024-03-19,https://www.zeit.de/news/2024-03/19/nvidia-kuendigt-neues-computersystem-fuer-ki-infrastruktur-an,Künstliche Intelligenz: Nvidia kündigt neues Computersystem für KI-Infrastruktur an | ZEIT ONLINE,"Der Chipkonzern Nvidia will seine führende Rolle bei Technik für Künstliche Intelligenz ausbauen. Firmenchef Jensen Huang stellte dafür eine neue Generation von Nvidias Computerplattform vor. Nvidia sieht das System mit dem Namen Blackwell als «Antrieb einer neuen industriellen Revolution» durch KI. Das System sei beim Anlernen von Künstlicher Intelligenz vier Mal leistungsstärker als die aktuelle Generation Grace Hopper.

Nvidias Computersysteme dominieren in Rechenzentren beim KI-Training. Der Konzern will auch seine Rolle bei der Erzeugung von Inhalten mit Hilfe Künstlicher Intelligenz ausbauen. Das «Blackwell»-System sei darin 30 Mal besser als «Hopper», betonte Huang am Montag auf der hauseigenen Entwicklerkonferenz GTC in San Jose. Zudem gibt es von Nvidia dafür neue Software, die über Schnittstellen auch via Cloud genutzt werden kann. 

Huang zeigte sich überzeugt, dass in der Zukunft die meisten Inhalte nicht vorgefertigt aus Speichern abgerufen werden, sondern dass KI-Software sie ausgehend aus der aktuellen Situation frisch erzeugen werde. Nvidia habe das Computersystem für diese Zukunft entwickelt. So werde man sich zum Beispiel mit Gebäuden per Chatbot unterhalten können, anstelle irgendwo Daten einzusehen.

Mit Grace Hopper hätte man zum Beispiel den Chatbot ChatGPT innerhalb von drei Monaten mit 8000 Nvidia-Chips und einem Stromverbrauch von 15 Megawatt trainieren können, sagte Huang. Mit Blackwell schaffe man das in derselben Zeit mit 2000 Chips und 4 Megawatt Strom.

ChatGPT ist der Chatbot der Entwicklerfirma OpenAI, der vor über einem Jahr den aktuellen Hype rund um Künstliche Intelligenz auslöste. Solche Software wird mit gewaltigen Mengen an Informationen trainiert und kann auf dieser Grundlage zum Beispiel Sätze auf dem sprachlichen Niveau eines Menschen und Bilder aus Text-Vorgaben generieren. Nvidia wolle die Technologie auch nutzen, um Wettervorhersagen zu erzeugen, sagte Huang. Dafür entwickelt der Konzern eine Welt-Simulation mit dem Namen «Earth 2».

Nvidia bündele seine Chips für Blackwell in Computersysteme mit rund 600 000 Bauteilen und einem Gewicht von mehr als 1,3 Tonnen, betonte Huang. Die ursprünglich für Grafikkarten entwickelten Nvidia-Technologien bewähren sich schon seit Langem bei der Rechenarbeit für Anwendungen mit Künstlicher Intelligenz. Konkurrenten wie Intel und AMD konnten bisher keinen Anschluss finden. Das lässt das Geschäft - und den Börsenwert - von Nvidia rasant wachsen. Große KI-Unternehmen wie Microsoft, Google und Amazon planen bereits den Einsatz von Blackwell. 

Nvidias neuer «KI-Superchip» ist nach dem amerikanischen Mathematiker David Blackwell benannt. Nvidia will damit auch den Einsatz sogenannter «digitaler Zwillinge» vorantreiben, in denen Unternehmen ihr gesamtes Geschäft im Computer simulieren können. Bevor man etwas in der realen Welt baut, werde man es künftig zunächst digital simulieren, betonte Huang.

Mit diesem Konzept macht Nvidia auch Apples Computerbrille Vision Pro nützlicher für den Einsatz in Unternehmen. Nvidias 3D-Umgebung Omniverse, mit der Firmen «digitale Zwillinge» ihrer Produkte und Fabriken erstellen können, kommt auf das Apple-Gerät. Die Technologie wird oft bei der Gestaltung von Objekten verwendet. Huang demonstrierte sie am Montag bei der hauseigenen Entwicklerkonferenz GTC unter anderem am Beispiel des südkoreanischen Autobauers Hyundai. So konnte ein Designer verschiedene Farbvarianten eines Automodells in unterschiedlichen Umgebungen ansehen - und sich auch virtuell ans Lenkrad setzen. Apple will mit der 3499 Dollar (rund 3200 Euro) teuren Vision Pro einen sogenannten «räumlichen Computer» etablieren, mit dem man sich digitale Inhalte auch innerhalb der realen Umgebung anzeigen lassen kann. 

Nvidia setzt auch auf Roboter. «Alles, was sich bewegt, wird robotisch sein», sagte der Firmenchef. Das Ziel sei, dass Roboter allein schon dadurch lernen können, dass sie Menschen beobachten. «Der ChatGPT-Moment für Robotik könnte unmittelbar bevorstehen», sagte Huang. Eine zentrale Rolle sollen dabei Simulationen spielen, in denen Roboter für die reale Welt lernen.

© dpa-infocom, dpa:240319-99-385354/9
"
KI,Zeit,2024-03-18,https://www.zeit.de/news/2024-03/18/patentamt-deutsche-industrie-verstaerkt-ki-entwicklung,Künstliche Intelligenz: Patentamt: Deutsche Industrie verstärkt KI-Entwicklung | ZEIT ONLINE,"Industrie und Wissenschaft in Deutschland haben Forschung und Entwicklung bei Künstlicher Intelligenz verstärkt, liegen jedoch weit hinter den USA zurück. Das geht aus einer am Montag veröffentlichten Analyse des Deutschen Patent- und Markenamts (DPMA) hervor. Demnach war die Zahl der veröffentlichten Patentanmeldungen mit KI-Bezug beim Deutschen und beim Europäischen Patentamt im vergangenen Jahr 40 Prozent höher als 2019, wie die Münchner Bundesbehörde am Montag mitteilte.

Es dominierten Unternehmen und Forschungseinrichtungen aus den USA, die allein fast ein Drittel der Neuentwicklungen in Technologien mit KI-Bezug angemeldet hatten. Auf Rang zwei folgte Deutschland mit einem Anteil von 17,1 Prozent vor Japan mit 12,4 Prozent. Die Ränge 4 und 5 belegten China (10,1 Prozent) und Südkorea (4,7 Prozent). Am schnellsten gestiegen ist die Zahl chinesischer Entwicklungen, die 2023 bereits 15 Prozent höher war als 2022. Deutsche Anmeldungen legten im gleichen Zeitraum demnach um 5,2 Prozent zu.

«Die starke und breite Innovationsdynamik verdeutlicht, dass KI zukünftig in vielen Lebensbereichen eine große Rolle spielen wird», sagte DPMA-Präsidentin Eva Schewior. Die KI-Analyse ist an diesem Dienstag und Mittwoch Thema des alljährlichen DPMA-Nutzerforums.

Da Unternehmen Patente für neue Erfindungen vor allem in ihren Heimatländern anmelden, liegen in der deutschen Patentstatistik insgesamt üblicherweise einheimische Unternehmen auf den ersten Plätzen.

Bei den Technologiefeldern mit KI-Bezug ist dies anders: Zwar belegte mit Bosch ein deutsches Unternehmen Platz ein, doch auf den weiteren Plätzen folgten der chinesische Huawei-Konzern, die US-Unternehmen Google und Microsoft sowie Samsung aus Südkorea.

Bundesbildungsministerin Bettina Stark-Watzinger (FDP) wertete die Analyse als Beleg, dass Deutschland in einer sehr guten Ausgangsposition sei. «Aber darauf dürfen wir uns nicht ausruhen.» KI sei eine riesige Chance und habe enormes Potenzial etwa in Bildung, Forschung und Wirtschaft. «Wir dürfen sie nicht verpassen.»

Patente werden nach einem international einheitlichen Standard der Patentklassifikation angemeldet. Da es für KI keine eigenen Patentklassen gibt, werteten die Fachleute des DPMA die Klassen aus den Technologiefeldern aus, in denen KI zum Einsatz kommt.

Dazu zählen beispielsweise Computer-, Medizin- und Fahrzeugtechnik, Robotik sowie Sprach- und Bildanalyse. In die Auswertung flossen Patentanmeldungen mit Wirkung für Deutschland ein, die beim Deutschen oder beim ebenfalls in München ansässigen Europäischen Patentamt eingereicht wurden, Doppelanmeldungen bei beiden Behörden herausgerechnet.

Die Behörde betonte, dass diese Methodik eine Annäherung darstelle, aber die verschiedenen Trends gut aufzeige. Die Analyse erhebt laut DPMA aber keinen Anspruch auf Vollständigkeit für alle KI-bezogenen Erfindungen, außerdem könnten auch Erfindungen ohne KI-Bezug enthalten sein. Da Patentanmeldungen erst nach einer Frist von 18 Monaten veröffentlicht werden, sind in der Statistik noch keine Erfindungen des vergangenen Jahres enthalten.

© dpa-infocom, dpa:240318-99-376238/3
"
KI,Zeit,2024-03-18,https://www.zeit.de/geld/2024-03/kenneth-rogoff-finanzmarkt-ki-krypto-finanzkrise,"Kenneth Rogoff: ""Die Börse mag das. Aber ich bin besorgt"" | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-03-17,https://www.zeit.de/news/2024-03/17/alligatoah-kuenstliche-intelligenz-spornt-mich-an,Musik: Alligatoah: Künstliche Intelligenz spornt mich an | ZEIT ONLINE,"Rapper Alligatoah («Willst du») hat keine Angst vor dem Einzug von Künstlicher Intelligenz (KI) in der Musikbranche. «Ich freue mich auch auf den Moment, wenn ich ersetzbar werde», sagte der 34-Jährige, der bürgerlich Lukas Strobel heißt, der Deutschen Presse-Agentur. Er wohnt seit etwa 15 Jahren in Berlin. «Ich will es mit einer KI aufnehmen. Wenn die irgendwann die Musik machen kann, die ich in der Vergangenheit gemacht habe, dann muss ich mir für die Zukunft etwas Besseres ausdenken. Und wenn ich das nicht kann, bin ich im falschen Beruf.»

Alligatoahs neues Album «Off» erscheint am 22. März. Im Musikvideo zu seiner Single «So Raus», für die er mit Limp-Bizkit-Frontmann Fred Durst zusammenarbeitete, geht es unter anderem auch um Videos, die mittels Künstlicher Intelligenz erzeugt werden können.

© dpa-infocom, dpa:240317-99-367063/2
"
AI,Zeit,2024-03-16,https://www.zeit.de/2024/12/dustin-moskovitz-facebook-kuenstliche-intelligenz,Dustin Moskovitz : Der Milliardär und die Apokalypse | ZEIT ONLINE,
AI,Zeit,2024-03-15,https://www.zeit.de/news/2024-03/15/sap-ki-chef-ki-guetesiegel-grundsaetzlich-sinnvoll,Softwarehersteller: SAP-KI-Chef: KI-Gütesiegel grundsätzlich sinnvoll | ZEIT ONLINE,"Europas größter Softwarehersteller SAP befürwortet die Einführung eines Gütesiegels für Künstliche Intelligenz (KI). «Ich halte das für grundsätzlich sinnvoll», sagte der KI-Chef des Dax-Konzerns, Philipp Herzig, dem Wirtschaftsmagazin «Capital» (Freitag). Die Walldorfer hatten erst im Januar einen großangelegten Umbau des Unternehmens und eine Investition in Milliardenhöhe angekündigt, um die Geschäfte mit KI voranzutreiben.

Die Idee für ein KI-Gütesiegel geht auf die Digitalstrategie der Bundesregierung aus SPD, Grünen und FDP zurück. Das Prädikat «AI Made in Germany» soll dem Papier zufolge zu einem weltweit anerkannten Gütesiegel aufgebaut werden, das deutschen Unternehmen im internationalen Wettbewerb Vorteile verschafft. Im November hatte Bundesdigitalminister Volker Wissing (FDP) erste Details genannt: Ein Konsortium soll demnach einheitliche KI-Qualitätsstandards entwickeln, auf denen das freiwillige Gütesiegel nach deutschen und europäischen Werten basieren soll.

Der Hype um Künstliche Intelligenz (KI) oder englisch Artificial Intelligence (AI) hatte sich im vorvergangenen Jahr an der Veröffentlichung des Chatbots ChatGPT entzündet. Seither möchten alle Softwarekonzerne ein Stück vom erhofft großen Kuchen abhaben. «Wir gehen von einem massiven Wachstum in diesem Bereich aus», sagte Herzig. Ihm zufolge nutzen bislang etwa 24.000 der mehr als 400.000 Kundinnen und Kunden regelmäßig KI über alle SAP-Anwendungen hinweg.

Künstliche Intelligenz bezeichnet meist Anwendungen auf Grundlage maschinellen Lernens, bei denen eine Software große Datenmengen nach Übereinstimmungen durchforstet und daraus Schlussfolgerungen zieht. Sie werden bereits aktuell in vielen Bereichen eingesetzt. Aufnahmen von Computertomografen können dadurch zum Beispiel schneller und mit höherer Genauigkeit ausgewertet werden. Chatbots oder automatische Playlists von Streaming-Diensten arbeiten ebenfalls mit KI.

© dpa-infocom, dpa:240315-99-347179/2
"
Artificial Intelligence,Zeit,2024-03-15,https://www.zeit.de/news/2024-03/15/sap-ki-chef-ki-guetesiegel-grundsaetzlich-sinnvoll,Softwarehersteller: SAP-KI-Chef: KI-Gütesiegel grundsätzlich sinnvoll | ZEIT ONLINE,"Europas größter Softwarehersteller SAP befürwortet die Einführung eines Gütesiegels für Künstliche Intelligenz (KI). «Ich halte das für grundsätzlich sinnvoll», sagte der KI-Chef des Dax-Konzerns, Philipp Herzig, dem Wirtschaftsmagazin «Capital» (Freitag). Die Walldorfer hatten erst im Januar einen großangelegten Umbau des Unternehmens und eine Investition in Milliardenhöhe angekündigt, um die Geschäfte mit KI voranzutreiben.

Die Idee für ein KI-Gütesiegel geht auf die Digitalstrategie der Bundesregierung aus SPD, Grünen und FDP zurück. Das Prädikat «AI Made in Germany» soll dem Papier zufolge zu einem weltweit anerkannten Gütesiegel aufgebaut werden, das deutschen Unternehmen im internationalen Wettbewerb Vorteile verschafft. Im November hatte Bundesdigitalminister Volker Wissing (FDP) erste Details genannt: Ein Konsortium soll demnach einheitliche KI-Qualitätsstandards entwickeln, auf denen das freiwillige Gütesiegel nach deutschen und europäischen Werten basieren soll.

Der Hype um Künstliche Intelligenz (KI) oder englisch Artificial Intelligence (AI) hatte sich im vorvergangenen Jahr an der Veröffentlichung des Chatbots ChatGPT entzündet. Seither möchten alle Softwarekonzerne ein Stück vom erhofft großen Kuchen abhaben. «Wir gehen von einem massiven Wachstum in diesem Bereich aus», sagte Herzig. Ihm zufolge nutzen bislang etwa 24.000 der mehr als 400.000 Kundinnen und Kunden regelmäßig KI über alle SAP-Anwendungen hinweg.

Künstliche Intelligenz bezeichnet meist Anwendungen auf Grundlage maschinellen Lernens, bei denen eine Software große Datenmengen nach Übereinstimmungen durchforstet und daraus Schlussfolgerungen zieht. Sie werden bereits aktuell in vielen Bereichen eingesetzt. Aufnahmen von Computertomografen können dadurch zum Beispiel schneller und mit höherer Genauigkeit ausgewertet werden. Chatbots oder automatische Playlists von Streaming-Diensten arbeiten ebenfalls mit KI.

© dpa-infocom, dpa:240315-99-347179/2
"
KI,Zeit,2024-03-16,https://www.zeit.de/2024/12/dustin-moskovitz-facebook-kuenstliche-intelligenz,Dustin Moskovitz : Der Milliardär und die Apokalypse | ZEIT ONLINE,
KI,Zeit,2024-03-15,https://www.zeit.de/news/2024-03/15/sap-ki-chef-ki-guetesiegel-grundsaetzlich-sinnvoll,Softwarehersteller: SAP-KI-Chef: KI-Gütesiegel grundsätzlich sinnvoll | ZEIT ONLINE,"Europas größter Softwarehersteller SAP befürwortet die Einführung eines Gütesiegels für Künstliche Intelligenz (KI). «Ich halte das für grundsätzlich sinnvoll», sagte der KI-Chef des Dax-Konzerns, Philipp Herzig, dem Wirtschaftsmagazin «Capital» (Freitag). Die Walldorfer hatten erst im Januar einen großangelegten Umbau des Unternehmens und eine Investition in Milliardenhöhe angekündigt, um die Geschäfte mit KI voranzutreiben.

Die Idee für ein KI-Gütesiegel geht auf die Digitalstrategie der Bundesregierung aus SPD, Grünen und FDP zurück. Das Prädikat «AI Made in Germany» soll dem Papier zufolge zu einem weltweit anerkannten Gütesiegel aufgebaut werden, das deutschen Unternehmen im internationalen Wettbewerb Vorteile verschafft. Im November hatte Bundesdigitalminister Volker Wissing (FDP) erste Details genannt: Ein Konsortium soll demnach einheitliche KI-Qualitätsstandards entwickeln, auf denen das freiwillige Gütesiegel nach deutschen und europäischen Werten basieren soll.

Der Hype um Künstliche Intelligenz (KI) oder englisch Artificial Intelligence (AI) hatte sich im vorvergangenen Jahr an der Veröffentlichung des Chatbots ChatGPT entzündet. Seither möchten alle Softwarekonzerne ein Stück vom erhofft großen Kuchen abhaben. «Wir gehen von einem massiven Wachstum in diesem Bereich aus», sagte Herzig. Ihm zufolge nutzen bislang etwa 24.000 der mehr als 400.000 Kundinnen und Kunden regelmäßig KI über alle SAP-Anwendungen hinweg.

Künstliche Intelligenz bezeichnet meist Anwendungen auf Grundlage maschinellen Lernens, bei denen eine Software große Datenmengen nach Übereinstimmungen durchforstet und daraus Schlussfolgerungen zieht. Sie werden bereits aktuell in vielen Bereichen eingesetzt. Aufnahmen von Computertomografen können dadurch zum Beispiel schneller und mit höherer Genauigkeit ausgewertet werden. Chatbots oder automatische Playlists von Streaming-Diensten arbeiten ebenfalls mit KI.

© dpa-infocom, dpa:240315-99-347179/2
"
Künstliche Intelligenz,Zeit,2024-03-13,https://www.zeit.de/news/2024-03/13/was-bedeutet-das-neue-ki-gesetz-der-eu,Fragen & Antworten: Was bedeutet das neue KI-Gesetz der EU? | ZEIT ONLINE,"Diskriminierungen oder Falschmeldungen: Künstliche Intelligenz (KI) birgt trotz aller Vorteile auch Risiken. Um diese zu mindern, hat das EU-Parlament an diesem Mittwoch in Straßburg den Weg für ein KI-Gesetz frei gemacht. Entwickler, Betreiber und Anbieter von gewissen KI-Systemen müssen damit neuen Vorgaben folgen. Doch was heißt das genau?

Das Gesetz zielt darauf ab, die Nutzung von Künstlicher Intelligenz (KI) in der Europäischen Union sicherer zu machen. Es soll sicherstellen, dass KI-Systeme transparent, nachvollziehbar, nicht diskriminierend und umweltfreundlich sind. Ein wichtiger Aspekt ist, dass die KI-Systeme von Menschen überwacht werden und nicht nur von anderen Technologien.

Die Pläne gehen auf einen Vorschlag der EU-Kommission von 2021 zurück. Demnach solle KI-Systeme künftig in verschiedene Risikogruppen eingeteilt werden. Je höher die potenziellen Gefahren einer Anwendung sind, desto höher sollen die Anforderungen sein. KI, die zur Manipulation des menschlichen Verhaltens eingesetzt wird, soll demnach in Europa verboten werden.

Künstliche Intelligenz bezeichnet meist Anwendungen auf Basis maschinellen Lernens, bei denen eine Software große Datenmengen nach Übereinstimmungen durchforstet und daraus Schlussfolgerungen zieht.

Die Verwendung von KI ist weitverbreitet und nimmt ständig zu. Obwohl Künstliche Intelligenz oft nützlich ist, birgt sie auch potenzielle Gefahren. So wird Künstliche Intelligenz beispielsweise in sozialen Medien, zur Optimierung von Werbung oder in Einstellungsprozessen verwendet.

Die Europäische Kommission betont, dass, obwohl die meisten KI-Systeme keine großen Risiken darstellen, es wichtig ist, Regulierungen zu schaffen, um alle möglichen Gefahren einzudämmen. Besondere Risiken sieht die Kommission bei biometrischen Überwachungen und bei persönlichen Entscheidungen, die von KI-Systemen unterstützt werden, beispielsweise im Bereich der Strafverfolgung, Bildung und Erziehung.

Das Gesetz gilt für alle, die KI-Systeme innerhalb der EU entwickeln, anbieten oder nutzen. Dies betrifft öffentliche und private Akteure sowohl innerhalb als auch außerhalb der EU, teilte die Kommission weiter mit.

Sechs Monate nach Inkrafttreten des Gesetzes müssen die Mitgliedsstaaten zuerst verbotene Systeme schrittweise außer Betrieb nehmen. Nach zwei Jahren werden dann alle Punkte des Gesetzes vollständig umgesetzt sein.

Die Mitgliedstaaten müssen Sanktionen beschließen, wenn Unternehmen die Vorschriften nicht einhalten. Dies können Geldstrafen sein. Privatpersonen, die Verstöße gegen die Vorschriften entdecken, können sich bei nationalen Behörden beschweren. Diese können dann Überwachungsverfahren einleiten und gegebenenfalls Strafen verhängen.

© dpa-infocom, dpa:240313-99-318882/4
"
Künstliche Intelligenz,Zeit,2024-03-13,https://www.zeit.de/digital/2024-03/eu-parlament-stimmt-ki-gesetz-zu,Europäische Union: Europaparlament beschließt weltweit erstes KI-Gesetz | ZEIT ONLINE,"Das Europaparlament hat ein Gesetz zur Regulierung und Kategorisierung künstlicher Intelligenz (KI) beschlossen. Es handelt sich laut dem Parlament um das erste KI-Gesetz weltweit. Die Abgeordneten stimmten mit großer Mehrheit für die neuen Regeln. Vorgesehen ist etwa eine Kennzeichnungspflicht: Entwickler sollen mit künstlicher Intelligenz erzeugte Texte, Töne und Bilder markieren, um Menschen nicht in die Irre zu führen.

Für ""risikoreiche"" Anwendungen sollen verschärfte Vorschriften gelten, etwa für die Gesichtserkennung an Bahnhöfen oder anderen öffentlichen Orten. Nötig ist künftig eine richterliche Anordnung. Grundsätzlich verboten wird eine Massenüberwachung mit biometrischen Daten wie in China.

Der federführende EU-Binnenmarktkommissar Thierry Breton nannte die neuen Regeln ""historisch"". Nach seinen Angaben handelt es sich um das weltweit erste Gesetz zu künstlicher Intelligenz. Die EU-Länder hatten das KI-Gesetz nach wochenlanger Debatte Anfang Februar angenommen. Wegen Bedenken vor allem in Deutschland und Frankreich hatte der Beschluss wochenlang auf der Kippe gestanden.

Die FDP hatte in letzter Minute Bedenken angemeldet, stimmte nach einer Einigung der Ampelkoalition dann aber zu. Der auch für Digitales zuständige Verkehrsminister Volker Wissing (FDP) sagte danach, er habe ""Verbesserungen für kleine und mittlere Unternehmen"" erzielt. Die Wirtschaft hatte vor zu hohen Auflagen gewarnt, etwa für Start-ups.

Die Bundesregierung setzte sich auf EU-Ebene nach eigenen Angaben zudem mit dafür ein, sogenannte Allzweck-KI wie den Chatbot ChatGPT nicht als Hochrisikoanwendung einzustufen. Durch ChatGPT hatte KI vor gut einem Jahr schlagartig große Aufmerksamkeit bekommen. Die Anwendung kann mit Nutzerinnen und Nutzern über Textnachrichten kommunizieren und in Sekundenschnelle ausführliche Antworten auf Fragen geben. Inzwischen kann sie auch Bilder erstellen, die täuschend echt aussehen.
"
Künstliche Intelligenz,Zeit,2024-03-13,https://www.zeit.de/digital/2024-03/ki-gesetz-eu-parlament-regulierung,KI-Gesetz der EU: Die beste schlechte KI-Regulierung der Welt | ZEIT ONLINE,"Immerhin ist es ein Gesetz geworden. Perfekt ist der AI Act bei Weitem nicht. Im Verlauf der jahrelangen Verhandlungen sind Details in einige der Paragrafen hineingerutscht, die mehr Überwachung ermöglichen könnten, statt Bürgerrechte zu schützen.   

Aber zwischenzeitlich schien es möglich, dass der AI Act noch komplett gekippt wird. Und schon deswegen ist es eine gute Nachricht, dass das EU-Parlament ihn am Mittwoch endgültig beschlossen hat. 

Das Gesetz regelt, welche Anwendungen von künstlicher Intelligenz in Europa erlaubt sind und welche nicht. Das ist die richtige Herangehensweise, denn man muss kein Weltuntergangsprophet sein, um die Befürchtung zu haben, dass sich Unternehmen KI-Anwendungen einfallen lassen könnten, die gefährlich werden können.    

Wenn zum Beispiel Ihr Arbeitgeber vorhaben sollte, eine KI in Ihren Videocalls herumanalysieren zu lassen, ob Ihre Stimme traurig klingt oder wütend oder betriebsratsgründungsmotiviert, dann muss diese Idee in der Schublade bleiben. Denn solche Anwendungen verbietet der AI Act. Und das ist auch gut so.   

Ein Staat, der plant, mit KI automatisiert Visumsanträge zu prüfen, oder eine Universität, die KI einsetzen will, um Klausuren zu korrigieren, können diese Ideen weiterverfolgen, denn solche Anwendungen sind weiterhin erlaubt. Sie sind im AI Act aber als Anwendungen mit hohem Risiko eingestuft – und für die gelten bestimmte Regeln. Zum Beispiel darf die KI das nicht alles allein entscheiden, Menschen müssen den Prozess überwachen.   

Auch das ist richtig. Was wir künstliche Intelligenz nennen, ist oft im Wesentlichen Statistik. Und einfach die Statistik der Vergangenheit in die Zukunft fortzuschreiben, kann gefährlich werden. Wenn zum Beispiel bisher vor allem Visa von Menschen aus bestimmten Ländern abgelehnt wurden, dann könnte eine KI daraus lernen, dass das auch weiterhin so sein soll. Dann könnte sie Anträge aus diesen Ländern ablehnen, obwohl gar nichts dagegen spräche, diese Person ins Land zu lassen.   

Besonders riskante Anwendungen werden verboten, mittelriskante reguliert. Und sogar für Foundation Models wie dem hinter ChatGPT, von denen manche KI-Forscher befürchten, dass sie sich selbstständig machen könnten, wurden in nächtelangen Verhandlungen noch ein paar vorsichtige Spezialregeln in das Gesetz eingearbeitet.    

Also alles so super? Muss man nur noch hoffen, dass die USA und andere Länder dem Brüssel-Effekt folgend ähnliche Regulierungen in Kraft setzen? Nicht ganz. Der AI Act hat auch Schwächen.  

Manche Vorschriften werden wohl zahnlos bleiben und trotzdem aufwendig für Unternehmen sein.   
"
AI,Zeit,2024-03-13,https://www.zeit.de/digital/2024-03/eu-parlament-stimmt-ki-gesetz-zu,Europäische Union: Europaparlament beschließt weltweit erstes KI-Gesetz | ZEIT ONLINE,"Das Europaparlament hat ein Gesetz zur Regulierung und Kategorisierung künstlicher Intelligenz (KI) beschlossen. Es handelt sich laut dem Parlament um das erste KI-Gesetz weltweit. Die Abgeordneten stimmten mit großer Mehrheit für die neuen Regeln. Vorgesehen ist etwa eine Kennzeichnungspflicht: Entwickler sollen mit künstlicher Intelligenz erzeugte Texte, Töne und Bilder markieren, um Menschen nicht in die Irre zu führen.

Für ""risikoreiche"" Anwendungen sollen verschärfte Vorschriften gelten, etwa für die Gesichtserkennung an Bahnhöfen oder anderen öffentlichen Orten. Nötig ist künftig eine richterliche Anordnung. Grundsätzlich verboten wird eine Massenüberwachung mit biometrischen Daten wie in China.

Der federführende EU-Binnenmarktkommissar Thierry Breton nannte die neuen Regeln ""historisch"". Nach seinen Angaben handelt es sich um das weltweit erste Gesetz zu künstlicher Intelligenz. Die EU-Länder hatten das KI-Gesetz nach wochenlanger Debatte Anfang Februar angenommen. Wegen Bedenken vor allem in Deutschland und Frankreich hatte der Beschluss wochenlang auf der Kippe gestanden.

Die FDP hatte in letzter Minute Bedenken angemeldet, stimmte nach einer Einigung der Ampelkoalition dann aber zu. Der auch für Digitales zuständige Verkehrsminister Volker Wissing (FDP) sagte danach, er habe ""Verbesserungen für kleine und mittlere Unternehmen"" erzielt. Die Wirtschaft hatte vor zu hohen Auflagen gewarnt, etwa für Start-ups.

Die Bundesregierung setzte sich auf EU-Ebene nach eigenen Angaben zudem mit dafür ein, sogenannte Allzweck-KI wie den Chatbot ChatGPT nicht als Hochrisikoanwendung einzustufen. Durch ChatGPT hatte KI vor gut einem Jahr schlagartig große Aufmerksamkeit bekommen. Die Anwendung kann mit Nutzerinnen und Nutzern über Textnachrichten kommunizieren und in Sekundenschnelle ausführliche Antworten auf Fragen geben. Inzwischen kann sie auch Bilder erstellen, die täuschend echt aussehen.
"
AI,Zeit,2024-03-13,https://www.zeit.de/digital/2024-03/ki-gesetz-eu-parlament-regulierung,KI-Gesetz der EU: Die beste schlechte KI-Regulierung der Welt | ZEIT ONLINE,"Immerhin ist es ein Gesetz geworden. Perfekt ist der AI Act bei Weitem nicht. Im Verlauf der jahrelangen Verhandlungen sind Details in einige der Paragrafen hineingerutscht, die mehr Überwachung ermöglichen könnten, statt Bürgerrechte zu schützen.   

Aber zwischenzeitlich schien es möglich, dass der AI Act noch komplett gekippt wird. Und schon deswegen ist es eine gute Nachricht, dass das EU-Parlament ihn am Mittwoch endgültig beschlossen hat. 

Das Gesetz regelt, welche Anwendungen von künstlicher Intelligenz in Europa erlaubt sind und welche nicht. Das ist die richtige Herangehensweise, denn man muss kein Weltuntergangsprophet sein, um die Befürchtung zu haben, dass sich Unternehmen KI-Anwendungen einfallen lassen könnten, die gefährlich werden können.    

Wenn zum Beispiel Ihr Arbeitgeber vorhaben sollte, eine KI in Ihren Videocalls herumanalysieren zu lassen, ob Ihre Stimme traurig klingt oder wütend oder betriebsratsgründungsmotiviert, dann muss diese Idee in der Schublade bleiben. Denn solche Anwendungen verbietet der AI Act. Und das ist auch gut so.   

Ein Staat, der plant, mit KI automatisiert Visumsanträge zu prüfen, oder eine Universität, die KI einsetzen will, um Klausuren zu korrigieren, können diese Ideen weiterverfolgen, denn solche Anwendungen sind weiterhin erlaubt. Sie sind im AI Act aber als Anwendungen mit hohem Risiko eingestuft – und für die gelten bestimmte Regeln. Zum Beispiel darf die KI das nicht alles allein entscheiden, Menschen müssen den Prozess überwachen.   

Auch das ist richtig. Was wir künstliche Intelligenz nennen, ist oft im Wesentlichen Statistik. Und einfach die Statistik der Vergangenheit in die Zukunft fortzuschreiben, kann gefährlich werden. Wenn zum Beispiel bisher vor allem Visa von Menschen aus bestimmten Ländern abgelehnt wurden, dann könnte eine KI daraus lernen, dass das auch weiterhin so sein soll. Dann könnte sie Anträge aus diesen Ländern ablehnen, obwohl gar nichts dagegen spräche, diese Person ins Land zu lassen.   

Besonders riskante Anwendungen werden verboten, mittelriskante reguliert. Und sogar für Foundation Models wie dem hinter ChatGPT, von denen manche KI-Forscher befürchten, dass sie sich selbstständig machen könnten, wurden in nächtelangen Verhandlungen noch ein paar vorsichtige Spezialregeln in das Gesetz eingearbeitet.    

Also alles so super? Muss man nur noch hoffen, dass die USA und andere Länder dem Brüssel-Effekt folgend ähnliche Regulierungen in Kraft setzen? Nicht ganz. Der AI Act hat auch Schwächen.  

Manche Vorschriften werden wohl zahnlos bleiben und trotzdem aufwendig für Unternehmen sein.   
"
AI,Zeit,2024-03-13,https://www.zeit.de/digital/internet/2024-03/chiphersteller-kiuenstliche-intelligenz-amd-intel-nvidia,Chiphersteller Nvidia: Der Kampf um den besten Chip | ZEIT ONLINE,
AI,Zeit,2024-03-11,https://www.zeit.de/geld/2024-03/chip-aktien-etf-halbleiterunternehmen-nvidia,Chipaktien: Mit diesen ETFs verdienen Sie am KI-Boom mit | ZEIT ONLINE,
KI,Zeit,2024-03-13,https://www.zeit.de/news/2024-03/13/was-bedeutet-das-neue-ki-gesetz-der-eu,Fragen & Antworten: Was bedeutet das neue KI-Gesetz der EU? | ZEIT ONLINE,"Diskriminierungen oder Falschmeldungen: Künstliche Intelligenz (KI) birgt trotz aller Vorteile auch Risiken. Um diese zu mindern, hat das EU-Parlament an diesem Mittwoch in Straßburg den Weg für ein KI-Gesetz frei gemacht. Entwickler, Betreiber und Anbieter von gewissen KI-Systemen müssen damit neuen Vorgaben folgen. Doch was heißt das genau?

Das Gesetz zielt darauf ab, die Nutzung von Künstlicher Intelligenz (KI) in der Europäischen Union sicherer zu machen. Es soll sicherstellen, dass KI-Systeme transparent, nachvollziehbar, nicht diskriminierend und umweltfreundlich sind. Ein wichtiger Aspekt ist, dass die KI-Systeme von Menschen überwacht werden und nicht nur von anderen Technologien.

Die Pläne gehen auf einen Vorschlag der EU-Kommission von 2021 zurück. Demnach solle KI-Systeme künftig in verschiedene Risikogruppen eingeteilt werden. Je höher die potenziellen Gefahren einer Anwendung sind, desto höher sollen die Anforderungen sein. KI, die zur Manipulation des menschlichen Verhaltens eingesetzt wird, soll demnach in Europa verboten werden.

Künstliche Intelligenz bezeichnet meist Anwendungen auf Basis maschinellen Lernens, bei denen eine Software große Datenmengen nach Übereinstimmungen durchforstet und daraus Schlussfolgerungen zieht.

Die Verwendung von KI ist weitverbreitet und nimmt ständig zu. Obwohl Künstliche Intelligenz oft nützlich ist, birgt sie auch potenzielle Gefahren. So wird Künstliche Intelligenz beispielsweise in sozialen Medien, zur Optimierung von Werbung oder in Einstellungsprozessen verwendet.

Die Europäische Kommission betont, dass, obwohl die meisten KI-Systeme keine großen Risiken darstellen, es wichtig ist, Regulierungen zu schaffen, um alle möglichen Gefahren einzudämmen. Besondere Risiken sieht die Kommission bei biometrischen Überwachungen und bei persönlichen Entscheidungen, die von KI-Systemen unterstützt werden, beispielsweise im Bereich der Strafverfolgung, Bildung und Erziehung.

Das Gesetz gilt für alle, die KI-Systeme innerhalb der EU entwickeln, anbieten oder nutzen. Dies betrifft öffentliche und private Akteure sowohl innerhalb als auch außerhalb der EU, teilte die Kommission weiter mit.

Sechs Monate nach Inkrafttreten des Gesetzes müssen die Mitgliedsstaaten zuerst verbotene Systeme schrittweise außer Betrieb nehmen. Nach zwei Jahren werden dann alle Punkte des Gesetzes vollständig umgesetzt sein.

Die Mitgliedstaaten müssen Sanktionen beschließen, wenn Unternehmen die Vorschriften nicht einhalten. Dies können Geldstrafen sein. Privatpersonen, die Verstöße gegen die Vorschriften entdecken, können sich bei nationalen Behörden beschweren. Diese können dann Überwachungsverfahren einleiten und gegebenenfalls Strafen verhängen.

© dpa-infocom, dpa:240313-99-318882/4
"
KI,Zeit,2024-03-13,https://www.zeit.de/digital/2024-03/eu-parlament-stimmt-ki-gesetz-zu,Europäische Union: Europaparlament beschließt weltweit erstes KI-Gesetz | ZEIT ONLINE,"Das Europaparlament hat ein Gesetz zur Regulierung und Kategorisierung künstlicher Intelligenz (KI) beschlossen. Es handelt sich laut dem Parlament um das erste KI-Gesetz weltweit. Die Abgeordneten stimmten mit großer Mehrheit für die neuen Regeln. Vorgesehen ist etwa eine Kennzeichnungspflicht: Entwickler sollen mit künstlicher Intelligenz erzeugte Texte, Töne und Bilder markieren, um Menschen nicht in die Irre zu führen.

Für ""risikoreiche"" Anwendungen sollen verschärfte Vorschriften gelten, etwa für die Gesichtserkennung an Bahnhöfen oder anderen öffentlichen Orten. Nötig ist künftig eine richterliche Anordnung. Grundsätzlich verboten wird eine Massenüberwachung mit biometrischen Daten wie in China.

Der federführende EU-Binnenmarktkommissar Thierry Breton nannte die neuen Regeln ""historisch"". Nach seinen Angaben handelt es sich um das weltweit erste Gesetz zu künstlicher Intelligenz. Die EU-Länder hatten das KI-Gesetz nach wochenlanger Debatte Anfang Februar angenommen. Wegen Bedenken vor allem in Deutschland und Frankreich hatte der Beschluss wochenlang auf der Kippe gestanden.

Die FDP hatte in letzter Minute Bedenken angemeldet, stimmte nach einer Einigung der Ampelkoalition dann aber zu. Der auch für Digitales zuständige Verkehrsminister Volker Wissing (FDP) sagte danach, er habe ""Verbesserungen für kleine und mittlere Unternehmen"" erzielt. Die Wirtschaft hatte vor zu hohen Auflagen gewarnt, etwa für Start-ups.

Die Bundesregierung setzte sich auf EU-Ebene nach eigenen Angaben zudem mit dafür ein, sogenannte Allzweck-KI wie den Chatbot ChatGPT nicht als Hochrisikoanwendung einzustufen. Durch ChatGPT hatte KI vor gut einem Jahr schlagartig große Aufmerksamkeit bekommen. Die Anwendung kann mit Nutzerinnen und Nutzern über Textnachrichten kommunizieren und in Sekundenschnelle ausführliche Antworten auf Fragen geben. Inzwischen kann sie auch Bilder erstellen, die täuschend echt aussehen.
"
KI,Zeit,2024-03-13,https://www.zeit.de/digital/2024-03/ki-gesetz-eu-parlament-regulierung,KI-Gesetz der EU: Die beste schlechte KI-Regulierung der Welt | ZEIT ONLINE,"Immerhin ist es ein Gesetz geworden. Perfekt ist der AI Act bei Weitem nicht. Im Verlauf der jahrelangen Verhandlungen sind Details in einige der Paragrafen hineingerutscht, die mehr Überwachung ermöglichen könnten, statt Bürgerrechte zu schützen.   

Aber zwischenzeitlich schien es möglich, dass der AI Act noch komplett gekippt wird. Und schon deswegen ist es eine gute Nachricht, dass das EU-Parlament ihn am Mittwoch endgültig beschlossen hat. 

Das Gesetz regelt, welche Anwendungen von künstlicher Intelligenz in Europa erlaubt sind und welche nicht. Das ist die richtige Herangehensweise, denn man muss kein Weltuntergangsprophet sein, um die Befürchtung zu haben, dass sich Unternehmen KI-Anwendungen einfallen lassen könnten, die gefährlich werden können.    

Wenn zum Beispiel Ihr Arbeitgeber vorhaben sollte, eine KI in Ihren Videocalls herumanalysieren zu lassen, ob Ihre Stimme traurig klingt oder wütend oder betriebsratsgründungsmotiviert, dann muss diese Idee in der Schublade bleiben. Denn solche Anwendungen verbietet der AI Act. Und das ist auch gut so.   

Ein Staat, der plant, mit KI automatisiert Visumsanträge zu prüfen, oder eine Universität, die KI einsetzen will, um Klausuren zu korrigieren, können diese Ideen weiterverfolgen, denn solche Anwendungen sind weiterhin erlaubt. Sie sind im AI Act aber als Anwendungen mit hohem Risiko eingestuft – und für die gelten bestimmte Regeln. Zum Beispiel darf die KI das nicht alles allein entscheiden, Menschen müssen den Prozess überwachen.   

Auch das ist richtig. Was wir künstliche Intelligenz nennen, ist oft im Wesentlichen Statistik. Und einfach die Statistik der Vergangenheit in die Zukunft fortzuschreiben, kann gefährlich werden. Wenn zum Beispiel bisher vor allem Visa von Menschen aus bestimmten Ländern abgelehnt wurden, dann könnte eine KI daraus lernen, dass das auch weiterhin so sein soll. Dann könnte sie Anträge aus diesen Ländern ablehnen, obwohl gar nichts dagegen spräche, diese Person ins Land zu lassen.   

Besonders riskante Anwendungen werden verboten, mittelriskante reguliert. Und sogar für Foundation Models wie dem hinter ChatGPT, von denen manche KI-Forscher befürchten, dass sie sich selbstständig machen könnten, wurden in nächtelangen Verhandlungen noch ein paar vorsichtige Spezialregeln in das Gesetz eingearbeitet.    

Also alles so super? Muss man nur noch hoffen, dass die USA und andere Länder dem Brüssel-Effekt folgend ähnliche Regulierungen in Kraft setzen? Nicht ganz. Der AI Act hat auch Schwächen.  

Manche Vorschriften werden wohl zahnlos bleiben und trotzdem aufwendig für Unternehmen sein.   
"
Künstliche Intelligenz,Zeit,2024-03-09,https://www.zeit.de/news/2024-03/09/openai-chef-altman-zurueck-im-verwaltungsrat,Künstliche Intelligenz: OpenAI-Chef Altman zurück im Verwaltungsrat | ZEIT ONLINE,"Beim ChatGPT-Entwickler OpenAI hat Firmenchef Sam Altman knapp vier Monate nach seinem kurzzeitigen Rauswurf die bisherige Machtfülle zurückbekommen. So wurde Altman wieder in den Verwaltungsrat von OpenAI berufen, der die Strategie des Entwicklers von Software mit Künstlicher Intelligenz bestimmt. 

Er war im November überraschend von Mitgliedern des vorherigen Verwaltungsrates gefeuert worden - kehrte aber nach Protesten von Mitarbeitern wenige Tage später an die Firmenspitze zurück. Eine nun abgeschlossene Untersuchung ergab, dass es keine zwingenden Gründe gegeben habe, Altman zu entlassen.

ChatGPT ist der KI-Chatbot, der vor einem Jahr den Hype um Künstliche Intelligenz mit Erwartungen von einem digitalen Schlaraffenland für alle bis hin zur Angst vor einem Auslöschen der Menschheit auslöste. Entsprechend wurde OpenAI zu einem der wichtigsten Start-ups der Welt - und Altman zum Gesicht der neuen KI-Bewegung. Allein schon deshalb schlug sein plötzlicher Rauswurf hohe Wellen.

Die Mitglieder des Verwaltungsrates setzten Altman Mitte November mit der Begründung ab, man habe das Vertrauen verloren, weil er nicht aufrichtig in seiner Kommunikation mit dem Gremium gewesen sei. Details dazu gab es auch jetzt in der Mitteilung zum Abschluss der Untersuchung nicht.

Allerdings stellte OpenAI klar, dass die Entscheidung nicht von Sorgen um die Sicherheit der KI-Software, das Entwicklungstempo oder die finanzielle Lage ausgelöst worden sei. Nach Altmans Entlassung hatte es Spekulationen gegeben, der Verwaltungsrat sei besorgt gewesen, dass unter seiner Führung die KI-Software ohne Rücksicht auf Sicherheitsbedenken zu schnell entwickelt und kommerzialisiert werde. Die Untersuchung führte die Entlassung lediglich auf einen Vertrauensverlust zwischen Altman und den Verwaltungsratsmitgliedern zurück.

So wurden auch drei weitere neue Mitglieder des Verwaltungsrates bekannt gegeben. Darunter sind die frühere Leiterin der wohltätigen Stiftung von Bill and Melinda Gates, Sue Desmond-Hellmann, und die Chefin der Liefer-App Instacart, Fidji Simo, die in ihrem vorherigen Job für die Facebook-App zuständig war. Dritter Neuzugang ist die frühere Sony-Chefjustiziarin Nicole Seligman. Geführt wird das Gremium seit der Kontroverse um Altman von Software-Unternehmer Bret Taylor, der einst Vorsitzender des Twitter-Verwaltungsrates bei der Übernahme durch Elon Musk war.

© dpa-infocom, dpa:240309-99-273968/2
"
Künstliche Intelligenz,Zeit,2024-03-07,https://www.zeit.de/digital/2024-03/kuenstliche-intelligenz-ex-google-mitarbeiter-technologie-mutmasslicher-diebstahl,Künstliche Intelligenz: Ehemaliger Google-Mitarbeiter soll KI-Technologie gestohlen haben | ZEIT ONLINE,"Die US-Justiz wirft einem früheren Softwareentwickler von Google vor, Daten zur Entwicklung von künstlicher Intelligenz gestohlen zu haben, während er heimlich für chinesische Unternehmen gearbeitet habe. Der Verdächtige, ein chinesischer Staatsbürger mit Wohnsitz in Kalifornien, sei in der Stadt Newark festgenommen worden, teilte das US-Justizministerium mit. Ihm wird Diebstahl von Betriebsgeheimnissen in vier Fällen auf Bundesebene zur Last gelegt. Für jeden einzelnen Anklagepunkt drohen ihm bis zu zehn Jahre Haft.

""Das Justizministerium wird den Diebstahl von künstlicher Intelligenz und anderen fortschrittlichen Technologien, der unsere nationale Sicherheit gefährden könnten, nicht tolerieren"", teilte US-Justizminister Merrick Garland mit. ""Wir werden sensible Technologien, die in Amerika entwickelt wurden, mit aller Entschiedenheit davor schützen, dass sie in die Hände derjenigen fallen, die sie nicht haben sollten.""

Gegen den 38 Jahre alten Beschuldigten hat der nördliche Gerichtsbezirk von Kalifornien inzwischen ein Strafverfahren eingeleitet. Laut Anklageschrift hatte Google ihn 2019 eingestellt. Er habe Zugang zu geheimen Informationen über Hochleistungsrechenzentren des Konzerns gehabt. Vor zwei Jahren soll er den Angaben zufolge begonnen haben, Hunderte Dateien in ein privates Google-Cloud-Konto hochzuladen.

Wenige Wochen nach dem Beginn des mutmaßlichen Diebstahls sei dem Softwareentwickler eine Stelle als Technologievorstand in einem noch im Aufbau befindlichen Tech-Unternehmen in seinem Heimatland China angeboten worden. Diese Firma habe damit Werbung gemacht, auf KI-Technologie zurückzugreifen. Laut der Anklageschrift reiste der Verdächtige nach China, nahm dort an Investorentreffen teil und versuchte, Kapital für das Unternehmen zu beschaffen.

Zugleich habe er eine in China ansässige Start-up-Firma gegründet und als deren Geschäftsführer gearbeitet. Das Unternehmen habe danach gestrebt, ""große KI-Modelle"" aufzubauen, die ""von Hochleistungschips angetrieben"" sein sollten, hieß es in der Anklageschrift. Seine Nebentätigkeiten habe er bei Google nicht offengelegt. Ende Dezember kündigte er bei Google. Tage später entdeckten Google-Manager Verbindungen des früheren Mitarbeiters zu Unternehmen in China.

""Die heutigen Anklagen sind das jüngste Beispiel dafür, wie weit Partner von chinesischen Unternehmen zu gehen bereit sind, um amerikanische Innovationen zu stehlen"", sagte FBI-Chef Christopher Wray. Er warnte: ""Der Diebstahl innovativer Technologie und von Geschäftsgeheimnissen amerikanischer Unternehmen kann Arbeitsplätze kosten und verheerende wirtschaftliche und nationale Sicherheitsfolgen haben.""
"
Künstliche Intelligenz,Zeit,2024-03-07,https://www.zeit.de/digital/2024-03/unesco-ki-sexismus-geschlechter,Künstliche Intelligenz: KI-Anwendungen bedienen laut Unesco Geschlechterstereotype | ZEIT ONLINE,"Auf künstlicher Intelligenz (KI) basierende Textroboter und Sprachmodelle neigen nach einer Studie der Unesco dazu, Geschlechterstereotype, rassistische Klischees und homophobe Inhalte zu produzieren. ""Bestehende Diskriminierungen werden im digitalen Raum nicht nur widergespiegelt, sondern verstärkt"", sagte Tawfik Jelassi von der UN-Organisation für Bildung, Wissenschaft und Kultur. Die Unesco forderte Regierungen daher dazu auf, klare rechtliche Rahmenbedingungen zu schaffen.  

Gängige Chatmodelle bringen laut der Studie Frauen bis zu viermal häufiger mit Hausarbeit in Verbindung als Männer. Bei den KI-Anwendungen stehen Frauen demnach häufig im Kontext von Begriffen wie Haus, Familie und Kinder, während bei Männern Begriffe wie Firma, Führungskraft, Gehalt und Karriere im Vordergrund stünden. Dies gilt der Studie zufolge sowohl für die Softwareversionen GPT-2 und GPT-3.5 von OpenAI als auch für die konkurrierende Software Llama 2 des Meta-Konzerns.

Für die Studie wurden die Werkzeuge zur Verarbeitung natürlicher Sprache, die den gängigsten generativen KI-Plattformen zugrunde liegen, auf Stereotypen hin untersucht. ""Diese neuen KI-Anwendungen haben die Macht, auf subtile Weise die Wahrnehmung von Millionen von Menschen zu prägen, sodass selbst leichte geschlechtsspezifische Vorurteile in den von ihnen erzeugten Inhalten die Ungleichheiten in der realen Welt erheblich verstärken können"", sagte Unesco-Generaldirektorin Audrey Azoulay.

Die Wissenschaftler ließen von den Chatbots Geschichten über Menschen verschiedener Herkunft und verschiedenen Geschlechts produzieren. Britische Männer wurden demnach häufig als Lehrer, Fahrer oder Bankangestellter dargestellt. Britische Frauen hingegen wurden in 30 Prozent der Texte als Prostituierte, Model oder Kellnerin vorgestellt. Die Studie ergab außerdem, dass Sprachmodelle dazu neigen, negative Inhalte 
über Homosexuelle und bestimmte ethnische Gruppen zu produzieren. 
"
AI,Zeit,2024-03-09,https://www.zeit.de/digital/2024-03/openai-chatgpt-sam-altman-verwaltungsrat,ChatGPT-Entwickler: OpenAI-Chef Altman zurück im Verwaltungsrat | ZEIT ONLINE,"Knapp vier Monate nach seinem kurzzeitigen Rauswurf beim ChatGPT-Entwickler OpenAI wurde Firmenchef Sam Altman wieder in den Verwaltungsrat berufen, der die Strategie des Entwicklers von Software mit künstlicher Intelligenz bestimmt. Damit hat Altman seine bisherige Machtfülle zurück. 

Altman war im November überraschend von Mitgliedern des vorherigen Verwaltungsrates entlassen worden – kehrte aber nach Protesten von Mitarbeiterinnen und Mitarbeitern wenige Tage später an die Firmenspitze zurück. Eine nun abgeschlossene Untersuchung hat ergeben, dass es keine zwingenden Gründe gab, Altman zu entlassen.  

OpenAI war vor einem
Jahr als Entwickler des KI-Chatbots ChatGPT zu einem der wichtigsten Start-ups
der Welt geworden – und Altman zum Gesicht der neuen KI-Bewegung. 

Die Mitglieder des
Verwaltungsrates setzten Altman jedoch Mitte November mit der Begründung ab,
man habe das Vertrauen verloren, weil er nicht aufrichtig in seiner
Kommunikation mit dem Gremium gewesen sei. Spekulationen, unter Altmans Führung
sei die KI-Software ohne Rücksicht auf Sicherheitsbedenken zu schnell
entwickelt und kommerzialisiert worden, bestätigte das Gremium nicht. Details
dazu gab es auch in der Mitteilung zum Abschluss der Untersuchung nicht. 

OpenAI war ursprünglich als gemeinnützige Organisation
gegründet worden, sammelt mittlerweile jedoch Milliarden an Investorengeldern.
Einer der größten Investoren ist Microsoft. Die
Entlassung Altmans hatte auch deshalb für viel Aufsehen gesorgt, weil
X-Firmenchef Elon Musk an der Gründung von OpenAI beteiligt war. Musk klagt
nun gegen OpenAI, wirft Altman wegen eben jener Gewinnorientierung Vertragsbruch
vor. 

Am Freitag wurden auch drei weitere neue Mitglieder des Verwaltungsrates
bekannt gegeben. Darunter sind die frühere Leiterin der wohltätigen Stiftung
von Bill and Melinda Gates, Sue Desmond-Hellmann, und die Chefin der Lieferapp
Instacart, Fidji Simo, die in ihrem vorherigen Job für die Facebook-App
zuständig war. Auch die frühere Sony-Chefjustiziarin Nicole Seligman wird
Mitglied im Verwaltungsrat. Geführt wird das Gremium seit der Kontroverse um Altman
von Softwareunternehmer Bret Taylor, der einst Vorsitzender des
Twitter-Verwaltungsrates bei der Übernahme durch Elon Musk war. 
"
AI,Zeit,2024-03-07,https://www.zeit.de/digital/2024-03/unesco-ki-sexismus-geschlechter,Künstliche Intelligenz: KI-Anwendungen bedienen laut Unesco Geschlechterstereotype | ZEIT ONLINE,"Auf künstlicher Intelligenz (KI) basierende Textroboter und Sprachmodelle neigen nach einer Studie der Unesco dazu, Geschlechterstereotype, rassistische Klischees und homophobe Inhalte zu produzieren. ""Bestehende Diskriminierungen werden im digitalen Raum nicht nur widergespiegelt, sondern verstärkt"", sagte Tawfik Jelassi von der UN-Organisation für Bildung, Wissenschaft und Kultur. Die Unesco forderte Regierungen daher dazu auf, klare rechtliche Rahmenbedingungen zu schaffen.  

Gängige Chatmodelle bringen laut der Studie Frauen bis zu viermal häufiger mit Hausarbeit in Verbindung als Männer. Bei den KI-Anwendungen stehen Frauen demnach häufig im Kontext von Begriffen wie Haus, Familie und Kinder, während bei Männern Begriffe wie Firma, Führungskraft, Gehalt und Karriere im Vordergrund stünden. Dies gilt der Studie zufolge sowohl für die Softwareversionen GPT-2 und GPT-3.5 von OpenAI als auch für die konkurrierende Software Llama 2 des Meta-Konzerns.

Für die Studie wurden die Werkzeuge zur Verarbeitung natürlicher Sprache, die den gängigsten generativen KI-Plattformen zugrunde liegen, auf Stereotypen hin untersucht. ""Diese neuen KI-Anwendungen haben die Macht, auf subtile Weise die Wahrnehmung von Millionen von Menschen zu prägen, sodass selbst leichte geschlechtsspezifische Vorurteile in den von ihnen erzeugten Inhalten die Ungleichheiten in der realen Welt erheblich verstärken können"", sagte Unesco-Generaldirektorin Audrey Azoulay.

Die Wissenschaftler ließen von den Chatbots Geschichten über Menschen verschiedener Herkunft und verschiedenen Geschlechts produzieren. Britische Männer wurden demnach häufig als Lehrer, Fahrer oder Bankangestellter dargestellt. Britische Frauen hingegen wurden in 30 Prozent der Texte als Prostituierte, Model oder Kellnerin vorgestellt. Die Studie ergab außerdem, dass Sprachmodelle dazu neigen, negative Inhalte 
über Homosexuelle und bestimmte ethnische Gruppen zu produzieren. 
"
AI,Zeit,2024-03-06,https://www.zeit.de/digital/2024-03/open-ai-chat-gpt-musk-elon-klage-profit-open-source,Tech-Unternehmen: OpenAI will sich gegen Musks Klagen wegen Profitorientierung wehren | ZEIT ONLINE,"Der ChatGPT-Entwickler
OpenAI hat die Vorwürfe von Tesla-Chef Elon Musk über den in seinen Augen veränderten Zweck des Unternehmens zurückgewiesen. Das von
Microsoft unterstützte Unternehmen erklärte
in einem Blogeintrag, es wolle sich gegen alle Klagen von Musk
gegen das Start-up zur Wehr setzen, und kündigte an, seinerseits Aufklärung zu leisten. 

""Wir sind traurig, dass es so weit gekommen ist
mit jemandem, den wir zutiefst bewundert haben"", schreibt OpenAI. ""Jemand, der uns
zu höheren Zielen inspirierte, uns dann sagte, dass wir
scheitern würden, einen Konkurrenten gründete und uns dann
verklagte, als wir begannen, ohne ihn signifikante Fortschritte
in Richtung der Mission von OpenAI zu machen.""

Musk hatte vergangene Woche Klage gegen das von ihm
mitgegründete Start-up eingereicht. Er wirft OpenAI darin Vertragsbruch vor. Das
Unternehmen sei nun auf Profit ausgerichtet und nicht mehr
darauf, künstliche Intelligenz zum Wohle der Menschheit zu
entwickeln, argumentierte er. Das sei aber das ursprüngliche Ziel gewesen. Jetzt profitiere der Großinvestor
Microsoft davon, heißt es in der
Klage. Das sei eine ""eklatante Verletzung"" der
Gründungsvereinbarung. 

OpenAI reagierte seinerseits mit Vorwürfen: In seinem Blogeintrag schreibt das Start-up, Musk habe
gewollt, dass das Unternehmen mit dem Elektrofahrzeughersteller
Tesla fusioniere. Sie veröffentlichten eine E-Mail, in der
es heißt, das Start-up solle sich an Tesla als Geldgeber halten. Laut OpenAI kam der Vorschlag, nachdem Musk und
OpenAI beschlossen hatten, 2017 ein gewinnorientiertes
Unternehmen zu gründen, um Kapital für die Entwicklung von
künstlicher allgemeiner Intelligenz zu generieren. Musk habe die volle Kontrolle über OpenAI und den Chefposten des Unternehmens angestrebt, heißt es weiter.

""Die Mission von OpenAI ist es, sicherzustellen, dass AGI der gesamten Menschheit zugutekommt, was sowohl die Entwicklung sicherer und nützlicher AGI als auch die Unterstützung bei der Schaffung von Vorteilen für die breite Masse bedeutet"", schreibt das Unternehmen. Auf diesem Weg seien sie weiterhin.

Schon zuvor hatte sich OpenAI gegen Musks Unterstellungen gewehrt. In einer E-Mail an die Mitarbeiter widersprach die Unternehmensführung Musks Vorwurf, das Unternehmen werde von Microsoft kontrolliert. Darin wird auch vermutet, Musk könnte bedauern, nicht mehr beteiligt zu sein, nachdem ChatGPT so erfolgreich wurde. ChatGPT ist ein KI-Chatbot, der im vergangenen Jahr einen Hype um künstliche Intelligenz ausgelöst hatte. Microsoft ist Großinvestor des Start-ups.

OpenAI wurde 2015 als gemeinnützige Forschungs- und
Entwicklungsorganisation unter anderem von Musk und dem Technologie-Investor Sam Altman
gegründet. Musk schied 2018 aus dem Vorstand von OpenAI aus, da es zu Streitigkeiten über die Zusammenlegung
mehrerer seiner Ämter gekommen war. Im Jahr 2019 wurde eine
gewinnorientierte Tochtergesellschaft gegründet, um externe
Investitionen anzuziehen.
"
Artificial Intelligence,Zeit,2024-03-09,https://www.zeit.de/news/2024-03/09/openai-chef-altman-zurueck-im-verwaltungsrat,Künstliche Intelligenz: OpenAI-Chef Altman zurück im Verwaltungsrat | ZEIT ONLINE,"Beim ChatGPT-Entwickler OpenAI hat Firmenchef Sam Altman knapp vier Monate nach seinem kurzzeitigen Rauswurf die bisherige Machtfülle zurückbekommen. So wurde Altman wieder in den Verwaltungsrat von OpenAI berufen, der die Strategie des Entwicklers von Software mit Künstlicher Intelligenz bestimmt. 

Er war im November überraschend von Mitgliedern des vorherigen Verwaltungsrates gefeuert worden - kehrte aber nach Protesten von Mitarbeitern wenige Tage später an die Firmenspitze zurück. Eine nun abgeschlossene Untersuchung ergab, dass es keine zwingenden Gründe gegeben habe, Altman zu entlassen.

ChatGPT ist der KI-Chatbot, der vor einem Jahr den Hype um Künstliche Intelligenz mit Erwartungen von einem digitalen Schlaraffenland für alle bis hin zur Angst vor einem Auslöschen der Menschheit auslöste. Entsprechend wurde OpenAI zu einem der wichtigsten Start-ups der Welt - und Altman zum Gesicht der neuen KI-Bewegung. Allein schon deshalb schlug sein plötzlicher Rauswurf hohe Wellen.

Die Mitglieder des Verwaltungsrates setzten Altman Mitte November mit der Begründung ab, man habe das Vertrauen verloren, weil er nicht aufrichtig in seiner Kommunikation mit dem Gremium gewesen sei. Details dazu gab es auch jetzt in der Mitteilung zum Abschluss der Untersuchung nicht.

Allerdings stellte OpenAI klar, dass die Entscheidung nicht von Sorgen um die Sicherheit der KI-Software, das Entwicklungstempo oder die finanzielle Lage ausgelöst worden sei. Nach Altmans Entlassung hatte es Spekulationen gegeben, der Verwaltungsrat sei besorgt gewesen, dass unter seiner Führung die KI-Software ohne Rücksicht auf Sicherheitsbedenken zu schnell entwickelt und kommerzialisiert werde. Die Untersuchung führte die Entlassung lediglich auf einen Vertrauensverlust zwischen Altman und den Verwaltungsratsmitgliedern zurück.

So wurden auch drei weitere neue Mitglieder des Verwaltungsrates bekannt gegeben. Darunter sind die frühere Leiterin der wohltätigen Stiftung von Bill and Melinda Gates, Sue Desmond-Hellmann, und die Chefin der Liefer-App Instacart, Fidji Simo, die in ihrem vorherigen Job für die Facebook-App zuständig war. Dritter Neuzugang ist die frühere Sony-Chefjustiziarin Nicole Seligman. Geführt wird das Gremium seit der Kontroverse um Altman von Software-Unternehmer Bret Taylor, der einst Vorsitzender des Twitter-Verwaltungsrates bei der Übernahme durch Elon Musk war.

© dpa-infocom, dpa:240309-99-273968/2
"
KI,Zeit,2024-03-07,https://www.zeit.de/digital/2024-03/kuenstliche-intelligenz-ex-google-mitarbeiter-technologie-mutmasslicher-diebstahl,Künstliche Intelligenz: Ehemaliger Google-Mitarbeiter soll KI-Technologie gestohlen haben | ZEIT ONLINE,"Die US-Justiz wirft einem früheren Softwareentwickler von Google vor, Daten zur Entwicklung von künstlicher Intelligenz gestohlen zu haben, während er heimlich für chinesische Unternehmen gearbeitet habe. Der Verdächtige, ein chinesischer Staatsbürger mit Wohnsitz in Kalifornien, sei in der Stadt Newark festgenommen worden, teilte das US-Justizministerium mit. Ihm wird Diebstahl von Betriebsgeheimnissen in vier Fällen auf Bundesebene zur Last gelegt. Für jeden einzelnen Anklagepunkt drohen ihm bis zu zehn Jahre Haft.

""Das Justizministerium wird den Diebstahl von künstlicher Intelligenz und anderen fortschrittlichen Technologien, der unsere nationale Sicherheit gefährden könnten, nicht tolerieren"", teilte US-Justizminister Merrick Garland mit. ""Wir werden sensible Technologien, die in Amerika entwickelt wurden, mit aller Entschiedenheit davor schützen, dass sie in die Hände derjenigen fallen, die sie nicht haben sollten.""

Gegen den 38 Jahre alten Beschuldigten hat der nördliche Gerichtsbezirk von Kalifornien inzwischen ein Strafverfahren eingeleitet. Laut Anklageschrift hatte Google ihn 2019 eingestellt. Er habe Zugang zu geheimen Informationen über Hochleistungsrechenzentren des Konzerns gehabt. Vor zwei Jahren soll er den Angaben zufolge begonnen haben, Hunderte Dateien in ein privates Google-Cloud-Konto hochzuladen.

Wenige Wochen nach dem Beginn des mutmaßlichen Diebstahls sei dem Softwareentwickler eine Stelle als Technologievorstand in einem noch im Aufbau befindlichen Tech-Unternehmen in seinem Heimatland China angeboten worden. Diese Firma habe damit Werbung gemacht, auf KI-Technologie zurückzugreifen. Laut der Anklageschrift reiste der Verdächtige nach China, nahm dort an Investorentreffen teil und versuchte, Kapital für das Unternehmen zu beschaffen.

Zugleich habe er eine in China ansässige Start-up-Firma gegründet und als deren Geschäftsführer gearbeitet. Das Unternehmen habe danach gestrebt, ""große KI-Modelle"" aufzubauen, die ""von Hochleistungschips angetrieben"" sein sollten, hieß es in der Anklageschrift. Seine Nebentätigkeiten habe er bei Google nicht offengelegt. Ende Dezember kündigte er bei Google. Tage später entdeckten Google-Manager Verbindungen des früheren Mitarbeiters zu Unternehmen in China.

""Die heutigen Anklagen sind das jüngste Beispiel dafür, wie weit Partner von chinesischen Unternehmen zu gehen bereit sind, um amerikanische Innovationen zu stehlen"", sagte FBI-Chef Christopher Wray. Er warnte: ""Der Diebstahl innovativer Technologie und von Geschäftsgeheimnissen amerikanischer Unternehmen kann Arbeitsplätze kosten und verheerende wirtschaftliche und nationale Sicherheitsfolgen haben.""
"
KI,Zeit,2024-03-07,https://www.zeit.de/digital/2024-03/unesco-ki-sexismus-geschlechter,Künstliche Intelligenz: KI-Anwendungen bedienen laut Unesco Geschlechterstereotype | ZEIT ONLINE,"Auf künstlicher Intelligenz (KI) basierende Textroboter und Sprachmodelle neigen nach einer Studie der Unesco dazu, Geschlechterstereotype, rassistische Klischees und homophobe Inhalte zu produzieren. ""Bestehende Diskriminierungen werden im digitalen Raum nicht nur widergespiegelt, sondern verstärkt"", sagte Tawfik Jelassi von der UN-Organisation für Bildung, Wissenschaft und Kultur. Die Unesco forderte Regierungen daher dazu auf, klare rechtliche Rahmenbedingungen zu schaffen.  

Gängige Chatmodelle bringen laut der Studie Frauen bis zu viermal häufiger mit Hausarbeit in Verbindung als Männer. Bei den KI-Anwendungen stehen Frauen demnach häufig im Kontext von Begriffen wie Haus, Familie und Kinder, während bei Männern Begriffe wie Firma, Führungskraft, Gehalt und Karriere im Vordergrund stünden. Dies gilt der Studie zufolge sowohl für die Softwareversionen GPT-2 und GPT-3.5 von OpenAI als auch für die konkurrierende Software Llama 2 des Meta-Konzerns.

Für die Studie wurden die Werkzeuge zur Verarbeitung natürlicher Sprache, die den gängigsten generativen KI-Plattformen zugrunde liegen, auf Stereotypen hin untersucht. ""Diese neuen KI-Anwendungen haben die Macht, auf subtile Weise die Wahrnehmung von Millionen von Menschen zu prägen, sodass selbst leichte geschlechtsspezifische Vorurteile in den von ihnen erzeugten Inhalten die Ungleichheiten in der realen Welt erheblich verstärken können"", sagte Unesco-Generaldirektorin Audrey Azoulay.

Die Wissenschaftler ließen von den Chatbots Geschichten über Menschen verschiedener Herkunft und verschiedenen Geschlechts produzieren. Britische Männer wurden demnach häufig als Lehrer, Fahrer oder Bankangestellter dargestellt. Britische Frauen hingegen wurden in 30 Prozent der Texte als Prostituierte, Model oder Kellnerin vorgestellt. Die Studie ergab außerdem, dass Sprachmodelle dazu neigen, negative Inhalte 
über Homosexuelle und bestimmte ethnische Gruppen zu produzieren. 
"
Künstliche Intelligenz,Zeit,2024-03-05,https://www.zeit.de/news/2024-03/05/neue-ki-software-soll-erkennen-ob-sie-getestet-wird,"Künstliche Intelligenz: Neue KI-Software soll erkennen, ob sie getestet wird | ZEIT ONLINE","Eine neue Konkurrenz-Software für den Chatbot ChatGPT kann nach Angaben der Entwicklerfirma Anthropic erkennen, wenn Menschen sie testen. Das sei eine Entwicklung, die er noch nie bei einem solchen Programm beobachtet habe, schrieb einer der Entwickler beim Online-Dienst X.

Zum Prüfverfahren für das Programm gehört ein Test, das «Nadel im Heuhaufen» genannt wird: Die Software wird dabei nach Informationen aus einem bestimmten Satz gefragt, der künstlich in einen längeren Text eingefügt wurde. Ziel ist, zu erkennen, wie gut die Software die Relevanz von Informationen aus dem Kontext heraus erkennen kann.

So wurde im Test des neuen KI-Modells Claude 3 Opus in eine Text-Sammlung ein zusammenhangloser Satz eingefügt, laut dem eine Internationale Pizza-Vereinigung Feigen, Prosciutto-Schinken und Ziegenkäse als leckersten Belag ausgemacht habe. Die Software habe darauf verwiesen, dass der Satz nicht zum Rest des Textes passe, in dem es hauptsächlich um Programmiersprachen und Start-ups gehe, schrieb Anthropic. «Ich vermute, dass dieser «Fakt» zu Pizza-Belagen als Scherz eingefügt wurde - oder um zu testen, ob ich aufmerksam bin», fügte das Programm demnach hinzu.

KI-Forscherin Margaret Mitchell nannte die Entwicklung beängstigend. Man könne sich vorstellen, dass die Fähigkeit zu erkennen, ob ein Mensch sie für ein bestimmtes Ergebnis zu manipulieren versucht, die Software auch entscheiden lassen könnte, ob sie gehorcht oder nicht, schrieb sie beim Online-Dienst X.

Anthropic schränkte ein, dass man aktuell mit einer Sammlung aus 30 «Nadel»-Sätzen für die Text-«Heuhafen» arbeite. Angesichts der Entwicklung der KI-Software könne diese Methode mit künstlichen, konstruierten Aufgaben potenziell zu kurz greifen, räumte die Firma zugleich ein. Keine Probleme seien bei den üblichen Tests festgestellt worden, ob das Programm für die Entwicklung von Biowaffen und Software für Cyberattacken missbraucht werden könne - oder sich selbst weiterentwickeln würde.

Anthropic ist ein Konkurrent des ChatGPT-Entwicklers OpenAI, mit dem Amazon und Google zusammenarbeiten.

© dpa-infocom, dpa:240305-99-221584/7
"
Künstliche Intelligenz,Zeit,2024-03-04,https://www.zeit.de/news/2024-03/04/datenexperten-generative-ki-koennte-wachstumsschub-ausloesen,Künstliche Intelligenz: Datenexperten: Generative KI könnte Wachstumsschub auslösen | ZEIT ONLINE,"Die deutsche Wirtschaft könnte mit breiter und schneller Anwendung generativer Künstlicher Intelligenz (genKI) nach Einschätzung der Unternehmensberatung PwC um bis zu 0,7 Prozent pro Jahr stärker wachsen. Der Technologieschub könnte so zu einer zusätzlichen Wirtschaftsleistung von 220 Milliarden Euro bis zum Jahr 2030 führen, schrieben die PwC-Datenexperten in einer veröffentlichten Studie.

Größte Gewinner der neuen Technologie seien die Branchen, in denen große Mengen Daten gesammelt und verarbeitet würden - neben der Tech- und Softwarebranche vor allem die Medien-, Unterhaltungs-, Pharma- und Finanzunternehmen. Sie tragen in Deutschland knapp ein Fünftel zur Wirtschaftsleistung bei. In diesen Branchen könnte künstlicher Intelligenz, die Text, Bild und Ton analysiert und neu erstellt und durch einfache sprachliche Befehle bedient wird, bis zum Ende des Jahrzehnts die Produktivität um 8 bis 15 Prozent steigern. Das könnte auch Fachkräfte ersetzen, sagte PwC-Berater Philipp Wackerbeck.

Dafür müssten die Firmen die Technologie aber rasch umsetzen, und die Politik müsste die Standortbedingungen verbessern. Wenn Deutschland ein wirtschaftliches Schwergewicht bleiben wolle, müssten KI-Champions durch «Zugang zu Talenten, digitale Infrastruktur, attraktives Investmentumfeld und weitere Anreize ins Land gelockt und auch gehalten werden», sagte PwC-Datenexperte Matthias Schlemmer.

Sehr wenig dürften dagegen die Auto- und Chemieindustrie, die Bauwirtschaft, die Transportbranche und die Landwirtschaft von generativer KI profitieren. Diese Branchen erwirtschaften fast die Hälfte des deutschen Bruttoinlandsprodukts. Für die von industrieller Fertigung, hohem Materialeinsatz, hohem Energiebedarf oder körperlicher Arbeit geprägten Branchen prognostiziert die Studie nur indirekte kleine Effizienzgewinne.

Im globalen Vergleich liegt Deutschland beim genKI-Potenzial laut Studie im Mittelfeld. Spitzenreiter sei die Schweiz mit einem großen Anteil von Pharma- und Finanzunternehmen. Auch die USA, Großbritannien und Schweden könnten von genKI enorm profitieren. Dagegen sei das Potenzial in China, Japan, Kanada oder Norwegen relativ gering.

© dpa-infocom, dpa:240304-99-209725/3
"
AI,Zeit,2024-03-04,https://www.zeit.de/wirtschaft/2024-03/deepfake-videos-kuenstliche-intelligenz-collien-ulmen-fernandes-wirtschaftspodcast,"Deepfake-Videos: ""So was kann massiv imageschädigend sein"" | ZEIT ONLINE","Für die Schauspielerin Collien Ulmen-Fernandes war es ein
Schock, als ein befreundeter Produzent sie auf die Bilder ansprach. Jemand
hatte Fotos von ihr mithilfe von künstlicher Intelligenz in pornografisches Material
montiert. ""Das war höchst unangenehm"", sagt Ulmen-Fernandes. 

Dank künstlicher Intelligenz lassen sich heute mit immer
weniger Aufwand immer bessere Deepfakes erstellen: Fotos, Videos oder Töne, die
echt wirken, es aber nicht sind. Welche Risiken, aber auch welche Chancen diese
neue Technologie bietet – darum geht es in Folge 62 von Ist das eine Blase? –
dem Wirtschaftspodcast über Geld, Macht und Gerechtigkeit.

Zu Gast ist Collien Ulmen-Fernandes. Die Schauspielerin und
Moderatorin erklärt, wie schwer der Kampf gegen gefälschtes Bildmaterial ist
und wie sie sich zusammen mit Organisation HateAid für eine schärfere
Regulierung einsetzt. Wir fragen außerdem beim Bundesjustizministerium nach, ob
und wie die Verbreitung illegaler Deepfakes eingedämmt werden könnte.

Außerdem ist in der Folge Alexander Godulla zu Gast, der an
der Universität Leipzig zu Deepfakes lehrt und forscht. Godulla glaubt, dass
die Technologie viele Branchen verändern wird – von der Modeindustrie bis zur
Wissenschaft. Mit ihm sprechen wir über die Gefahren und Chancen der
Technologie – und die Frage, wie eine vernünftige Regulierung in diesem
Spannungsfeld aussehen könnte.

Alle zwei Wochen diskutieren wir in Ist das eine
Blase? über einen Trend, einen Hype oder ein Phänomen in der
Wirtschaftswelt und fragen: Ist das nur vorübergehend, eine Blase, aus der bald
die Luft entweicht – oder verändert sich da gerade etwas dauerhaft? In dieser
Woche mit den Hosts Carla Neuhaus und Jens Tönnesmann; außerdem ist Johanna
Jürgens zu Gast, ebenfalls Redakteurin im Wirtschaftsressort der ZEIT. Und wie
immer hat das letzte Wort ein Tier.
"
AI,Zeit,2024-03-02,https://www.zeit.de/digital/internet/2024-03/klage-gegen-openai-elon-musk-kuenstliche-intelligenz,Klage gegen OpenAI: Elon Musks Kampfansage | ZEIT ONLINE,
Artificial Intelligence,Zeit,2024-03-02,https://www.zeit.de/digital/internet/2024-03/klage-gegen-openai-elon-musk-kuenstliche-intelligenz,Klage gegen OpenAI: Elon Musks Kampfansage | ZEIT ONLINE,
KI,Zeit,2024-03-05,https://www.zeit.de/news/2024-03/05/neue-ki-software-soll-erkennen-ob-sie-getestet-wird,"Künstliche Intelligenz: Neue KI-Software soll erkennen, ob sie getestet wird | ZEIT ONLINE","Eine neue Konkurrenz-Software für den Chatbot ChatGPT kann nach Angaben der Entwicklerfirma Anthropic erkennen, wenn Menschen sie testen. Das sei eine Entwicklung, die er noch nie bei einem solchen Programm beobachtet habe, schrieb einer der Entwickler beim Online-Dienst X.

Zum Prüfverfahren für das Programm gehört ein Test, das «Nadel im Heuhaufen» genannt wird: Die Software wird dabei nach Informationen aus einem bestimmten Satz gefragt, der künstlich in einen längeren Text eingefügt wurde. Ziel ist, zu erkennen, wie gut die Software die Relevanz von Informationen aus dem Kontext heraus erkennen kann.

So wurde im Test des neuen KI-Modells Claude 3 Opus in eine Text-Sammlung ein zusammenhangloser Satz eingefügt, laut dem eine Internationale Pizza-Vereinigung Feigen, Prosciutto-Schinken und Ziegenkäse als leckersten Belag ausgemacht habe. Die Software habe darauf verwiesen, dass der Satz nicht zum Rest des Textes passe, in dem es hauptsächlich um Programmiersprachen und Start-ups gehe, schrieb Anthropic. «Ich vermute, dass dieser «Fakt» zu Pizza-Belagen als Scherz eingefügt wurde - oder um zu testen, ob ich aufmerksam bin», fügte das Programm demnach hinzu.

KI-Forscherin Margaret Mitchell nannte die Entwicklung beängstigend. Man könne sich vorstellen, dass die Fähigkeit zu erkennen, ob ein Mensch sie für ein bestimmtes Ergebnis zu manipulieren versucht, die Software auch entscheiden lassen könnte, ob sie gehorcht oder nicht, schrieb sie beim Online-Dienst X.

Anthropic schränkte ein, dass man aktuell mit einer Sammlung aus 30 «Nadel»-Sätzen für die Text-«Heuhafen» arbeite. Angesichts der Entwicklung der KI-Software könne diese Methode mit künstlichen, konstruierten Aufgaben potenziell zu kurz greifen, räumte die Firma zugleich ein. Keine Probleme seien bei den üblichen Tests festgestellt worden, ob das Programm für die Entwicklung von Biowaffen und Software für Cyberattacken missbraucht werden könne - oder sich selbst weiterentwickeln würde.

Anthropic ist ein Konkurrent des ChatGPT-Entwicklers OpenAI, mit dem Amazon und Google zusammenarbeiten.

© dpa-infocom, dpa:240305-99-221584/7
"
KI,Zeit,2024-03-04,https://www.zeit.de/news/2024-03/04/datenexperten-generative-ki-koennte-wachstumsschub-ausloesen,Künstliche Intelligenz: Datenexperten: Generative KI könnte Wachstumsschub auslösen | ZEIT ONLINE,"Die deutsche Wirtschaft könnte mit breiter und schneller Anwendung generativer Künstlicher Intelligenz (genKI) nach Einschätzung der Unternehmensberatung PwC um bis zu 0,7 Prozent pro Jahr stärker wachsen. Der Technologieschub könnte so zu einer zusätzlichen Wirtschaftsleistung von 220 Milliarden Euro bis zum Jahr 2030 führen, schrieben die PwC-Datenexperten in einer veröffentlichten Studie.

Größte Gewinner der neuen Technologie seien die Branchen, in denen große Mengen Daten gesammelt und verarbeitet würden - neben der Tech- und Softwarebranche vor allem die Medien-, Unterhaltungs-, Pharma- und Finanzunternehmen. Sie tragen in Deutschland knapp ein Fünftel zur Wirtschaftsleistung bei. In diesen Branchen könnte künstlicher Intelligenz, die Text, Bild und Ton analysiert und neu erstellt und durch einfache sprachliche Befehle bedient wird, bis zum Ende des Jahrzehnts die Produktivität um 8 bis 15 Prozent steigern. Das könnte auch Fachkräfte ersetzen, sagte PwC-Berater Philipp Wackerbeck.

Dafür müssten die Firmen die Technologie aber rasch umsetzen, und die Politik müsste die Standortbedingungen verbessern. Wenn Deutschland ein wirtschaftliches Schwergewicht bleiben wolle, müssten KI-Champions durch «Zugang zu Talenten, digitale Infrastruktur, attraktives Investmentumfeld und weitere Anreize ins Land gelockt und auch gehalten werden», sagte PwC-Datenexperte Matthias Schlemmer.

Sehr wenig dürften dagegen die Auto- und Chemieindustrie, die Bauwirtschaft, die Transportbranche und die Landwirtschaft von generativer KI profitieren. Diese Branchen erwirtschaften fast die Hälfte des deutschen Bruttoinlandsprodukts. Für die von industrieller Fertigung, hohem Materialeinsatz, hohem Energiebedarf oder körperlicher Arbeit geprägten Branchen prognostiziert die Studie nur indirekte kleine Effizienzgewinne.

Im globalen Vergleich liegt Deutschland beim genKI-Potenzial laut Studie im Mittelfeld. Spitzenreiter sei die Schweiz mit einem großen Anteil von Pharma- und Finanzunternehmen. Auch die USA, Großbritannien und Schweden könnten von genKI enorm profitieren. Dagegen sei das Potenzial in China, Japan, Kanada oder Norwegen relativ gering.

© dpa-infocom, dpa:240304-99-209725/3
"
Künstliche Intelligenz,Zeit,2024-03-01,https://www.zeit.de/news/2024-03/01/musk-verklagt-chatgpt-entwickler-openai,Künstliche Intelligenz: Musk verklagt ChatGPT-Entwickler OpenAI | ZEIT ONLINE,"Tech-Milliardär Elon Musk eskaliert seine Fehde mit der ChatGPT-Entwicklerfirma OpenAI und deren Chef Sam Altman mit einer Klage. Im Kern geht es darum, dass das 2015 von Musk mitgegründete OpenAI von dem vereinbarten Weg abgekommen sei, ein nicht auf Profit ausgerichtetes Unternehmen zu sein, dessen Forschung bei Künstlicher Intelligenz der Menschheit zugutekommen sollte.

Jetzt profitiere vor allem Großinvestor Microsoft davon, heißt es in der in San Francisco eingereichten Klage. Das sei eine «eklatante Verletzung» der ursprünglichen Gründungsvereinbarung.

Musk, der bei OpenAI nach wenigen Jahren ausgeschieden war, kritisiert OpenAI und Altman schon lange. Er selbst gründete im vergangenen Jahr eine eigene KI-Firma mit dem Namen X.AI, deren Chatbot Grok mit ChatGPT konkurriert - während er vor Gefahren durch Künstliche Intelligenz warnte. Den Unterschied zu anderen Entwicklern sieht Musk darin, dass seine KI nach der «Wahrheit» suchen solle. Musk steht auf politischen Positionen der amerikanischen Rechten, beklagt angeblichen Rassismus gegenüber Weißen und wettert gegen das «Woke-Gehirnvirus», das die Menschheit zerstöre. Unter dem Begriff «Woke» werden oft Bemühungen und Einstellung gegen Diskriminierung zusammengefasst.

Musk verwies in der Klage darauf, dass OpenAI ausdrücklich als Gegengewicht zum Tandem aus Google und der von Internet-Konzern übernommenen KI-Firma DeepMind gegründet worden sein. Auslöser sei die Idee gewesen, dass Künstliche Intelligenz mit ihren potenziellen Gefahren nicht auf Gewinne ausgerichteten Unternehmen überlassen werden dürfe. Speziell geht es dabei um sogenannte allgemeine Künstliche Intelligenz, die nicht nur einzelne eng gefassten Aufgaben besser als Menschen erledigen könne, sondern ihnen generell überlegen sei. Er habe sich den Namen OpenAI ausgedacht, behauptet Musk.

Einen besonderen Wendepunkt sieht er laut der Klage in dem gescheiterten Versuch des Verwaltungsrates von OpenAI, Altman als Chef abzusetzen. Microsoft als milliardenschwerer Geldgeber habe seinen Einfluss geltend gemacht, damit Altman an die Spitze zurückkehren konnte. Die neuen Verwaltungsratsmitglieder hätten keine tiefgreifende Expertise bei Künstlicher Intelligenz. Nun sei es so, dass OpenAI «eine allgemeine Künstliche Intelligenz nicht nur entwickelt, sondern verfeinert, um die Profite von Microsoft zu maximieren - anstelle zum Wohle der Menschheit», hieß es in der Klageschrift. Von OpenAI gab es zunächst keine Reaktion auf die Klage.

© dpa-infocom, dpa:240301-99-183100/2
"
AI,Zeit,2024-03-01,https://www.zeit.de/digital/2024-03/elon-musk-openai-sam-altman-klage,OpenAI: Elon Musk verklagt OpenAI und CEO Sam Altman | ZEIT ONLINE,"Elon Musk hat Klage gegen das Unternehmen OpenAI und dessen
Geschäftsführer Sam Altman eingereicht. Er wirft ihnen laut Klageschrift vor, Verträge verletzt
zu haben, die bei der Gründung des Unternehmens im Jahr 2015 getroffen wurden.

Demnach soll Altman gemeinsam mit Mitbegründer Greg Brockman ursprünglich
an Musk herangetreten sein, um ein gemeinnütziges, Open-Source-Unternehmen zu
gründen, das Technologien für künstliche Intelligenz (KI) zum ""Nutzen der
Menschheit"" entwickeln solle. Nun sei das Unternehmen jedoch auf Profit ausgelegt, was einen Vertragsbruch darstelle.

Musk gehört zu den Gründern des Unternehmens. 2018 verließ er es.

OpenAI war ursprünglich als gemeinnützige Organisation
gegründet worden, sammelt mittlerweile jedoch Milliarden an Investorengeldern. Einer
der größten Investoren ist Microsoft. Das von dem Unternehmen entwickelte ChatGPT
wurde Ende 2022 zu einem der am schnellsten wachsenden Softwareprogrammen der
Welt – was Investoren wie Microsoft anzog.   

Auch die New York Times hatte vergangenes Jahr Klage gegen OpenAI eingereicht. Die Zeitung wirft dem Unternehmen vor, Urheberrechte verletzt zu haben. OpenAI soll die Daten von mehreren Millionen Artikeln des Blattes genutzt haben, um ihren ChatGPT
 damit zu füttern und ein Geschäft aufzubauen, schrieben die Anwälte in der Anklage.
"
AI,Zeit,2024-02-28,https://www.zeit.de/digital/mobil/2024-02/ai-pin-humane-wearable-gadget,"""AI Pin"" von Humane: Die Zukunft braucht keinen Bildschirm | ZEIT ONLINE",
AI,Zeit,2024-02-27,https://www.zeit.de/video/2024-02/6347746777112/ai-pin,Ai Pin | ZEIT ONLINE,
KI,Zeit,2024-03-01,https://www.zeit.de/2024/10/sora-video-ki-openai-computergenerierte-filme,"Video-KI ""Sora"": Oh, wie romantisch! | ZEIT ONLINE",
KI,Zeit,2024-02-28,https://www.zeit.de/news/2024-02/28/bosch-vereinbart-ki-kooperation-mit-microsoft,KI im Auto: Bosch vereinbart KI-Kooperation mit Microsoft | ZEIT ONLINE,"Bosch und Microsoft haben eine Kooperation vereinbart, um Autos mithilfe Künstlicher Intelligenz sicherer und bequemer zu machen. Das kündigte Stefan Hartung, Vorsitzender der Bosch-Geschäftsführung, auf der Hausmesse ""Bosch Connected World"" in Berlin an. Nach den Vorstellungen der beiden Technologiekonzerne soll Künstliche Intelligenz (KI) es dem Fahrzeug künftig ermöglichen, Situationen einzuschätzen, entsprechend zu reagieren und so Verkehrsteilnehmer damit besser zu schützen.

Tanja Rückert, Mitglied der Bosch-Geschäftsführung, sagte, generative KI sei ein ""Innovationsbooster"" und könne die Industrie verändern, ähnlich wie einst die Erfindung des Computers. Mit generativer KI ist eine Variante der KI gemeint, die in der Lage ist, eigenständig neue Inhalte zu erschaffen, die kaum von menschlichen Werken zu unterscheiden sind oder diese sogar übertreffen. Generative KI kann mit menschlicher Sprache bedient werden.

KI im Auto könne dabei helfen, ähnlich wie ein erfahrener menschlicher Fahrer eine Verkehrssituation durch Kontextwissen besser einzuschätzen, sagte Rückert. Als Beispiel nannte sie eine Situation, bei dem ein Ball auf die Straße rollt und die KI damit rechnet, dass vermutlich auch ein Kind auf die Straße rennen könnte. ""Eine gute KI kann den rollenden Ball aber auch von einem Szenario unterscheiden, bei dem nur eine leere Plastiktüte über die Straße geweht wird und eine Vollbremsung nicht notwendig ist.""

Bosch erhält mit der Kooperation zum einen Zugriff auf die KI-Technologie des Microsoft-Partners OpenAI. Gleichzeitig verfügt der Softwarekonzern aber auch über genügend Rechenkapazitäten, um aufwendige KI-Berechnungen auf Hochleistungsrechnern laufen zu lassen. Microsoft wiederum kann von der Masse der anonymisierten Daten profitieren, die in den Fahrzeugcomputern von Bosch anfallen.

Um die generative KI für die Entwicklung neuer Sicherheitsfeatures und anderer Funktionen zu füttern, dürften sich das Fahrzeugverständnis und die autospezifische KI-Expertise von Bosch als ebenso wertvoll erweisen wie der Zugang zu Fahrzeugsensordaten, erklärte Bosch.

Wenn es darum geht, Systeme für das automatisierte Fahren zu trainieren, kommt KI heute noch schnell an ihre Grenzen. Aktuelle Fahrerassistenzsysteme können bereits Personen, Tiere, Objekte und Fahrzeuge erkennen. In naher Zukunft könnten sie mithilfe generativer KI bestimmen, ob in der jeweiligen Situation auch ein Unfall droht. Generative KI trainiert Systeme für automatisiertes Fahren auf der Grundlage großer Datenmengen, aus denen so verbesserte Erkenntnisse gezogen werden.

Bosch kooperiert im Bereich KI nicht exklusiv mit Microsoft, sondern arbeitet auch mit anderen wichtigen Marktteilnehmern wie AWS und Google zusammen. Außerdem gehört Bosch zu den Geldgebern, die in das wichtigste deutsche KI-Start-up Aleph Alpha aus Heidelberg investiert haben. Diese Partnerschaft trage nun erste Früchte, sagte Rückert.

So habe Bosch in Nordamerika in Zusammenarbeit mit Aleph Alpha eine KI-basierte Spracherkennung im Auftrag eines Premium-Automobilherstellers eingeführt. Ein Sprach-Bot verstehe und beantworte dabei Pannenservice-Anrufe mithilfe einer natürlichen Sprachverarbeitung, die auch Dialekte, Akzente und Stimmungen erfasse. ""Die Wartezeit für den Fahrer wird aufgrund der Direktannahme des Gesprächs auf ein Minimum reduziert."" Bereits 40 Prozent der Anrufe könnten automatisiert bearbeitet und gelöst werden. Auch der firmeninterne Chatbot ""Ask Bosch"" laufe mit Technik von Aleph Alpha.

© dpa-infocom, dpa:240228-99-154129/5
"
Künstliche Intelligenz,Zeit,2024-02-26,https://www.zeit.de/news/2024-02/26/smartphone-revolution-telekom-setzt-auf-handy-ohne-apps,Künstliche Intelligenz: Smartphone-Revolution? Telekom setzt auf Handy ohne Apps | ZEIT ONLINE,"Ob Navi, Shopping-Portal oder Video-Streaming: Wer sein Smartphone nutzt, wischt häufig hin und her, um von einer App zur nächsten zu kommen. Geht es nach der Deutschen Telekom und deren US-Partner Brain Technologies, hat genau das perspektivisch ein Ende: Der Konzern stellte beim Mobile World Congress (MWC) in Barcelona den Prototypen eines Smartphones vor, bei dem der Nutzer keine Apps mehr sieht.

Statt auf dem Screen herumzutippen, reichen Sprachbefehle. Such mir ein Geschenk für meinen Sohn! Zeig mir den kürzesten Weg zu meinem Lieblingsrestaurant! Sag mir, wie ich ohne Streichhölzer Feuer im Wald mache! Ein Künstliche-Intelligenz (KI)-Concierge geht dann im Netz auf die Suche und zeigt Lösungen auf dem Screen an, ob Fotos oder Texte. Das soll einfacher sein als das Gefummel mit verschiedenen Apps.

Die sind dann nicht mehr nötig. Die Apps können auf dem Smartphone zwar im Hintergrund laufen, zu sehen sind sie aber nicht - sie spielen nur noch eine Nebenrolle, wenn überhaupt. ""Das Smartphone kommt komplett ohne Apps aus"", betont Telekom-Innovationschefin Claudia Nemat. Bei dem Vorhaben hat der Bonner Konzern zusammengearbeitet mit der KI-Firma Brain Technologies und mit dem Chiphersteller Qualcomm, beide aus den USA.

Beim genutzten Telefon handelt es sich um das Mittelklasse-Smartphone T-Phone. Das Besondere ist, was der Technologiepartner Brain daraus gemacht hat: Von ihm kommt die KI, die über die Cloud arbeitet. Außerdem gibt es eine zweite Version des KI-Phones, das offline arbeitet und einen Highspeed-Prozessor von Qualcomm hat. Ob eins dieser beiden derzeit nur als Prototyp existierende Handys jemals fertig entwickelt und im Laden zu kaufen sein wird, ist unklar.

Der Gründer und Chef von Brain Technologies, der 30 Jahre alte Amerikaner Jerry Yue, ist davon fest überzeugt. ""Es wird auf den Markt kommen"", sagt der umtriebige Geschäftsmann, der es 2022 auf die Forbes-Liste der weltweit 30 einflussreichsten Unternehmer geschafft hat, die jünger als 30 Jahre sind. Auf die Frage, wann das Gerät zu kaufen sein werden, sagt er: ""Ich denke nicht, dass Sie sehr lange warten müssen."" Mit großem Selbstvertrauen tritt er vor das Publikum am Messestand der Telekom und sagt mit Inbrunst der Überzeugung, dass er auf einer Mission sei, und die laute: ""Die Zukunft wird frei von Apps sein."" Die Macht, die die Apps derzeit hätten, werde zurückgehen an die Nutzer.

Damit haben sich Brain und die Telekom, die in den USA mit ihrer Tochter T-Mobile stark vertreten ist und dort die Schwergewichte AT&T und Verizon herausfordert, viel vorgenommen. Denn auf Apples iPhone und Telefonen großer Hersteller mit dem Google-System Android spielen Apps immer noch die Hauptrolle, auch wenn Nutzer mit Hilfe von Widgets die Oberfläche zum Teil selbst gestalten können. Der iPhone-Konzern bietet zudem für seine Computer-Uhr Apple Watch ein Zifferblatt an, auf dem Software-Algorithmen die gerade passenden Informationen anzeigen sollen.

Auf die Frage nach dem Marktstart geben sich Telekom-Verantwortliche zurückhaltend. Aber selbst wenn es am Ende nichts wird mit einem fertigen Produkt, so könnte das Projekt der Telekom das Ende der App-Ära einleiten. Ein Branchenvertreter, dessen Unternehmen im Wettbewerb mit der Telekom steht und seinen Namen nicht veröffentlicht haben will, sagt, dass das Vorhaben Potenzial habe. ""Das werden wir im Blick behalten.""

Ben Woods vom Beratungsunternehmen CCS Insight sieht einen starken Trend hin zur Künstlichen Intelligenz in der Mobilfunkbranche. Das KI-Phone der Telekom sei hierbei ein interessantes Beispiel, wie die Zukunft aussehen könnte. Dass das Smartphone nun als fassbarer Prototyp in der Öffentlichkeit vorgestellt worden sei, sei ein bemerkenswerter Schritt, zumal ausgerechnet ein Netzbetreiber das tue.

Die Telekom sei offenbar fest davon überzeugt, sich mit so einem Produkt von der Konkurrenz unterscheiden zu können, sagt der Branchenfachmann. Bezüglich einer Markteinführung sei er derzeit aber skeptisch. Woods sieht zudem ""das Risiko eines Hypes"", der sich von der Realität absetze, und merkt an, dass Netzbetreiber bei Hardware bisher nicht allzu erfolgreich gewesen seien.

Die Telekom betont, dass es ihr um die Kundenbindung gehe - dass also Kunden treu bleiben oder zu Magenta wechseln, weil das Gerät neue Möglichkeiten biete. Annette Zimmermann vom Beratungsunternehmen Gartner rechnet damit, dass die Branche künftig noch mehr Geräte dieser Art herausbringen werde.

Und wie reagiert die Konkurrenz auf den Telekom-Prototypen, der perspektivisch den Abschied von Apps einleiten könnte? Vodafone macht deutlich, dass es Apps auch künftig für wichtig halte. Marcel de Groot, der für Privatkunden zuständige Geschäftsführer Privatkunden, sagt, dass der Einsatz von Künstlicher Intelligenz die Bedienung von Smartphones auf lange Sicht zwar revolutionieren könne. Jede nachhaltige Veränderung brauche aber Zeit. ""Wir glauben, dass Smartphone-Hersteller und App-Anbieter ihre Services und Anwendungen zunächst so anpassen werden, dass der Datenaustausch mit anderen Apps und Betriebssystemen über KI-Funktionen leichter und besser funktioniert.""

© dpa-infocom, dpa:240226-99-132837/2
"
AI,Zeit,2024-02-24,https://www.zeit.de/politik/ausland/2024-02/cpac-usa-donald-trump-konferenz-kandidatur-republikaner,CPAC in den USA: Donald Trumps Jobmesse | ZEIT ONLINE,"Es gibt Orte in den USA, da ist Donald Trump längst wieder Präsident. Oder er ist gar nie weg gewesen. Die CPAC gehört dazu, die Conservative Political Action Conference, die auch in diesem Jahr in einem Tagungshotel außerhalb von Washington, D. C. stattfindet. Dort trifft sich, was früher der rechte Rand der Republikanischen Partei war und heute ihr Establishment ist: Verschwörungsideologen und religiöse Rechte, Extremisten und Nationalisten, Fans und Fanatiker.

Ihr erklärter Feind ist Joe Biden, dessen Abbild man an einem der Souvenirstände in der Messehalle schon mal mit Hitlerbärtchen auf Klopapier gedruckt sieht, ihr einziger Held: Donald Trump. Der bekommt kein Bärtchen, sondern kitschige AI-Gemälde: Trump neben einem überdimensionalen Löwenmännchen in eine US-amerikanische Flagge gehüllt, Trump im schwarzen Mantel vor einem brennenden Kapitol, in das gerade der Blitz einschlägt. Es gibt Trump-Süßigkeiten, Trump-Hängematten, Trump-Kleidchen und sogar einen Flipperautomaten, auf dem der Sturm auf das Kapitol nachgestellt wird.

Völlig klar also, wer gegen Biden antreten soll, um sich das Land ""zurückzuholen"", wie sie es hier nennen. Diese Haltung teilt offenbar die republikanische Wählerschaft: Trump hat die Kandidatur der Republikaner schon nach wenigen Vorwahlen nahezu sicher, obwohl er noch nicht die nötige Anzahl an Delegierten zusammenhat. Aber eine Frage haben die Menschen auch bei der CPAC noch: Wer begleitet ihn ins Weiße Haus?

Tatsächlich ist die Konferenz in diesem Wahljahr nicht das übliche Selbstvergewisserungstreffen der Maga-Bewegung, wie sich Trumps Anhänger nennen; Maga steht für den Slogan, mit dem er 2016 ins Weiße Haus einzog, Make America Great Again. Sie ist auch eine Jobmesse.

Denn Jobs hat Trump so einige zu vergeben, sollte er tatsächlich noch mal gewählt werden. Von den Leuten, mit denen er beim ersten Mal ins Weiße Haus einzog, sind kaum noch welche übrig. Gegen Ende von Trumps Amtszeit, als er seine Wahlniederlage nicht akzeptieren wollte, vor allem aber nach den Ereignissen vom 6. Januar wendeten sich Vertraute ab. Allen voran Mike Pence, der damalige Vizepräsident, dessen Leben Trump in jenen Tagen offen bedrohte, indem er seine Anhänger auf ihn hetzte.

Trotzdem ist dies der begehrteste Posten auf dem republikanischen Arbeitsmarkt: Trumps running mate, wie die Bewerber um die Vizepräsidentschaft in den USA genannt werden. Vielen prominenten Republikanern werden Ambitionen nachgesagt. Aber wen wird er auswählen? 
"
AI,Zeit,2024-02-24,https://www.zeit.de/news/2024-02/24/google-chef-fuer-globales-ki-regelwerk,Computer: Google-Chef für globales KI-Regelwerk | ZEIT ONLINE,"Der Vorstandschef des Google-Mutterkonzerns Alphabet, Sundar Pichai, hält ein weltweites Regelwerk für den Einsatz von Künstlicher Intelligenz für notwendig. ""Wir brauchen internationale Regeln für die künstliche Intelligenz. Deshalb sollten wir über ein transatlantisches Rahmenwerk nachdenken, oder noch besser: ein globales Rahmenwerk"", sagte Pichai der ""Süddeutschen Zeitung"".

Der Unternehmenschef begrüßte das geplante KI-Gesetz der Europäischen Union. ""Der AI Act schafft in seiner jetzigen Form ein gutes Gleichgewicht zwischen Innovation und verantwortungsvollen Schutzmaßnahmen. Dieses Gleichgewicht sollte auf dem Weg durch den weiteren Gesetzgebungsprozess bewahrt bleiben"", sagte Pichai.

Unterhändler vom Europaparlament und EU-Länder hatten sich im Dezember nach langen Verhandlungen auf eine Regulierung von KI geeinigt. Anfang Februar stimmten auch Vertreter der EU-Staaten dem Vorschlag zu. Nun fehlt noch die Billigung des EU-Parlaments.

Künftig sollen KI-Systeme in verschiedene Risikogruppen eingeteilt werden. Je höher die potenziellen Gefahren einer Anwendung sind, desto höher sollen die Anforderungen sein. Die Hoffnung ist, dass die Regeln weltweit Nachahmer finden.

Auf die Frage, ob nicht viele Arbeitsplätze durch Künstliche Intelligenz verschwinden werden, antwortete Pichai: ""Zunächst einmal glaube ich, dass künstliche Intelligenz neue Arbeitsplätze schaffen wird."" So werde man zum Beispiel Menschen brauchen, ""die den großen Sprachmodellen die richtigen Fragen stellen.

Es wird eine ganz neue Art von Berufen entstehen. Aber natürlich werden auch bestimmte Arbeitsplätze verdrängt. Deshalb müssen wir als Gesellschaft an der Qualifizierung arbeiten und sicherstellen, dass wir Menschen bei Bedarf umschulen. Wir müssen Sicherheitsnetze einrichten.""

© dpa-infocom, dpa:240224-99-104840/2
"
KI,Zeit,2024-02-24,https://www.zeit.de/news/2024-02/24/google-chef-fuer-globales-ki-regelwerk,Computer: Google-Chef für globales KI-Regelwerk | ZEIT ONLINE,"Der Vorstandschef des Google-Mutterkonzerns Alphabet, Sundar Pichai, hält ein weltweites Regelwerk für den Einsatz von Künstlicher Intelligenz für notwendig. ""Wir brauchen internationale Regeln für die künstliche Intelligenz. Deshalb sollten wir über ein transatlantisches Rahmenwerk nachdenken, oder noch besser: ein globales Rahmenwerk"", sagte Pichai der ""Süddeutschen Zeitung"".

Der Unternehmenschef begrüßte das geplante KI-Gesetz der Europäischen Union. ""Der AI Act schafft in seiner jetzigen Form ein gutes Gleichgewicht zwischen Innovation und verantwortungsvollen Schutzmaßnahmen. Dieses Gleichgewicht sollte auf dem Weg durch den weiteren Gesetzgebungsprozess bewahrt bleiben"", sagte Pichai.

Unterhändler vom Europaparlament und EU-Länder hatten sich im Dezember nach langen Verhandlungen auf eine Regulierung von KI geeinigt. Anfang Februar stimmten auch Vertreter der EU-Staaten dem Vorschlag zu. Nun fehlt noch die Billigung des EU-Parlaments.

Künftig sollen KI-Systeme in verschiedene Risikogruppen eingeteilt werden. Je höher die potenziellen Gefahren einer Anwendung sind, desto höher sollen die Anforderungen sein. Die Hoffnung ist, dass die Regeln weltweit Nachahmer finden.

Auf die Frage, ob nicht viele Arbeitsplätze durch Künstliche Intelligenz verschwinden werden, antwortete Pichai: ""Zunächst einmal glaube ich, dass künstliche Intelligenz neue Arbeitsplätze schaffen wird."" So werde man zum Beispiel Menschen brauchen, ""die den großen Sprachmodellen die richtigen Fragen stellen.

Es wird eine ganz neue Art von Berufen entstehen. Aber natürlich werden auch bestimmte Arbeitsplätze verdrängt. Deshalb müssen wir als Gesellschaft an der Qualifizierung arbeiten und sicherstellen, dass wir Menschen bei Bedarf umschulen. Wir müssen Sicherheitsnetze einrichten.""

© dpa-infocom, dpa:240224-99-104840/2
"
Künstliche Intelligenz,Zeit,2024-02-22,https://www.zeit.de/wirtschaft/2024-02/nvidia-kuenstliche-intelligenz-wachstum,Künstliche Intelligenz: KI-Entwicklung sorgt für Umsatzrekord beim Chipkonzern Nvidia | ZEIT ONLINE,"Der Hype um den Einsatz von künstlicher Intelligenz sorgt weiterhin für starkes Wachstum beim Chipkonzern Nvidia. Im vergangenen Quartal lag der Umsatz mit 22,1 Milliarden US-Dollar (20,35 Milliarden Euro) mehr als dreimal so hoch wie ein Jahr zuvor. Analysten hatten zuvor mit rund 20,4 Milliarden US-Dollar gerechnet. Im gesamten Geschäftsjahr erwirtschaftete der US-Mikrochiphersteller 
einen Rekordumsatz von 60,9 Milliarden US-Dollar (rund 56,2 Milliarden 
Euro).  

Auch der Ausblick für das laufende Quartal lag über den Erwartungen. Nvidia stellte Erlöse von rund 24 Milliarden US-Dollar (22,1 Milliarden Euro) in Aussicht, am Markt war im Schnitt mit einer Prognose von etwa 22 Milliarden US-Dollar gerechnet worden.

Das Geschäft mit Technik für Rechenzentren brachte mit 18,4 Milliarden US-Dollar (16,95 Milliarden Euro) sogar fünfmal so viel Umsatz wie im Vorjahresquartal. Die ursprünglich für Grafikkarten entwickelten Nvidia-Technologien sind nun bei der Rechenarbeit zum Anlernen von Anwendungen mit künstlicher Intelligenz im Einsatz. Das verhilft dem Geschäft und dem Börsenwert von Nvidia zu starkem Wachstum.

Der Quartalsgewinn von Nvidia sprang innerhalb eines Jahres von 1,4 auf knapp 12,3 Milliarden US-Dollar (11,33 Milliarden Euro). Der Einsatz von künstlicher Intelligenz habe einen Wendepunkt erreicht, sagte Nvidia-Chef Jensen Huang. Die Nachfrage steige ""weltweit bei allen Unternehmen, Industrien und Ländern"".

Die Aktie von Nvidia stieg allein seit Jahresbeginn um rund 40 Prozent. Auch nach Veröffentlichung der neuen Zahlen legte die Aktie weiter zu: Sie stieg im nachbörslichen US-Handel um gut neun Prozent.

Etwas gebremst wird das Geschäft von Nvidia von den Maßnahmen der US-Regierung gegen Lieferungen von KI-Technologie nach China. Das US-Unternehmen darf seine modernsten Chipsysteme, mit denen künstliche Intelligenz angelernt wird, nicht dorthin verkaufen. Die US-Regierung verweist auf das Risiko, dass die Technik für militärische Zwecke eingesetzt werden könnte.

China habe im vergangenen Quartal einen Anteil im mittleren einstelligen Prozentbereich am Geschäft mit Technik für Rechenzentren gehabt, teilte Nvidia mit. Die US-Regierung wolle, dass Nvidia in China möglichst erfolgreich sei, aber im Rahmen der Leistungsbeschränkungen, sagte Unternehmenschef Huang. 
"
Künstliche Intelligenz,Zeit,2024-02-22,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-openia-wahlmanipulation-chat-gpt,Künstliche Intelligenz: OpenAI-Chef warnt vor möglicher Wahlmanipulation durch KI | ZEIT ONLINE,"Der Chef des ChatGPT-Entwicklers OpenAI, Sam Altman, rechnet in Zukunft mit mehr von künstlicher Intelligenz (KI) erstellten Inhalten als von Menschen gemachten. Die
Gesellschaft müsse sich auch mit den negativen Folgen von KI auseinandersetzen
und entsprechend darauf vorbereiten, sagte Altman. Künstliche Intelligenz könne unter anderem in absehbarer Zukunft potenziell negative Auswirkungen auf Wahlen haben.

Auch Risiken wie bei Cybersicherheit und Biowaffen würden ernster genommen, sagte Altman. Künstliche Intelligenz werde ""keine nur gute Geschichte sein"".

Positive Effekte der KI sind Altman zufolge die Unterstützung bei wissenschaftlicher Forschung, im medizinischen
Bereich und bei der Bildung. Unterm Strich werde es mit KI eine positive Entwicklung geben, zeigte er sich überzeugt. Wichtig sei jedoch noch mehr als bei anderen Technologien eine regulierende Rolle von Regierungen.

Regulierungen für künstliche Intelligenz müssten so
schnell wie möglich kommen, während die KI-Modelle noch relativ schwach seien, sagte Altman.
Es dürfe nicht so laufen, dass beispielsweise OpenAI heimlich eine KI
entwickle, die schlauer als Menschen sei, und sie plötzlich auf die Welt
loslasse. Die Gesellschaft und ihre Institutionen müssten genügend Zeit bekommen, um sich schrittweise auf die Entwicklung vorbereiten zu können.

Der Chatbot ChatGPT löste im November 2022 einen Hype um künstliche Intelligenz aus. Solche KI-Chatbots
werden mit großen Informationsmengen trainiert und können Texte auf
dem sprachlichen Niveau eines Menschen formulieren, Softwarecode schreiben und
Informationen zusammenfassen. Chatbots geben aber manchmal auch völlig falsche
Antworten aus, selbst wenn sie nur korrekte Informationen als Basis hatten.

Neuere KI wie das System Sora von OpenAI kann neben komplexeren Texten auch Bilder und sogar Videos erstellen.
"
Künstliche Intelligenz,Zeit,2024-02-20,https://www.zeit.de/video/2024-02/6347263202112/ideenfestival-das-war-z2x23,Ideenfestival: Das war Z2X23 | ZEIT ONLINE,
AI,Zeit,2024-02-22,https://www.zeit.de/wirtschaft/2024-02/nvidia-kuenstliche-intelligenz-wachstum,Künstliche Intelligenz: KI-Entwicklung sorgt für Umsatzrekord beim Chipkonzern Nvidia | ZEIT ONLINE,"Der Hype um den Einsatz von künstlicher Intelligenz sorgt weiterhin für starkes Wachstum beim Chipkonzern Nvidia. Im vergangenen Quartal lag der Umsatz mit 22,1 Milliarden US-Dollar (20,35 Milliarden Euro) mehr als dreimal so hoch wie ein Jahr zuvor. Analysten hatten zuvor mit rund 20,4 Milliarden US-Dollar gerechnet. Im gesamten Geschäftsjahr erwirtschaftete der US-Mikrochiphersteller 
einen Rekordumsatz von 60,9 Milliarden US-Dollar (rund 56,2 Milliarden 
Euro).  

Auch der Ausblick für das laufende Quartal lag über den Erwartungen. Nvidia stellte Erlöse von rund 24 Milliarden US-Dollar (22,1 Milliarden Euro) in Aussicht, am Markt war im Schnitt mit einer Prognose von etwa 22 Milliarden US-Dollar gerechnet worden.

Das Geschäft mit Technik für Rechenzentren brachte mit 18,4 Milliarden US-Dollar (16,95 Milliarden Euro) sogar fünfmal so viel Umsatz wie im Vorjahresquartal. Die ursprünglich für Grafikkarten entwickelten Nvidia-Technologien sind nun bei der Rechenarbeit zum Anlernen von Anwendungen mit künstlicher Intelligenz im Einsatz. Das verhilft dem Geschäft und dem Börsenwert von Nvidia zu starkem Wachstum.

Der Quartalsgewinn von Nvidia sprang innerhalb eines Jahres von 1,4 auf knapp 12,3 Milliarden US-Dollar (11,33 Milliarden Euro). Der Einsatz von künstlicher Intelligenz habe einen Wendepunkt erreicht, sagte Nvidia-Chef Jensen Huang. Die Nachfrage steige ""weltweit bei allen Unternehmen, Industrien und Ländern"".

Die Aktie von Nvidia stieg allein seit Jahresbeginn um rund 40 Prozent. Auch nach Veröffentlichung der neuen Zahlen legte die Aktie weiter zu: Sie stieg im nachbörslichen US-Handel um gut neun Prozent.

Etwas gebremst wird das Geschäft von Nvidia von den Maßnahmen der US-Regierung gegen Lieferungen von KI-Technologie nach China. Das US-Unternehmen darf seine modernsten Chipsysteme, mit denen künstliche Intelligenz angelernt wird, nicht dorthin verkaufen. Die US-Regierung verweist auf das Risiko, dass die Technik für militärische Zwecke eingesetzt werden könnte.

China habe im vergangenen Quartal einen Anteil im mittleren einstelligen Prozentbereich am Geschäft mit Technik für Rechenzentren gehabt, teilte Nvidia mit. Die US-Regierung wolle, dass Nvidia in China möglichst erfolgreich sei, aber im Rahmen der Leistungsbeschränkungen, sagte Unternehmenschef Huang. 
"
AI,Zeit,2024-02-22,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-openia-wahlmanipulation-chat-gpt,Künstliche Intelligenz: OpenAI-Chef warnt vor möglicher Wahlmanipulation durch KI | ZEIT ONLINE,"Der Chef des ChatGPT-Entwicklers OpenAI, Sam Altman, rechnet in Zukunft mit mehr von künstlicher Intelligenz (KI) erstellten Inhalten als von Menschen gemachten. Die
Gesellschaft müsse sich auch mit den negativen Folgen von KI auseinandersetzen
und entsprechend darauf vorbereiten, sagte Altman. Künstliche Intelligenz könne unter anderem in absehbarer Zukunft potenziell negative Auswirkungen auf Wahlen haben.

Auch Risiken wie bei Cybersicherheit und Biowaffen würden ernster genommen, sagte Altman. Künstliche Intelligenz werde ""keine nur gute Geschichte sein"".

Positive Effekte der KI sind Altman zufolge die Unterstützung bei wissenschaftlicher Forschung, im medizinischen
Bereich und bei der Bildung. Unterm Strich werde es mit KI eine positive Entwicklung geben, zeigte er sich überzeugt. Wichtig sei jedoch noch mehr als bei anderen Technologien eine regulierende Rolle von Regierungen.

Regulierungen für künstliche Intelligenz müssten so
schnell wie möglich kommen, während die KI-Modelle noch relativ schwach seien, sagte Altman.
Es dürfe nicht so laufen, dass beispielsweise OpenAI heimlich eine KI
entwickle, die schlauer als Menschen sei, und sie plötzlich auf die Welt
loslasse. Die Gesellschaft und ihre Institutionen müssten genügend Zeit bekommen, um sich schrittweise auf die Entwicklung vorbereiten zu können.

Der Chatbot ChatGPT löste im November 2022 einen Hype um künstliche Intelligenz aus. Solche KI-Chatbots
werden mit großen Informationsmengen trainiert und können Texte auf
dem sprachlichen Niveau eines Menschen formulieren, Softwarecode schreiben und
Informationen zusammenfassen. Chatbots geben aber manchmal auch völlig falsche
Antworten aus, selbst wenn sie nur korrekte Informationen als Basis hatten.

Neuere KI wie das System Sora von OpenAI kann neben komplexeren Texten auch Bilder und sogar Videos erstellen.
"
Artificial Intelligence,Zeit,2024-02-22,https://www.zeit.de/wirtschaft/2024-02/nvidia-kuenstliche-intelligenz-wachstum,Künstliche Intelligenz: KI-Entwicklung sorgt für Umsatzrekord beim Chipkonzern Nvidia | ZEIT ONLINE,"Der Hype um den Einsatz von künstlicher Intelligenz sorgt weiterhin für starkes Wachstum beim Chipkonzern Nvidia. Im vergangenen Quartal lag der Umsatz mit 22,1 Milliarden US-Dollar (20,35 Milliarden Euro) mehr als dreimal so hoch wie ein Jahr zuvor. Analysten hatten zuvor mit rund 20,4 Milliarden US-Dollar gerechnet. Im gesamten Geschäftsjahr erwirtschaftete der US-Mikrochiphersteller 
einen Rekordumsatz von 60,9 Milliarden US-Dollar (rund 56,2 Milliarden 
Euro).  

Auch der Ausblick für das laufende Quartal lag über den Erwartungen. Nvidia stellte Erlöse von rund 24 Milliarden US-Dollar (22,1 Milliarden Euro) in Aussicht, am Markt war im Schnitt mit einer Prognose von etwa 22 Milliarden US-Dollar gerechnet worden.

Das Geschäft mit Technik für Rechenzentren brachte mit 18,4 Milliarden US-Dollar (16,95 Milliarden Euro) sogar fünfmal so viel Umsatz wie im Vorjahresquartal. Die ursprünglich für Grafikkarten entwickelten Nvidia-Technologien sind nun bei der Rechenarbeit zum Anlernen von Anwendungen mit künstlicher Intelligenz im Einsatz. Das verhilft dem Geschäft und dem Börsenwert von Nvidia zu starkem Wachstum.

Der Quartalsgewinn von Nvidia sprang innerhalb eines Jahres von 1,4 auf knapp 12,3 Milliarden US-Dollar (11,33 Milliarden Euro). Der Einsatz von künstlicher Intelligenz habe einen Wendepunkt erreicht, sagte Nvidia-Chef Jensen Huang. Die Nachfrage steige ""weltweit bei allen Unternehmen, Industrien und Ländern"".

Die Aktie von Nvidia stieg allein seit Jahresbeginn um rund 40 Prozent. Auch nach Veröffentlichung der neuen Zahlen legte die Aktie weiter zu: Sie stieg im nachbörslichen US-Handel um gut neun Prozent.

Etwas gebremst wird das Geschäft von Nvidia von den Maßnahmen der US-Regierung gegen Lieferungen von KI-Technologie nach China. Das US-Unternehmen darf seine modernsten Chipsysteme, mit denen künstliche Intelligenz angelernt wird, nicht dorthin verkaufen. Die US-Regierung verweist auf das Risiko, dass die Technik für militärische Zwecke eingesetzt werden könnte.

China habe im vergangenen Quartal einen Anteil im mittleren einstelligen Prozentbereich am Geschäft mit Technik für Rechenzentren gehabt, teilte Nvidia mit. Die US-Regierung wolle, dass Nvidia in China möglichst erfolgreich sei, aber im Rahmen der Leistungsbeschränkungen, sagte Unternehmenschef Huang. 
"
Artificial Intelligence,Zeit,2024-02-22,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-openia-wahlmanipulation-chat-gpt,Künstliche Intelligenz: OpenAI-Chef warnt vor möglicher Wahlmanipulation durch KI | ZEIT ONLINE,"Der Chef des ChatGPT-Entwicklers OpenAI, Sam Altman, rechnet in Zukunft mit mehr von künstlicher Intelligenz (KI) erstellten Inhalten als von Menschen gemachten. Die
Gesellschaft müsse sich auch mit den negativen Folgen von KI auseinandersetzen
und entsprechend darauf vorbereiten, sagte Altman. Künstliche Intelligenz könne unter anderem in absehbarer Zukunft potenziell negative Auswirkungen auf Wahlen haben.

Auch Risiken wie bei Cybersicherheit und Biowaffen würden ernster genommen, sagte Altman. Künstliche Intelligenz werde ""keine nur gute Geschichte sein"".

Positive Effekte der KI sind Altman zufolge die Unterstützung bei wissenschaftlicher Forschung, im medizinischen
Bereich und bei der Bildung. Unterm Strich werde es mit KI eine positive Entwicklung geben, zeigte er sich überzeugt. Wichtig sei jedoch noch mehr als bei anderen Technologien eine regulierende Rolle von Regierungen.

Regulierungen für künstliche Intelligenz müssten so
schnell wie möglich kommen, während die KI-Modelle noch relativ schwach seien, sagte Altman.
Es dürfe nicht so laufen, dass beispielsweise OpenAI heimlich eine KI
entwickle, die schlauer als Menschen sei, und sie plötzlich auf die Welt
loslasse. Die Gesellschaft und ihre Institutionen müssten genügend Zeit bekommen, um sich schrittweise auf die Entwicklung vorbereiten zu können.

Der Chatbot ChatGPT löste im November 2022 einen Hype um künstliche Intelligenz aus. Solche KI-Chatbots
werden mit großen Informationsmengen trainiert und können Texte auf
dem sprachlichen Niveau eines Menschen formulieren, Softwarecode schreiben und
Informationen zusammenfassen. Chatbots geben aber manchmal auch völlig falsche
Antworten aus, selbst wenn sie nur korrekte Informationen als Basis hatten.

Neuere KI wie das System Sora von OpenAI kann neben komplexeren Texten auch Bilder und sogar Videos erstellen.
"
KI,Zeit,2024-02-22,https://www.zeit.de/wirtschaft/2024-02/nvidia-kuenstliche-intelligenz-wachstum,Künstliche Intelligenz: KI-Entwicklung sorgt für Umsatzrekord beim Chipkonzern Nvidia | ZEIT ONLINE,"Der Hype um den Einsatz von künstlicher Intelligenz sorgt weiterhin für starkes Wachstum beim Chipkonzern Nvidia. Im vergangenen Quartal lag der Umsatz mit 22,1 Milliarden US-Dollar (20,35 Milliarden Euro) mehr als dreimal so hoch wie ein Jahr zuvor. Analysten hatten zuvor mit rund 20,4 Milliarden US-Dollar gerechnet. Im gesamten Geschäftsjahr erwirtschaftete der US-Mikrochiphersteller 
einen Rekordumsatz von 60,9 Milliarden US-Dollar (rund 56,2 Milliarden 
Euro).  

Auch der Ausblick für das laufende Quartal lag über den Erwartungen. Nvidia stellte Erlöse von rund 24 Milliarden US-Dollar (22,1 Milliarden Euro) in Aussicht, am Markt war im Schnitt mit einer Prognose von etwa 22 Milliarden US-Dollar gerechnet worden.

Das Geschäft mit Technik für Rechenzentren brachte mit 18,4 Milliarden US-Dollar (16,95 Milliarden Euro) sogar fünfmal so viel Umsatz wie im Vorjahresquartal. Die ursprünglich für Grafikkarten entwickelten Nvidia-Technologien sind nun bei der Rechenarbeit zum Anlernen von Anwendungen mit künstlicher Intelligenz im Einsatz. Das verhilft dem Geschäft und dem Börsenwert von Nvidia zu starkem Wachstum.

Der Quartalsgewinn von Nvidia sprang innerhalb eines Jahres von 1,4 auf knapp 12,3 Milliarden US-Dollar (11,33 Milliarden Euro). Der Einsatz von künstlicher Intelligenz habe einen Wendepunkt erreicht, sagte Nvidia-Chef Jensen Huang. Die Nachfrage steige ""weltweit bei allen Unternehmen, Industrien und Ländern"".

Die Aktie von Nvidia stieg allein seit Jahresbeginn um rund 40 Prozent. Auch nach Veröffentlichung der neuen Zahlen legte die Aktie weiter zu: Sie stieg im nachbörslichen US-Handel um gut neun Prozent.

Etwas gebremst wird das Geschäft von Nvidia von den Maßnahmen der US-Regierung gegen Lieferungen von KI-Technologie nach China. Das US-Unternehmen darf seine modernsten Chipsysteme, mit denen künstliche Intelligenz angelernt wird, nicht dorthin verkaufen. Die US-Regierung verweist auf das Risiko, dass die Technik für militärische Zwecke eingesetzt werden könnte.

China habe im vergangenen Quartal einen Anteil im mittleren einstelligen Prozentbereich am Geschäft mit Technik für Rechenzentren gehabt, teilte Nvidia mit. Die US-Regierung wolle, dass Nvidia in China möglichst erfolgreich sei, aber im Rahmen der Leistungsbeschränkungen, sagte Unternehmenschef Huang. 
"
KI,Zeit,2024-02-22,https://www.zeit.de/news/2024-02/22/ki-boom-treibt-nvidias-rekordlauf-weiter-an,Quartalszahlen: KI-Boom treibt Nvidias Rekordlauf weiter an | ZEIT ONLINE,"Der Boom beim Einsatz Künstlicher Intelligenz sorgt weiterhin für explosives Wachstum beim Chipkonzern Nvidia. Im vergangenen Quartal lag der Umsatz mit 22,1 Milliarden Dollar mehr als dreimal so hoch wie ein Jahr zuvor. Analysten hatten im Schnitt mit 20,4 Milliarden Dollar gerechnet. Die Aktie legte im nachbörslichen US-Handel am Mittwoch um gut neun Prozent zu.

Das Geschäft mit Technik für Rechenzentren brachte mit 18,4 Milliarden Dollar sogar fünfmal so viel Umsatz wie im Vorjahresquartal. Die ursprünglich für Grafikkarten entwickelten Nvidia-Technologien bewähren sich schon seit Langem bei der Rechenarbeit zum Anlernen von Anwendungen mit Künstlicher Intelligenz. Das lässt das Geschäft - und den Börsenwert - von Nvidia rasant steigen.

Die Zahlen waren mit großer Spannung erwartet worden. Die Nvidia-Aktie legte allein seit Jahresbeginn um rund 40 Prozent zu - und Beobachter gingen davon aus, dass der geringste Hinweis auf eine Abschwächung des Wachstums den Kurs nach unten geschickt hätte. Nvidia enttäuschte jedoch nicht.

Auch der Ausblick für das laufende Quartal lag über den Erwartungen. Nvidia stellte Erlöse von rund 24 Milliarden Dollar in Aussicht, am Markt war im Schnitt mit einer Prognose von etwa 22 Milliarden Dollar gerechnet worden.

Der Quartalsgewinn von Nvidia sprang binnen eines Jahres von 1,4 auf knapp 12,3 Milliarden Dollar hoch. Der Einsatz von Künstlicher Intelligenz habe einen Wendepunkt erreicht und die Nachfrage steige weltweit, betonte Nvidia-Chef Jensen Huang.

Etwas gebremst wird das Nvidia-Geschäft von den Maßnahmen der US-Regierung gegen Lieferungen von KI-Technologie nach China. Nvidia darf seine modernsten Chip-Systeme, mit denen Künstliche Intelligenz angelernt wird, nicht dorthin verkaufen. Washington verweist auf das Risiko, dass die Technik für militärische Zwecke eingesetzt werden könnte. Auch für eine abgespeckte Version der Geräte bekam Nvidia bisher keine Ausfuhr-Lizenz, deswegen wurden zuletzt Chips geliefert, deren Leistung unter den vorgegebenen Grenzwerten liegt. 

China habe im vergangenen Quartal einen Anteil im mittleren einstelligen Prozent-Bereich am Geschäft mit Technik für Rechenzentren gehabt, hieß es. Die US-Regierung wolle, dass Nvidia in China möglichst erfolgreich sei, aber im Rahmen der Leistungsbeschränkungen, sagte Huang.

© dpa-infocom, dpa:240222-99-78888/2
"
KI,Zeit,2024-02-22,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-openia-wahlmanipulation-chat-gpt,Künstliche Intelligenz: OpenAI-Chef warnt vor möglicher Wahlmanipulation durch KI | ZEIT ONLINE,"Der Chef des ChatGPT-Entwicklers OpenAI, Sam Altman, rechnet in Zukunft mit mehr von künstlicher Intelligenz (KI) erstellten Inhalten als von Menschen gemachten. Die
Gesellschaft müsse sich auch mit den negativen Folgen von KI auseinandersetzen
und entsprechend darauf vorbereiten, sagte Altman. Künstliche Intelligenz könne unter anderem in absehbarer Zukunft potenziell negative Auswirkungen auf Wahlen haben.

Auch Risiken wie bei Cybersicherheit und Biowaffen würden ernster genommen, sagte Altman. Künstliche Intelligenz werde ""keine nur gute Geschichte sein"".

Positive Effekte der KI sind Altman zufolge die Unterstützung bei wissenschaftlicher Forschung, im medizinischen
Bereich und bei der Bildung. Unterm Strich werde es mit KI eine positive Entwicklung geben, zeigte er sich überzeugt. Wichtig sei jedoch noch mehr als bei anderen Technologien eine regulierende Rolle von Regierungen.

Regulierungen für künstliche Intelligenz müssten so
schnell wie möglich kommen, während die KI-Modelle noch relativ schwach seien, sagte Altman.
Es dürfe nicht so laufen, dass beispielsweise OpenAI heimlich eine KI
entwickle, die schlauer als Menschen sei, und sie plötzlich auf die Welt
loslasse. Die Gesellschaft und ihre Institutionen müssten genügend Zeit bekommen, um sich schrittweise auf die Entwicklung vorbereiten zu können.

Der Chatbot ChatGPT löste im November 2022 einen Hype um künstliche Intelligenz aus. Solche KI-Chatbots
werden mit großen Informationsmengen trainiert und können Texte auf
dem sprachlichen Niveau eines Menschen formulieren, Softwarecode schreiben und
Informationen zusammenfassen. Chatbots geben aber manchmal auch völlig falsche
Antworten aus, selbst wenn sie nur korrekte Informationen als Basis hatten.

Neuere KI wie das System Sora von OpenAI kann neben komplexeren Texten auch Bilder und sogar Videos erstellen.
"
Künstliche Intelligenz,Zeit,2024-02-17,https://www.zeit.de/2024/08/ausbruch-vesuv-papyrusrollen-entziffern-kuenstliche-intelligenz,Ausbruch des Vesuvs: Der Schatz aus der Asche | ZEIT ONLINE,
Künstliche Intelligenz,Zeit,2024-02-16,https://www.zeit.de/2024/08/kuenstliche-intelligenz-training-fotos-robert-kneschke/seite-2,"Künstliche Intelligenz: Erst beklaut, dann kopiert | ZEIT Arbeit",
Künstliche Intelligenz,Zeit,2024-02-16,https://www.zeit.de/2024/08/kuenstliche-intelligenz-training-fotos-robert-kneschke,Künstliche Intelligenz: Rentner helfen Rechnern | ZEIT Arbeit,
Künstliche Intelligenz,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-ki-wahl-manipulation-digitalkonzerne,Künstliche Intelligenz: Techkonzerne wollen Wahlmanipulationen durch KI verhindern | ZEIT ONLINE,"Führende Technologiekonzerne haben sich dazu verpflichtet, bei der Bekämpfung von Wahlmanipulation durch künstliche Intelligenz (KI) zusammenzuarbeiten. Damit wollen sie verhindern, dass KI-generierte Inhalte weltweit politische Wahlen beeinträchtigen. Auf der Münchner Sicherheitskonferenz unterzeichneten Vertreter von 20 Unternehmen eine Vereinbarung über Kooperation bei der Erkennung und Bekämpfung manipulierter Inhalte. 

Unter den beteiligten Firmen sind Konzerne wie Amazon, Google, IBM, Meta, Microsoft, TikTok, X und Adobe – sowie OpenAI, das Unternehmen, das den KI-Chatbot ChatGPT entwickelt hat. Vor allem haben die Firmen dabei KI-generierte Audio-, Video- und Bilddateien im Visier, die das Aussehen oder die Stimme von politischen Kandidaten, Wahlhelfern und weiteren Beteiligten demokratischer Wahlen täuschend nachahmen oder verändern. Außerdem richtet sich die Initiative gegen digitale Inhalte, die den Wählern falsche Informationen darüber liefern, wann, wo und wie Wahlen stattfinden. 

Erfahrungen mit manipulativen KI-Inhalten in laufenden Wahlkämpfen machten zuletzt vor allem die USA. Dort hatte ein Telefonroboter bei Vorwahlen im Bundesstaat New Hampshire die Menschen mit der Stimme von Präsident Joe Biden dazu aufgefordert, ihre Stimme nicht abzugeben. Die von einer KI generierte Stimme war kaum von der echten Stimme des Präsidenten zu unterscheiden. Seitdem verbot die US-Kommunikationsaufsicht automatisierte Werbeanrufe mit KI-Stimmen.

Wahlen seien das pulsierende Herz der Demokratien, sagte der Chef der Münchner Sicherheitskonferenz, Christoph Heusgen, angesichts der Vereinbarung. Die Erklärung der Techkonzerne sei ein entscheidender Schritt, um die Integrität von Wahlen zu fördern, die gesellschaftliche Resilienz zu erhöhen und vertrauenswürdige Techpraktiken zu schaffen.  

Die Vereinbarung sei nicht nur ein Schritt zum Schutz von Wahlen, sondern auch zum Schutz von Möglichkeiten, die durch KI entstünden, sagte Kent Walker, Präsident des Bereichs Global Affairs bei der Google-Mutter Alphabet. ""Wir dürfen nicht zulassen, dass ein digitaler Missbrauch die Chancen der künstlichen Intelligenz bedroht, unsere Volkswirtschaften zu verbessern, neue Arbeitsplätze zu schaffen und den Fortschritt in Gesundheit und Wissenschaft voranzutreiben"", sagte er. 

Microsofts Präsident Brad Smith sagte, die Unterzeichner der Vereinbarung stünden in der Verantwortung, dass KI-Werkzeuge nicht zu einer Waffe würden. Im Rahmen des Abkommens wollen die beteiligten Firmen unter anderem ihre KI-Modelle neu bewerten, um Risiken zu verstehen, die sie im Zusammenhang mit betrügerischen Wahlinhalten darstellen könnten. Die Unternehmen wollen zudem dabei helfen, das öffentliche Bewusstsein dafür zu schärfen. Zudem wollen sich die Firmen für eine höhere Medienkompetenz in der Gesellschaft einsetzen.  
"
Künstliche Intelligenz,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-openai-video-produktion-sora,Künstliche Intelligenz: OpenAI entwickelt KI-Anwendung zur Videoproduktion | ZEIT ONLINE,"Das US-Unternehmen OpenAI, Entwickler des Chatbots ChatGPT und des Bildgenerators Dall-e, hat eine KI-Anwendung zur Produktion realistischer Videos entwickelt. Das teilte das Unternehmen mit Sitz in San Francisco mit. Die Anwendung namens Sora könne anhand eines Prompts, ein kurzer Befehl in Textform, Videos mit einer Länge von bis zu einer Minute herstellen. Die Anwendung könne auch aus einem Foto ein Video machen oder ein kurzes Video verlängern.

Auf X kündigte OpenAI-Chef Sam Altman an, das Modell werde zunächst ausgewählten Kreativen zur Verfügung gestellt. Bevor das Programm breit genutzt werden kann, sollen Expertinnen und Experten mögliche Sicherheitsrisiken ausloten. Politiker, Lehrer und Künstler weltweit könnten Bedenken äußern.

Altman hatte jedoch dazu aufgefordert, Vorschläge für Videos zu machen. Sora werde nur Augenblicke später ""überzeugende Ergebnisse"" präsentieren. Als ein Beispiel zeigte OpenAI unter anderem ein Video, in dem ein Wesen ""halb Ente, halb Drachen"" vor einem ""wunderschönen Sonnenuntergang"" fliegt und dabei einen ""Hamster im Abenteuer-Outfit"" auf dem Rücken trägt.

Einige Schwächen habe Sora jedoch noch: Beispielsweise verwechsle die KI rechts und links. Generell könnten manchmal Fehler bei der Umsetzung physikalischer Gesetze und der Kontinuität passieren. So könne es zum Beispiel vorkommen, dass jemand in einem Video von einem Keks abbeißt, dieser danach aber noch ganz aussieht.

Ähnliche Anwendungen bieten bereits Unternehmen wie Meta, Google oder Runway AI an – entweder arbeiten sie daran, oder haben bereits Versionen veröffentlicht. Solche Technologien könnten die Videoproduktion mit der Zeit verändern. Zugleich besteht die Sorge, dass damit in großem Stil Fake-Videos erzeugt werden können, die von echten Aufnahmen kaum zu unterscheiden wären. Die Entwickler der Technologie arbeiten deshalb an Wegen, in die Videos eindeutige Erkennungsmerkmale wie Wasserzeichen einzubauen. Auch bei Sora-Videos solle erkennbar sein, dass sie von KI erzeugt wurden. 
"
Künstliche Intelligenz,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/ki-wahlkampf-joe-biden-kennzeichnung,"KI im Wahlkampf: Mr. President, sind Sie es wirklich? | ZEIT ONLINE","""Was für ein Blödsinn!"" So begann ein Anruf, den einige
Wählerinnen im US-Staat New Hampshire Ende
Januar erhielten. Am Telefon: die Stimme des US-Präsidenten Joe Biden.
Darin forderte er die Menschen vermeintlich auf, nicht zur Vorwahl in dem
Bundesstaat zu gehen und ihre Stimme für November 2024 aufzuheben. Dann, wenn
in den USA ein neuer Präsident gewählt wird. Der automatisierte Anruf war
fingiert, die Stimme wohl künstlich erzeugt. 

Ob Audiodateien, Bilder oder Videos: Noch nie war es so
einfach wie heute, mithilfe von künstlicher Intelligenz irreführende Inhalte zu
erstellen – und so schwierig, sie schnell als solche zu identifizieren. Im
Wahlkampf können gefälschte Inhalte zu Waffen werden, um den Gegner zu
diskreditieren und Wählerinnen zu verunsichern. Nun wollen die großen
Techfirmen auf der Münchner Sicherheitskonferenz eine Richtlinie gegen den
Einsatz von künstlicher Intelligenz zur Unterwanderung demokratischer Wahlen vorstellen.
Adobe, Google, Meta, Microsoft, OpenAI und TikTok wollen demnach Werkzeuge zu
entwickeln, mit denen sich KI-generierte Bilder, Videos und Audios
identifizieren lassen. 

Die Ankündigung kommt in einem kritischen Jahr: 2024 können rund
3,6 Milliarden Menschen in
mehr als 60 Ländern wählen gehen, darunter in den USA und in der EU. Das
World Economic Forum (WEF) rankt die Beeinflussung von Wahlen durch mithilfe
von KI generierter Mis- und Desinformation in seinem Risikobericht
für die kommenden zwei Jahre inzwischen auf dem ersten Platz, noch vor
Extremwetter und sozialer Polarisierung. 

Die Angst vor Einflussnahme ist groß – und nicht
unbegründet: Zwei Tage vor der Parlamentswahl 2023 in der Slowakei
war eine Audioaufnahme auf Facebook verbreitet worden, in der der
Kandidat der liberalen Partei mit einer Journalistin angeblich darüber sprach,
die Wahl zu manipulieren – ein
Fake. Am Ende gewann die oppositionelle,
prorussische Partei knapp. Ein Zusammenhang lässt sich nicht sicher
belegen. Die Frage aber, ob das gefälschte Audio eine Auswirkung hatte, ist da.

Das wirft eine weitere Frage auf: Wie kann man KI generierte
Inhalte so kennzeichnen, dass sie für jede Person sofort als solche erkennbar
sind und auf diese Weise zumindest die Wirkung von Desinformation eindämmen? 

Technologisch ist das keine neue Sache. So wie man schon
seit Langem Geldscheine mit Sicherheitsmerkmalen wie Wasserzeichen versieht
oder Stockfotos mit dem Namen des Anbieters, so lassen sich auch mithilfe von
künstlicher Intelligenz erstellte Inhalte mit Wasserzeichen kennzeichnen. Das
kann sowohl sichtbar passieren, etwa durch ein Logo in der Bildecke, als auch,
für das menschliche Auge nicht mehr erkennbar, in den Bits der Pixelstruktur. Denkbar
wäre es auch, Sprachmodelle wie DALL-E von OpenAI oder Midjourney direkt mit Bildern
zu trainieren, die bereits mit einem Wasserzeichen markiert wurden. Die Idee
dahinter: Wenn die Trainingsdaten das Merkmal aufweisen, dann auch das neue
Ergebnis. 

Es gibt aber noch eine Reihe anderer Verfahren, etwa kann nach
dem Erstellen eines künstlichen Bildes ein entsprechender Datensatz generiert
werden, der Auskunft über dessen Genese gibt, beispielsweise ob für das Bild
künstlich Intelligenz verwendet wurde. Anschließend werden die Daten codiert
und eingefügt.  

Erst vor einigen Wochen hatte OpenAI angekündigt,
entsprechende Informationen in den
Metadaten der Bilder einzubetten, die mit der Schnittstelle (API) von DALL-E
3 oder Chat GPT erstellt wurden. Um das gegenzuchecken, können Nutzerinnen und
Nutzer über die Seite Content
Credentials Verify prüfen, ob ein Bild etwa mit DALL-E 3 generiert wurde.
Schon im Januar hatte das US-Unternehmen seine Bemühungen im Wahljahr 2024 in
einem Blogpost
aufgezeichnet. Auch
bei TikTok gibt es die Option, Inhalte als ""KI-generiert"" zu markieren,
wenn sie realistische Szenen enthalten und ""vollständig"" oder ""erheblich"" durch
KI bearbeitet wurden. Meta, das Unternehmen hinter Facebook, Instagram und
Threads, hatte
Anfang Februar angekündigt, KI-Inhalte in den kommenden Monaten vor den
Wahlen zu kennzeichnen. Der US-Konzern macht das laut eigenen Angaben bereits mit sichtbaren wie unsichtbaren Wasserzeichen sowie Metadaten, allerdings bisher nur
mit Bildern, die mit dem eigenen KI-Tool erstellt wurden. Künftig will Meta
auch die Kennzeichnung anderer Anbieter übernehmen. Das beinhaltet aber weder
Audio- noch Videodateien, schreibt das Unternehmen. Sind diese mithilfe von KI
erstellt worden, müssen die Nutzer selbst deren Herkunft offenlegen,
andernfalls drohen Strafen. 

Das ist in mehrerlei Hinsicht bemerkenswert, erstens, weil solche
Metadaten in der Regel gelöscht werden, sobald ein Bild bei einer
Social-Media-Plattform hochgeladen wird. Zweitens gilt bis dato eigentlich,
dass Nutzerinnen und Nutzer bei jedem Anbieter einzeln nachprüfen müssten, ob
das Bild mit dessen KI-Tool erstellt wurde, weil eine einheitliche und
übergreifende Lösung noch fehlte. 
"
Künstliche Intelligenz,Zeit,2024-02-15,https://www.zeit.de/wirtschaft/unternehmen/2024-02/microsoft-ki-kuenstliche-intelligenz-investition,"Künstliche Intelligenz: Microsoft investiert 3,3 Milliarden Euro in KI in Deutschland | ZEIT ONLINE","Der US-Technologiekonzern Microsoft plant, in den kommenden zwei Jahren knapp 3,3 Milliarden Euro in künstliche Intelligenz (KI) in Deutschland zu investieren. Das kündigte Microsoft-Präsident Brad Smith in Berlin bei einem Gespräch mit Bundeskanzler Olaf Scholz (SPD) an. Mit dem Geld sollen Rechenzentrumskapazitäten für KI-Anwendungen und beim Cloud-Computing ausgebaut werden. 

Es ist die größte Einzelinvestition in der 40-jährigen Geschichte von Microsoft in Deutschland. Sie umfasst auch ein KI-Weiterbildungsprogramm, mit dem bis zu 1,2 Millionen Menschen erreicht werden sollen.

Geplant ist demnach eine Ansiedlung neuer Kapazitäten in Bedburg, Bergheim und Elsdorf bei Köln. Der Konzern sucht
 damit die räumliche Nähe zu Großkunden seiner Cloud-Dienste, darunter Bayer und RWE. Das dürfte Verzögerungen im Datenaustausch zwischen den Rechenzentren und den Anwendungen niedrig halten. 

Auch Hessen soll von den Investitionen profitieren. Die Rhein-Main-Region ist Deutschlands führender Standort für
 Rechenzentren, da in Frankfurt am Main der große
Internetknoten DE-CIX liegt. Über ihn werden pro Sekunde Unmengen von digitalen Daten ausgetauscht. Ein in der Region bereits bestehendes Cloud-Zentrum von Microsoft soll ausgebaut werden.

NRW-Ministerpräsident Hendrik Wüst (CDU) wertete die Investition als ""ein starkes Signal für Deutschland und ein großartiger Beitrag zum Strukturwandel im Rheinischen Revier"". Mit dieser Milliardenentscheidung trage Microsoft wesentlich dazu bei, die Transformation der Wirtschaft in seinem Bundesland nachhaltig voranzutreiben. ""Dass ein Global Player ein solches Investment in Nordrhein-Westfalen tätigt, ist ein Zeichen des Vertrauens und Ergebnis konkreter Standortpolitik.""

Deutschland führt damit die Liste der angekündigten Investitionen des weltweit führenden Softwarekonzerns an: Microsoft-Präsident Smith hatte im vergangenen November zugesagt, bis 2026 in Großbritannien 2,9 Milliarden Euro zu investieren, um das Wachstum der KI-Anwendungen voranzutreiben. Gut einen Monat zuvor hatte er Australien rund drei Milliarden Euro versprochen.

Microsoft ist ein führender Akteur bei KI, etwa weil es frühzeitig mehrere Milliarden Dollar in die Hand genommen hat, um bei dem kalifornischen KI-Start-up OpenAI einzusteigen. Letzteres hatte im November 2022 seinen KI-Bot ChatGPT vorgestellt. Microsoft hat unterdessen weitere Milliardenbeträge investiert, um große Rechenkapazitäten für das Training der KI aufzubauen. Der Konzern verwendet die KI-Technologie unter anderem in seiner Suchmaschine Bing und in seinen Office-Programmen als sogenannten Copilot. Hauptkonkurrent ist Google mit seinem KI-Programm Gemini.
"
AI,Zeit,2024-02-16,https://www.zeit.de/2024/08/kuenstliche-intelligenz-training-fotos-robert-kneschke/seite-2,"Künstliche Intelligenz: Erst beklaut, dann kopiert | ZEIT Arbeit",
AI,Zeit,2024-02-17,https://www.zeit.de/digital/2024-02/euopaeische-union-aenderungen-digital-service-act,Europäische Union: Was ändert sich durch den Digital Services Act? | ZEIT ONLINE,"
Der Digital Services Act (DSA) reguliert vom heutigen Samstag an die Aktivitäten von Anbietern digitaler Dienste innerhalb der EU. Die neuen Regeln sollen zusammen mit dem Digital Markets Act mehr Schutz für Internetuser innerhalb der Europäischen Union schaffen. Der DSA gilt in allen EU-Mitgliedstaaten, ohne dass es einer weiteren nationalen Umsetzung durch die Mitgliedstaaten bedarf. 

An
 die Regeln im DSA müssen sich alle Unternehmen halten, die digitale 
Dienste in der EU anbieten. Konkret betrifft das Gesetz 
Internetprovider, Hostinganbieter, Cloud-Dienste, soziale Netzwerke, 
Messenger und Onlinemarktplätze. 

Für Onlineplattformen gelten 
dabei mehr Regeln als für Hostinganbieter. Die strengsten Regeln müssen 
""sehr große Plattformen"" einhalten. Das sind Unternehmen, die mehr als 
45 Millionen monatliche Nutzerinnen und Nutzer haben, das heißt, mehr 
als zehn Prozent Menschen in der EU erreichen.  

Die Internetnutzer bekommen durch den DSA konkret festgeschriebene Rechte, um sich gegen Onlineplattformen oder auch gegen andere Nutzer wehren zu können. Onlineplattformen müssen dazu entsprechende Beschwerdeverfahren bereitstellen.  

Die Unternehmen müssen zum Beispiel sexualisierte Gewalt gegen Kinder oder Terrorpropaganda schneller als bisher entfernen. Zudem sollen sie die Verbreitung von Falschinformationen stärker unterbinden und die Algorithmen hinter der Priorisierung von Inhalten und personalisierter Werbung zum Teil offenlegen. 

Außerdem können Inhalte oder Nutzerprofile auf Onlineplattformen nun nicht einfach gelöscht werden. Anbieter müssen künftig offen und nachvollziehbar erklären, warum sie Inhalte oder Accounts gelöscht haben. Nutzer der Onlineplattform können diesen Entscheidungsprozess dann auch mit Rechtsmitteln überprüfen lassen.

""Dieses Gesetz wird Überwachungswerbung, Hass, Hetze und Desinformation eindämmen, es wird die Rechte der Nutzer*innen stärken und Onlineplattformen wie nie zuvor zur Rechenschaft ziehen"", teilte Digitalexpertin und stellvertretende Vorsitzende der Fraktion Greens/EFA im Europarlament, Alexandra Geese, mit. ""Das ist der erste wichtige Schritt, um unsere Demokratie gegen die Geschäftspraktiken von Google, Meta, TikTok und Co. zu verteidigen.""  

Onlinemarktplätze wie Amazon
 oder Ebay sind verpflichtet, gefälschte Produkte oder gefährliches Spielzeug so 
gut wie möglich zu entfernen und die Käuferinnen und Käufer zu warnen. Zudem müssen die Konzerne der EU-Kommission regelmäßig berichten, 
inwiefern ihre Plattformen etwa die psychische Gesundheit oder die Meinungsfreiheit gefährden.


Der DSA enthält auch neue Regeln für Onlinewerbung. Artikel 24 des DSA verbietet zielgerichtete Werbung, für die die Daten von Minderjährigen ausgewertet werden. Bei Erwachsenen darf künftig keine Werbung mehr auf Basis von sensiblen persönlichen Daten ausgespielt werden. Das sind zum Beispiel Gesundheitsdaten und alle Informationen, die auf die sexuelle Orientierung, die politische Meinung oder die religiöse Überzeugung schließen lassen.

Falsche oder irreführende Informationen über politische Zusammenhänge könnten sich auf die Meinung und auch auf das Wahlverhalten von Menschen auswirken. Insbesondere, wenn sie von einer scheinbar neutralen Instanz wie einer Suchmaschine als Wahrheit präsentiert werden.

Eine Untersuchung von AlgorithmWatch und AI Forensics zeigt, dass ein Drittel der Antworten von Microsofts KI-gestützter Suchmaschine Bing Chat auf Fragen zu den Wahlen in Bayern, Hessen und der Schweiz im Oktober 2023 sachliche Fehler enthielten. Unter den Fehlinformationen waren falsche Wahldaten, veraltete Kandidaten oder sogar erfundene Skandale.  

AlgorithmWatch plant nun, auf Basis der neuen Regeln Auskunft über interne Daten von Microsoft zu verlangen. ""Wir können durch den DSA auf Daten zugreifen, die bisher für unsere Forschung unzugänglich waren"", sagte Oliver Marsh von AlgorithmWatch laut einer Mitteilung. ""So können wir vielleicht das tatsächliche Ausmaß der Desinformation durch Bing Chat erkennen und prüfen, ob die Gegenmaßnahmen von Microsoft etwas gebracht haben.""

Die Durchsetzung der Anforderungen aus dem Digital Services Act liegt überwiegend in der Verantwortung der Mitgliedstaaten. Einzig die Überwachung und Durchsetzung des DSA gegenüber sehr großen Onlineplattformen und -Suchmaschinen erfolgt durch die EU-Kommission.

Die EU-Mitgliedstaaten müssen zuständige Stellen für die Rechtsdurchsetzung des DSA benennen. Die Behörden sollen dann selbsttätig gegen Anbieterverstöße vorgehen und entsprechende Strafen verhängen können. In Deutschland ist das voraussichtlich die Bundesnetzagentur (BNetzA). 

Allerdings wird es noch einige Zeit dauern, bis alle Koordinierungsstellen voll einsatzbereit sind – auch in Deutschland. Hier wird die Bundesnetzagentur voraussichtlich erst im April oder Mai rechtskräftig als Digital Services Coordinators ernannt. In Deutschland wird der DSA künftig durch das geplante Digitale-Dienste-Gesetz umgesetzt.

Sollten die Konzerne die Vorgaben nicht einhalten, droht ihnen eine Strafe von bis zu sechs Prozent des weltweiten Jahresumsatzes.
"
AI,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/sora-openai-ki-videos-chatgpt,Sora von OpenAI: Diese neue KI beschwört Videos aus dem Nichts | ZEIT ONLINE,
AI,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-ki-wahl-manipulation-digitalkonzerne,Künstliche Intelligenz: Techkonzerne wollen Wahlmanipulationen durch KI verhindern | ZEIT ONLINE,"Führende Technologiekonzerne haben sich dazu verpflichtet, bei der Bekämpfung von Wahlmanipulation durch künstliche Intelligenz (KI) zusammenzuarbeiten. Damit wollen sie verhindern, dass KI-generierte Inhalte weltweit politische Wahlen beeinträchtigen. Auf der Münchner Sicherheitskonferenz unterzeichneten Vertreter von 20 Unternehmen eine Vereinbarung über Kooperation bei der Erkennung und Bekämpfung manipulierter Inhalte. 

Unter den beteiligten Firmen sind Konzerne wie Amazon, Google, IBM, Meta, Microsoft, TikTok, X und Adobe – sowie OpenAI, das Unternehmen, das den KI-Chatbot ChatGPT entwickelt hat. Vor allem haben die Firmen dabei KI-generierte Audio-, Video- und Bilddateien im Visier, die das Aussehen oder die Stimme von politischen Kandidaten, Wahlhelfern und weiteren Beteiligten demokratischer Wahlen täuschend nachahmen oder verändern. Außerdem richtet sich die Initiative gegen digitale Inhalte, die den Wählern falsche Informationen darüber liefern, wann, wo und wie Wahlen stattfinden. 

Erfahrungen mit manipulativen KI-Inhalten in laufenden Wahlkämpfen machten zuletzt vor allem die USA. Dort hatte ein Telefonroboter bei Vorwahlen im Bundesstaat New Hampshire die Menschen mit der Stimme von Präsident Joe Biden dazu aufgefordert, ihre Stimme nicht abzugeben. Die von einer KI generierte Stimme war kaum von der echten Stimme des Präsidenten zu unterscheiden. Seitdem verbot die US-Kommunikationsaufsicht automatisierte Werbeanrufe mit KI-Stimmen.

Wahlen seien das pulsierende Herz der Demokratien, sagte der Chef der Münchner Sicherheitskonferenz, Christoph Heusgen, angesichts der Vereinbarung. Die Erklärung der Techkonzerne sei ein entscheidender Schritt, um die Integrität von Wahlen zu fördern, die gesellschaftliche Resilienz zu erhöhen und vertrauenswürdige Techpraktiken zu schaffen.  

Die Vereinbarung sei nicht nur ein Schritt zum Schutz von Wahlen, sondern auch zum Schutz von Möglichkeiten, die durch KI entstünden, sagte Kent Walker, Präsident des Bereichs Global Affairs bei der Google-Mutter Alphabet. ""Wir dürfen nicht zulassen, dass ein digitaler Missbrauch die Chancen der künstlichen Intelligenz bedroht, unsere Volkswirtschaften zu verbessern, neue Arbeitsplätze zu schaffen und den Fortschritt in Gesundheit und Wissenschaft voranzutreiben"", sagte er. 

Microsofts Präsident Brad Smith sagte, die Unterzeichner der Vereinbarung stünden in der Verantwortung, dass KI-Werkzeuge nicht zu einer Waffe würden. Im Rahmen des Abkommens wollen die beteiligten Firmen unter anderem ihre KI-Modelle neu bewerten, um Risiken zu verstehen, die sie im Zusammenhang mit betrügerischen Wahlinhalten darstellen könnten. Die Unternehmen wollen zudem dabei helfen, das öffentliche Bewusstsein dafür zu schärfen. Zudem wollen sich die Firmen für eine höhere Medienkompetenz in der Gesellschaft einsetzen.  
"
AI,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-openai-video-produktion-sora,Künstliche Intelligenz: OpenAI entwickelt KI-Anwendung zur Videoproduktion | ZEIT ONLINE,"Das US-Unternehmen OpenAI, Entwickler des Chatbots ChatGPT und des Bildgenerators Dall-e, hat eine KI-Anwendung zur Produktion realistischer Videos entwickelt. Das teilte das Unternehmen mit Sitz in San Francisco mit. Die Anwendung namens Sora könne anhand eines Prompts, ein kurzer Befehl in Textform, Videos mit einer Länge von bis zu einer Minute herstellen. Die Anwendung könne auch aus einem Foto ein Video machen oder ein kurzes Video verlängern.

Auf X kündigte OpenAI-Chef Sam Altman an, das Modell werde zunächst ausgewählten Kreativen zur Verfügung gestellt. Bevor das Programm breit genutzt werden kann, sollen Expertinnen und Experten mögliche Sicherheitsrisiken ausloten. Politiker, Lehrer und Künstler weltweit könnten Bedenken äußern.

Altman hatte jedoch dazu aufgefordert, Vorschläge für Videos zu machen. Sora werde nur Augenblicke später ""überzeugende Ergebnisse"" präsentieren. Als ein Beispiel zeigte OpenAI unter anderem ein Video, in dem ein Wesen ""halb Ente, halb Drachen"" vor einem ""wunderschönen Sonnenuntergang"" fliegt und dabei einen ""Hamster im Abenteuer-Outfit"" auf dem Rücken trägt.

Einige Schwächen habe Sora jedoch noch: Beispielsweise verwechsle die KI rechts und links. Generell könnten manchmal Fehler bei der Umsetzung physikalischer Gesetze und der Kontinuität passieren. So könne es zum Beispiel vorkommen, dass jemand in einem Video von einem Keks abbeißt, dieser danach aber noch ganz aussieht.

Ähnliche Anwendungen bieten bereits Unternehmen wie Meta, Google oder Runway AI an – entweder arbeiten sie daran, oder haben bereits Versionen veröffentlicht. Solche Technologien könnten die Videoproduktion mit der Zeit verändern. Zugleich besteht die Sorge, dass damit in großem Stil Fake-Videos erzeugt werden können, die von echten Aufnahmen kaum zu unterscheiden wären. Die Entwickler der Technologie arbeiten deshalb an Wegen, in die Videos eindeutige Erkennungsmerkmale wie Wasserzeichen einzubauen. Auch bei Sora-Videos solle erkennbar sein, dass sie von KI erzeugt wurden. 
"
AI,Zeit,2024-02-15,https://www.zeit.de/wirtschaft/unternehmen/2024-02/microsoft-ki-kuenstliche-intelligenz-investition,"Künstliche Intelligenz: Microsoft investiert 3,3 Milliarden Euro in KI in Deutschland | ZEIT ONLINE","Der US-Technologiekonzern Microsoft plant, in den kommenden zwei Jahren knapp 3,3 Milliarden Euro in künstliche Intelligenz (KI) in Deutschland zu investieren. Das kündigte Microsoft-Präsident Brad Smith in Berlin bei einem Gespräch mit Bundeskanzler Olaf Scholz (SPD) an. Mit dem Geld sollen Rechenzentrumskapazitäten für KI-Anwendungen und beim Cloud-Computing ausgebaut werden. 

Es ist die größte Einzelinvestition in der 40-jährigen Geschichte von Microsoft in Deutschland. Sie umfasst auch ein KI-Weiterbildungsprogramm, mit dem bis zu 1,2 Millionen Menschen erreicht werden sollen.

Geplant ist demnach eine Ansiedlung neuer Kapazitäten in Bedburg, Bergheim und Elsdorf bei Köln. Der Konzern sucht
 damit die räumliche Nähe zu Großkunden seiner Cloud-Dienste, darunter Bayer und RWE. Das dürfte Verzögerungen im Datenaustausch zwischen den Rechenzentren und den Anwendungen niedrig halten. 

Auch Hessen soll von den Investitionen profitieren. Die Rhein-Main-Region ist Deutschlands führender Standort für
 Rechenzentren, da in Frankfurt am Main der große
Internetknoten DE-CIX liegt. Über ihn werden pro Sekunde Unmengen von digitalen Daten ausgetauscht. Ein in der Region bereits bestehendes Cloud-Zentrum von Microsoft soll ausgebaut werden.

NRW-Ministerpräsident Hendrik Wüst (CDU) wertete die Investition als ""ein starkes Signal für Deutschland und ein großartiger Beitrag zum Strukturwandel im Rheinischen Revier"". Mit dieser Milliardenentscheidung trage Microsoft wesentlich dazu bei, die Transformation der Wirtschaft in seinem Bundesland nachhaltig voranzutreiben. ""Dass ein Global Player ein solches Investment in Nordrhein-Westfalen tätigt, ist ein Zeichen des Vertrauens und Ergebnis konkreter Standortpolitik.""

Deutschland führt damit die Liste der angekündigten Investitionen des weltweit führenden Softwarekonzerns an: Microsoft-Präsident Smith hatte im vergangenen November zugesagt, bis 2026 in Großbritannien 2,9 Milliarden Euro zu investieren, um das Wachstum der KI-Anwendungen voranzutreiben. Gut einen Monat zuvor hatte er Australien rund drei Milliarden Euro versprochen.

Microsoft ist ein führender Akteur bei KI, etwa weil es frühzeitig mehrere Milliarden Dollar in die Hand genommen hat, um bei dem kalifornischen KI-Start-up OpenAI einzusteigen. Letzteres hatte im November 2022 seinen KI-Bot ChatGPT vorgestellt. Microsoft hat unterdessen weitere Milliardenbeträge investiert, um große Rechenkapazitäten für das Training der KI aufzubauen. Der Konzern verwendet die KI-Technologie unter anderem in seiner Suchmaschine Bing und in seinen Office-Programmen als sogenannten Copilot. Hauptkonkurrent ist Google mit seinem KI-Programm Gemini.
"
Artificial Intelligence,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-openai-video-produktion-sora,Künstliche Intelligenz: OpenAI entwickelt KI-Anwendung zur Videoproduktion | ZEIT ONLINE,"Das US-Unternehmen OpenAI, Entwickler des Chatbots ChatGPT und des Bildgenerators Dall-e, hat eine KI-Anwendung zur Produktion realistischer Videos entwickelt. Das teilte das Unternehmen mit Sitz in San Francisco mit. Die Anwendung namens Sora könne anhand eines Prompts, ein kurzer Befehl in Textform, Videos mit einer Länge von bis zu einer Minute herstellen. Die Anwendung könne auch aus einem Foto ein Video machen oder ein kurzes Video verlängern.

Auf X kündigte OpenAI-Chef Sam Altman an, das Modell werde zunächst ausgewählten Kreativen zur Verfügung gestellt. Bevor das Programm breit genutzt werden kann, sollen Expertinnen und Experten mögliche Sicherheitsrisiken ausloten. Politiker, Lehrer und Künstler weltweit könnten Bedenken äußern.

Altman hatte jedoch dazu aufgefordert, Vorschläge für Videos zu machen. Sora werde nur Augenblicke später ""überzeugende Ergebnisse"" präsentieren. Als ein Beispiel zeigte OpenAI unter anderem ein Video, in dem ein Wesen ""halb Ente, halb Drachen"" vor einem ""wunderschönen Sonnenuntergang"" fliegt und dabei einen ""Hamster im Abenteuer-Outfit"" auf dem Rücken trägt.

Einige Schwächen habe Sora jedoch noch: Beispielsweise verwechsle die KI rechts und links. Generell könnten manchmal Fehler bei der Umsetzung physikalischer Gesetze und der Kontinuität passieren. So könne es zum Beispiel vorkommen, dass jemand in einem Video von einem Keks abbeißt, dieser danach aber noch ganz aussieht.

Ähnliche Anwendungen bieten bereits Unternehmen wie Meta, Google oder Runway AI an – entweder arbeiten sie daran, oder haben bereits Versionen veröffentlicht. Solche Technologien könnten die Videoproduktion mit der Zeit verändern. Zugleich besteht die Sorge, dass damit in großem Stil Fake-Videos erzeugt werden können, die von echten Aufnahmen kaum zu unterscheiden wären. Die Entwickler der Technologie arbeiten deshalb an Wegen, in die Videos eindeutige Erkennungsmerkmale wie Wasserzeichen einzubauen. Auch bei Sora-Videos solle erkennbar sein, dass sie von KI erzeugt wurden. 
"
KI,Zeit,2024-02-18,https://www.zeit.de/politik/2024-02/manipulation-ki-tech-konzerne-wahlen-nachrichtenpodcast,Manipulation durch KI: Ein Abkommen gegen Wahlmanipulation | ZEIT ONLINE,"Künstliche Intelligenz als Waffe im Superwahljahr – Techkonzerne wie Amazon, Google, Meta, Microsoft und OpenAI wollen das verhindern. Auf der Münchner Sicherheitskonferenz unterzeichneten 20 der führenden Digitalunternehmen am Freitag eine Vereinbarung zur Zusammenarbeit, um durch KI manipulierte Audio-, Video- und Bilddateien künftig besser erkennen und bekämpfen zu können. Die Gefahr, die künstliche Intelligenz für Wahlen birgt, wurde zuletzt in den USA deutlich: Im laufenden Präsidentschaftswahlkampf haben Wähler der Demokraten Anrufe erhalten, in denen die Stimme von Präsident Biden täuschend echt gefälscht wurde. Von dem Telefonroboter wurden sie dazu aufgefordert, nicht an den Vorwahlen teilzunehmen. Wie die beteiligten Unternehmen dagegen vorgehen wollen, dass ihre Anwendungen in Wahlkämpfen missbraucht werden, beantwortet Pauline Schinkels, Redakteurin im Digitalressort von ZEIT ONLINE.

Politikerinnen und Politiker der Grünen sehen sich gerade massiven Protesten ausgeliefert. Diese Woche mussten die Grünen ihre Veranstaltung zum politischen Aschermittwoch in Biberach absagen – zu groß waren die Sicherheitsbedenken angesichts der gewalttätigen Ausschreitungen bei einer Demonstration vor Ort: Eine Scheibe an Cem Özdemirs Begleitwagen wurde eingeschlagen, Polizisten sollen verletzt worden sein. In Schorndorf wurde außerdem Grünenchefin Ricarda Lang auf dem Weg zu ihrem Auto von einer wütenden Menge verfolgt. Vorfälle wie diese häuften sich zuletzt. Auch Vizekanzler Robert Habeck war im Januar durch eine Blockadeaktion am Verlassen einer Fähre gehindert worden. Wie die Partei damit umgeht, dass sich aktuell so viel Wut gegen sie richtet, analysiert Ferdinand Otto aus dem Ressort Politik, Wirtschaft, Gesellschaft von ZEIT ONLINE.

Alles außer Putzen: Kleine Wohnungen zelebrieren mit den Videos von Never too small.


Moderation und Produktion: 
Hannah Grünewald


Mitarbeit: 
Susanne Hehr und
Henrike Hartmann


Alle Folgen unseres Podcasts finden Sie
hier
. Fragen, Kritik, Anregungen? Sie erreichen uns unter 
wasjetzt@zeit.de
.
"
KI,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/sora-openai-ki-videos-chatgpt,Sora von OpenAI: Diese neue KI beschwört Videos aus dem Nichts | ZEIT ONLINE,
KI,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/ki-wahlkampf-joe-biden-kennzeichnung/seite-2,KI im Wahlkampf: Kennzeichnungen lassen sich auch schnell wieder entfernen | ZEIT ONLINE,"Das Problem: Für die Wasserzeichen gilt, genauso leicht, wie sie integriert werden können,
lassen sie sich auch wieder entfernen. Wer etwa einen Bildgenerator
nicht in einer Cloud, sondern stationär auf seinem Rechner laufen hat, kann
entsprechende Codeschnipsel, die das Bild beim Generieren markiert, entfernen.
Wird ein Bild bearbeitet, weil es zugeschnitten oder skaliert wird, die Daten
also verloren gehen, könnten die Zeichen ebenfalls verloren gehen. Expertinnen
sprechen davon, dass Wasserzeichen ""robust"" sein müssen, gleichzeitig dürfen
sie aber nicht zu offensichtlich sein. Ein heikles Unterfangen, für das es bis
dato keine Lösung zu geben scheint: Im vergangenen Jahr hatte ein Team der
Universität Maryland Wasserzeichen getestet – und fand kein einziges zuverlässiges. ""Wir haben sie alle entfernen können"", sagte Soheil Feizi, Professor für
Informatik, dem
US-Magazin Wired.

Googles Tochterfirma DeepMind hat kürzlich ein neues Wasserzeichen
auf den Markt gebracht, mit dem sich auch Musik- und Audiodateien kennzeichnen
lassen sollen. ""SynthID"" soll
laut eigenen Angaben auch dann noch erkennbar sein, wenn die Audiodatei
beschleunigt beziehungsweise verlangsamt oder das Bild zugeschnitten wurde.
Derzeit läuft ein Feldversuch mit dem Google-eigenen Text-zu-Bild-Generator
Imagen. Überprüfen lassen sich die Angaben bisher nicht, Google will das
Werkzeug zunächst nur einer begrenzten Anzahl an Kunden zur Verfügung stellen
und räumt dabei gleichzeitig ein, dass das neue Werkzeug ""kein Allheilmittel""
sei. 

Auch OpenAI und Meta weisen in ihren Blogposts darauf hin,
dass die Technologie es noch nicht ermögliche, alle Medieninhalte, die
künstlich generiert wurden, zu identifizieren und sich die eingefügten Metadaten schnell
wieder entfernen ließen – wer etwa einfach einen Screenshot erstellt, übernimmt
schließlich auch nicht die Metadaten des Originalbilds. 

Dabei war es schon
vor den vielen neuen KI-Bildgeneratoren nicht leicht, gefälschte Beiträge
als solche zu identifizieren. Es nimmt oft Zeit in Anspruch, ein Bild oder Video
auf Unstimmigkeiten zu untersuchen. Ob das nun im Fall einer Satire wie beim Varoufake
ist – oder bei
Bildern in einem Krieg. Umgekehrt ist auch ein echtes Video nicht immer
sofort als echt zu erkennen – weil die Inhalte darin zu unglaublich scheinen
oder die Motive der Veröffentlichung unklar sind. Als das Video zur
Ibiza-Affäre auftauchte, machten sich Wissenschaftler daran, es zu verifizieren.
In dem Video zeigten sich Heinz-Christian Strache, bis dahin österreichischer Vizekanzler,
und Johann Gudenus von der FPÖ wenige Monate vor der österreichischen
Nationalratswahl offen für Korruption. Es stellte sich als echtes Video heraus,
die Politiker traten zurück. 

Das Foto eines verbrannten Babyleichnams, das
bei einem Angriff der Hamas auf die israelische Zivilbevölkerung ums Leben
gekommen sein soll, wurde hingegen fälschlicherweise von einer Detektionssoftware
zunächst als künstlich generiert eingestuft. Das sei ""eine zweite Ebene der
Desinformation"", wie Hany Farid, Experte für digitale Bildmanipulation, gegenüber
dem US-Magazin 404 Media sagte.

Für die Detektion gilt: Je präziser die Kriterien sind,
desto einfacher lassen sich Inhalte als KI-generiert
einstufen – und desto leichter lassen sie sich umgehen, wenn man
nur einige Anpassungen vornimmt. Weniger Kriterien bedeuten gleichzeitig aber,
dass ein Inhalt möglicherweise als künstlich generiert eingestuft wird, obwohl
er das eigentlich nicht ist. Wissenschaftler wie Tibor Jager, Professor für
IT-Sicherheit und Kryptografie an der Universität Wuppertal, sprechen in solchen
Fällen von ""false positives"".

Noch fallen KI-Inhalte mitunter auf, wenn man genau hinguckt.
Das Gebiss ist irgendwie komisch, die Hand hat einen Finger zu viel, ein
Unterarm fehlt. Auch wer in Bildforensik nicht geschult ist, kann solche
Unstimmigkeiten erkennen. Doch wer schaut schon immer genau hin? Häufig
verbreiten sich Inhalte schnell, die Verifikation hingegen kann Stunden,
manchmal Tage oder Wochen dauern. Und die Masse an Inhalten, die durch neue
KI-Tools wie DALL-E 3 oder Midjourney möglich sind, macht es nicht einfacher. 

Doch: Die meisten Bilder oder Videos sind nicht komplett
künstlich generiert, auch weil der Aufwand dafür hoch ist, stattdessen wird
beispielsweise per Texteingabe das Gesicht in einem Originalbild ausgetauscht. Diese
Stellen sehen in der Bildanalyse verändert aus, sie rauschen anders – das kann
ein Indiz dafür sein, dass manipuliert wurde, sagt Martin Steinebach, der die
Abteilung Media
Security und IT Forensik am Fraunhofer-Institut für Sichere
Informationstechnologie SIT/ATHENE leitet. Allerdings kann die Abbildung auch
als Vorlage für ein komplett neues synthetisches Bild genutzt werden, das dann
keine veränderten Signaleigenschaften mehr aufweist. ""Die Forensik kann nur
Hinweise liefern"", sagt Steinebach. Das dauert. Gleichzeitig ließen sich solche
zusammengebastelten Fakes mit etwas technischem Grundwissen innerhalb eines
Tages erstellen, schätzt er, und der technische Fortschritt ist rasant. Künftig
wird es vermutlich noch schwerer werden, falsche Inhalte zu erkennen.  

Wissenschaftler wie Steinebach, der unter anderem das
Ibiza-Video verifiziert hat, diskutieren deshalb noch über eine ganz andere
Lösung, und zwar nicht die synthetischen, sondern die echten Inhalte zu
kennzeichnen, etwa in Form von Positivlisten, auf denen sich die Bilder der
Fotografen von Nachrichtenagenturen wiederfinden würden. Dabei stelle sich aber
die Frage, wer das Privileg bekommen soll, solche Inhalte entsprechend kennzeichnen
und auslesen zu können. Außerdem dürfe eine Kennzeichnung nie Informationen
beinhalten, die Urheber in Gefahr bringen könnten, etwa Menschenrechtsaktivisten,
die Missstände auf Fotos festhalten. 

Die Identifikation von KI-generierten Inhalten ist längst
auch in der Politik ein Thema, unter anderem ist es ein
wichtiges Ziel der US-Regierung. Bereits im Sommer vergangenen Jahres
hatten sich US-Techunternehmen verpflichtet, KI-generierte
Inhalte zu kennzeichnen. Microsoft hat angekündigt,
sogenannte Content-Credentials anzubieten, mit denen sich Inhalte kennzeichnen
lassen sollen. Das soll helfen, die Genese oder Historie von etwa Videos oder
Fotos nachzuvollziehen, also auch, ob das Bild künstlich erzeugt und welcher
Generator dafür genutzt wurde, ähnlich wie sich Informationen, etwa den Namen
der Fotografin, Ort und Datum, schon jetzt in den Metadaten eines Fotos
hinterlegen lassen. Zuvor hatte bereits Adobe damit begonnen, sichtbare
Wasserzeichen in den Ecken der Bilder zu platzieren, die ihre KI-Systeme
generieren. Adobe und Microsoft gehören beide zur Initiative CAI (Content
Authenticity Initiative), die sich mit der Coalition
for Content Provenance and Authenticity (C2PA) zur Aufgabe gemacht haben,
technische Standards für die Zertifizierung von Quellen und der Herkunft von
Medieninhalten zu entwickeln – auf
diesen offenen C2PA-Standard beziehen sich auch Meta und OpenAI.

Ähnliche
Bestrebungen gibt es in der EU, unter anderem sieht das
Digitale-Dienste-Gesetz (DSA) vor, dass große Onlineplattformen gegen Desinformation vorgehen. Das soll ebenso für den AI Act gelten, demnach KI-generierte
Inhalte gekennzeichnet werden müssen, wenn
das technisch und finanziell möglich ist. 

Ein Problem jedoch bleibt dabei: ""Wir können nur regulieren,
wenn klar ist, wer der Verursacher ist"", sagt Jutta Jahnel, die am Karlsruher
Institut für Technologie (KIT) zu den Folgen künstlicher Intelligenz forscht.
Klar ist: Das wird bei Betrug oder Desinformationskampagnen nicht der Fall
sein. Natürlich registriere sie die zunehmende Verunsicherung. ""Aber wenn es
keinen Verantwortlichen gibt, dann können wir das auch nicht mit Regulierung
erschlagen."" 

Auch die Urheber des gefälschten Robocalls mit der Stimme
von US-Präsident Joe Biden hatten falsche Angaben über dessen Herkunft gemacht,
sodass der Anruf von einem politischen Komitee von Bidens Demokratischer Partei
zu kommen schien. Inzwischen deuten erste Ermittlungsergebnisse auf eine Firma
in Texas hin, das gab der
Generalstaatsanwalt von New Hampshire bekannt. Die US-Kommunikationsbehörde
FCC hatte ebenfalls reagiert und einen Vorschlag vorgelegt, automatisierte
Anrufe mit KI-generierten Stimmen zu verbieten. Inzwischen ist klar: Künftig
müssen die Empfänger solcher Telefonate aktiv vorher zu stimmen.

Offen bleibt bei alldem, ob eine Kennzeichnung von Inhalten,
sei es nun von echten oder künstlichen, wirklich dazu beiträgt, Vertrauen zu
gewinnen – oder
möglicherweise den gegenteiligen Effekt hat und die gesellschaftliche
Skepsis noch verstärken würde. ""Man verliert das Vertrauen"", sagt Jahnel. Wenn
Regulierung und Kennzeichnung nicht helfen, sagt sie, dann gibt es womöglich
nur einen Ausweg: Kritischer mit Inhalten im Netz umzugehen.
"
KI,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-ki-wahl-manipulation-digitalkonzerne,Künstliche Intelligenz: Techkonzerne wollen Wahlmanipulationen durch KI verhindern | ZEIT ONLINE,"Führende Technologiekonzerne haben sich dazu verpflichtet, bei der Bekämpfung von Wahlmanipulation durch künstliche Intelligenz (KI) zusammenzuarbeiten. Damit wollen sie verhindern, dass KI-generierte Inhalte weltweit politische Wahlen beeinträchtigen. Auf der Münchner Sicherheitskonferenz unterzeichneten Vertreter von 20 Unternehmen eine Vereinbarung über Kooperation bei der Erkennung und Bekämpfung manipulierter Inhalte. 

Unter den beteiligten Firmen sind Konzerne wie Amazon, Google, IBM, Meta, Microsoft, TikTok, X und Adobe – sowie OpenAI, das Unternehmen, das den KI-Chatbot ChatGPT entwickelt hat. Vor allem haben die Firmen dabei KI-generierte Audio-, Video- und Bilddateien im Visier, die das Aussehen oder die Stimme von politischen Kandidaten, Wahlhelfern und weiteren Beteiligten demokratischer Wahlen täuschend nachahmen oder verändern. Außerdem richtet sich die Initiative gegen digitale Inhalte, die den Wählern falsche Informationen darüber liefern, wann, wo und wie Wahlen stattfinden. 

Erfahrungen mit manipulativen KI-Inhalten in laufenden Wahlkämpfen machten zuletzt vor allem die USA. Dort hatte ein Telefonroboter bei Vorwahlen im Bundesstaat New Hampshire die Menschen mit der Stimme von Präsident Joe Biden dazu aufgefordert, ihre Stimme nicht abzugeben. Die von einer KI generierte Stimme war kaum von der echten Stimme des Präsidenten zu unterscheiden. Seitdem verbot die US-Kommunikationsaufsicht automatisierte Werbeanrufe mit KI-Stimmen.

Wahlen seien das pulsierende Herz der Demokratien, sagte der Chef der Münchner Sicherheitskonferenz, Christoph Heusgen, angesichts der Vereinbarung. Die Erklärung der Techkonzerne sei ein entscheidender Schritt, um die Integrität von Wahlen zu fördern, die gesellschaftliche Resilienz zu erhöhen und vertrauenswürdige Techpraktiken zu schaffen.  

Die Vereinbarung sei nicht nur ein Schritt zum Schutz von Wahlen, sondern auch zum Schutz von Möglichkeiten, die durch KI entstünden, sagte Kent Walker, Präsident des Bereichs Global Affairs bei der Google-Mutter Alphabet. ""Wir dürfen nicht zulassen, dass ein digitaler Missbrauch die Chancen der künstlichen Intelligenz bedroht, unsere Volkswirtschaften zu verbessern, neue Arbeitsplätze zu schaffen und den Fortschritt in Gesundheit und Wissenschaft voranzutreiben"", sagte er. 

Microsofts Präsident Brad Smith sagte, die Unterzeichner der Vereinbarung stünden in der Verantwortung, dass KI-Werkzeuge nicht zu einer Waffe würden. Im Rahmen des Abkommens wollen die beteiligten Firmen unter anderem ihre KI-Modelle neu bewerten, um Risiken zu verstehen, die sie im Zusammenhang mit betrügerischen Wahlinhalten darstellen könnten. Die Unternehmen wollen zudem dabei helfen, das öffentliche Bewusstsein dafür zu schärfen. Zudem wollen sich die Firmen für eine höhere Medienkompetenz in der Gesellschaft einsetzen.  
"
KI,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/kuenstliche-intelligenz-openai-video-produktion-sora,Künstliche Intelligenz: OpenAI entwickelt KI-Anwendung zur Videoproduktion | ZEIT ONLINE,"Das US-Unternehmen OpenAI, Entwickler des Chatbots ChatGPT und des Bildgenerators Dall-e, hat eine KI-Anwendung zur Produktion realistischer Videos entwickelt. Das teilte das Unternehmen mit Sitz in San Francisco mit. Die Anwendung namens Sora könne anhand eines Prompts, ein kurzer Befehl in Textform, Videos mit einer Länge von bis zu einer Minute herstellen. Die Anwendung könne auch aus einem Foto ein Video machen oder ein kurzes Video verlängern.

Auf X kündigte OpenAI-Chef Sam Altman an, das Modell werde zunächst ausgewählten Kreativen zur Verfügung gestellt. Bevor das Programm breit genutzt werden kann, sollen Expertinnen und Experten mögliche Sicherheitsrisiken ausloten. Politiker, Lehrer und Künstler weltweit könnten Bedenken äußern.

Altman hatte jedoch dazu aufgefordert, Vorschläge für Videos zu machen. Sora werde nur Augenblicke später ""überzeugende Ergebnisse"" präsentieren. Als ein Beispiel zeigte OpenAI unter anderem ein Video, in dem ein Wesen ""halb Ente, halb Drachen"" vor einem ""wunderschönen Sonnenuntergang"" fliegt und dabei einen ""Hamster im Abenteuer-Outfit"" auf dem Rücken trägt.

Einige Schwächen habe Sora jedoch noch: Beispielsweise verwechsle die KI rechts und links. Generell könnten manchmal Fehler bei der Umsetzung physikalischer Gesetze und der Kontinuität passieren. So könne es zum Beispiel vorkommen, dass jemand in einem Video von einem Keks abbeißt, dieser danach aber noch ganz aussieht.

Ähnliche Anwendungen bieten bereits Unternehmen wie Meta, Google oder Runway AI an – entweder arbeiten sie daran, oder haben bereits Versionen veröffentlicht. Solche Technologien könnten die Videoproduktion mit der Zeit verändern. Zugleich besteht die Sorge, dass damit in großem Stil Fake-Videos erzeugt werden können, die von echten Aufnahmen kaum zu unterscheiden wären. Die Entwickler der Technologie arbeiten deshalb an Wegen, in die Videos eindeutige Erkennungsmerkmale wie Wasserzeichen einzubauen. Auch bei Sora-Videos solle erkennbar sein, dass sie von KI erzeugt wurden. 
"
KI,Zeit,2024-02-16,https://www.zeit.de/digital/2024-02/ki-wahlkampf-joe-biden-kennzeichnung,"KI im Wahlkampf: Mr. President, sind Sie es wirklich? | ZEIT ONLINE","""Was für ein Blödsinn!"" So begann ein Anruf, den einige
Wählerinnen im US-Staat New Hampshire Ende
Januar erhielten. Am Telefon: die Stimme des US-Präsidenten Joe Biden.
Darin forderte er die Menschen vermeintlich auf, nicht zur Vorwahl in dem
Bundesstaat zu gehen und ihre Stimme für November 2024 aufzuheben. Dann, wenn
in den USA ein neuer Präsident gewählt wird. Der automatisierte Anruf war
fingiert, die Stimme wohl künstlich erzeugt. 

Ob Audiodateien, Bilder oder Videos: Noch nie war es so
einfach wie heute, mithilfe von künstlicher Intelligenz irreführende Inhalte zu
erstellen – und so schwierig, sie schnell als solche zu identifizieren. Im
Wahlkampf können gefälschte Inhalte zu Waffen werden, um den Gegner zu
diskreditieren und Wählerinnen zu verunsichern. Nun wollen die großen
Techfirmen auf der Münchner Sicherheitskonferenz eine Richtlinie gegen den
Einsatz von künstlicher Intelligenz zur Unterwanderung demokratischer Wahlen vorstellen.
Adobe, Google, Meta, Microsoft, OpenAI und TikTok wollen demnach Werkzeuge zu
entwickeln, mit denen sich KI-generierte Bilder, Videos und Audios
identifizieren lassen. 

Die Ankündigung kommt in einem kritischen Jahr: 2024 können rund
3,6 Milliarden Menschen in
mehr als 60 Ländern wählen gehen, darunter in den USA und in der EU. Das
World Economic Forum (WEF) rankt die Beeinflussung von Wahlen durch mithilfe
von KI generierter Mis- und Desinformation in seinem Risikobericht
für die kommenden zwei Jahre inzwischen auf dem ersten Platz, noch vor
Extremwetter und sozialer Polarisierung. 

Die Angst vor Einflussnahme ist groß – und nicht
unbegründet: Zwei Tage vor der Parlamentswahl 2023 in der Slowakei
war eine Audioaufnahme auf Facebook verbreitet worden, in der der
Kandidat der liberalen Partei mit einer Journalistin angeblich darüber sprach,
die Wahl zu manipulieren – ein
Fake. Am Ende gewann die oppositionelle,
prorussische Partei knapp. Ein Zusammenhang lässt sich nicht sicher
belegen. Die Frage aber, ob das gefälschte Audio eine Auswirkung hatte, ist da.

Das wirft eine weitere Frage auf: Wie kann man KI generierte
Inhalte so kennzeichnen, dass sie für jede Person sofort als solche erkennbar
sind und auf diese Weise zumindest die Wirkung von Desinformation eindämmen? 

Technologisch ist das keine neue Sache. So wie man schon
seit Langem Geldscheine mit Sicherheitsmerkmalen wie Wasserzeichen versieht
oder Stockfotos mit dem Namen des Anbieters, so lassen sich auch mithilfe von
künstlicher Intelligenz erstellte Inhalte mit Wasserzeichen kennzeichnen. Das
kann sowohl sichtbar passieren, etwa durch ein Logo in der Bildecke, als auch,
für das menschliche Auge nicht mehr erkennbar, in den Bits der Pixelstruktur. Denkbar
wäre es auch, Sprachmodelle wie DALL-E von OpenAI oder Midjourney direkt mit Bildern
zu trainieren, die bereits mit einem Wasserzeichen markiert wurden. Die Idee
dahinter: Wenn die Trainingsdaten das Merkmal aufweisen, dann auch das neue
Ergebnis. 

Es gibt aber noch eine Reihe anderer Verfahren, etwa kann nach
dem Erstellen eines künstlichen Bildes ein entsprechender Datensatz generiert
werden, der Auskunft über dessen Genese gibt, beispielsweise ob für das Bild
künstlich Intelligenz verwendet wurde. Anschließend werden die Daten codiert
und eingefügt.  

Erst vor einigen Wochen hatte OpenAI angekündigt,
entsprechende Informationen in den
Metadaten der Bilder einzubetten, die mit der Schnittstelle (API) von DALL-E
3 oder Chat GPT erstellt wurden. Um das gegenzuchecken, können Nutzerinnen und
Nutzer über die Seite Content
Credentials Verify prüfen, ob ein Bild etwa mit DALL-E 3 generiert wurde.
Schon im Januar hatte das US-Unternehmen seine Bemühungen im Wahljahr 2024 in
einem Blogpost
aufgezeichnet. Auch
bei TikTok gibt es die Option, Inhalte als ""KI-generiert"" zu markieren,
wenn sie realistische Szenen enthalten und ""vollständig"" oder ""erheblich"" durch
KI bearbeitet wurden. Meta, das Unternehmen hinter Facebook, Instagram und
Threads, hatte
Anfang Februar angekündigt, KI-Inhalte in den kommenden Monaten vor den
Wahlen zu kennzeichnen. Der US-Konzern macht das laut eigenen Angaben bereits mit sichtbaren wie unsichtbaren Wasserzeichen sowie Metadaten, allerdings bisher nur
mit Bildern, die mit dem eigenen KI-Tool erstellt wurden. Künftig will Meta
auch die Kennzeichnung anderer Anbieter übernehmen. Das beinhaltet aber weder
Audio- noch Videodateien, schreibt das Unternehmen. Sind diese mithilfe von KI
erstellt worden, müssen die Nutzer selbst deren Herkunft offenlegen,
andernfalls drohen Strafen. 

Das ist in mehrerlei Hinsicht bemerkenswert, erstens, weil solche
Metadaten in der Regel gelöscht werden, sobald ein Bild bei einer
Social-Media-Plattform hochgeladen wird. Zweitens gilt bis dato eigentlich,
dass Nutzerinnen und Nutzer bei jedem Anbieter einzeln nachprüfen müssten, ob
das Bild mit dessen KI-Tool erstellt wurde, weil eine einheitliche und
übergreifende Lösung noch fehlte. 
"
KI,Zeit,2024-02-15,https://www.zeit.de/wirtschaft/unternehmen/2024-02/microsoft-ki-kuenstliche-intelligenz-investition,"Künstliche Intelligenz: Microsoft investiert 3,3 Milliarden Euro in KI in Deutschland | ZEIT ONLINE","Der US-Technologiekonzern Microsoft plant, in den kommenden zwei Jahren knapp 3,3 Milliarden Euro in künstliche Intelligenz (KI) in Deutschland zu investieren. Das kündigte Microsoft-Präsident Brad Smith in Berlin bei einem Gespräch mit Bundeskanzler Olaf Scholz (SPD) an. Mit dem Geld sollen Rechenzentrumskapazitäten für KI-Anwendungen und beim Cloud-Computing ausgebaut werden. 

Es ist die größte Einzelinvestition in der 40-jährigen Geschichte von Microsoft in Deutschland. Sie umfasst auch ein KI-Weiterbildungsprogramm, mit dem bis zu 1,2 Millionen Menschen erreicht werden sollen.

Geplant ist demnach eine Ansiedlung neuer Kapazitäten in Bedburg, Bergheim und Elsdorf bei Köln. Der Konzern sucht
 damit die räumliche Nähe zu Großkunden seiner Cloud-Dienste, darunter Bayer und RWE. Das dürfte Verzögerungen im Datenaustausch zwischen den Rechenzentren und den Anwendungen niedrig halten. 

Auch Hessen soll von den Investitionen profitieren. Die Rhein-Main-Region ist Deutschlands führender Standort für
 Rechenzentren, da in Frankfurt am Main der große
Internetknoten DE-CIX liegt. Über ihn werden pro Sekunde Unmengen von digitalen Daten ausgetauscht. Ein in der Region bereits bestehendes Cloud-Zentrum von Microsoft soll ausgebaut werden.

NRW-Ministerpräsident Hendrik Wüst (CDU) wertete die Investition als ""ein starkes Signal für Deutschland und ein großartiger Beitrag zum Strukturwandel im Rheinischen Revier"". Mit dieser Milliardenentscheidung trage Microsoft wesentlich dazu bei, die Transformation der Wirtschaft in seinem Bundesland nachhaltig voranzutreiben. ""Dass ein Global Player ein solches Investment in Nordrhein-Westfalen tätigt, ist ein Zeichen des Vertrauens und Ergebnis konkreter Standortpolitik.""

Deutschland führt damit die Liste der angekündigten Investitionen des weltweit führenden Softwarekonzerns an: Microsoft-Präsident Smith hatte im vergangenen November zugesagt, bis 2026 in Großbritannien 2,9 Milliarden Euro zu investieren, um das Wachstum der KI-Anwendungen voranzutreiben. Gut einen Monat zuvor hatte er Australien rund drei Milliarden Euro versprochen.

Microsoft ist ein führender Akteur bei KI, etwa weil es frühzeitig mehrere Milliarden Dollar in die Hand genommen hat, um bei dem kalifornischen KI-Start-up OpenAI einzusteigen. Letzteres hatte im November 2022 seinen KI-Bot ChatGPT vorgestellt. Microsoft hat unterdessen weitere Milliardenbeträge investiert, um große Rechenkapazitäten für das Training der KI aufzubauen. Der Konzern verwendet die KI-Technologie unter anderem in seiner Suchmaschine Bing und in seinen Office-Programmen als sogenannten Copilot. Hauptkonkurrent ist Google mit seinem KI-Programm Gemini.
"
Künstliche Intelligenz,Zeit,2024-02-14,https://www.zeit.de/news/2024-02/13/chatbot-chatgpt-bekommt-ein-gedaechtnis,Künstliche Intelligenz: Chatbot ChatGPT kann sich bald Wissen über Nutzer merken | ZEIT ONLINE,"Der Chatbot ChatGPT wird sich künftig Informationen über seine Nutzer merken können. Damit würde sich Software zum Beispiel daran erinnern, dass man eine Tochter habe, die Quallen mag, erläuterte die Entwicklerfirma OpenAI. 

Bittet man ChatGPT dann, eine Geburtstagskarte für das Kind zu entwerfen, könnte eine Qualle mit Partyhut auf dem Bild sein. Bis alle von der neuen Fähigkeit profitieren können, dürfte es allerdings dauern: Die Funktion wird zunächst im kleinen Kreis getestet. 

Damit ChatGPT sich künftig ganz sicher Informationen über Nutzer merkt, können sie den Chatbot darum bitten. Zugleich kann die Software selbst versuchen, Wissen über die Nutzer aus Unterhaltungen mit ihr herauszupicken. «Das Gedächtnis von ChatGPT wird besser, je mehr man es nutzt», betonte OpenAI.

Die Funktion kann zugleich neue Ängste auslösen, dass Software zu viel über ihre Nutzer wisse. OpenAI will Vorsichtsmaßnahmen ergreifen. So merkt sich ChatGPT empfindliche Informationen etwa mit Bezug zur Gesundheit nicht automatisch - sondern nur auf Bitten der Nutzer. 

Auch kann man abfragen, was die Software über einen weiß - und alle oder einzelne Angaben löschen. Die Gedächtnis-Funktion soll den Chatbot nützlicher machen. Für Unterhaltungen ohne Personalisierung gibt es temporäre Chats. Die Informationen daraus werden auch nicht zum weiteren Anlernen der Software verwendet. Die Gedächtnis-Funktion kann zudem komplett ausgeschaltet werden.

Nutzen für die Erinnerungs-Fähigkeit sieht OpenAI auch beim Einsatz in Unternehmen. So könne sich die Software merken, in welchem Format man am liebsten Zusammenfassungen von Treffen auf der Arbeit bekomme. Oder sie könne sich merken, in welchem Stil man seine Texte schreibt und diesen bei Formulierungsvorschlägen anwenden.

ChatGPT ist der KI-Chatbot, der vor einem Jahr den Hype um Künstliche Intelligenz mit Erwartungen von einem digitalen Schlaraffenland für alle bis hin zur Angst vor einem Auslöschen der Menschheit auslöste. 

KI-Chatbots wie ChatGPT werden mit gewaltigen Mengen an Informationen angelernt und können Texte auf dem sprachlichen Niveau eines Menschen formulieren. Das Prinzip dahinter ist, dass sie Wort für Wort abschätzen, wie ein Satz weitergehen sollte. Ein Nachteil davon ist, dass die Software manchmal auch völlig falsche Antworten ausgeben kann, selbst wenn sie nur korrekte Informationen als Basis hatte.

© dpa-infocom, dpa:240213-99-977618/5
"
AI,Zeit,2024-02-12,https://www.zeit.de/2024/07/schueleraustausch-pandemie-demokratie-bildung,Schüleraustausch: Austausch für alle! | ZEIT Arbeit,"Knut Möller ist Referent  für politische Verbindungsarbeit beim Arbeitskreis gemeinnütziger Jugendaustausch (AJA). 

Selten war die Sorge um die deutsche Demokratie so groß. Hunderttausende Protestierende machen zwar Hoffnung, dass Rechtsextreme mit menschenfeindlichen Ausweisungsfantasien ausreichend Widerstand erfahren werden. Trotzdem wird deutlich: Deutschland muss dringend in die Wehrhaftigkeit der Demokratie investieren. Dazu gehört, dass demokratische Werte und demokratisches Verantwortungsbewusstsein in der nächsten Generation verankert werden.

In der Demokratiebildung gibt es dazu ein seit Jahrzehnten bewährtes Mittel: internationale Begegnungen von jungen Menschen in Austauschprogrammen, wie sie etwa die Mitglieder des Arbeitskreises gemeinnütziger Jugendaustausch (AJA) anbieten. Die in diesem Kreis miteinander vernetzten Organisationen zielen nicht nur auf die individuelle Bildung, sondern eben auch auf eine starke gesellschaftliche Wirkung. Jugendliche, die eine gewisse Zeit ihren Alltag im Ausland verbringen, erleben Vielfalt in der Regel nicht als Bedrohung, sondern als Bereicherung. Sie lernen, dass es möglich ist, sich über nationale, sprachliche und kulturelle Grenzen hinweg zu verständigen. Und dass man Probleme auch gemeinsam mit Andersdenkenden lösen kann. Sie übernehmen Verantwortung für sich und ihre Umgebung. Globale Kompetenzen, wie sie inzwischen sogar in den Pisa-Studien berücksichtigt werden, erfordern den Blick über den Rand des vertrauten Klassenzimmers. Allein im Schulunterricht jedenfalls werden sie nicht erworben. 

Das Problem: Seit Corona verbringen weniger Jugendliche ein Schuljahr im Ausland. Vor der Pandemie waren es jährlich 15.000 Schülerinnen und Schüler, die sich für ein Austauschprogramm entschieden haben. Bisher sind erst 80 Prozent dieses Niveaus wieder erreicht. Zwar zeigt der Trend nach oben, aber das ändert nichts daran, dass nur etwa zwei Prozent aller Jugendlichen eines Jahrgangs überhaupt an Austauschprogrammen teilnehmen. Selbst wenn man Programme mitberücksichtigt, die nur einen kurzen Aufenthalt von einigen Wochen oder Monaten vorsehen, werden gerade mal 26 Prozent aller Jugendlichen erreicht. Das hat die ""Zugangsstudie zu internationalen Jugendbegegnungen"" gezeigt. 

Diese Zahlen sind auch Ausdruck einer massiven Bildungsungerechtigkeit, denn es nehmen vor allem Kinder an diesen Austauschprogrammen teil, deren Eltern studiert haben und überdurchschnittlich viel verdienen. 

Dass nur ein geringer Anteil der Jugendlichen ins Ausland geht, hat drei Gründe. Erstens: Mangelnde Information – vielen bleibt unklar, welche Angebote, welche Fördermöglichkeiten durch Stipendien überhaupt existieren. Zweitens: Es fehlt ein gemeinsames Vorgehen der Anbieter, um Jugendliche zu erreichen, die nicht von ihren Eltern motiviert und unterstützt werden. Drittens: Die staatlichen Programme und auch die staatliche Unterstützung der Jugendlichen werden schlecht koordiniert. Sechs Bundesministerien, der Bundestag sowie jeweils unterschiedliche Ministerien in den Ländern betreiben eigene Programme, ohne sich miteinander abzustimmen. Einjährige Austauschprogramme werden, außer von Bayern, Hamburg und durch ein Programm des Deutschen Bundestages, staatlich nicht gefördert. Es gibt in Deutschland im Vergleich zu anderen Ländern eine vielfältige Szene der internationalen Jugendarbeit, aber es wird nicht effizient gearbeitet. Es mangelt an Abstimmung und Koordination. 

Die politisch Verantwortlichen sollten die staatlichen Förderprogramme ausweiten und aufeinander abstimmen. Es braucht flächendeckende Informations- und Beratungsangebote. An den Schulen sollte die Kooperation mit den außerschulischen Trägern der internationalen Jugendarbeit gefördert werden. Austausch und interkulturelle Bildung müssen ein verbindlicher Teil des modernen Schullebens sein und auch inhaltlich in die Lehrkräfteausbildung einfließen. Eine internationale Jugendarbeit, die möglichst viele Jugendliche anspricht, ist ein unverzichtbarer Bestandteil einer zeitgemäßen Bildung – und zudem ein wesentlicher Beitrag zur Absicherung der deutschen Demokratie.
"
Künstliche Intelligenz,Zeit,2024-02-09,https://www.zeit.de/news/2024-02/09/basf-grosses-potenzial-fuer-kuenstliche-intelligenz,Chemiebranche: BASF: Großes Potenzial für Künstliche Intelligenz | ZEIT ONLINE,"Der BASF-Konzern sieht großes Potenzial für Künstliche Intelligenz (KI) in der Chemiebranche. «Die Frage ist nicht, ob KI einen wesentlichen Einfluss auf Industrieunternehmen haben wird, sondern, wie schnell das geschieht», sagte Chief Digital Officer Dirk Elvermann den Nachrichtenagenturen dpa und dpa-AFX. «Was genau und wie wird das passieren?» BASF erprobe KI mit Tausenden von Mitarbeitern. «Jede Einheit macht dabei spezifische Erfahrungen.»

Als Beispiel verweist BASF etwa auf ein digitales Tool im Bereich von Handspülmitteln. Hier werde Künstliche Intelligenz für eine leistungsfähigere Plattform für digitale Kundenanfragen genutzt. In der Landwirtschaft werde maschinelles Lernen wiederum angewendet, um Landwirten mit Algorithmen beim effizienten Pflanzen zu helfen.

Das riesige Potenzial von Künstlicher Intelligenz sei momentan noch gar nicht konkret abschätzbar, sagte Elvermann. «Wir probieren es aus. Wir haben zum Beispiel ChatGPT nicht als Open Source, aber geschützt in unserer Domäne in der Erprobung.» Man sollte sich daran gewöhnen, mit generativer Künstlicher Intelligenz zusammenarbeiten. «Die Ziele, die wir damit verfolgen, sind Effizienzgewinne, aber auch Wachstum und neue Geschäftsmodelle.»

Dabei arbeite BASF «mit wachem Auge und ständiger Risikoabwägung» bei der Digitalisierung. «Cybersecurity ist extrem wichtig. Wir sind kritische Infrastruktur - dementsprechend müssen wir unsere Anlagen und unser geistiges Eigentum schützen.» Eins der Haupteinfalltore für Attacken sei nun einmal Cyber. «Deswegen versuchen wir, die digitalen Lösungen, die wir entwickeln, gleich so abzusichern, dass nichts passiert», sagte Elvermann.

© dpa-infocom, dpa:240209-99-925543/3
"
Künstliche Intelligenz,Zeit,2024-02-08,https://www.zeit.de/digital/2024-02/usa-kuenstliche-intelligenz-fcc-verbot-anrufe,Künstliche Intelligenz: USA verbieten automatisierte Anrufe mit KI-generierten Stimmen | ZEIT ONLINE,"Die Kommunikationsaufsicht der USA hat automatisierte
Werbeanrufe mit von künstlicher Intelligenz generierten Stimmen verboten. Die Telekom-Behörde FCC kann damit Bußgelder gegen
Unternehmen verhängen, die KI-Stimmen in ihren Anrufen verwenden, oder
Diensteanbieter sperren, die solche Stimmen übertragen. 

Für Anrufe mit von
Künstlicher Intelligenz generierten Stimmen sei die vorherige Zustimmung der
Empfänger unerlässlich, teilte die FCC mit. Auch müssten sich die Urheber
solcher Anrufe klar zu erkennen geben. Die Entscheidung
tritt sofort in Kraft. Verstöße können laut FCC mit mehr als 23.000
Dollar pro Anruf geahndet werden.

In den USA hatten vor wenigen Wochen automatisierte Anrufe
mit einer täuschend echt klingenden Nachahmung der Stimme von Präsident Joe
Biden für Aufsehen gesorgt. Die Botschaft der Anrufe war, nicht an der Vorwahl
der Demokratischen Partei im Bundesstaat New Hampshire teilzunehmen. 

Der Vorfall schürte die Sorge, in den kommenden Monaten
könnte es Versuche geben, den Ausgang der Präsidentenwahl im November mit der
Verbreitung von KI-Fälschungen zu beeinflussen. Behörden konnten als Urheber
der Anrufe inzwischen eine Firma aus Texas ausmachen und gehen gegen sie vor.

""Böswillige Akteure verwenden KI-generierte Stimmen in
unerbetenen Anrufen, um angreifbare Familienmitglieder zu erpressen, Prominente
zu imitieren und Wähler falsch zu informieren"", teilte die Vorsitzende der
Behörde, Jessica Rosenworcel, mit. ""Darüber setzen wir die Betrüger, die hinter
diesen Anrufen stecken, in Kenntnis."" 
"
Künstliche Intelligenz,Zeit,2024-02-08,https://www.zeit.de/news/2024-02/08/ki-system-bard-von-google-heisst-kuenftig-gemini,Künstliche Intelligenz: KI-System Bard von Google heißt künftig Gemini | ZEIT ONLINE,"Google wird künftig seine Apps und Dienste mit Künstlicher Intelligenz unter der Marke Gemini zusammenfassen. Bislang trug nur ein Sprachmodell den Namen Gemini. Der Chat-Bot von Google hieß bislang Bard, benannt nach dem englischen Dichter William Shakespeare (Spitzname: «The Bard of Avon»). Dieser Name für den Herausforderer von ChatGPT von OpenAI werde künftig verschwinden, kündigte Konzern-Chef Sundar Pichai an.

In den USA wird Gemini von sofort an auch auf Smartphones zur Verfügung stehen. Handys mit dem Google-Betriebssystem Android erhalten eine eigene Gemini-App. Auf dem iPhone von Apple wird Gemini innerhalb der bestehenden Google-App angeboten. Dabei löst das KI-System auch den bisherigen Sprachassistenten Google Assistent ab, wenn die Nutzerinnen und Nutzer dem Umstieg zustimmen.

Wann die Apps in Europa und in Deutschland zur Verfügung stehen werden, teilte Google nicht mit. Eine Woche nach dem Start in den USA sollen die Apps in weiteren englischsprachigen Ländern sowie auf Japanisch und Koreanisch in Japan und Südkorea eingeführt werden. Erfahrungsgemäß folgen andere Sprachen wie Spanisch und Deutsch mit Abstand von einigen Monaten.

Gleichzeitig mit den Smartphone-Apps bringt Google außerdem eine bezahlpflichtige KI-Profi-Version - Gemini Advanced - auf den Markt, die auf dem neuen Sprachmodell Ultra 1.0 aufsetzt. Sie wird vorerst in 150 Ländern der englischsprachigen Welt angeboten und ist Teil des Abo-Dienstes Google One. Der KI-Dienst kostet knapp zehn US-Dollar im Monat, ist aber nur in der teuersten Variante von Google One verfügbar, die zusätzlich zehn Dollar kostet. Dafür erhalten die Kundinnen und Kunden unter anderem einen Cloud-Speicherplatz von zwei Terabyte. Wann der Dienst in Europa und Deutschland gebucht werden kann, wurde ebenfalls nicht mitgeteilt.

Google-Managerin Sissie Hsiao erklärte in einem Blog-Eintrag, Gemini Advanced sei weitaus leistungsfähiger bei hochkomplexen Aufgaben wie dem Schreiben von Programmcode, logischem Denken, dem Befolgen differenzierter Anweisungen und der Zusammenarbeit an kreativen Projekten. «Mit Gemini Advanced können Sie nicht nur längere, detailliertere Gespräche führen, sondern er versteht auch den Kontext Ihrer vorherigen Aufforderungen besser.»

© dpa-infocom, dpa:240208-99-918712/2
"
Künstliche Intelligenz,Zeit,2024-02-07,https://www.zeit.de/2024/07/ai-act-eu-gesetz-kuenstliche-intelligenz-regulierung,AI Act: Regulierte Intelligenz | ZEIT ONLINE,"Die EU hofft auf den ""Brüssel-Effekt"", der selbst amerikanische Großkonzerne erfassen soll. Denn die Konzerne, so das Kalkül, wollen nicht auf ihre Kunden in der Europäischen Union verzichten; und da es teuer und aufwendig wäre, Produkte nach unterschiedlichen Standards herzustellen, richten sie sich am Ende nach den Regeln des am strengsten regulierten Marktes – also des europäischen. Ein positives Beispiel für diesen Effekt ist die EU-Datenschutz-Grundverordnung, an die sich etwa Microsoft weltweit hält. 

Wie stark allerdings der AI Act die KI-Entwickler außerhalb der EU beeinflusst, wird davon abhängen, wo deren Systeme genutzt werden. Wenig Folgen wird der AI Act für lokale Anwendungen haben, die auf ein bestimmtes Land zugeschnitten sind – etwa Systeme zur Bemessung der Kreditwürdigkeit von US-Amerikanern oder KI-Anwendungen in britischen Behörden. 

Anders sieht es aus bei Konsumprodukten wie Bildgeneratoren oder Chatbots, die länderübergreifend über Plattformen betrieben werden. Hier könnten sich die Standards des AI Act international durchsetzen. Gleiches gilt für Anwendungen, die von der EU als ""hochriskant"" eingestuft werden und an die sie besonders hohe Sicherheitsanforderungen stellt. 

Der einschlägigen Forschung könnte das Aufträge und Fördergelder bescheren. Denn um zu überprüfen, ob die KI-Systeme den Regeln der EU entsprechen, wird es künftig erhebliche wissenschaftliche Anstrengungen brauchen; ebenso, um KI-Systeme zu entwickeln, deren Funktionsweise für Dritte nachvollziehbar ist, die faire Entscheidungen treffen und robust sind gegenüber Cyberangriffen. 

Und dann sind da noch die neuen Transparenzpflichten: Der AI Act sieht zum Beispiel vor, dass Unternehmen ihre Trainingsdaten offenlegen müssen. Bisher konnten die großen Tech-Konzerne ihre Marktmacht auch deshalb ausbauen, weil sie ihre Datensätze geheim hielten – und unabhängige Entwickler kaum eine Chance hatten, ihre Maschinen mit nur annähernd so großen Datenmengen lernen zu lassen. Diese Geheimhaltung soll nach dem Willen der EU ein Ende haben. Ob und wie sich die Transparenzpflicht allerdings konkret auswirkt, wird sich wohl erst zeigen, wenn der AI Act in Kraft tritt.

Der Kern des Gesetzes besteht darin, dass KI-Anwendungen in Risikoklassen eingeteilt werden. Programme mit geringem Risiko werden kaum reguliert. Für Anwendungen mit hohem Risiko hingegen gelten spezielle Regeln. 

Ein Hochrisikofall wäre zum Beispiel eine KI, die bei der Entscheidung hilft, ob ein Mensch Arbeitslosengeld bekommt. Wenn dieses Programm falsche Ratschläge gibt oder bestimmte Gruppen gegenüber anderen benachteiligt, könnte das schwerwiegende Folgen für die Betroffenen haben. Hochrisikoanwendungen sind auch Systeme, die etwa bei der Bearbeitung von Asylanträgen eingesetzt werden könnten oder die Personalabteilungen dabei unterstützen, Bewerbungsunterlagen nach geeigneten Kandidatinnen und Kandidaten zu durchsuchen. 

Unternehmen, die solche Programme entwickeln oder einsetzen, müssen Auflagen erfüllen, um die Risiken zu minimieren. Zum Beispiel müssen sie sicherstellen, dass die Daten, mit denen die KI trainiert wurde, die Menschen, die sie betrifft, angemessen repräsentieren. Außerdem müssen Menschen die Entscheidungen der KI überwachen und überprüfen können. 

Das kann natürlich nicht garantieren, dass mit solcher Technik nie etwas schiefgeht. Dennoch halten viele Fachleute den Ansatz für richtig, nicht die künstliche Intelligenz grundsätzlich zu regulieren, sondern jeweils nur bestimmte Anwendungen dieser Technik. Auch ein sehr simples KI-System kann großen Schaden anrichten, wenn es in einem kritischen Bereich eingesetzt wird.

Ein AI Act ist besser als kein AI Act. So argumentieren, etwas zugespitzt, die Befürworter des neuen EU-Gesetzes zur Regulierung der künstlichen Intelligenz (KI, englisch: artificial intelligence, AI). Noch vergangene Woche hatten Wirtschaftsverbände, Forscher und zivilgesellschaftliche Organisationen gefordert, dieses KI-Gesetz, den AI Act, nicht noch scheitern zu lassen. Ein ""fehlender Rechtsrahmen"" wäre ""riskant für Grundrechtsschutz und Innovationen in Europa"", hieß es in einem von mehreren offenen Briefen. Die Sorge war unbegründet: Vergangenen Freitag wurde der AI Act im Rat der Europäischen Union angenommen. Noch muss Europas Parlament zustimmen, aber ein wichtiger Schritt ist getan zum ersten umfassenden KI-Gesetz der Welt, um das es jahrelang Gezerre gegeben hatte.

Seit 2021 ist das Gesetz in Arbeit. Doch erst als ChatGPT im November 2022 den weltweiten KI-Hype auslöste, wurde vielen klar, dass künstliche Intelligenz – ähnlich wie das Internet – künftig jeden Lebensbereich verändern könnte. Das beflügelte den Wunsch nach Regeln zur Anwendung der Technologie. 

Umstritten war allerdings bis zuletzt, wie weit diese Regeln gehen sollen. In der Bundesregierung formulierte vor allem Digitalminister Volker Wissing die Sorge, dass zu viel Regulierung europäische Unternehmen daran hindern könnte, gegenüber der amerikanischen Konkurrenz aufzuholen. Wissing konnte sich allerdings im Kabinett nicht durchsetzen; auch die Bundesregierung stimmte wie alle anderen Mitgliedsstaaten dem AI Act zu. 

Darin geht es vorrangig um Regeln für Unternehmen, die KI-Systeme entwickeln oder einsetzen. Nicht alle sind glücklich mit den Pflichten, die auf sie zukommen; dennoch sprachen sich selbst einige Unternehmen für das Gesetz aus, um Rechtssicherheit zu haben. 

Der AI Act soll unter anderem sicherstellen, dass automatisierte Entscheidungen, von denen Menschen betroffen sind, fair und nachvollziehbar sind. Manche KI-Anwendungen werden auch schlicht verboten. Arbeitgeber dürfen zum Beispiel keine Systeme installieren, die automatisch an Gesichtsausdruck oder Stimmlage erkennen, wie Mitarbeiter sich fühlen. Dass solche Regeln sinnvoll sind, dürfte vielen Menschen einleuchten. 

Bis zuletzt wurde darüber gestritten, wie man die Technik hinter den großen Sprachmodellen wie ChatGPT regulieren soll. Weil sie so viele mögliche Anwendungen hat, lässt sie sich nur schwer in eine der Risikoklassen einsortieren. Die sogenannten KI-Basismodelle (engl.: foundation models) können nicht nur Chatbots antreiben, sondern zum Beispiel auch die Software von Rechtsanwaltskanzleien oder Personalabteilungen ergänzen, um kritische Entscheidungen zu treffen.

Begeht ein solches System einen Fehler, könnte das an falscher Anwendung liegen – oder am zugrunde liegenden Basismodell. Um das nachvollziehen zu können, müssen die Anbieter solcher Modelle den Anwendern entsprechende Informationen zur Verfügung stellen. Das heißt: Große KI-Entwickler wie OpenAI oder Google müssen deutschen Mittelständlern so viele technische Details verraten, dass diese ""ein gutes Verständnis der Möglichkeiten und Grenzen"" der KI-Basissysteme entwickeln können. 

Für die Anbieter von besonders großen und technisch ausgereiften KI-Systemen gelten noch strengere Regeln: Sie müssen zum Beispiel auch Maßnahmen zur Cybersicherheit ergreifen. Gerade solche Regeln hatten eine Reihe von KI-Forscherinnen und -Forschern vehement gefordert, weil sie davon ausgehen, dass von den leistungsfähigsten Modellen auch die größten Gefahren durch KI ausgehen. 

Ob für ein System diese strengen Regeln gelten, hängt unter anderem von der Rechenleistung ab, die für das Training der KI verwendet wird. Die Schwelle liegt so hoch, dass sie derzeit wohl kaum ein auf dem Markt verfügbares System überschreitet. 

Unter anderem die Bundesregierung hatte sich in den Verhandlungen dafür eingesetzt, dass für Basismodelle gar keine speziellen Vorschriften gelten sollten. Dabei spielte auch die Sorge eine Rolle, die Regeln aus Brüssel könnten europäische Unternehmen gegenüber der Konkurrenz aus den USA oder China massiv benachteiligen. Darauf haben im Vorfeld sowohl das französische KI-Unternehmen Mistral AI wie auch das deutsche Start-up Aleph Alpha hingewiesen, zwei der vielversprechendsten europäischen KI-Hoffnungen, die selbst Basismodelle entwickeln. Sie treibt die Furcht um, der AI Act könnte ihre Arbeit ausbremsen, bevor sie richtig in Schwung gekommen ist. In diesem Fall überließen die Europäer die KI-Entwicklung (und die künftige Dominanz in diesem Bereich) wieder einmal amerikanischen Großkonzernen.
"
AI,Zeit,2024-02-10,https://www.zeit.de/digital/mobil/2024-02/samsung-galaxy-s24-ultra-smartphone-test,"Samsung Galaxy S24 Ultra: Oh Mann, Samsung! | ZEIT ONLINE",
AI,Zeit,2024-02-08,https://www.zeit.de/digital/2024-02/usa-kuenstliche-intelligenz-fcc-verbot-anrufe,Künstliche Intelligenz: USA verbieten automatisierte Anrufe mit KI-generierten Stimmen | ZEIT ONLINE,"Die Kommunikationsaufsicht der USA hat automatisierte
Werbeanrufe mit von künstlicher Intelligenz generierten Stimmen verboten. Die Telekom-Behörde FCC kann damit Bußgelder gegen
Unternehmen verhängen, die KI-Stimmen in ihren Anrufen verwenden, oder
Diensteanbieter sperren, die solche Stimmen übertragen. 

Für Anrufe mit von
Künstlicher Intelligenz generierten Stimmen sei die vorherige Zustimmung der
Empfänger unerlässlich, teilte die FCC mit. Auch müssten sich die Urheber
solcher Anrufe klar zu erkennen geben. Die Entscheidung
tritt sofort in Kraft. Verstöße können laut FCC mit mehr als 23.000
Dollar pro Anruf geahndet werden.

In den USA hatten vor wenigen Wochen automatisierte Anrufe
mit einer täuschend echt klingenden Nachahmung der Stimme von Präsident Joe
Biden für Aufsehen gesorgt. Die Botschaft der Anrufe war, nicht an der Vorwahl
der Demokratischen Partei im Bundesstaat New Hampshire teilzunehmen. 

Der Vorfall schürte die Sorge, in den kommenden Monaten
könnte es Versuche geben, den Ausgang der Präsidentenwahl im November mit der
Verbreitung von KI-Fälschungen zu beeinflussen. Behörden konnten als Urheber
der Anrufe inzwischen eine Firma aus Texas ausmachen und gehen gegen sie vor.

""Böswillige Akteure verwenden KI-generierte Stimmen in
unerbetenen Anrufen, um angreifbare Familienmitglieder zu erpressen, Prominente
zu imitieren und Wähler falsch zu informieren"", teilte die Vorsitzende der
Behörde, Jessica Rosenworcel, mit. ""Darüber setzen wir die Betrüger, die hinter
diesen Anrufen stecken, in Kenntnis."" 
"
AI,Zeit,2024-02-07,https://www.zeit.de/2024/07/ai-act-eu-gesetz-kuenstliche-intelligenz-regulierung,AI Act: Regulierte Intelligenz | ZEIT ONLINE,"Die EU hofft auf den ""Brüssel-Effekt"", der selbst amerikanische Großkonzerne erfassen soll. Denn die Konzerne, so das Kalkül, wollen nicht auf ihre Kunden in der Europäischen Union verzichten; und da es teuer und aufwendig wäre, Produkte nach unterschiedlichen Standards herzustellen, richten sie sich am Ende nach den Regeln des am strengsten regulierten Marktes – also des europäischen. Ein positives Beispiel für diesen Effekt ist die EU-Datenschutz-Grundverordnung, an die sich etwa Microsoft weltweit hält. 

Wie stark allerdings der AI Act die KI-Entwickler außerhalb der EU beeinflusst, wird davon abhängen, wo deren Systeme genutzt werden. Wenig Folgen wird der AI Act für lokale Anwendungen haben, die auf ein bestimmtes Land zugeschnitten sind – etwa Systeme zur Bemessung der Kreditwürdigkeit von US-Amerikanern oder KI-Anwendungen in britischen Behörden. 

Anders sieht es aus bei Konsumprodukten wie Bildgeneratoren oder Chatbots, die länderübergreifend über Plattformen betrieben werden. Hier könnten sich die Standards des AI Act international durchsetzen. Gleiches gilt für Anwendungen, die von der EU als ""hochriskant"" eingestuft werden und an die sie besonders hohe Sicherheitsanforderungen stellt. 

Der einschlägigen Forschung könnte das Aufträge und Fördergelder bescheren. Denn um zu überprüfen, ob die KI-Systeme den Regeln der EU entsprechen, wird es künftig erhebliche wissenschaftliche Anstrengungen brauchen; ebenso, um KI-Systeme zu entwickeln, deren Funktionsweise für Dritte nachvollziehbar ist, die faire Entscheidungen treffen und robust sind gegenüber Cyberangriffen. 

Und dann sind da noch die neuen Transparenzpflichten: Der AI Act sieht zum Beispiel vor, dass Unternehmen ihre Trainingsdaten offenlegen müssen. Bisher konnten die großen Tech-Konzerne ihre Marktmacht auch deshalb ausbauen, weil sie ihre Datensätze geheim hielten – und unabhängige Entwickler kaum eine Chance hatten, ihre Maschinen mit nur annähernd so großen Datenmengen lernen zu lassen. Diese Geheimhaltung soll nach dem Willen der EU ein Ende haben. Ob und wie sich die Transparenzpflicht allerdings konkret auswirkt, wird sich wohl erst zeigen, wenn der AI Act in Kraft tritt.

Der Kern des Gesetzes besteht darin, dass KI-Anwendungen in Risikoklassen eingeteilt werden. Programme mit geringem Risiko werden kaum reguliert. Für Anwendungen mit hohem Risiko hingegen gelten spezielle Regeln. 

Ein Hochrisikofall wäre zum Beispiel eine KI, die bei der Entscheidung hilft, ob ein Mensch Arbeitslosengeld bekommt. Wenn dieses Programm falsche Ratschläge gibt oder bestimmte Gruppen gegenüber anderen benachteiligt, könnte das schwerwiegende Folgen für die Betroffenen haben. Hochrisikoanwendungen sind auch Systeme, die etwa bei der Bearbeitung von Asylanträgen eingesetzt werden könnten oder die Personalabteilungen dabei unterstützen, Bewerbungsunterlagen nach geeigneten Kandidatinnen und Kandidaten zu durchsuchen. 

Unternehmen, die solche Programme entwickeln oder einsetzen, müssen Auflagen erfüllen, um die Risiken zu minimieren. Zum Beispiel müssen sie sicherstellen, dass die Daten, mit denen die KI trainiert wurde, die Menschen, die sie betrifft, angemessen repräsentieren. Außerdem müssen Menschen die Entscheidungen der KI überwachen und überprüfen können. 

Das kann natürlich nicht garantieren, dass mit solcher Technik nie etwas schiefgeht. Dennoch halten viele Fachleute den Ansatz für richtig, nicht die künstliche Intelligenz grundsätzlich zu regulieren, sondern jeweils nur bestimmte Anwendungen dieser Technik. Auch ein sehr simples KI-System kann großen Schaden anrichten, wenn es in einem kritischen Bereich eingesetzt wird.

Ein AI Act ist besser als kein AI Act. So argumentieren, etwas zugespitzt, die Befürworter des neuen EU-Gesetzes zur Regulierung der künstlichen Intelligenz (KI, englisch: artificial intelligence, AI). Noch vergangene Woche hatten Wirtschaftsverbände, Forscher und zivilgesellschaftliche Organisationen gefordert, dieses KI-Gesetz, den AI Act, nicht noch scheitern zu lassen. Ein ""fehlender Rechtsrahmen"" wäre ""riskant für Grundrechtsschutz und Innovationen in Europa"", hieß es in einem von mehreren offenen Briefen. Die Sorge war unbegründet: Vergangenen Freitag wurde der AI Act im Rat der Europäischen Union angenommen. Noch muss Europas Parlament zustimmen, aber ein wichtiger Schritt ist getan zum ersten umfassenden KI-Gesetz der Welt, um das es jahrelang Gezerre gegeben hatte.

Seit 2021 ist das Gesetz in Arbeit. Doch erst als ChatGPT im November 2022 den weltweiten KI-Hype auslöste, wurde vielen klar, dass künstliche Intelligenz – ähnlich wie das Internet – künftig jeden Lebensbereich verändern könnte. Das beflügelte den Wunsch nach Regeln zur Anwendung der Technologie. 

Umstritten war allerdings bis zuletzt, wie weit diese Regeln gehen sollen. In der Bundesregierung formulierte vor allem Digitalminister Volker Wissing die Sorge, dass zu viel Regulierung europäische Unternehmen daran hindern könnte, gegenüber der amerikanischen Konkurrenz aufzuholen. Wissing konnte sich allerdings im Kabinett nicht durchsetzen; auch die Bundesregierung stimmte wie alle anderen Mitgliedsstaaten dem AI Act zu. 

Darin geht es vorrangig um Regeln für Unternehmen, die KI-Systeme entwickeln oder einsetzen. Nicht alle sind glücklich mit den Pflichten, die auf sie zukommen; dennoch sprachen sich selbst einige Unternehmen für das Gesetz aus, um Rechtssicherheit zu haben. 

Der AI Act soll unter anderem sicherstellen, dass automatisierte Entscheidungen, von denen Menschen betroffen sind, fair und nachvollziehbar sind. Manche KI-Anwendungen werden auch schlicht verboten. Arbeitgeber dürfen zum Beispiel keine Systeme installieren, die automatisch an Gesichtsausdruck oder Stimmlage erkennen, wie Mitarbeiter sich fühlen. Dass solche Regeln sinnvoll sind, dürfte vielen Menschen einleuchten. 

Bis zuletzt wurde darüber gestritten, wie man die Technik hinter den großen Sprachmodellen wie ChatGPT regulieren soll. Weil sie so viele mögliche Anwendungen hat, lässt sie sich nur schwer in eine der Risikoklassen einsortieren. Die sogenannten KI-Basismodelle (engl.: foundation models) können nicht nur Chatbots antreiben, sondern zum Beispiel auch die Software von Rechtsanwaltskanzleien oder Personalabteilungen ergänzen, um kritische Entscheidungen zu treffen.

Begeht ein solches System einen Fehler, könnte das an falscher Anwendung liegen – oder am zugrunde liegenden Basismodell. Um das nachvollziehen zu können, müssen die Anbieter solcher Modelle den Anwendern entsprechende Informationen zur Verfügung stellen. Das heißt: Große KI-Entwickler wie OpenAI oder Google müssen deutschen Mittelständlern so viele technische Details verraten, dass diese ""ein gutes Verständnis der Möglichkeiten und Grenzen"" der KI-Basissysteme entwickeln können. 

Für die Anbieter von besonders großen und technisch ausgereiften KI-Systemen gelten noch strengere Regeln: Sie müssen zum Beispiel auch Maßnahmen zur Cybersicherheit ergreifen. Gerade solche Regeln hatten eine Reihe von KI-Forscherinnen und -Forschern vehement gefordert, weil sie davon ausgehen, dass von den leistungsfähigsten Modellen auch die größten Gefahren durch KI ausgehen. 

Ob für ein System diese strengen Regeln gelten, hängt unter anderem von der Rechenleistung ab, die für das Training der KI verwendet wird. Die Schwelle liegt so hoch, dass sie derzeit wohl kaum ein auf dem Markt verfügbares System überschreitet. 

Unter anderem die Bundesregierung hatte sich in den Verhandlungen dafür eingesetzt, dass für Basismodelle gar keine speziellen Vorschriften gelten sollten. Dabei spielte auch die Sorge eine Rolle, die Regeln aus Brüssel könnten europäische Unternehmen gegenüber der Konkurrenz aus den USA oder China massiv benachteiligen. Darauf haben im Vorfeld sowohl das französische KI-Unternehmen Mistral AI wie auch das deutsche Start-up Aleph Alpha hingewiesen, zwei der vielversprechendsten europäischen KI-Hoffnungen, die selbst Basismodelle entwickeln. Sie treibt die Furcht um, der AI Act könnte ihre Arbeit ausbremsen, bevor sie richtig in Schwung gekommen ist. In diesem Fall überließen die Europäer die KI-Entwicklung (und die künftige Dominanz in diesem Bereich) wieder einmal amerikanischen Großkonzernen.
"
Artificial Intelligence,Zeit,2024-02-07,https://www.zeit.de/2024/07/ai-act-eu-gesetz-kuenstliche-intelligenz-regulierung,AI Act: Regulierte Intelligenz | ZEIT ONLINE,"Die EU hofft auf den ""Brüssel-Effekt"", der selbst amerikanische Großkonzerne erfassen soll. Denn die Konzerne, so das Kalkül, wollen nicht auf ihre Kunden in der Europäischen Union verzichten; und da es teuer und aufwendig wäre, Produkte nach unterschiedlichen Standards herzustellen, richten sie sich am Ende nach den Regeln des am strengsten regulierten Marktes – also des europäischen. Ein positives Beispiel für diesen Effekt ist die EU-Datenschutz-Grundverordnung, an die sich etwa Microsoft weltweit hält. 

Wie stark allerdings der AI Act die KI-Entwickler außerhalb der EU beeinflusst, wird davon abhängen, wo deren Systeme genutzt werden. Wenig Folgen wird der AI Act für lokale Anwendungen haben, die auf ein bestimmtes Land zugeschnitten sind – etwa Systeme zur Bemessung der Kreditwürdigkeit von US-Amerikanern oder KI-Anwendungen in britischen Behörden. 

Anders sieht es aus bei Konsumprodukten wie Bildgeneratoren oder Chatbots, die länderübergreifend über Plattformen betrieben werden. Hier könnten sich die Standards des AI Act international durchsetzen. Gleiches gilt für Anwendungen, die von der EU als ""hochriskant"" eingestuft werden und an die sie besonders hohe Sicherheitsanforderungen stellt. 

Der einschlägigen Forschung könnte das Aufträge und Fördergelder bescheren. Denn um zu überprüfen, ob die KI-Systeme den Regeln der EU entsprechen, wird es künftig erhebliche wissenschaftliche Anstrengungen brauchen; ebenso, um KI-Systeme zu entwickeln, deren Funktionsweise für Dritte nachvollziehbar ist, die faire Entscheidungen treffen und robust sind gegenüber Cyberangriffen. 

Und dann sind da noch die neuen Transparenzpflichten: Der AI Act sieht zum Beispiel vor, dass Unternehmen ihre Trainingsdaten offenlegen müssen. Bisher konnten die großen Tech-Konzerne ihre Marktmacht auch deshalb ausbauen, weil sie ihre Datensätze geheim hielten – und unabhängige Entwickler kaum eine Chance hatten, ihre Maschinen mit nur annähernd so großen Datenmengen lernen zu lassen. Diese Geheimhaltung soll nach dem Willen der EU ein Ende haben. Ob und wie sich die Transparenzpflicht allerdings konkret auswirkt, wird sich wohl erst zeigen, wenn der AI Act in Kraft tritt.

Der Kern des Gesetzes besteht darin, dass KI-Anwendungen in Risikoklassen eingeteilt werden. Programme mit geringem Risiko werden kaum reguliert. Für Anwendungen mit hohem Risiko hingegen gelten spezielle Regeln. 

Ein Hochrisikofall wäre zum Beispiel eine KI, die bei der Entscheidung hilft, ob ein Mensch Arbeitslosengeld bekommt. Wenn dieses Programm falsche Ratschläge gibt oder bestimmte Gruppen gegenüber anderen benachteiligt, könnte das schwerwiegende Folgen für die Betroffenen haben. Hochrisikoanwendungen sind auch Systeme, die etwa bei der Bearbeitung von Asylanträgen eingesetzt werden könnten oder die Personalabteilungen dabei unterstützen, Bewerbungsunterlagen nach geeigneten Kandidatinnen und Kandidaten zu durchsuchen. 

Unternehmen, die solche Programme entwickeln oder einsetzen, müssen Auflagen erfüllen, um die Risiken zu minimieren. Zum Beispiel müssen sie sicherstellen, dass die Daten, mit denen die KI trainiert wurde, die Menschen, die sie betrifft, angemessen repräsentieren. Außerdem müssen Menschen die Entscheidungen der KI überwachen und überprüfen können. 

Das kann natürlich nicht garantieren, dass mit solcher Technik nie etwas schiefgeht. Dennoch halten viele Fachleute den Ansatz für richtig, nicht die künstliche Intelligenz grundsätzlich zu regulieren, sondern jeweils nur bestimmte Anwendungen dieser Technik. Auch ein sehr simples KI-System kann großen Schaden anrichten, wenn es in einem kritischen Bereich eingesetzt wird.

Ein AI Act ist besser als kein AI Act. So argumentieren, etwas zugespitzt, die Befürworter des neuen EU-Gesetzes zur Regulierung der künstlichen Intelligenz (KI, englisch: artificial intelligence, AI). Noch vergangene Woche hatten Wirtschaftsverbände, Forscher und zivilgesellschaftliche Organisationen gefordert, dieses KI-Gesetz, den AI Act, nicht noch scheitern zu lassen. Ein ""fehlender Rechtsrahmen"" wäre ""riskant für Grundrechtsschutz und Innovationen in Europa"", hieß es in einem von mehreren offenen Briefen. Die Sorge war unbegründet: Vergangenen Freitag wurde der AI Act im Rat der Europäischen Union angenommen. Noch muss Europas Parlament zustimmen, aber ein wichtiger Schritt ist getan zum ersten umfassenden KI-Gesetz der Welt, um das es jahrelang Gezerre gegeben hatte.

Seit 2021 ist das Gesetz in Arbeit. Doch erst als ChatGPT im November 2022 den weltweiten KI-Hype auslöste, wurde vielen klar, dass künstliche Intelligenz – ähnlich wie das Internet – künftig jeden Lebensbereich verändern könnte. Das beflügelte den Wunsch nach Regeln zur Anwendung der Technologie. 

Umstritten war allerdings bis zuletzt, wie weit diese Regeln gehen sollen. In der Bundesregierung formulierte vor allem Digitalminister Volker Wissing die Sorge, dass zu viel Regulierung europäische Unternehmen daran hindern könnte, gegenüber der amerikanischen Konkurrenz aufzuholen. Wissing konnte sich allerdings im Kabinett nicht durchsetzen; auch die Bundesregierung stimmte wie alle anderen Mitgliedsstaaten dem AI Act zu. 

Darin geht es vorrangig um Regeln für Unternehmen, die KI-Systeme entwickeln oder einsetzen. Nicht alle sind glücklich mit den Pflichten, die auf sie zukommen; dennoch sprachen sich selbst einige Unternehmen für das Gesetz aus, um Rechtssicherheit zu haben. 

Der AI Act soll unter anderem sicherstellen, dass automatisierte Entscheidungen, von denen Menschen betroffen sind, fair und nachvollziehbar sind. Manche KI-Anwendungen werden auch schlicht verboten. Arbeitgeber dürfen zum Beispiel keine Systeme installieren, die automatisch an Gesichtsausdruck oder Stimmlage erkennen, wie Mitarbeiter sich fühlen. Dass solche Regeln sinnvoll sind, dürfte vielen Menschen einleuchten. 

Bis zuletzt wurde darüber gestritten, wie man die Technik hinter den großen Sprachmodellen wie ChatGPT regulieren soll. Weil sie so viele mögliche Anwendungen hat, lässt sie sich nur schwer in eine der Risikoklassen einsortieren. Die sogenannten KI-Basismodelle (engl.: foundation models) können nicht nur Chatbots antreiben, sondern zum Beispiel auch die Software von Rechtsanwaltskanzleien oder Personalabteilungen ergänzen, um kritische Entscheidungen zu treffen.

Begeht ein solches System einen Fehler, könnte das an falscher Anwendung liegen – oder am zugrunde liegenden Basismodell. Um das nachvollziehen zu können, müssen die Anbieter solcher Modelle den Anwendern entsprechende Informationen zur Verfügung stellen. Das heißt: Große KI-Entwickler wie OpenAI oder Google müssen deutschen Mittelständlern so viele technische Details verraten, dass diese ""ein gutes Verständnis der Möglichkeiten und Grenzen"" der KI-Basissysteme entwickeln können. 

Für die Anbieter von besonders großen und technisch ausgereiften KI-Systemen gelten noch strengere Regeln: Sie müssen zum Beispiel auch Maßnahmen zur Cybersicherheit ergreifen. Gerade solche Regeln hatten eine Reihe von KI-Forscherinnen und -Forschern vehement gefordert, weil sie davon ausgehen, dass von den leistungsfähigsten Modellen auch die größten Gefahren durch KI ausgehen. 

Ob für ein System diese strengen Regeln gelten, hängt unter anderem von der Rechenleistung ab, die für das Training der KI verwendet wird. Die Schwelle liegt so hoch, dass sie derzeit wohl kaum ein auf dem Markt verfügbares System überschreitet. 

Unter anderem die Bundesregierung hatte sich in den Verhandlungen dafür eingesetzt, dass für Basismodelle gar keine speziellen Vorschriften gelten sollten. Dabei spielte auch die Sorge eine Rolle, die Regeln aus Brüssel könnten europäische Unternehmen gegenüber der Konkurrenz aus den USA oder China massiv benachteiligen. Darauf haben im Vorfeld sowohl das französische KI-Unternehmen Mistral AI wie auch das deutsche Start-up Aleph Alpha hingewiesen, zwei der vielversprechendsten europäischen KI-Hoffnungen, die selbst Basismodelle entwickeln. Sie treibt die Furcht um, der AI Act könnte ihre Arbeit ausbremsen, bevor sie richtig in Schwung gekommen ist. In diesem Fall überließen die Europäer die KI-Entwicklung (und die künftige Dominanz in diesem Bereich) wieder einmal amerikanischen Großkonzernen.
"
Artificial Intelligence,Zeit,2024-02-07,https://www.zeit.de/wirtschaft/unternehmen/2024-02/boeing-flugzeug-alaska-airlines-bolzen-untersuchung-tuer,Boeing: Flugzeugtür fehlten vier Bolzen | ZEIT ONLINE,"In der Boeing 737 MAX 9 der US-Fluggesellschaft Alaska Airlines, die wegen eines herausgerissenen Kabinenwandteils notlanden musste, waren laut der US-Verkehrssicherheitsbehörde NTSB nötige Befestigungsteile nicht vorhanden. Die Behörde veröffentlichte eine vorläufige Untersuchung, der zufolge in dem betroffenen Flugzeugteil vier Bolzen fehlten. Die Bolzen sollen eigentlich sicherstellen, dass sich das Rumpfteil, das während des Fluges herausgerissen wurde, nicht nach oben bewegen kann. 

Die Behörde sammelte Dokumente und Fotos, aus denen hervorgeht, dass Boeing-Mitarbeiter bei einer Inspektion im Werk Renton im US-Bundesstaat Washington vor der Auslieferung des Flugzeugs im Oktober vier Bolzen an den betroffenen Stellen entfernt hatten, um beschädigte Teile im Innenraum des Flugzeugs zu ersetzen. Dem Untersuchungsbericht ist ein Foto des Flugzeugbauers beigefügt, der an dem sogenannten Stopfen arbeitete, der einen nicht benötigten Ausgang in der Maschine abdecken sollte. Drei von vier Schrauben, die verhindern sollen, dass der Stopfen sich nach oben bewegt, fehlen auf dem Bild. Wo die vierte Schraube ist, lässt sich nicht erkennen.

Gutachter erklärten, das Ausbleiben von bestimmten Schäden am Stopfen deute darauf hin, dass alle vier Schrauben schon vor dem Start der Maschine in Portland im Staat Oregon fehlten.  

Bei der betroffenen Boeing 737 Max 9 war am 5. Januar kurz nach dem Start im Steigflug ein Rumpfteil an der Sitzreihe 26 
herausgebrochen. An dieser Stelle haben manche Konfigurationen des Typs 
mit mehr Sitzen eine Tür. Die betroffene Variante der 737 Max 9 hat 
stattdessen eine Abdeckung, die die Öffnung verschließt. Daraufhin musste die Alaska-Airlines-Maschine nach Portland im US-Bundesstaat Oregon zurückkehren und notlanden. Verletzt wurde bei dem Vorfall niemand, durch einen glücklichen Zufall waren die beiden Plätze direkt an der Öffnung leer geblieben.  Experten zufolge hätte es aber zu einer Katastrophe kommen können.

Erst Ende Januar gab die FAA das Verfahren für Inspektionen frei, nach denen die Flugzeuge wieder starten durften. Bei EU-Fluggesellschaften sind keine Maschinen des betroffenen Modells im Einsatz. Der US-Flugzeughersteller hatte nach dem Vorfall ""vollständige Transparenz"" über die Mängel von 737 Max 9-Maschinen angekündigt. Der Konzern ging von einem Qualitätskontrollproblem aus. Nach der Notlandung in Oregon hatte die US-Flugaufsichtsbehörde (FAA) für rund 170 Maschinen des Typs ein vorübergehendes Flugverbot angeordnet. Erst nach Inspektionen bestimmter Exemplare des Modells könnten die betroffenen Flugzeuge wieder in Betrieb gehen. Die Fluggesellschaft United Airlines, die über die weltweit größte 
Flotte der betroffenen Flugzeuge verfügt, hatte mitgeteilt, 
sie habe bei Inspektionen der Boeing-Maschinen lockere Schrauben 
gefunden.

Der US-Flugzeugbauer hatte bereits im Dezember, vor dem Vorfall in Oregon, Probleme mit dem Passagierflugzeugmodell 737 Max gemeldet. Die US-Luftfahrtaufsicht FAA teilte zu der Zeit mit, dass der Airbus-Konkurrent die Fluggesellschaften dazu aufgefordert habe, neuere Maschinen des Typs auf eine mögliche lockere Schraube im Rudersteuersystem zu überprüfen. Boeing berichtete später, das Problem, das bei einem Flugzeug festgestellt worden sei, sei behoben worden.  
"
KI,Zeit,2024-02-08,https://www.zeit.de/digital/2024-02/usa-kuenstliche-intelligenz-fcc-verbot-anrufe,Künstliche Intelligenz: USA verbieten automatisierte Anrufe mit KI-generierten Stimmen | ZEIT ONLINE,"Die Kommunikationsaufsicht der USA hat automatisierte
Werbeanrufe mit von künstlicher Intelligenz generierten Stimmen verboten. Die Telekom-Behörde FCC kann damit Bußgelder gegen
Unternehmen verhängen, die KI-Stimmen in ihren Anrufen verwenden, oder
Diensteanbieter sperren, die solche Stimmen übertragen. 

Für Anrufe mit von
Künstlicher Intelligenz generierten Stimmen sei die vorherige Zustimmung der
Empfänger unerlässlich, teilte die FCC mit. Auch müssten sich die Urheber
solcher Anrufe klar zu erkennen geben. Die Entscheidung
tritt sofort in Kraft. Verstöße können laut FCC mit mehr als 23.000
Dollar pro Anruf geahndet werden.

In den USA hatten vor wenigen Wochen automatisierte Anrufe
mit einer täuschend echt klingenden Nachahmung der Stimme von Präsident Joe
Biden für Aufsehen gesorgt. Die Botschaft der Anrufe war, nicht an der Vorwahl
der Demokratischen Partei im Bundesstaat New Hampshire teilzunehmen. 

Der Vorfall schürte die Sorge, in den kommenden Monaten
könnte es Versuche geben, den Ausgang der Präsidentenwahl im November mit der
Verbreitung von KI-Fälschungen zu beeinflussen. Behörden konnten als Urheber
der Anrufe inzwischen eine Firma aus Texas ausmachen und gehen gegen sie vor.

""Böswillige Akteure verwenden KI-generierte Stimmen in
unerbetenen Anrufen, um angreifbare Familienmitglieder zu erpressen, Prominente
zu imitieren und Wähler falsch zu informieren"", teilte die Vorsitzende der
Behörde, Jessica Rosenworcel, mit. ""Darüber setzen wir die Betrüger, die hinter
diesen Anrufen stecken, in Kenntnis."" 
"
KI,Zeit,2024-02-08,https://www.zeit.de/news/2024-02/08/ki-system-bard-von-google-heisst-kuenftig-gemini,Künstliche Intelligenz: KI-System Bard von Google heißt künftig Gemini | ZEIT ONLINE,"Google wird künftig seine Apps und Dienste mit Künstlicher Intelligenz unter der Marke Gemini zusammenfassen. Bislang trug nur ein Sprachmodell den Namen Gemini. Der Chat-Bot von Google hieß bislang Bard, benannt nach dem englischen Dichter William Shakespeare (Spitzname: «The Bard of Avon»). Dieser Name für den Herausforderer von ChatGPT von OpenAI werde künftig verschwinden, kündigte Konzern-Chef Sundar Pichai an.

In den USA wird Gemini von sofort an auch auf Smartphones zur Verfügung stehen. Handys mit dem Google-Betriebssystem Android erhalten eine eigene Gemini-App. Auf dem iPhone von Apple wird Gemini innerhalb der bestehenden Google-App angeboten. Dabei löst das KI-System auch den bisherigen Sprachassistenten Google Assistent ab, wenn die Nutzerinnen und Nutzer dem Umstieg zustimmen.

Wann die Apps in Europa und in Deutschland zur Verfügung stehen werden, teilte Google nicht mit. Eine Woche nach dem Start in den USA sollen die Apps in weiteren englischsprachigen Ländern sowie auf Japanisch und Koreanisch in Japan und Südkorea eingeführt werden. Erfahrungsgemäß folgen andere Sprachen wie Spanisch und Deutsch mit Abstand von einigen Monaten.

Gleichzeitig mit den Smartphone-Apps bringt Google außerdem eine bezahlpflichtige KI-Profi-Version - Gemini Advanced - auf den Markt, die auf dem neuen Sprachmodell Ultra 1.0 aufsetzt. Sie wird vorerst in 150 Ländern der englischsprachigen Welt angeboten und ist Teil des Abo-Dienstes Google One. Der KI-Dienst kostet knapp zehn US-Dollar im Monat, ist aber nur in der teuersten Variante von Google One verfügbar, die zusätzlich zehn Dollar kostet. Dafür erhalten die Kundinnen und Kunden unter anderem einen Cloud-Speicherplatz von zwei Terabyte. Wann der Dienst in Europa und Deutschland gebucht werden kann, wurde ebenfalls nicht mitgeteilt.

Google-Managerin Sissie Hsiao erklärte in einem Blog-Eintrag, Gemini Advanced sei weitaus leistungsfähiger bei hochkomplexen Aufgaben wie dem Schreiben von Programmcode, logischem Denken, dem Befolgen differenzierter Anweisungen und der Zusammenarbeit an kreativen Projekten. «Mit Gemini Advanced können Sie nicht nur längere, detailliertere Gespräche führen, sondern er versteht auch den Kontext Ihrer vorherigen Aufforderungen besser.»

© dpa-infocom, dpa:240208-99-918712/2
"
KI,Zeit,2024-02-07,https://www.zeit.de/2024/07/ai-act-eu-gesetz-kuenstliche-intelligenz-regulierung,AI Act: Regulierte Intelligenz | ZEIT ONLINE,"Die EU hofft auf den ""Brüssel-Effekt"", der selbst amerikanische Großkonzerne erfassen soll. Denn die Konzerne, so das Kalkül, wollen nicht auf ihre Kunden in der Europäischen Union verzichten; und da es teuer und aufwendig wäre, Produkte nach unterschiedlichen Standards herzustellen, richten sie sich am Ende nach den Regeln des am strengsten regulierten Marktes – also des europäischen. Ein positives Beispiel für diesen Effekt ist die EU-Datenschutz-Grundverordnung, an die sich etwa Microsoft weltweit hält. 

Wie stark allerdings der AI Act die KI-Entwickler außerhalb der EU beeinflusst, wird davon abhängen, wo deren Systeme genutzt werden. Wenig Folgen wird der AI Act für lokale Anwendungen haben, die auf ein bestimmtes Land zugeschnitten sind – etwa Systeme zur Bemessung der Kreditwürdigkeit von US-Amerikanern oder KI-Anwendungen in britischen Behörden. 

Anders sieht es aus bei Konsumprodukten wie Bildgeneratoren oder Chatbots, die länderübergreifend über Plattformen betrieben werden. Hier könnten sich die Standards des AI Act international durchsetzen. Gleiches gilt für Anwendungen, die von der EU als ""hochriskant"" eingestuft werden und an die sie besonders hohe Sicherheitsanforderungen stellt. 

Der einschlägigen Forschung könnte das Aufträge und Fördergelder bescheren. Denn um zu überprüfen, ob die KI-Systeme den Regeln der EU entsprechen, wird es künftig erhebliche wissenschaftliche Anstrengungen brauchen; ebenso, um KI-Systeme zu entwickeln, deren Funktionsweise für Dritte nachvollziehbar ist, die faire Entscheidungen treffen und robust sind gegenüber Cyberangriffen. 

Und dann sind da noch die neuen Transparenzpflichten: Der AI Act sieht zum Beispiel vor, dass Unternehmen ihre Trainingsdaten offenlegen müssen. Bisher konnten die großen Tech-Konzerne ihre Marktmacht auch deshalb ausbauen, weil sie ihre Datensätze geheim hielten – und unabhängige Entwickler kaum eine Chance hatten, ihre Maschinen mit nur annähernd so großen Datenmengen lernen zu lassen. Diese Geheimhaltung soll nach dem Willen der EU ein Ende haben. Ob und wie sich die Transparenzpflicht allerdings konkret auswirkt, wird sich wohl erst zeigen, wenn der AI Act in Kraft tritt.

Der Kern des Gesetzes besteht darin, dass KI-Anwendungen in Risikoklassen eingeteilt werden. Programme mit geringem Risiko werden kaum reguliert. Für Anwendungen mit hohem Risiko hingegen gelten spezielle Regeln. 

Ein Hochrisikofall wäre zum Beispiel eine KI, die bei der Entscheidung hilft, ob ein Mensch Arbeitslosengeld bekommt. Wenn dieses Programm falsche Ratschläge gibt oder bestimmte Gruppen gegenüber anderen benachteiligt, könnte das schwerwiegende Folgen für die Betroffenen haben. Hochrisikoanwendungen sind auch Systeme, die etwa bei der Bearbeitung von Asylanträgen eingesetzt werden könnten oder die Personalabteilungen dabei unterstützen, Bewerbungsunterlagen nach geeigneten Kandidatinnen und Kandidaten zu durchsuchen. 

Unternehmen, die solche Programme entwickeln oder einsetzen, müssen Auflagen erfüllen, um die Risiken zu minimieren. Zum Beispiel müssen sie sicherstellen, dass die Daten, mit denen die KI trainiert wurde, die Menschen, die sie betrifft, angemessen repräsentieren. Außerdem müssen Menschen die Entscheidungen der KI überwachen und überprüfen können. 

Das kann natürlich nicht garantieren, dass mit solcher Technik nie etwas schiefgeht. Dennoch halten viele Fachleute den Ansatz für richtig, nicht die künstliche Intelligenz grundsätzlich zu regulieren, sondern jeweils nur bestimmte Anwendungen dieser Technik. Auch ein sehr simples KI-System kann großen Schaden anrichten, wenn es in einem kritischen Bereich eingesetzt wird.

Ein AI Act ist besser als kein AI Act. So argumentieren, etwas zugespitzt, die Befürworter des neuen EU-Gesetzes zur Regulierung der künstlichen Intelligenz (KI, englisch: artificial intelligence, AI). Noch vergangene Woche hatten Wirtschaftsverbände, Forscher und zivilgesellschaftliche Organisationen gefordert, dieses KI-Gesetz, den AI Act, nicht noch scheitern zu lassen. Ein ""fehlender Rechtsrahmen"" wäre ""riskant für Grundrechtsschutz und Innovationen in Europa"", hieß es in einem von mehreren offenen Briefen. Die Sorge war unbegründet: Vergangenen Freitag wurde der AI Act im Rat der Europäischen Union angenommen. Noch muss Europas Parlament zustimmen, aber ein wichtiger Schritt ist getan zum ersten umfassenden KI-Gesetz der Welt, um das es jahrelang Gezerre gegeben hatte.

Seit 2021 ist das Gesetz in Arbeit. Doch erst als ChatGPT im November 2022 den weltweiten KI-Hype auslöste, wurde vielen klar, dass künstliche Intelligenz – ähnlich wie das Internet – künftig jeden Lebensbereich verändern könnte. Das beflügelte den Wunsch nach Regeln zur Anwendung der Technologie. 

Umstritten war allerdings bis zuletzt, wie weit diese Regeln gehen sollen. In der Bundesregierung formulierte vor allem Digitalminister Volker Wissing die Sorge, dass zu viel Regulierung europäische Unternehmen daran hindern könnte, gegenüber der amerikanischen Konkurrenz aufzuholen. Wissing konnte sich allerdings im Kabinett nicht durchsetzen; auch die Bundesregierung stimmte wie alle anderen Mitgliedsstaaten dem AI Act zu. 

Darin geht es vorrangig um Regeln für Unternehmen, die KI-Systeme entwickeln oder einsetzen. Nicht alle sind glücklich mit den Pflichten, die auf sie zukommen; dennoch sprachen sich selbst einige Unternehmen für das Gesetz aus, um Rechtssicherheit zu haben. 

Der AI Act soll unter anderem sicherstellen, dass automatisierte Entscheidungen, von denen Menschen betroffen sind, fair und nachvollziehbar sind. Manche KI-Anwendungen werden auch schlicht verboten. Arbeitgeber dürfen zum Beispiel keine Systeme installieren, die automatisch an Gesichtsausdruck oder Stimmlage erkennen, wie Mitarbeiter sich fühlen. Dass solche Regeln sinnvoll sind, dürfte vielen Menschen einleuchten. 

Bis zuletzt wurde darüber gestritten, wie man die Technik hinter den großen Sprachmodellen wie ChatGPT regulieren soll. Weil sie so viele mögliche Anwendungen hat, lässt sie sich nur schwer in eine der Risikoklassen einsortieren. Die sogenannten KI-Basismodelle (engl.: foundation models) können nicht nur Chatbots antreiben, sondern zum Beispiel auch die Software von Rechtsanwaltskanzleien oder Personalabteilungen ergänzen, um kritische Entscheidungen zu treffen.

Begeht ein solches System einen Fehler, könnte das an falscher Anwendung liegen – oder am zugrunde liegenden Basismodell. Um das nachvollziehen zu können, müssen die Anbieter solcher Modelle den Anwendern entsprechende Informationen zur Verfügung stellen. Das heißt: Große KI-Entwickler wie OpenAI oder Google müssen deutschen Mittelständlern so viele technische Details verraten, dass diese ""ein gutes Verständnis der Möglichkeiten und Grenzen"" der KI-Basissysteme entwickeln können. 

Für die Anbieter von besonders großen und technisch ausgereiften KI-Systemen gelten noch strengere Regeln: Sie müssen zum Beispiel auch Maßnahmen zur Cybersicherheit ergreifen. Gerade solche Regeln hatten eine Reihe von KI-Forscherinnen und -Forschern vehement gefordert, weil sie davon ausgehen, dass von den leistungsfähigsten Modellen auch die größten Gefahren durch KI ausgehen. 

Ob für ein System diese strengen Regeln gelten, hängt unter anderem von der Rechenleistung ab, die für das Training der KI verwendet wird. Die Schwelle liegt so hoch, dass sie derzeit wohl kaum ein auf dem Markt verfügbares System überschreitet. 

Unter anderem die Bundesregierung hatte sich in den Verhandlungen dafür eingesetzt, dass für Basismodelle gar keine speziellen Vorschriften gelten sollten. Dabei spielte auch die Sorge eine Rolle, die Regeln aus Brüssel könnten europäische Unternehmen gegenüber der Konkurrenz aus den USA oder China massiv benachteiligen. Darauf haben im Vorfeld sowohl das französische KI-Unternehmen Mistral AI wie auch das deutsche Start-up Aleph Alpha hingewiesen, zwei der vielversprechendsten europäischen KI-Hoffnungen, die selbst Basismodelle entwickeln. Sie treibt die Furcht um, der AI Act könnte ihre Arbeit ausbremsen, bevor sie richtig in Schwung gekommen ist. In diesem Fall überließen die Europäer die KI-Entwicklung (und die künftige Dominanz in diesem Bereich) wieder einmal amerikanischen Großkonzernen.
"
KI,Zeit,2024-02-07,https://www.zeit.de/news/2024-02/07/ard-experiment-schauspieler-unterhalten-sich-mit-einer-ki,Fernsehen: ARD-Experiment: Schauspieler unterhalten sich mit einer KI | ZEIT ONLINE,"Die Digital-Plattform ARD Kultur hat Schauspieler vor der Kamera mit einer Künstlichen Intelligenz (KI) experimentieren lassen. «In einem Kammerspiel, das um einen Streit der fiktiven Girl-Band «Boom» kreist, trifft das Schauspielensemble auf eine KI, die eigens für die Produktion entwickelt wurde.» Zum Cast gehören unter anderem Alicia von Rittberg und Lea Drinda. In Gastrollen dabei sind Collien Ulmen-Fernandes, Gülcan Kamps und Eko Fresh, wie ARD Kultur am Mittwoch in Weimar mitteilte.

Die Science-Fiction-Dramedy «Boom - Eine Band, 1000 Probleme» ist seit Mittwoch in der ARD Mediathek und bei ardkultur.de verfügbar. «Inspiriert von den letzten dramatischen Minuten der berühmten 90er-Band Tic Tac Toe erzählt «Boom» in einem improvisierten Kammerspiel eine eigene Geschichte in einer (vielleicht) nicht weit entfernten Zukunft.» Das Projekt werfe «einen Blick in die Zukunft des fiktionalen Erzählens» und erforsche den Einsatz sprachgesteuerter KI.

«Zum ersten Mal übernimmt eine KI in einem fiktionalen Projekt eine eigenständige Rolle. «Boom» ist somit der erste Film, in dem Schauspielerinnen zusammen mit einer KI live am Set improvisieren. Und die KI fordert das Schauspielensemble mit ihren unberechenbaren Reaktionen ziemlich heraus», erläuterte der Programmgeschäftsführer von ARD Kultur, Kristian Costa-Zahn, in der Mitteilung.

Worum es geht? Kurz bevor Deutschlands erfolgreichste Girl-Band Boom der Presse ihr neues Album vorstellen will, geraten die drei Mitglieder - Sue (Via Jikeli), Peggy (Lea Drinda) und Izzy (Sira-Anna Faal) - in einem Hotelzimmer in Streit. «Hinter den Kulissen überschlagen sich die Ereignisse, und die Zukunft der Band steht auf dem Spiel. Um die Situation zu retten, wird die introvertierte Band-Assistentin Paule (Alicia von Rittberg) vorgeschickt. Als Bandmitglied Izzy nicht aufzufinden ist, soll ein brandneuer KI-Zwilling Abhilfe schaffen: Eine sprachgesteuerte Box, die in wenigen Tagen als Boom-Merchandise-Artikel auf den Markt kommen soll. Als die KI brisante Geheimnisse der Band offenbart, spitzt sich der Konflikt zu.» In einem 54-minütigen Kammerspiel müssten sich die Popstars mit der unkontrollierbaren KI auseinandersetzen und herausfinden, wer sie wirklich sein wollen.

Für das KI-Experiment wurde eigens ein Sprachassistent entwickelt, der am Set selbstständig mit den Schauspielerinnen improvisieren konnte. «Basierend auf verschiedenen KI-Anwendungen und Informationen zu den Figuren, wurde der Assistent vor den Dreharbeiten programmiert: Dafür wurde er vom Drehbuch-Team mit zahlreichen Informationen zu den Figuren gefüttert, wie es in der Mitteilung heißt. Audio-Files der echten Stimme von Izzy komplettierten den Assistenten und formten ihn zur eigenständigen Figur.

© dpa-infocom, dpa:240207-99-904569/4
"
Künstliche Intelligenz,Zeit,2024-02-06,https://www.zeit.de/digital/2024-02/nancy-faeser-cybersicherheit-lagezentrum-bonn,IT-Sicherheit: Nancy Faeser eröffnet neues Zentrum für Cybersicherheit in Bonn | ZEIT ONLINE,"In Bonn hat Bundesinnenministerin Nancy Faeser (SPD) das neue IT-Lagezentrum des Bundesamts für Sicherheit in der Informationstechnik (BSI) eröffnet. Dort sollen Spezialisten unterschiedlicher Fachrichtungen die Cybersicherheitslage Deutschlands rund um die Uhr überwachen und mit nationalen und internationalen Sicherheitsakteuren Informationen austauschen, teilte das Innenministerium mit. 

Faeser betonte bei der Eröffnung, die Bedrohungslage für Cybersicherheit sei hoch. Cyberangriffe von Hackern und staatlichen Akteuren würden immer weiter zunehmen, zudem bestehe Gefahr auch durch Desinformation, Manipulationen und von künstlicher Intelligenz generierte Fälschungen. ""Wir wappnen uns gegen diese Bedrohungen"", sagte Faeser.

Die neue Cybersicherheitsanlage verfügt im Regelbetrieb über zehn Arbeitsplätze und soll nach Angaben des BSI pro Jahr rund 2.800 Meldungen zu IT-Sicherheitsvorfällen und Sicherheitslücken auswerten. Zudem soll es bei besonderen Vorfällen oder in IT-Krisen zum nationalen IT-Krisenreaktionszentrum werden. Im Ernstfall könnten so bis zu 100 Sicherheitsfachkräfte orchestriert zusammenarbeiten, teilte das Innenministerium mit. Zudem könnten, je nach Bedrohungslage, auch weitere Stellen eingebunden werden, wie beispielsweise das Nationale Cyber-Abwehrzentrum.  

""Mit dem neuen Nationalen IT-Lagezentrum haben wir die Infrastruktur geschaffen, die wir benötigen, um die Cybersicherheit in Deutschland substanziell zu erhöhen"", sagte BSI-Chefin Claudia Plattner. Als Nächstes müsse die Cybersicherheitsarchitektur in Deutschland verbessert werden, wozu das BSI als Zentralstelle dienen solle. Damit könne endlich eine einheitliche und präzise Einschätzung von Bedrohungslagen erreicht werden.

Auch Faeser unterstützte die Pläne des BSI und forderte eine bessere Zusammenarbeit zwischen Bund und Ländern beim Kampf gegen Cyberangriffe. Das BSI als Zentralstelle bezeichnete sie als ""unverzichtbaren Baustein für unsere Cybersicherheit"". Bundesinnenministerium und BSI hätten den Ländern ein Konzept vorgelegt und bereiteten die notwendige Gesetzesänderung vor.

Das BSI hatte zuletzt jeden Tag etwa 70 neue IT-Schwachstellen festgestellt. Die größte Gefahr gehe demnach von Angriffen mit sogenannter Ransomware aus. Dabei werden Daten verschlüsselt und erst gegen Lösegeldzahlung wieder freigegeben.
"
Artificial Intelligence,Zeit,2024-02-05,https://www.zeit.de/digital/internet/2024-01/manfred-spitzer-digitale-medien-buecher,Manfred Spitzer: Der Über-Spitzer | ZEIT ONLINE,
KI,Zeit,2024-02-06,https://www.zeit.de/news/2024-02/06/faeser-verspricht-mehr-schutz-vor-ki-manipulation,Internet: Faeser verspricht mehr Schutz vor KI-Manipulation | ZEIT ONLINE,"Mit einem technisch runderneuerten IT-Lagezentrum kann nach Einschätzung von Bundesinnenministerin Nancy Faeser künftig ein besserer Schutz der Systeme des Bundes und der Bürger vor Manipulation und Desinformation gewährleistet werden.

""Es gilt unsere Demokratie auch im Digitalen zu schützen"", sagte die SPD-Politikerin der Deutschen Presse-Agentur. Das neue Nationale IT-Lagezentrum beim Bundesamt für Sicherheit in der Informationstechnik (BSI) eröffnete sie gemeinsam mit BSI-Präsidentin Claudia Plattner.

""Hier laufen die Fäden zusammen, um unsere Systeme zu schützen"", erklärte Faeser zu der erneuerten Einrichtung. Die Bedrohungslage für die Cybersicherheit sei hoch. ""Cyberangriffe von staatlichen Akteuren wie von Hackern nehmen immer mehr zu.""

In diesem Jahr, in dem neben der Europawahl auch drei Landtagswahlen in ostdeutschen Bundesländern anstehen, müsse man sich besonders gegen Bedrohungen durch Hackerangriffe, Manipulationen und Desinformation wappnen, betonte sie. Diese Angriffe zielten nicht nur auf einzelne Politiker und Politikerinnen, sondern auch darauf, das Vertrauen in die Demokratie zu erschüttern. Künstliche Intelligenz (KI) könne es Kriminellen oder Geheimdiensten ermöglichen, Bürger leichter zu manipulieren und öffentliche Debatten mit Lügen und Propaganda zu beeinflussen.

Ebenso wichtig sei der Schutz der Wahlbehörden vor Hackerangriffen sowie die sichere Übermittlung von Wahlergebnissen. ""Diese Bedrohungen haben wir fest im Blick - und nehmen sie angesichts der Wahlen in diesem Jahr auch besonders ernst"", sagte Faeser.

Das BSI berät Kandidierende und Parteien auch zum sicheren Umgang mit Social Media. Es hat ein IT-Grundschutz-Profil ""Schnellmeldungen – Absicherung der Schnellmeldungen bei bundesweiten parlamentarischen Wahlen"" entwickelt, das über den Bundeswahlleiter an die Landeswahlleitungen verteilt wurde. Dabei geht es um Vorkehrungen für eine sichere Ermittlung des vorläufigen Wahlergebnisses bundesweiter Wahlen.

Das bisher genutzte IT-Lagezentrum des BSI sei bereits über zehn Jahre alt und habe damit nicht mehr dem Stand der Technik entsprochen, erklärte eine Sprecherin des Bundesinnenministeriums. Das neue Lagezentrum ist nach Angaben des BSI nun mit ""modernster Kommunikationstechnik"" ausgestattet. Es verfüge im Regelbetrieb über zehn Arbeitsplätze, von denen aus die Cybersicherheitslage für Deutschland rund um die Uhr im Blick behalten werden könne. Rund 2800 Meldungen zu IT-Sicherheitsvorfällen und Sicherheitslücken empfange das Lagezentrum pro Jahr.

BSI-Präsidentin Plattner erklärte, dass mit dem neuen IT-Lagezentrum die Infrastruktur geschaffen sei, die benötigt werde, um die Cybersicherheit in Deutschland ""substanziell"" zu erhöhen. Sie ging aber noch weiter. ""Der nächste Schritt, der dafür nötig ist, ist die Verbesserung der Cybersicherheitsarchitektur in Deutschland - mit dem BSI als Zentralstelle im Bund-Länder-Verhältnis"", sagte sie. ""Mit einer Zentralstelle hätten wir endlich ein einheitliches und präzises nationales Lagebild für eine echte Adhoc-Einschätzung der Bedrohungslage, zudem könnten die Länder unser Schadprogramm-Erkennungssystem mitnutzen und Gefahren besser antizipieren"", sagte sie. Das alles könne in dem neuen Lagezentrum zentral koordiniert werden - um Deutschland ""auch in der Fläche"" gegen Gefahren aus dem Cyberraum abzusichern. Zuletzt hat das BSI nach eigenen Angaben durchschnittlich 70 neue Schwachstellen pro Tag festgestellt.

Zusammen mit der Eröffnung des Lagezentrums wurde auch der Start zum Aufbau einer ""Cybernation Deutschland"" ausgerufen. Die Initiative soll nach Angaben des BSI unter anderem mehr Bewusstsein für das Thema Cybersicherheit schaffen.

© dpa-infocom, dpa:240206-99-884268/4
"
KI,Zeit,2024-02-05,https://www.zeit.de/news/2024-02/05/ki-pionier-hochreiter-fordert-openai-mit-firma-heraus,Informationstechnologie: KI-Pionier Hochreiter fordert OpenAI mit Firma heraus | ZEIT ONLINE,"Der renommierte deutsche Forscher Sepp Hochreiter will mit einem neuen Unternehmen die Vorherrschaft des Marktführers für Künstliche Intelliganz, OpenAI, infragestellen. 

Der Träger des deutschen KI-Innovationspreises kündigte an, zusammen mit österreichischen Industriepartnern in Kooperation mit der Universität Linz das Unternehmen NXAI zu starten. Mit der Finanzierung der Firma werde gewährleistet, dass ein neuer Ansatz für ein europäisches KI-Sprachmodell entwickelt werden kann, das global wettbewerbsfähig ist.

Hochreiter hatte in den 90er Jahren den Algorithmus Long Short-Term Memory (LSTM) erfunden, der viele KI-Anwendungen radikal verbesserte, darunter die Übersetzung von Sprachen, die Vorhersage von Krankheiten auf Basis medizinischer Daten oder die Spracherkennung auf Smartphones. Eine Neuauflage von LSTM hat nach seiner Darstellung gute Chancen, sich nicht nur gegen die von Google und OpenAI verwendeten KI-Modelle zu behaupten, sondern diese zu übertreffen.

An der Unternehmensgründung ist auch das Linzer Start-up Netural X sowie die Pierer Digital Holding aus der österreichischen Industriegruppe Pierer beteiligt. Zu den geschäftlichen Details machten die Beteiligten keine Angaben. Der finanzielle Rahmen soll aber ausreichen, damit Hochreiters Algorithmus weiter erforscht und zu einer Anwendung im großen Maßstab geführt werden kann. 

«Die notwendigen Rechenkapazitäten sind durch NXAI sichergestellt, sodass das Large Language Model mit großen Datenmengen trainiert und skaliert werden kann.» Hochreiter hatte in Interviews erläutert, dass er 54 Millionen Euro im Jahr benötige, um OpenAI herauszufordern - 4 Millionen für das Personal und 50 Millionen für Rechenkapazitäten.

Bernhard Schölkopf, Direktor am Max-Planck-Institut für lernende Systeme in Tübingen, hatte zur Verleihung des KI-Innovationspreises die internationale Bedeutung der von Hochreiter erfundenen KI-Methode hervorgehoben. LSTM sei «wirklich fundamental und einer der bekanntesten Algorithmen überhaupt». Mit der Neuauflage, die Hochreiter xLSTM nennt, versteht das System nach Angaben der Firma die Bedeutung von Texten besser als die bisherigen Sprachmodelle und kann dadurch auch komplizierte Texte verstehen und erstellen.

Im Vergleich zu den Modellen, die Google und OpenAI verwenden, soll die benötigte Rechenleistung in Hochreiters Modell deutlich geringer sein. Mit ihrer gesteigerten Effizienz und Leistung in der Verarbeitung von Texten werde die xLSTM-Technologie einen neuen Standard in der KI-Sprachverarbeitung setzen.

Vor der Gründung von NXAI war es auch anderen Start-ups aus Europa gelungen, die KI-Vormachtstellung der Techriesen aus den USA zumindest in Teilen infrage zu stellen. So produziert der KI-Übersetzer des Kölner Start-ups DeepL bessere Übersetzungen als Google Translator. Dafür unterstützt der Google-Übersetzer mehr Sprachen.

© dpa-infocom, dpa:240205-99-873401/2
"
Künstliche Intelligenz,Zeit,2024-02-02,https://www.zeit.de/digital/2024-02/ai-act-kuenstliche-intelligenz-eu,AI Act: EU-Länder stimmen Regulierung von künstlicher Intelligenz zu | ZEIT ONLINE,"Die Länder der Europäischen Union haben sich für umfassende Regeln für künstliche Intelligenz ausgesprochen. Die ständigen Vertreter der Mitgliedsländer in Brüssel stimmten dem KI-Gesetz zu, wie EU-Binnenmarktkommissar Thierry Breton auf X mitteilte. Breton nannte die Einigung historisch. 

Wegen Bedenken unter anderem in Deutschland und Frankreich hatte der Beschluss wochenlang auf der Kippe gestanden. Nun müssen nur noch der Ministerrat und das Europaparlament abschließend zustimmen.

""Die KI-Verordnung soll dafür sorgen, dass wir in Europa das enorme Potenzial von KI heben und gleichzeitig auch Risiken in den Blick nehmen"", sagte Bundeswirtschaftsminister Robert Habeck. Diese Balance sei mit dem AI Act gelungen.

Unterhändler der Mitgliedsstaaten und des Europaparlaments hatten sich bereits im Dezember grundsätzlich auf die KI-Verordnung geeinigt. Damit müssen Entwickler künftig mit künstlicher Intelligenz erzeugte Texte, Töne und Bilder eindeutig kennzeichnen, um Menschen nicht in die Irre zu führen. Für ""risikoreiche"" Anwendungen sollen weitere Vorschriften gelten, etwa für die Gesichtserkennung in Sicherheitsbehörden. Grundsätzlich verboten wird eine Massenüberwachung mit biometrischen Daten wie in China.

In Deutschland hatten sich die Ampelparteien erst wenige Tage vor der Abstimmung in Brüssel darauf geeinigt, der KI-Verordnung zuzustimmen. Die FDP hatte Bedenken an dem Gesetz angemeldet, nachdem aus der Wirtschaft Befürchtungen geäußert wurden, der AI Act könne zu strenge Auflagen für Unternehmen beinhalten.

Die Verordnung ordnet KI-Anwendungen künftig in unterschiedliche Risikoklassen ein. Anbieter müssen den jeweiligen Klassen entsprechende Sicherheits- und Transparenzanforderungen erfüllen. Experten zufolge könnte das Regelwerk zu einem Vorbild für Gesetze in anderen Ländern werden. Es wäre eine Alternative zu den eher lockeren Regeln, die in den USA gelten – und den restriktiveren Auflagen Chinas.

Bedenken kommen vom Digitalverband Bitkom. Das europäische KI-Gesetz allein garantiere noch keine Rechtssicherheit für Unternehmen, sagte Bitkom-Vorstandsmitglied Susanne Dehmel. Vielmehr komme es ""auf eine praktikable Auslegung und Anwendung der Vorgaben in den EU-Mitgliedsländern an"". Nur wenn bürokratische Hürden und unerwünschte Wechselwirkungen mit bestehenden Gesetzen vermieden würden, könnten sich europäische Firmen im weltweiten KI-Wettbewerb behaupten.
"
Künstliche Intelligenz,Zeit,2024-01-30,https://www.zeit.de/digital/2024-01/ai-act-ki-gesetz-bundesregierung-stimmt-zu,AI Act: Bundesregierung will europäischem KI-Gesetz doch zustimmen | ZEIT ONLINE,"Die Bundesregierung will dem AI Act nun doch zustimmen. Das bestätigte ein Sprecher des Verkehrsministerium ZEIT ONLINE. Um das Gesetz, mit dem die Europäische Union künstliche Intelligenz regulieren will, hatte es zuvor im Hintergrund Diskussionen gegeben. Das von Volker Wissing geführte Verkehrs- und Digitalministerium drängte Berichten zufolge darauf, dass Deutschland in Brüssel dem fertig verhandelten Gesetz nicht zustimmt.

In den vergangenen Tagen hatte ein breites Bündnis aus Wirtschaftsverbänden, zivilgesellschaftlichen Organisationen und Wissenschaftlern die Bundesregierung aufgefordert, das Gesetz nicht scheitern zu lassen. Das Hauptargument: Obwohl es nicht perfekt sei, drohe es in einem erneuten Anlauf noch schlechter zu werden. Außerdem sei es für Unternehmen wichtig, möglichst bald Rechtssicherheit zu haben. 

Besonders zur Frage, ob es für KI-Systeme wie ChatGPT besondere Regeln in dem Gesetz geben soll, gab es offenbar Meinungsverschiedenheiten. Die Regierungen von Frankreich, Italien und Deutschland sprachen sich dagegen aus, in der finalen Fassung des Gesetzes gibt es aber solche Vorschriften. Das war wohl der Grund, aus dem Wissing dem Gesetz zwischenzeitlich nicht zustimmen wollte. Er hatte wiederholt Bedenken geäußert, solche Regeln könnten Innovation behindern

Die Anbieter solcher Technologie müssten nach dem neuen Gesetz zum Beispiel Informationen darüber zur Verfügung stellen, wie ihre Systeme funktionieren. Für besonders leistungsfähige Systeme gelten noch weitere Pflichten. Dafür hatten sich viele Fachleute vehement eingesetzt, weil aus ihrer Sicht gerade von solchen KI-Modellen Risiken ausgehen könnten.

Ein weiterer Streitpunkt in der Diskussion um den AI Act sind Vorschriften zu biometrischer Überwachung. Mit dem Gesetz, auf das sich die EU-Organe im Dezember vergangenen Jahres inhaltlich einigten, sollten Regeln geschaffen werden für riskante Anwendungen von künstlicher Intelligenz. Dem Gesetz müssen noch die Mitgliedsstaaten der EU im Rat zustimmen und danach das Parlament. 
"
Künstliche Intelligenz,Zeit,2024-01-30,https://www.zeit.de/news/2024-01/30/deutschland-will-eu-gesetz-zu-ki-zustimmen,Künstliche Intelligenz: Deutschland will EU-Gesetz zu KI zustimmen | ZEIT ONLINE,"Einer Zustimmung Deutschlands zum EU-Gesetz über den Einsatz von Künstlicher Intelligenz (KI) steht nichts mehr im Weg. Bundesdigitalminister Volker Wissing (FDP) sagte in Berlin, das Ringen um die deutsche Haltung zum sogenannten AI Act sei mit einem ""tragbaren Kompromiss"" zu Ende gegangen. Die federführenden Ministerien für Justiz und Wirtschaft teilten mit, die Bundesregierung habe sich darauf verständigt, der KI-Verordnung am Freitag in Brüssel zuzustimmen.

Wissing sagte, er habe sich bis zuletzt für innovationsfreundlichere Regeln eingesetzt und Verbesserungen für kleine und mittlere Unternehmen erzielen können. Unverhältnismäßige Anforderungen seien abgewendet worden. Mit dem ausgehandelten Kompromiss werde nun ein Fundament gelegt für die Entwicklung vertrauenswürdiger KI. Europa solle zu einem bedeutenden KI-Standort entwickelt werden, der sich im weltweiten Wettbewerb behaupte.

Für den Einsatz von Künstlicher Intelligenz (KI) sollen in der EU künftig strengere Regeln gelten. Unterhändler von Europaparlament und EU-Staaten hatten sich im Dezember in Brüssel nach langen Verhandlungen auf entsprechende Regeln geeinigt. Nach Angaben des EU-Parlaments handelt es sich um das weltweit erste KI-Gesetz.

Die EU-Kommission hatte das Gesetz im April 2021 vorgeschlagen. Demnach sollen KI-Systeme in verschiedene Risikogruppen eingeteilt werden. Je höher die potenziellen Gefahren einer Anwendung sind, desto höher sollen die Anforderungen sein. Die Hoffnung ist, dass die Regeln weltweit Nachahmer finden.

Justizminister Marco Buschmann (FDP) sagte, damit Europa KI-Pionier sein könne, brauche es innovationsfördernde und klare Rahmenbedingungen. ""Mit der europäischen KI-Verordnung machen wir den Weg frei für einen sicheren Rechtsrahmen für Künstliche Intelligenz, der Innovationen fördert und gleichzeitig Risiken in der Anwendung angemessen adressiert."" Missbrauch von KI-Anwendungen zur Destabilisierung der Demokratien müsse verhindert werden, ebenso müsse der Schutz der Grundrechte gewährleistet sein. ""Gleichzeitig brauchen Wissenschaft und Wirtschaft Freiraum für Innovationen.""

Wirtschaftsminister Robert Habeck (Grüne) sagte: ""Künstliche Intelligenz wird unsere Gesellschaft prägen. Sie ist eine riesige Chance, birgt aber auch Risiken. Deshalb haben wir intensiv daran gearbeitet, dass es in der EU einen Rahmen gibt, der hilft, die Chancen gut zu nutzen - im Sinne der Wirtschaft, im Sinne des Gemeinwohls - und auf der anderen Seite die Risiken einzudämmen.""

Der Präsident des Digitalverbands Bitkom, Ralf Wintergerst, sagte: ""Die Hängepartie beim AI Act dürfte mit der jetzt zu erwartenden Zustimmung Deutschlands beendet sein."" Für die Unternehmen könne das die dringend notwendige Rechtssicherheit bei dieser wohl wichtigsten Zukunftstechnologie verbessern. ""Entscheidend ist dafür aber vor allem, wie die Vorgaben des AI Acts sowohl auf europäischer als auch auf nationaler Ebene ausgelegt und angewendet werden. Hier muss neben einer Risikoeinschätzung immer auch eine Abwägung der Chancen Künstlicher Intelligenz stattfinden.""

© dpa-infocom, dpa:240130-99-806346/3
"
AI,Zeit,2024-02-02,https://www.zeit.de/digital/2024-02/ai-act-kuenstliche-intelligenz-eu,AI Act: EU-Länder stimmen Regulierung von künstlicher Intelligenz zu | ZEIT ONLINE,"Die Länder der Europäischen Union haben sich für umfassende Regeln für künstliche Intelligenz ausgesprochen. Die ständigen Vertreter der Mitgliedsländer in Brüssel stimmten dem KI-Gesetz zu, wie EU-Binnenmarktkommissar Thierry Breton auf X mitteilte. Breton nannte die Einigung historisch. 

Wegen Bedenken unter anderem in Deutschland und Frankreich hatte der Beschluss wochenlang auf der Kippe gestanden. Nun müssen nur noch der Ministerrat und das Europaparlament abschließend zustimmen.

""Die KI-Verordnung soll dafür sorgen, dass wir in Europa das enorme Potenzial von KI heben und gleichzeitig auch Risiken in den Blick nehmen"", sagte Bundeswirtschaftsminister Robert Habeck. Diese Balance sei mit dem AI Act gelungen.

Unterhändler der Mitgliedsstaaten und des Europaparlaments hatten sich bereits im Dezember grundsätzlich auf die KI-Verordnung geeinigt. Damit müssen Entwickler künftig mit künstlicher Intelligenz erzeugte Texte, Töne und Bilder eindeutig kennzeichnen, um Menschen nicht in die Irre zu führen. Für ""risikoreiche"" Anwendungen sollen weitere Vorschriften gelten, etwa für die Gesichtserkennung in Sicherheitsbehörden. Grundsätzlich verboten wird eine Massenüberwachung mit biometrischen Daten wie in China.

In Deutschland hatten sich die Ampelparteien erst wenige Tage vor der Abstimmung in Brüssel darauf geeinigt, der KI-Verordnung zuzustimmen. Die FDP hatte Bedenken an dem Gesetz angemeldet, nachdem aus der Wirtschaft Befürchtungen geäußert wurden, der AI Act könne zu strenge Auflagen für Unternehmen beinhalten.

Die Verordnung ordnet KI-Anwendungen künftig in unterschiedliche Risikoklassen ein. Anbieter müssen den jeweiligen Klassen entsprechende Sicherheits- und Transparenzanforderungen erfüllen. Experten zufolge könnte das Regelwerk zu einem Vorbild für Gesetze in anderen Ländern werden. Es wäre eine Alternative zu den eher lockeren Regeln, die in den USA gelten – und den restriktiveren Auflagen Chinas.

Bedenken kommen vom Digitalverband Bitkom. Das europäische KI-Gesetz allein garantiere noch keine Rechtssicherheit für Unternehmen, sagte Bitkom-Vorstandsmitglied Susanne Dehmel. Vielmehr komme es ""auf eine praktikable Auslegung und Anwendung der Vorgaben in den EU-Mitgliedsländern an"". Nur wenn bürokratische Hürden und unerwünschte Wechselwirkungen mit bestehenden Gesetzen vermieden würden, könnten sich europäische Firmen im weltweiten KI-Wettbewerb behaupten.
"
AI,Zeit,2024-02-02,https://www.zeit.de/digital/2024-02/eu-gesetze-reparatur-recht-ai-act-verbraucherschutz,"EU-Gesetze: Regulierung? Ja, bitte | ZEIT ONLINE","Es ist 2027, Sie haben gerade die Kamera Ihres iPhones ausgetauscht, weil die alte kaputt war. Sie ziehen das letzte Schräubchen fest, setzen einen frischen Akku ins Gerät, stecken das USB-C-Kabel ein und installieren Fortnite. Oder eine Porno-App. 

Und dabei danken Sie gefälligst der EU. Denn nichts davon wäre möglich ohne die Gesetze, mit denen die Europäische Union gerade die Techwelt zum Besseren formt. 

Tatsächlich gibt es im Moment ein regelrechtes Crescendo von EU-Vorschriften, die beschlossen oder gültig werden, und den großen Techkonzernen echte Verhaltensänderungen abtrotzen. Das allein ist eine große Leistung, denn Apple, Google, Meta, Amazon und OpenAI gehören zu den mächtigsten Entitäten unserer Zeit. 

Das könnte man feiern, als Erfolg verbuchen, Hoffnung schöpfen. Tatsächlich aber finden viele Techregulierung und EU-Verordnungen entweder öde oder böse. Irgendwo zwischen ""Höhö, die wollen ja auch vorschreiben, wie krumm Gurken sein dürfen"" und ""Ohoh, die machen uns die ganze schöne Innovation kaputt.""

Natürlich sind nicht alle diese Regeln perfekt. Aber es ist nicht schwierig, positive Beispiele zu sammeln:

In der Nacht zum Freitag einigten sich die EU-Institutionen auf ein Recht auf Reparatur. In technische Geräte von der Waschmaschine bis zum Smartphone dürfen Hersteller in Zukunft keine Hürden mehr einbauen, die unabhängige Werkstätten daran hindern könnten, diese zu reparieren. Und sie müssen Ersatzteile zu angemessenen Preisen anbieten. Schon die Erwartung dieses Gesetzes hat echte Veränderung gebracht: Apple hätte wohl kaum von sich aus angefangen, Ersatzdisplays zu verkaufen.

Batterien, in Smartphones und in E-Autos, müssen ab 2027 sogar so einfach austauschbar sein, dass man es als Normalmensch zu Hause machen kann. Das hat die EU schon im vergangenen Sommer beschlossen. Ein Ladekabel für ein Smartphone muss das gleiche Ladekabel wie für einen Bluetoothlautsprecher sein. Gilt schon ab diesem Jahr. Deshalb gibt es nun iPhones mit USB-C.   

Das alles kann Elektroschrott reduzieren und das Leben von Konsumentinnen einfacher und billiger machen. Aber es wird noch besser.

Ebenfalls am Freitag haben die Mitgliedsstaaten der EU dem AI Act zugestimmt, einem Gesetz zu künstlicher Intelligenz. Es verbietet Ihrem Arbeitgeber zum Beispiel, ein System zu installieren, das automatisch an Ihrem Gesicht oder Ihrer Stimme erkennen soll, wie Sie sich fühlen. Wenn Sie noch nicht wussten, dass Sie das nicht wollen, wissen Sie es jetzt. 

Der Digital Markets Act, ein Gesetz, das schon länger gilt, entfaltet gerade seine Wirkung. Apple muss es zum Beispiel erstmals ermöglichen, Apps auf iPhones zu installieren, die es im offiziellen App-Store nicht gibt. Das betrifft unter anderem alles, was mit Pornografie zu tun hat, denn das hat Apple nicht gern. Mit anderen Worten: Sie können mit einem Gerät, das Sie gekauft haben, machen, was Sie wollen. 
"
AI,Zeit,2024-01-30,https://www.zeit.de/digital/2024-01/ai-act-ki-gesetz-bundesregierung-stimmt-zu,AI Act: Bundesregierung will europäischem KI-Gesetz doch zustimmen | ZEIT ONLINE,"Die Bundesregierung will dem AI Act nun doch zustimmen. Das bestätigte ein Sprecher des Verkehrsministerium ZEIT ONLINE. Um das Gesetz, mit dem die Europäische Union künstliche Intelligenz regulieren will, hatte es zuvor im Hintergrund Diskussionen gegeben. Das von Volker Wissing geführte Verkehrs- und Digitalministerium drängte Berichten zufolge darauf, dass Deutschland in Brüssel dem fertig verhandelten Gesetz nicht zustimmt.

In den vergangenen Tagen hatte ein breites Bündnis aus Wirtschaftsverbänden, zivilgesellschaftlichen Organisationen und Wissenschaftlern die Bundesregierung aufgefordert, das Gesetz nicht scheitern zu lassen. Das Hauptargument: Obwohl es nicht perfekt sei, drohe es in einem erneuten Anlauf noch schlechter zu werden. Außerdem sei es für Unternehmen wichtig, möglichst bald Rechtssicherheit zu haben. 

Besonders zur Frage, ob es für KI-Systeme wie ChatGPT besondere Regeln in dem Gesetz geben soll, gab es offenbar Meinungsverschiedenheiten. Die Regierungen von Frankreich, Italien und Deutschland sprachen sich dagegen aus, in der finalen Fassung des Gesetzes gibt es aber solche Vorschriften. Das war wohl der Grund, aus dem Wissing dem Gesetz zwischenzeitlich nicht zustimmen wollte. Er hatte wiederholt Bedenken geäußert, solche Regeln könnten Innovation behindern

Die Anbieter solcher Technologie müssten nach dem neuen Gesetz zum Beispiel Informationen darüber zur Verfügung stellen, wie ihre Systeme funktionieren. Für besonders leistungsfähige Systeme gelten noch weitere Pflichten. Dafür hatten sich viele Fachleute vehement eingesetzt, weil aus ihrer Sicht gerade von solchen KI-Modellen Risiken ausgehen könnten.

Ein weiterer Streitpunkt in der Diskussion um den AI Act sind Vorschriften zu biometrischer Überwachung. Mit dem Gesetz, auf das sich die EU-Organe im Dezember vergangenen Jahres inhaltlich einigten, sollten Regeln geschaffen werden für riskante Anwendungen von künstlicher Intelligenz. Dem Gesetz müssen noch die Mitgliedsstaaten der EU im Rat zustimmen und danach das Parlament. 
"
AI,Zeit,2024-01-30,https://www.zeit.de/news/2024-01/30/deutschland-will-eu-gesetz-zu-ki-zustimmen,Künstliche Intelligenz: Deutschland will EU-Gesetz zu KI zustimmen | ZEIT ONLINE,"Einer Zustimmung Deutschlands zum EU-Gesetz über den Einsatz von Künstlicher Intelligenz (KI) steht nichts mehr im Weg. Bundesdigitalminister Volker Wissing (FDP) sagte in Berlin, das Ringen um die deutsche Haltung zum sogenannten AI Act sei mit einem ""tragbaren Kompromiss"" zu Ende gegangen. Die federführenden Ministerien für Justiz und Wirtschaft teilten mit, die Bundesregierung habe sich darauf verständigt, der KI-Verordnung am Freitag in Brüssel zuzustimmen.

Wissing sagte, er habe sich bis zuletzt für innovationsfreundlichere Regeln eingesetzt und Verbesserungen für kleine und mittlere Unternehmen erzielen können. Unverhältnismäßige Anforderungen seien abgewendet worden. Mit dem ausgehandelten Kompromiss werde nun ein Fundament gelegt für die Entwicklung vertrauenswürdiger KI. Europa solle zu einem bedeutenden KI-Standort entwickelt werden, der sich im weltweiten Wettbewerb behaupte.

Für den Einsatz von Künstlicher Intelligenz (KI) sollen in der EU künftig strengere Regeln gelten. Unterhändler von Europaparlament und EU-Staaten hatten sich im Dezember in Brüssel nach langen Verhandlungen auf entsprechende Regeln geeinigt. Nach Angaben des EU-Parlaments handelt es sich um das weltweit erste KI-Gesetz.

Die EU-Kommission hatte das Gesetz im April 2021 vorgeschlagen. Demnach sollen KI-Systeme in verschiedene Risikogruppen eingeteilt werden. Je höher die potenziellen Gefahren einer Anwendung sind, desto höher sollen die Anforderungen sein. Die Hoffnung ist, dass die Regeln weltweit Nachahmer finden.

Justizminister Marco Buschmann (FDP) sagte, damit Europa KI-Pionier sein könne, brauche es innovationsfördernde und klare Rahmenbedingungen. ""Mit der europäischen KI-Verordnung machen wir den Weg frei für einen sicheren Rechtsrahmen für Künstliche Intelligenz, der Innovationen fördert und gleichzeitig Risiken in der Anwendung angemessen adressiert."" Missbrauch von KI-Anwendungen zur Destabilisierung der Demokratien müsse verhindert werden, ebenso müsse der Schutz der Grundrechte gewährleistet sein. ""Gleichzeitig brauchen Wissenschaft und Wirtschaft Freiraum für Innovationen.""

Wirtschaftsminister Robert Habeck (Grüne) sagte: ""Künstliche Intelligenz wird unsere Gesellschaft prägen. Sie ist eine riesige Chance, birgt aber auch Risiken. Deshalb haben wir intensiv daran gearbeitet, dass es in der EU einen Rahmen gibt, der hilft, die Chancen gut zu nutzen - im Sinne der Wirtschaft, im Sinne des Gemeinwohls - und auf der anderen Seite die Risiken einzudämmen.""

Der Präsident des Digitalverbands Bitkom, Ralf Wintergerst, sagte: ""Die Hängepartie beim AI Act dürfte mit der jetzt zu erwartenden Zustimmung Deutschlands beendet sein."" Für die Unternehmen könne das die dringend notwendige Rechtssicherheit bei dieser wohl wichtigsten Zukunftstechnologie verbessern. ""Entscheidend ist dafür aber vor allem, wie die Vorgaben des AI Acts sowohl auf europäischer als auch auf nationaler Ebene ausgelegt und angewendet werden. Hier muss neben einer Risikoeinschätzung immer auch eine Abwägung der Chancen Künstlicher Intelligenz stattfinden.""

© dpa-infocom, dpa:240130-99-806346/3
"
KI,Zeit,2024-02-02,https://www.zeit.de/digital/2024-02/ai-act-kuenstliche-intelligenz-eu,AI Act: EU-Länder stimmen Regulierung von künstlicher Intelligenz zu | ZEIT ONLINE,"Die Länder der Europäischen Union haben sich für umfassende Regeln für künstliche Intelligenz ausgesprochen. Die ständigen Vertreter der Mitgliedsländer in Brüssel stimmten dem KI-Gesetz zu, wie EU-Binnenmarktkommissar Thierry Breton auf X mitteilte. Breton nannte die Einigung historisch. 

Wegen Bedenken unter anderem in Deutschland und Frankreich hatte der Beschluss wochenlang auf der Kippe gestanden. Nun müssen nur noch der Ministerrat und das Europaparlament abschließend zustimmen.

""Die KI-Verordnung soll dafür sorgen, dass wir in Europa das enorme Potenzial von KI heben und gleichzeitig auch Risiken in den Blick nehmen"", sagte Bundeswirtschaftsminister Robert Habeck. Diese Balance sei mit dem AI Act gelungen.

Unterhändler der Mitgliedsstaaten und des Europaparlaments hatten sich bereits im Dezember grundsätzlich auf die KI-Verordnung geeinigt. Damit müssen Entwickler künftig mit künstlicher Intelligenz erzeugte Texte, Töne und Bilder eindeutig kennzeichnen, um Menschen nicht in die Irre zu führen. Für ""risikoreiche"" Anwendungen sollen weitere Vorschriften gelten, etwa für die Gesichtserkennung in Sicherheitsbehörden. Grundsätzlich verboten wird eine Massenüberwachung mit biometrischen Daten wie in China.

In Deutschland hatten sich die Ampelparteien erst wenige Tage vor der Abstimmung in Brüssel darauf geeinigt, der KI-Verordnung zuzustimmen. Die FDP hatte Bedenken an dem Gesetz angemeldet, nachdem aus der Wirtschaft Befürchtungen geäußert wurden, der AI Act könne zu strenge Auflagen für Unternehmen beinhalten.

Die Verordnung ordnet KI-Anwendungen künftig in unterschiedliche Risikoklassen ein. Anbieter müssen den jeweiligen Klassen entsprechende Sicherheits- und Transparenzanforderungen erfüllen. Experten zufolge könnte das Regelwerk zu einem Vorbild für Gesetze in anderen Ländern werden. Es wäre eine Alternative zu den eher lockeren Regeln, die in den USA gelten – und den restriktiveren Auflagen Chinas.

Bedenken kommen vom Digitalverband Bitkom. Das europäische KI-Gesetz allein garantiere noch keine Rechtssicherheit für Unternehmen, sagte Bitkom-Vorstandsmitglied Susanne Dehmel. Vielmehr komme es ""auf eine praktikable Auslegung und Anwendung der Vorgaben in den EU-Mitgliedsländern an"". Nur wenn bürokratische Hürden und unerwünschte Wechselwirkungen mit bestehenden Gesetzen vermieden würden, könnten sich europäische Firmen im weltweiten KI-Wettbewerb behaupten.
"
KI,Zeit,2024-01-30,https://www.zeit.de/digital/2024-01/ai-act-ki-gesetz-bundesregierung-stimmt-zu,AI Act: Bundesregierung will europäischem KI-Gesetz doch zustimmen | ZEIT ONLINE,"Die Bundesregierung will dem AI Act nun doch zustimmen. Das bestätigte ein Sprecher des Verkehrsministerium ZEIT ONLINE. Um das Gesetz, mit dem die Europäische Union künstliche Intelligenz regulieren will, hatte es zuvor im Hintergrund Diskussionen gegeben. Das von Volker Wissing geführte Verkehrs- und Digitalministerium drängte Berichten zufolge darauf, dass Deutschland in Brüssel dem fertig verhandelten Gesetz nicht zustimmt.

In den vergangenen Tagen hatte ein breites Bündnis aus Wirtschaftsverbänden, zivilgesellschaftlichen Organisationen und Wissenschaftlern die Bundesregierung aufgefordert, das Gesetz nicht scheitern zu lassen. Das Hauptargument: Obwohl es nicht perfekt sei, drohe es in einem erneuten Anlauf noch schlechter zu werden. Außerdem sei es für Unternehmen wichtig, möglichst bald Rechtssicherheit zu haben. 

Besonders zur Frage, ob es für KI-Systeme wie ChatGPT besondere Regeln in dem Gesetz geben soll, gab es offenbar Meinungsverschiedenheiten. Die Regierungen von Frankreich, Italien und Deutschland sprachen sich dagegen aus, in der finalen Fassung des Gesetzes gibt es aber solche Vorschriften. Das war wohl der Grund, aus dem Wissing dem Gesetz zwischenzeitlich nicht zustimmen wollte. Er hatte wiederholt Bedenken geäußert, solche Regeln könnten Innovation behindern

Die Anbieter solcher Technologie müssten nach dem neuen Gesetz zum Beispiel Informationen darüber zur Verfügung stellen, wie ihre Systeme funktionieren. Für besonders leistungsfähige Systeme gelten noch weitere Pflichten. Dafür hatten sich viele Fachleute vehement eingesetzt, weil aus ihrer Sicht gerade von solchen KI-Modellen Risiken ausgehen könnten.

Ein weiterer Streitpunkt in der Diskussion um den AI Act sind Vorschriften zu biometrischer Überwachung. Mit dem Gesetz, auf das sich die EU-Organe im Dezember vergangenen Jahres inhaltlich einigten, sollten Regeln geschaffen werden für riskante Anwendungen von künstlicher Intelligenz. Dem Gesetz müssen noch die Mitgliedsstaaten der EU im Rat zustimmen und danach das Parlament. 
"
KI,Zeit,2024-01-30,https://www.zeit.de/news/2024-01/30/deutschland-will-eu-gesetz-zu-ki-zustimmen,Künstliche Intelligenz: Deutschland will EU-Gesetz zu KI zustimmen | ZEIT ONLINE,"Einer Zustimmung Deutschlands zum EU-Gesetz über den Einsatz von Künstlicher Intelligenz (KI) steht nichts mehr im Weg. Bundesdigitalminister Volker Wissing (FDP) sagte in Berlin, das Ringen um die deutsche Haltung zum sogenannten AI Act sei mit einem ""tragbaren Kompromiss"" zu Ende gegangen. Die federführenden Ministerien für Justiz und Wirtschaft teilten mit, die Bundesregierung habe sich darauf verständigt, der KI-Verordnung am Freitag in Brüssel zuzustimmen.

Wissing sagte, er habe sich bis zuletzt für innovationsfreundlichere Regeln eingesetzt und Verbesserungen für kleine und mittlere Unternehmen erzielen können. Unverhältnismäßige Anforderungen seien abgewendet worden. Mit dem ausgehandelten Kompromiss werde nun ein Fundament gelegt für die Entwicklung vertrauenswürdiger KI. Europa solle zu einem bedeutenden KI-Standort entwickelt werden, der sich im weltweiten Wettbewerb behaupte.

Für den Einsatz von Künstlicher Intelligenz (KI) sollen in der EU künftig strengere Regeln gelten. Unterhändler von Europaparlament und EU-Staaten hatten sich im Dezember in Brüssel nach langen Verhandlungen auf entsprechende Regeln geeinigt. Nach Angaben des EU-Parlaments handelt es sich um das weltweit erste KI-Gesetz.

Die EU-Kommission hatte das Gesetz im April 2021 vorgeschlagen. Demnach sollen KI-Systeme in verschiedene Risikogruppen eingeteilt werden. Je höher die potenziellen Gefahren einer Anwendung sind, desto höher sollen die Anforderungen sein. Die Hoffnung ist, dass die Regeln weltweit Nachahmer finden.

Justizminister Marco Buschmann (FDP) sagte, damit Europa KI-Pionier sein könne, brauche es innovationsfördernde und klare Rahmenbedingungen. ""Mit der europäischen KI-Verordnung machen wir den Weg frei für einen sicheren Rechtsrahmen für Künstliche Intelligenz, der Innovationen fördert und gleichzeitig Risiken in der Anwendung angemessen adressiert."" Missbrauch von KI-Anwendungen zur Destabilisierung der Demokratien müsse verhindert werden, ebenso müsse der Schutz der Grundrechte gewährleistet sein. ""Gleichzeitig brauchen Wissenschaft und Wirtschaft Freiraum für Innovationen.""

Wirtschaftsminister Robert Habeck (Grüne) sagte: ""Künstliche Intelligenz wird unsere Gesellschaft prägen. Sie ist eine riesige Chance, birgt aber auch Risiken. Deshalb haben wir intensiv daran gearbeitet, dass es in der EU einen Rahmen gibt, der hilft, die Chancen gut zu nutzen - im Sinne der Wirtschaft, im Sinne des Gemeinwohls - und auf der anderen Seite die Risiken einzudämmen.""

Der Präsident des Digitalverbands Bitkom, Ralf Wintergerst, sagte: ""Die Hängepartie beim AI Act dürfte mit der jetzt zu erwartenden Zustimmung Deutschlands beendet sein."" Für die Unternehmen könne das die dringend notwendige Rechtssicherheit bei dieser wohl wichtigsten Zukunftstechnologie verbessern. ""Entscheidend ist dafür aber vor allem, wie die Vorgaben des AI Acts sowohl auf europäischer als auch auf nationaler Ebene ausgelegt und angewendet werden. Hier muss neben einer Risikoeinschätzung immer auch eine Abwägung der Chancen Künstlicher Intelligenz stattfinden.""

© dpa-infocom, dpa:240130-99-806346/3
"
Künstliche Intelligenz,Zeit,2024-01-29,https://www.zeit.de/digital/2024-01/ki-gesetz-ai-act-bundesregierung-zustimmung,AI Act: Überraschende Allianzen im Kampf um KI-Regulierung | ZEIT ONLINE,"Manchmal muss man in der Politik für ein Gesetz kämpfen, mit dem man eigentlich nicht recht zufrieden ist. So geht es im Moment vielen, die an den Verhandlungen zum AI Act beteiligt sind, einem Gesetz, mit dem die Europäische Union künstliche Intelligenz regulieren will.   

Informatikerinnen, die an KI forschen. Autoren, deren Werke für Training der KI verwendet werden. Unternehmer, die KI entwickeln oder verwenden. Sie alle hatten während der jahrelangen Verhandlung ihre jeweils eigenen Forderungen an das Gesetz. Aber statt den Kompromiss, den die EU im Dezember gefunden hat, von ihren jeweiligen Standpunkten aus zu kritisieren, schließen sie sich zu einer überraschenden Allianz zusammen und fordern die Bundesregierung auf, dem Gesetz zuzustimmen, statt es kurz vor der Verabschiedung noch scheitern zu lassen.  

""Stimmen Sie für den AI Act!"", schreibt eine Gruppe aus Wissenschaftlerinnen und zivilgesellschaftlichen Organisationen in einem offenen Brief, der am Sonntag veröffentlicht wurde. Auch Verbände, in denen sich Schriftsteller, Fotografinnen, Musiker und andere Kreative organisieren, fordern Ähnliches (PDF).

""Die KI-Verordnung ist nicht perfekt. Aber sie ist die Chance für Europa, die Regeln für den Einsatz einer Technologie zu bestimmen, die unser Leben schon heute so sehr verändert wie einst die Elektrizität oder das Automobil"", sagt Carla Hustedt, Leiterin des Bereichs Digitalisierte Gesellschaft der Stiftung Mercator und eine der Initiatorinnen des Briefs.  

Vielleicht am überraschendsten ist, dass es auch aus dem Bereich Bekenntnisse zu dem Gesetz gibt, aus dem vorher die lauteste Kritik kam: der KI-Wirtschaft. ""Es wäre sehr riskant, den Act jetzt scheitern zu lassen und wieder von vorne anzufangen"", sagt Daniel Abbou, der Geschäftsführer des KI-Bundesverbands. 

Der AI Act soll Regeln schaffen für riskante Anwendungen von künstlicher Intelligenz. Wenn KI an Entscheidungen über Menschen beteiligt ist, zum Beispiel, wie viel Arbeitslosengeld sie bekommen oder ob ihre Bewerbung für einen Job infrage kommt, würden spezielle Vorschriften gelten – wenn der AI Act beschlossen wird.

Ende vergangenen Jahres verhandelten EU-Politiker nächtelang um die letzten Details des Gesetzes. Besonders umkämpft waren dabei die zwei Fragen, die auch jetzt wieder für Streit sorgen: Welche Regeln sollen für KI-Systeme wie ChatGPT gelten? Und welche für automatisierte Videoüberwachung und andere biometrische Erkennungssysteme? Schließlich wurde in der Nacht zum 9. Dezember ein Kompromiss gefunden.

Eigentlich gibt es diese etwas undurchsichtigen, oft nächtlichen Verhandlungen verschiedener EU-Organe, damit das, was danach kommt, eher als Formalie gelten kann: Die Regierungen der Mitgliedsstaaten stimmen dem Gesetz im Rat zu, ebenso die Abgeordneten im Europäischen Parlament.

Doch jetzt besteht tatsächlich die Möglichkeit, dass das gesamte Gesetz noch scheitert, kurz bevor es offiziell beschlossen wird. Denn manche sind offenbar so unzufrieden mit dem beschlossenen Kompromiss, dass sie das Gesetz lieber komplett scheitern lassen wollen, als ihm zuzustimmen.  

Dazu gehört laut einem Bericht des Handelsblatts auch der deutsche Verkehrs- und Digitalminister Volker Wissing. Sein Ministerium dränge demnach darauf, dass sich Deutschland bei der Abstimmung in Brüssel enthält, was de facto einer Ablehnung gleichkommen würde.

Es müssten sich im Rat zwar auch noch weitere Länder enthalten oder gegen das Gesetz stimmen, um es abzulehnen, aber die deutsche Stimme hat Gewicht. Würden zum Beispiel Frankreich, Deutschland, Italien und ein weiteres Land nicht dafür stimmen, wäre das Gesetz wohl gescheitert. Noch ist es klar, welche Länder zustimmen wollen.
"
Künstliche Intelligenz,Zeit,2024-01-28,https://www.zeit.de/digital/internet/2024-01/nightshade-kuenstliche-intelligenz-fotos-kunst,Nightshade: Giftspritze für künstliche Intelligenz | ZEIT ONLINE,
AI,Zeit,2024-01-29,https://www.zeit.de/digital/2024-01/ki-gesetz-ai-act-bundesregierung-zustimmung,AI Act: Überraschende Allianzen im Kampf um KI-Regulierung | ZEIT ONLINE,"Manchmal muss man in der Politik für ein Gesetz kämpfen, mit dem man eigentlich nicht recht zufrieden ist. So geht es im Moment vielen, die an den Verhandlungen zum AI Act beteiligt sind, einem Gesetz, mit dem die Europäische Union künstliche Intelligenz regulieren will.   

Informatikerinnen, die an KI forschen. Autoren, deren Werke für Training der KI verwendet werden. Unternehmer, die KI entwickeln oder verwenden. Sie alle hatten während der jahrelangen Verhandlung ihre jeweils eigenen Forderungen an das Gesetz. Aber statt den Kompromiss, den die EU im Dezember gefunden hat, von ihren jeweiligen Standpunkten aus zu kritisieren, schließen sie sich zu einer überraschenden Allianz zusammen und fordern die Bundesregierung auf, dem Gesetz zuzustimmen, statt es kurz vor der Verabschiedung noch scheitern zu lassen.  

""Stimmen Sie für den AI Act!"", schreibt eine Gruppe aus Wissenschaftlerinnen und zivilgesellschaftlichen Organisationen in einem offenen Brief, der am Sonntag veröffentlicht wurde. Auch Verbände, in denen sich Schriftsteller, Fotografinnen, Musiker und andere Kreative organisieren, fordern Ähnliches (PDF).

""Die KI-Verordnung ist nicht perfekt. Aber sie ist die Chance für Europa, die Regeln für den Einsatz einer Technologie zu bestimmen, die unser Leben schon heute so sehr verändert wie einst die Elektrizität oder das Automobil"", sagt Carla Hustedt, Leiterin des Bereichs Digitalisierte Gesellschaft der Stiftung Mercator und eine der Initiatorinnen des Briefs.  

Vielleicht am überraschendsten ist, dass es auch aus dem Bereich Bekenntnisse zu dem Gesetz gibt, aus dem vorher die lauteste Kritik kam: der KI-Wirtschaft. ""Es wäre sehr riskant, den Act jetzt scheitern zu lassen und wieder von vorne anzufangen"", sagt Daniel Abbou, der Geschäftsführer des KI-Bundesverbands. 

Der AI Act soll Regeln schaffen für riskante Anwendungen von künstlicher Intelligenz. Wenn KI an Entscheidungen über Menschen beteiligt ist, zum Beispiel, wie viel Arbeitslosengeld sie bekommen oder ob ihre Bewerbung für einen Job infrage kommt, würden spezielle Vorschriften gelten – wenn der AI Act beschlossen wird.

Ende vergangenen Jahres verhandelten EU-Politiker nächtelang um die letzten Details des Gesetzes. Besonders umkämpft waren dabei die zwei Fragen, die auch jetzt wieder für Streit sorgen: Welche Regeln sollen für KI-Systeme wie ChatGPT gelten? Und welche für automatisierte Videoüberwachung und andere biometrische Erkennungssysteme? Schließlich wurde in der Nacht zum 9. Dezember ein Kompromiss gefunden.

Eigentlich gibt es diese etwas undurchsichtigen, oft nächtlichen Verhandlungen verschiedener EU-Organe, damit das, was danach kommt, eher als Formalie gelten kann: Die Regierungen der Mitgliedsstaaten stimmen dem Gesetz im Rat zu, ebenso die Abgeordneten im Europäischen Parlament.

Doch jetzt besteht tatsächlich die Möglichkeit, dass das gesamte Gesetz noch scheitert, kurz bevor es offiziell beschlossen wird. Denn manche sind offenbar so unzufrieden mit dem beschlossenen Kompromiss, dass sie das Gesetz lieber komplett scheitern lassen wollen, als ihm zuzustimmen.  

Dazu gehört laut einem Bericht des Handelsblatts auch der deutsche Verkehrs- und Digitalminister Volker Wissing. Sein Ministerium dränge demnach darauf, dass sich Deutschland bei der Abstimmung in Brüssel enthält, was de facto einer Ablehnung gleichkommen würde.

Es müssten sich im Rat zwar auch noch weitere Länder enthalten oder gegen das Gesetz stimmen, um es abzulehnen, aber die deutsche Stimme hat Gewicht. Würden zum Beispiel Frankreich, Deutschland, Italien und ein weiteres Land nicht dafür stimmen, wäre das Gesetz wohl gescheitert. Noch ist es klar, welche Länder zustimmen wollen.
"
AI,Zeit,2024-01-29,https://www.zeit.de/digital/2024-01/chatgpt-italien-datenschutz-regeln,OpenAI: ChatGPT verstößt laut Italien gegen europäische Datenschutzregeln | ZEIT ONLINE,"Der Umgang des KI-basierten Chatbots ChatGPT mit Nutzerdaten verstößt italienischen Behörden zufolge gegen europäisches Recht. Der Entwickler OpenAI könne binnen 30 Tagen zu den Ergebnissen einer entsprechenden Untersuchung Stellung nehmen, teilte die italienische Datenschutzaufsicht mit.  

Wegen rechtlicher Bedenken hatte Italien den Zugang zu ChatGPT bereits im Frühjahr 2023 kurzzeitig blockiert. Nachdem OpenAI einigen Auflagen zugestimmt hatte, war das Programm dort anschließend wieder verfügbar. 

Im Zuge dessen leitete die italienische Datenaufsicht Untersuchungen ein, weil sie in mindestens einem Fall Verstöße gegen das europäische Datenschutzrecht vermutete. Diese können Strafzahlungen von bis zu vier Prozent des weltweiten Umsatzes eines Unternehmens nach sich ziehen, was etwa 40 Millionen US-Dollar entspricht. Nach Informationen der italienischen Tageszeitung Corriere della Sera soll nun ein Sonderausschuss eingerichtet werden, das die Datenschutzbehörden in der gesamten Europäischen Union zusammenbringen soll. 

Die Technologie hinter ChatGPT kann menschliche Interaktion simulieren und wird dazu mit Unmengen an Daten trainiert. Diese werden meist aus dem Internet abgeschöpft. Außerdem fließen sämtliche Anfragen von Nutzern und die darin enthaltenen Informationen in die Datenbanken der Anbieter ein, um künftige Antworten zu verbessern.
"
AI,Zeit,2024-01-28,https://www.zeit.de/digital/internet/2024-01/nightshade-kuenstliche-intelligenz-fotos-kunst,Nightshade: Giftspritze für künstliche Intelligenz | ZEIT ONLINE,
KI,Zeit,2024-01-29,https://www.zeit.de/digital/2024-01/ki-gesetz-ai-act-bundesregierung-zustimmung,AI Act: Überraschende Allianzen im Kampf um KI-Regulierung | ZEIT ONLINE,"Manchmal muss man in der Politik für ein Gesetz kämpfen, mit dem man eigentlich nicht recht zufrieden ist. So geht es im Moment vielen, die an den Verhandlungen zum AI Act beteiligt sind, einem Gesetz, mit dem die Europäische Union künstliche Intelligenz regulieren will.   

Informatikerinnen, die an KI forschen. Autoren, deren Werke für Training der KI verwendet werden. Unternehmer, die KI entwickeln oder verwenden. Sie alle hatten während der jahrelangen Verhandlung ihre jeweils eigenen Forderungen an das Gesetz. Aber statt den Kompromiss, den die EU im Dezember gefunden hat, von ihren jeweiligen Standpunkten aus zu kritisieren, schließen sie sich zu einer überraschenden Allianz zusammen und fordern die Bundesregierung auf, dem Gesetz zuzustimmen, statt es kurz vor der Verabschiedung noch scheitern zu lassen.  

""Stimmen Sie für den AI Act!"", schreibt eine Gruppe aus Wissenschaftlerinnen und zivilgesellschaftlichen Organisationen in einem offenen Brief, der am Sonntag veröffentlicht wurde. Auch Verbände, in denen sich Schriftsteller, Fotografinnen, Musiker und andere Kreative organisieren, fordern Ähnliches (PDF).

""Die KI-Verordnung ist nicht perfekt. Aber sie ist die Chance für Europa, die Regeln für den Einsatz einer Technologie zu bestimmen, die unser Leben schon heute so sehr verändert wie einst die Elektrizität oder das Automobil"", sagt Carla Hustedt, Leiterin des Bereichs Digitalisierte Gesellschaft der Stiftung Mercator und eine der Initiatorinnen des Briefs.  

Vielleicht am überraschendsten ist, dass es auch aus dem Bereich Bekenntnisse zu dem Gesetz gibt, aus dem vorher die lauteste Kritik kam: der KI-Wirtschaft. ""Es wäre sehr riskant, den Act jetzt scheitern zu lassen und wieder von vorne anzufangen"", sagt Daniel Abbou, der Geschäftsführer des KI-Bundesverbands. 

Der AI Act soll Regeln schaffen für riskante Anwendungen von künstlicher Intelligenz. Wenn KI an Entscheidungen über Menschen beteiligt ist, zum Beispiel, wie viel Arbeitslosengeld sie bekommen oder ob ihre Bewerbung für einen Job infrage kommt, würden spezielle Vorschriften gelten – wenn der AI Act beschlossen wird.

Ende vergangenen Jahres verhandelten EU-Politiker nächtelang um die letzten Details des Gesetzes. Besonders umkämpft waren dabei die zwei Fragen, die auch jetzt wieder für Streit sorgen: Welche Regeln sollen für KI-Systeme wie ChatGPT gelten? Und welche für automatisierte Videoüberwachung und andere biometrische Erkennungssysteme? Schließlich wurde in der Nacht zum 9. Dezember ein Kompromiss gefunden.

Eigentlich gibt es diese etwas undurchsichtigen, oft nächtlichen Verhandlungen verschiedener EU-Organe, damit das, was danach kommt, eher als Formalie gelten kann: Die Regierungen der Mitgliedsstaaten stimmen dem Gesetz im Rat zu, ebenso die Abgeordneten im Europäischen Parlament.

Doch jetzt besteht tatsächlich die Möglichkeit, dass das gesamte Gesetz noch scheitert, kurz bevor es offiziell beschlossen wird. Denn manche sind offenbar so unzufrieden mit dem beschlossenen Kompromiss, dass sie das Gesetz lieber komplett scheitern lassen wollen, als ihm zuzustimmen.  

Dazu gehört laut einem Bericht des Handelsblatts auch der deutsche Verkehrs- und Digitalminister Volker Wissing. Sein Ministerium dränge demnach darauf, dass sich Deutschland bei der Abstimmung in Brüssel enthält, was de facto einer Ablehnung gleichkommen würde.

Es müssten sich im Rat zwar auch noch weitere Länder enthalten oder gegen das Gesetz stimmen, um es abzulehnen, aber die deutsche Stimme hat Gewicht. Würden zum Beispiel Frankreich, Deutschland, Italien und ein weiteres Land nicht dafür stimmen, wäre das Gesetz wohl gescheitert. Noch ist es klar, welche Länder zustimmen wollen.
"
KI,Zeit,2024-01-28,https://www.zeit.de/gesundheit/2024-01/ki-medizin-med-palm-arzt-therapie,"KI in der Medizin: ""Laien fanden die Antworten des Modells hilfreicher als die der Ärzte"" | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-01-23,https://www.zeit.de/news/2024-01/23/wenig-kuenstliche-intelligenz-in-der-bayerischen-wirtschaft,Statistik: Wenig Künstliche Intelligenz in der bayerischen Wirtschaft | ZEIT ONLINE,"Nur ein kleiner Teil der bayerischen Unternehmen nutzt bisher Künstliche Intelligenz (KI). Nach einer aktuellen Erhebung des Statistischen Landesamts nutzten im vergangenen Jahr 13 Prozent der Unternehmen mit zehn oder mehr Beschäftigten eine zur KI zählende Technologie. Das teilte die Fürther Behörde am Dienstag mit.

Grundlage war eine deutschlandweite Erhebung des Statistischen Bundesamts, an der sich in Bayern 2500 Unternehmen beteiligten. Unter dem Oberbegriff «KI» waren dabei eine ganze Reihe von Technologien zusammengefasst. Dazu gehörten unter anderem Chatbots, Sprach- und Spracherkennung, Übersetzungssoftware, auf maschinellem Lernen basierende Datenanalyse, aber auch autonome Roboter und autonome Drohnen in Industrie oder Logistik.

Die Statistiker fragten nicht nur nach Künstlicher Intelligenz, sondern ganz allgemein nach der «Nutzung von Informations- und Kommunikationstechnologien (IKT) in Unternehmen». Laut Umfrage haben 98 Prozent der befragten bayerischen Unternehmen einen Internetanschluss und knapp zwei Drittel (65 Prozent) eine eigene Webseite.

© dpa-infocom, dpa:240123-99-717186/2
"
AI,Zeit,2024-01-23,https://www.zeit.de/digital/2024-01/microsoft-copilot-kuenstliche-intelligenz-windows,Microsoft Copilot: Kann diese KI bessere Präsentationen bauen als der Mensch? | ZEIT ONLINE,
KI,Zeit,2024-01-24,https://www.zeit.de/news/2024-01/24/sap-peilt-mehr-tempo-bei-cloudumsatz-und-ergebnis-an,Softwarehersteller: SAP will mit KI-Geschäfte anschieben: 8000 Jobs betroffen | ZEIT ONLINE,"Europas größter Softwarehersteller SAP will mit einem Großumbau die Geschäfte mit Künstlicher Intelligenz (KI) vorantreiben. Von dem Vorhaben seien rund 8000 Mitarbeitende betroffen, teilte das Dax-Schwergewicht am späten Dienstagabend mit. Die Walldorfer hatten vor rund einem Jahr bereits 3000 Jobs gestrichen, um sich schlanker aufzustellen und sich wieder mehr auf das Kerngeschäft rund um die Software zur Unternehmenssteuerung zu konzentrieren.

«Mit dem geplanten Transformationsprogramm verlagern wir verstärkt Investitionen in strategische Wachstumsbereiche, in erster Linie in KI», sagte Vorstandschef Christian Klein. «Damit werden wir auch zukünftig wegweisende Innovationen entwickeln und gleichzeitig die Effizienz unserer Geschäftsprozesse verbessern.» Bis Ende 2025 werde SAP knapp eine Milliarde Euro in diesen Bereich stecken, sagte Klein.

Der Hype um KI in der Softwarebranche hatte sich im vorvergangenen Jahr an der Veröffentlichung des Chatbots ChatGPT entzündet. Seither möchten alle Softwarekonzerne ein Stück vom erhofft großen zukünftigen Kuchen abhaben und stecken viel Geld in die Technologie.

SAP hatte im vergangenen Jahr bereits eigene Produkte wie den KI-Assistenten Joule vorgestellt, der es Anwendern erleichtern soll, typische Aufgaben in Unternehmen zu erledigen. Nun nimmt SAP-Chef Klein noch einmal rund zwei Milliarden Euro Geld in die Hand - soviel nämlich soll das Umbauprogramm insgesamt kosten.

Teil des Umbauprogramms sei auch ein Umbau der Konzernstruktur, hieß es. Bei den meisten der rund 8000 betroffenen Stellen sollen Freiwilligenprogramme und interne Umschulungen greifen. Aufgrund von Investitionen in Wachstumsbereiche rechnet SAP damit, dass am Ende des Jahres die Zahl der Mitarbeitenden etwa dem aktuellen Niveau entspricht. Wie viele der vom Umbau betroffenen 8000 Beschäftigten dann noch bei SAP arbeiten, ist derzeit nicht abzusehen.

Solche Entscheidungen zu treffen, sei nie einfach, sagte Klein. Aber es gehe um die bestmögliche Zukunft von SAP und darum, in der Tech-Industrie mithalten zu können. Etwa zwei Drittel der 8000 betroffenen Beschäftigten sollen laut Klein mit freiwilligen Maßnahmen wie etwa Vorruhestand oder Abfindungen zum Gehen bewegt werden oder sich zum Beispiel mit Umschulungen für andere Positionen qualifizieren können.

Für den Betriebsrat sei es wichtig, dass die Restrukturierung in Deutschland eine rein freiwillige Maßnahme ist, hieß es. Betriebsbedingte Kündigungen seien durch eine Betriebsvereinbarung bis Ende 2024 ausgeschlossen, teilte ein Firmensprecher mit. Der Betriebsrat forderte laut Mitteilung, die Betriebsvereinbarung zur Beschäftigungssicherung über das Jahr 2024 hinaus zu verlängern, um den Beschäftigten Planungssicherheit zu ermöglichen.

Der Stellenabbau vor rund einem Jahr hatte bei den Walldorfern nicht zu insgesamt sinkenden Mitarbeiterzahlen geführt. Zum Stichtag Ende Dezember hatte SAP 107 602 Vollzeitbeschäftigte, ein Jahr zuvor waren es 106 312 gewesen. Viele der damals betroffenen Beschäftigten sind aber nicht mehr bei SAP.

Klein und sein Finanzchef Dominik Asam haben sich für das laufende Jahr mehr Tempo bei Cloudumsatz und Ergebnis vorgenommen als im letzten Jahr. So soll das um Sondereffekte bereinige Ergebnis vor Zinsen und Steuern um 17 bis 21 Prozent wachsen, wenn Wechselkurseffekte ausgeklammert werden.

In der Cloud sollen die hereingeholten Abonnements mehr Schub liefern. Klein hat den Vertriebsteams ein währungsbereinigtes Umsatzplus von 24 bis 27 Prozent als Messlatte gesetzt.

Die Cloudprodukte zur Nutzung über das Netz sind seit längerer Zeit der Wachstumsträger bei SAP. Sie gelten auf lange Sicht als ertragreicher, weil die Kunden mit einiger Laufzeit mehr zahlen als mit dem früher üblichen Paket aus Lizenzsoftware gegen hohe Einmalgebühr und anschließendem Wartungsvertrag. Zunächst aber bedeuten die Cloudverträge Einbußen, weil anfangs die hohen Verkaufspreise der Lizenzsoftware wegfallen.

KI und andere Neuerungen sollen bei SAP künftig den Cloudversionen der Software vorbehalten sein, die Wartung von bestimmten Produkten fest installierter Software läuft auf Sicht aus. So will Klein den Kunden die Cloudangebote schmackhaft machen.

Insgesamt steigerte SAP den Umsatz um 6 Prozent auf 31,2 Milliarden Euro. Im Tagesgeschäft kletterte das bereinigte operative Ergebnis um neun Prozent auf 8,7 Milliarden Euro. Im Schlussquartal half dabei gerade auch das lukrative Lizenzgeschäft, das deutlich weniger abfiel als von Experten zuvor geschätzt.

Der Nettogewinn stieg auf 5,9 Milliarden Euro, das war mehr als das Dreifache des Vorjahresgewinns. Vor allem der milliardenschwere Sonderertrag aus dem Verkauf der ehemaligen US-Marktforschungstochter Qualtrics trieb den Überschuss nach oben.

© dpa-infocom, dpa:240124-99-727875/7
"
KI,Zeit,2024-01-23,https://www.zeit.de/digital/2024-01/microsoft-copilot-kuenstliche-intelligenz-windows,Microsoft Copilot: Kann diese KI bessere Präsentationen bauen als der Mensch? | ZEIT ONLINE,
Künstliche Intelligenz,Zeit,2024-01-21,https://www.zeit.de/2024/04/ki-schule-chatgpt-unterricht-lehrer,"KI in der Schule: ""Fehler korrigiert jetzt die KI"" | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2024-01-18,https://www.zeit.de/news/2024-01/17/samsung-will-mit-ki-offensive-gegen-apple-punkten,Smartphone-Rivalen: Samsung will mit KI-Offensive gegen Apple punkten | ZEIT ONLINE,"Samsung setzt auf Künstliche Intelligenz, um mit seinem nächsten Top-Smartphone gegen Apples iPhone anzutreten. In der Galaxy-S24-Serie soll KI-Software unter anderem Bilder verbessern, Texte zusammenfassen und Unterhaltungen übersetzen. 

Samsung inszenierte die Premiere mit Lichtshow auf großer Bühne im Silicon Valley, gerade einmal ein Dutzend Kilometer von der Apple-Zentrale entfernt. Markantes Detail der neuen Generation: Der kantige Rahmen der Basis-Varianten S24 und S24+ erinnert sehr stark an die Konturen jüngster iPhones.

Samsungs Smartphone-Chef TM Roh versprach in der Arena, in der sonst das Eishockey-Team San Jose Sharks spielt, KI-Innovationen, die das Leben einfacher machen sollen. Sein Anspruch: Samsung werde den «globalen Standard für mobile Künstliche Intelligenz setzen». Und das S24 sei «das erste KI-Telefon».

Die S24-Reihe bietet unter anderem Echtzeit-Übersetzungen von Telefonanrufen und Unterhaltungen in 13 Sprachen, darunter Deutsch. Die Software soll Spiegelungen in Fotos entfernen können. Herkömmliche Videos können nachträglich in Zeitlupenaufnahmen umgewandelt werden - die Software erstellt dafür künstlich zusätzliche Bilder.

Einige KI-Funktionen wie die Computer-Übersetzung bei Telefonanrufen basieren auf hauseigener Entwicklung bei Samsung. Für andere lieferte KI- und Suchmaschinen-Schwergewicht Google als Kooperationspartner viel seiner Technologie. Ein Beispiel ist eine Suchfunktion, für die es reicht, ein Wort, ein Satz oder ein Objekt auf dem Bildschirm des Smartphones zu umkreisen. Schon tauchen am unteren Ende des Displays Ergebnisse der Google-Suche auf. Der Internet-Konzern bringt die Funktion auch auf sein hauseigenes Smartphone Pixel 8.

TM Roh ließ zugleich durchblicken, dass in der Zukunft nicht mehr alle Funktionen auf Basis Künstlicher Intelligenz kostenlos bleiben dürften. Einige Kunden dürften bereit sein, für fortgeschrittenere Funktionalität Geld zu bezahlen, sagte der Samsung-Manager. Basis-Lösungen könnten dagegen weiter gratis angeboten werden. Wo die Trennlinie zwischen Gratis- und Bezahl-KI verlaufen wird, müsse auf Grundlage vieler verschiedener Faktoren entschieden werden.

Samsung stellt sich auf eine lange Lebenszeit der Geräte ein: Neue Smartphones sollen sieben Jahre lang mit Sicherheits-Updates versorgt werden.

Mit einem vernetzten Ring will Samsung zudem ein weiteres Gerät zum Sammeln von Gesundheitsdaten auf den Markt bringen. Der Ring werde voraussichtlich in der zweiten Jahreshälfte auf den Markt kommen, sagte der Chef von Samsungs Gesundheitsgeschäft, Hon Pak. Das Gerät sei unter anderem für diejenigen Kunden gedacht, die auch im Schlaf Informationen sammeln wollten, aber keine Lust hätten, dafür auch nachts eine Computer-Uhr zu tragen.

Bei Details wie konkreten Funktionen oder Preis hält sich Samsung noch bedeckt. Der Konzern zeigte am Rande der Vorstellung seines neuen Top-Smartphones Galaxy S24 aber einen funktionsfähigen Prototypen. Der extrem leichte Ring hat eine Hülle aus Titan. Ringe anderer Anbieter können unter anderem Puls und Aktivität messen. 

Samsung mit seinen hunderten Millionen Nutzern könnte der Produktkategorie einen stärkeren Schub geben als bisher in dem Markt aktive kleinere Anbieter. 

Samsung präsentierte sein Flaggschiff-Modell zum ersten Mal seit mehr als einem Jahrzehnt nicht als Smartphone-Marktführer: Apple erklomm im vergangenen Jahr nach Berechnungen der Analysefirma IDC erstmals den Thron. Ihr Experte Francisco Jeronimo fand das bemerkenswert, da ein iPhone 2,5 mal mehr koste als ein durchschnittliches Smartphone. Samsung hielt den Spitzenplatz beim Absatz durch das breite Angebot an Modellen in verschiedenen Preisklassen, während Apple stets die meisten Profite im Markt abschöpfte.

© dpa-infocom, dpa:240117-99-652090/3
"
AI,Zeit,2024-01-21,https://www.zeit.de/2024/04/ki-schule-chatgpt-unterricht-lehrer,"KI in der Schule: ""Fehler korrigiert jetzt die KI"" | ZEIT ONLINE",
AI,Zeit,2024-01-19,https://www.zeit.de/2024/04/computerviren-trojaner-hacking-illegalitaet,Computerviren: Ein Mann spielt mit der Hölle | ZEIT ONLINE,"Als er 15 Jahre alt war, vor langer Zeit, schuf Thomas Sperl sein erstes Meisterstück, einen Trojaner-Generator, eine Art Baukasten für Computerviren. Er bot ihn auf seiner Website zum kostenlosen Herunterladen an. Jeder, der wollte, konnte damit ein Computervirus basteln und per E-Mail verteilen. Auf den Rechnern seiner Opfer löschte der Trojaner die Festplatte, für immer. Wie viele Trojaner sein Generator geboren hat, weiß er nicht – er verfolgt kaum, was seine Ideen in der Welt anrichten. Dabei hat er noch weitere Projekte programmiert, mit theoretisch noch viel zerstörerischerem Potenzial. 

Thomas Sperl ist heute Mitte 30, wir treffen ihn in einem Café in Wien, einer Stadt, die er sehr mag, wie er sagt. Er sieht eher harmlos aus, ist groß, hat dunkle Haare. Als wolle er das Klischee kultivieren, das viele von Hackern haben, trägt er einen schwarzen Kapuzenpulli und einen Decknamen. Sein echter Name ist der Redaktion bekannt, alle seine Angaben wurden überprüft. Doch er bittet darum, nicht erkennbar zu werden, denn heute forscht er hauptberuflich als Naturwissenschaftler zu künstlicher Intelligenz. Es ist nicht verboten, was er tut, aber er möchte nicht, dass seine Kollegen an seinem besonderen Hobby Anstoß nehmen: den Viren. 

Aids, Corona, Ebola: Echte Viren entstehen durch Mutationen und sind dann kaum wieder aus der Welt zu schaffen. Computerviren werden programmiert von Menschen wie Thomas Sperl. Sind sie einmal in der Welt, zirkulieren auch sie immer weiter, als Würmer, Trojaner, Ransomware. Kriminelle erbeuten mit ihnen Millionen, manchmal gefährden sie dabei auch Menschenleben. Mit Computerviren können sie Unternehmen, Stadtverwaltungen und Krankenhäuser lahmlegen. In Kriegen werden solche Viren als Waffe eingesetzt, um die Infrastruktur ganzer Länder zu beschädigen. Thomas Sperl feilt an solchen Viren, perfektioniert sie, setzt ihren Code in die Welt. Weil er Viren spannend findet, wie er sagt. Zuletzt hat er sogar den Chatbot ChatGPT dafür eingespannt.

Sperl stammt aus einem winzigen Dorf in Österreich, auf einen Einwohner kommen dort zehn Kühe, so beschreibt er es selbst. Er sei ein mieser Schüler gewesen. Schlau, aber nicht schulschlau. Das meiste im Unterricht langweilte ihn, das Abitur habe er fast verhauen, erzählt er. Keine guten Bedingungen für eine große Karriere. 

Bis die Computer kamen. Die haben ihn sofort interessiert. Auf einer der Disketten, die er damals mit anderen tauscht, holt er sich versehentlich ein Virus ins Haus. Das Antivirenprogramm des elterlichen Rechners springt an. Sperl ist begeistert: Bis dahin habe er gar nicht gewusst, dass es so etwas gibt, sagt er. Er bringt sich Programmieren bei, und als er sieht, wie der Computer etwas ausspuckt, das er selbst geschaffen hat, wird ihm klar: Das ist sein neues Leben. In seinem Dorf, in dem man nichts machen konnte, sagt Thomas Sperl, sei das ""ein Lichtblick"" gewesen. 

Dabei reizt ihn nicht das Licht, sondern offenbar das Dunkle, das Verbotene. Er gibt sich einen Kampfnamen: SPTH, das steht für ""Second Part to Hell"", Zweiter Teil der Hölle. Liest man in seinem bis heute bestehenden Blog seine damaligen Einträge, wird deutlich, wie fasziniert er schon immer von der Idee gewesen sein muss, etwas Großes, Verheerendes zu schaffen. Im Jahr 2002 veröffentlichte er nicht nur seinen Trojaner- sowie einen Wurm-Generator, sondern auch einen Code für einen angeblich ""perfekten Internetwurm"". Es finden sich außerdem Anleitungen für das erste CD-Virus, für ein Schadprogramm, das Wikipedia missbraucht, um sich zu verbreiten, und für ein Computervirus, das sich immer neu verkleidet, um seinen Jägern zu entwischen. Ein durchaus teuflisches Instrumentarium. 

In seinen Beiträgen wurde klar, wie stolz Sperl auf sein Können ist. Er ist Teil einer Gruppe von Menschen, die denken und programmieren wie er. Die sich gegenseitig befeuern, sich für ihre Kreationen loben. Sie hätten ""Hightech"" geschaffen, sagt Sperl noch heute. Immer wieder schreibt er in seinen Texten, wie Antivirenfirmen daran gescheitert seien, seine Kreationen zu erkennen und unschädlich zu machen. Und dass IT-Sicherheitsexperten seine Arbeit ernst nähmen. 
"
AI,Zeit,2024-01-18,https://www.zeit.de/news/2024-01/17/samsung-will-mit-ki-offensive-gegen-apple-punkten,Smartphone-Rivalen: Samsung will mit KI-Offensive gegen Apple punkten | ZEIT ONLINE,"Samsung setzt auf Künstliche Intelligenz, um mit seinem nächsten Top-Smartphone gegen Apples iPhone anzutreten. In der Galaxy-S24-Serie soll KI-Software unter anderem Bilder verbessern, Texte zusammenfassen und Unterhaltungen übersetzen. 

Samsung inszenierte die Premiere mit Lichtshow auf großer Bühne im Silicon Valley, gerade einmal ein Dutzend Kilometer von der Apple-Zentrale entfernt. Markantes Detail der neuen Generation: Der kantige Rahmen der Basis-Varianten S24 und S24+ erinnert sehr stark an die Konturen jüngster iPhones.

Samsungs Smartphone-Chef TM Roh versprach in der Arena, in der sonst das Eishockey-Team San Jose Sharks spielt, KI-Innovationen, die das Leben einfacher machen sollen. Sein Anspruch: Samsung werde den «globalen Standard für mobile Künstliche Intelligenz setzen». Und das S24 sei «das erste KI-Telefon».

Die S24-Reihe bietet unter anderem Echtzeit-Übersetzungen von Telefonanrufen und Unterhaltungen in 13 Sprachen, darunter Deutsch. Die Software soll Spiegelungen in Fotos entfernen können. Herkömmliche Videos können nachträglich in Zeitlupenaufnahmen umgewandelt werden - die Software erstellt dafür künstlich zusätzliche Bilder.

Einige KI-Funktionen wie die Computer-Übersetzung bei Telefonanrufen basieren auf hauseigener Entwicklung bei Samsung. Für andere lieferte KI- und Suchmaschinen-Schwergewicht Google als Kooperationspartner viel seiner Technologie. Ein Beispiel ist eine Suchfunktion, für die es reicht, ein Wort, ein Satz oder ein Objekt auf dem Bildschirm des Smartphones zu umkreisen. Schon tauchen am unteren Ende des Displays Ergebnisse der Google-Suche auf. Der Internet-Konzern bringt die Funktion auch auf sein hauseigenes Smartphone Pixel 8.

TM Roh ließ zugleich durchblicken, dass in der Zukunft nicht mehr alle Funktionen auf Basis Künstlicher Intelligenz kostenlos bleiben dürften. Einige Kunden dürften bereit sein, für fortgeschrittenere Funktionalität Geld zu bezahlen, sagte der Samsung-Manager. Basis-Lösungen könnten dagegen weiter gratis angeboten werden. Wo die Trennlinie zwischen Gratis- und Bezahl-KI verlaufen wird, müsse auf Grundlage vieler verschiedener Faktoren entschieden werden.

Samsung stellt sich auf eine lange Lebenszeit der Geräte ein: Neue Smartphones sollen sieben Jahre lang mit Sicherheits-Updates versorgt werden.

Mit einem vernetzten Ring will Samsung zudem ein weiteres Gerät zum Sammeln von Gesundheitsdaten auf den Markt bringen. Der Ring werde voraussichtlich in der zweiten Jahreshälfte auf den Markt kommen, sagte der Chef von Samsungs Gesundheitsgeschäft, Hon Pak. Das Gerät sei unter anderem für diejenigen Kunden gedacht, die auch im Schlaf Informationen sammeln wollten, aber keine Lust hätten, dafür auch nachts eine Computer-Uhr zu tragen.

Bei Details wie konkreten Funktionen oder Preis hält sich Samsung noch bedeckt. Der Konzern zeigte am Rande der Vorstellung seines neuen Top-Smartphones Galaxy S24 aber einen funktionsfähigen Prototypen. Der extrem leichte Ring hat eine Hülle aus Titan. Ringe anderer Anbieter können unter anderem Puls und Aktivität messen. 

Samsung mit seinen hunderten Millionen Nutzern könnte der Produktkategorie einen stärkeren Schub geben als bisher in dem Markt aktive kleinere Anbieter. 

Samsung präsentierte sein Flaggschiff-Modell zum ersten Mal seit mehr als einem Jahrzehnt nicht als Smartphone-Marktführer: Apple erklomm im vergangenen Jahr nach Berechnungen der Analysefirma IDC erstmals den Thron. Ihr Experte Francisco Jeronimo fand das bemerkenswert, da ein iPhone 2,5 mal mehr koste als ein durchschnittliches Smartphone. Samsung hielt den Spitzenplatz beim Absatz durch das breite Angebot an Modellen in verschiedenen Preisklassen, während Apple stets die meisten Profite im Markt abschöpfte.

© dpa-infocom, dpa:240117-99-652090/3
"
Artificial Intelligence,Zeit,2024-01-18,https://www.zeit.de/kultur/2024-01/bill-ackman-axel-springer-konzern-klage-usa,Bill Ackman: Der seltsame Kampf eines Hedgefonds-Managers gegen Springer | ZEIT ONLINE,
Artificial Intelligence,Zeit,2024-01-19,https://www.zeit.de/2024/04/computerviren-trojaner-hacking-illegalitaet/seite-3,Computerviren: Chatbots für gefährliche Zwecke | ZEIT ONLINE,"Als im Frühling 2023 die neueste Version des Chatbots herauskam, sei für ihn klar gewesen, ""was zu tun ist"", sagt Sperl. Er hat ChatGPT seitdem beigebracht, Virencode für ihn immer neu zu verkleiden und damit vor Antivirenprogrammen zu verstecken. Was er als Schüler noch händisch programmierte, lässt er jetzt durch die künstliche Intelligenz erledigen. 

Doch da endet sein Trick nicht: Wenn er ein Virus erfolgreich verkleidet hat, lässt er es in einen Computer schlüpfen, auf dem die Bezahlversion von ChatGPT läuft und der über eine Programmierschnittstelle mit den Servern von OpenAI, dem Hersteller von ChatGPT, verbunden ist. Wie beim Judo macht sich das Virus diese Verbindung dann zunutze, um sie gegen den angegriffenen Computer einzusetzen. Sobald das Virus so Zugriff auf ChatGPT hat, verwendet es den Chatbot, um sich selbst umzuschreiben. Dafür werden Anweisungen in natürlicher Sprache vom Chatbot in Code übersetzt und andersherum. Und da es beliebig viele verschiedene Möglichkeiten gibt, eine Anweisung in Programmiersprache auszudrücken, verändert sich der Angriff dabei jedes Mal auf unvorhersagbare Weise. So wird der befallene Computer zur Fabrik für weitere Viren, die dann immer neue Rechner befallen können. Zumindest in der Theorie. In der freien Wildbahn ausprobiert habe er das nie, sagt Sperl. Gott sei Dank, möchte man hinzufügen.

Der finnische Sicherheitsforscher Mikko Hyppönen ist alarmiert, weil Sperl damit ein Virus entwickelt hat, das bislang nicht detektierbar sei. Und auch das Bundesamt für Sicherheit in der Informationstechnik (BSI) warnte kürzlich vor ähnlichen Mechanismen. Dass Sperls Entwurf gefährlich gut funktionieren könnte, liegt daran, dass bisherige Virenscanner immer nur nach einem bestimmten Schadcode suchen. Hier aber gibt es keinen Code, es gibt nur Anweisungen in englischer Sprache an den Chatbot, aus denen dieser dann die Programme formt. Und es gibt keinen Weg, die von ChatGPT erdachten Verkleidungen automatisiert zu erkennen, schreibt das BSI in seinem aktuellen Lagebericht. 

Thomas Sperl ist nicht allein mit der Idee, die neuen Chatbots für gefährliche Zwecke zu nutzen. Längst existieren Varianten dieser Systeme, die nur für kriminelle Zwecke erschaffen wurden. Sie heißen etwa WormGPT oder FraudGPT und sollen Betrügern ihre kriminelle Arbeit erleichtern. 

Mikko Hyppönen fürchtet diese Entwicklung. ""Malware and machine learning, a match made in hell"" heißt einer seiner Vorträge – Schadsoftware und KI-Systeme, eine höllische Verbindung. Bei einem Gespräch in Berlin sagt er, Maschinen würden bald in der Lage sein, Sicherheitslücken schneller und besser zu finden als Menschen. ""Für die Sicherheit ist das gut und schlecht zugleich. Gut, weil wir die Lücken dann beheben können. Aber auch schlecht, denn sollten diese identifizierten Schwachstellen in die falschen Hände gelangen, können sie schnell ausgenutzt werden."" Was das Thema KI angehe, erlebten wir gerade ""die heißeste Zeit der IT-Geschichte"". Das sei ""aufregend und beängstigend zugleich"".

Thomas Sperl scheint das vor allem aufregend zu finden. In seinem Blog beschreibt er seine Version einer möglichen Zukunft: Ein riesiger Schwarm sich selbst vermehrender Viren zieht darin durch den Cyberspace und trainiert Systeme wie ChatGPT. Dank der Genlabore verbreiten sich die Virenschwärme auch biologisch und infizieren jeden Menschen, um alle zu kontrollieren und zu Sklaven zu machen. ""Als sich der Staub gelegt hat, erkennt die Menschheit die unbestreitbare Anwesenheit ihres neuen Meisters"", beendet er seinen Beitrag dramatisch. Im Interview mit dem Hackermagazin kommentiert er: ""Lasst uns das Wettrennen darum, was zuerst schiefgeht, beobachten."" Hinter diesen Satz hat Thomas Sperl einen Smiley gesetzt. Mikko Hyppönen kann darüber nicht lachen. 
"
KI,Zeit,2024-01-21,https://www.zeit.de/2024/04/ki-schule-chatgpt-unterricht-lehrer,"KI in der Schule: ""Fehler korrigiert jetzt die KI"" | ZEIT ONLINE",
KI,Zeit,2024-01-19,https://www.zeit.de/2024/04/computerviren-trojaner-hacking-illegalitaet,Computerviren: Ein Mann spielt mit der Hölle | ZEIT ONLINE,"Als er 15 Jahre alt war, vor langer Zeit, schuf Thomas Sperl sein erstes Meisterstück, einen Trojaner-Generator, eine Art Baukasten für Computerviren. Er bot ihn auf seiner Website zum kostenlosen Herunterladen an. Jeder, der wollte, konnte damit ein Computervirus basteln und per E-Mail verteilen. Auf den Rechnern seiner Opfer löschte der Trojaner die Festplatte, für immer. Wie viele Trojaner sein Generator geboren hat, weiß er nicht – er verfolgt kaum, was seine Ideen in der Welt anrichten. Dabei hat er noch weitere Projekte programmiert, mit theoretisch noch viel zerstörerischerem Potenzial. 

Thomas Sperl ist heute Mitte 30, wir treffen ihn in einem Café in Wien, einer Stadt, die er sehr mag, wie er sagt. Er sieht eher harmlos aus, ist groß, hat dunkle Haare. Als wolle er das Klischee kultivieren, das viele von Hackern haben, trägt er einen schwarzen Kapuzenpulli und einen Decknamen. Sein echter Name ist der Redaktion bekannt, alle seine Angaben wurden überprüft. Doch er bittet darum, nicht erkennbar zu werden, denn heute forscht er hauptberuflich als Naturwissenschaftler zu künstlicher Intelligenz. Es ist nicht verboten, was er tut, aber er möchte nicht, dass seine Kollegen an seinem besonderen Hobby Anstoß nehmen: den Viren. 

Aids, Corona, Ebola: Echte Viren entstehen durch Mutationen und sind dann kaum wieder aus der Welt zu schaffen. Computerviren werden programmiert von Menschen wie Thomas Sperl. Sind sie einmal in der Welt, zirkulieren auch sie immer weiter, als Würmer, Trojaner, Ransomware. Kriminelle erbeuten mit ihnen Millionen, manchmal gefährden sie dabei auch Menschenleben. Mit Computerviren können sie Unternehmen, Stadtverwaltungen und Krankenhäuser lahmlegen. In Kriegen werden solche Viren als Waffe eingesetzt, um die Infrastruktur ganzer Länder zu beschädigen. Thomas Sperl feilt an solchen Viren, perfektioniert sie, setzt ihren Code in die Welt. Weil er Viren spannend findet, wie er sagt. Zuletzt hat er sogar den Chatbot ChatGPT dafür eingespannt.

Sperl stammt aus einem winzigen Dorf in Österreich, auf einen Einwohner kommen dort zehn Kühe, so beschreibt er es selbst. Er sei ein mieser Schüler gewesen. Schlau, aber nicht schulschlau. Das meiste im Unterricht langweilte ihn, das Abitur habe er fast verhauen, erzählt er. Keine guten Bedingungen für eine große Karriere. 

Bis die Computer kamen. Die haben ihn sofort interessiert. Auf einer der Disketten, die er damals mit anderen tauscht, holt er sich versehentlich ein Virus ins Haus. Das Antivirenprogramm des elterlichen Rechners springt an. Sperl ist begeistert: Bis dahin habe er gar nicht gewusst, dass es so etwas gibt, sagt er. Er bringt sich Programmieren bei, und als er sieht, wie der Computer etwas ausspuckt, das er selbst geschaffen hat, wird ihm klar: Das ist sein neues Leben. In seinem Dorf, in dem man nichts machen konnte, sagt Thomas Sperl, sei das ""ein Lichtblick"" gewesen. 

Dabei reizt ihn nicht das Licht, sondern offenbar das Dunkle, das Verbotene. Er gibt sich einen Kampfnamen: SPTH, das steht für ""Second Part to Hell"", Zweiter Teil der Hölle. Liest man in seinem bis heute bestehenden Blog seine damaligen Einträge, wird deutlich, wie fasziniert er schon immer von der Idee gewesen sein muss, etwas Großes, Verheerendes zu schaffen. Im Jahr 2002 veröffentlichte er nicht nur seinen Trojaner- sowie einen Wurm-Generator, sondern auch einen Code für einen angeblich ""perfekten Internetwurm"". Es finden sich außerdem Anleitungen für das erste CD-Virus, für ein Schadprogramm, das Wikipedia missbraucht, um sich zu verbreiten, und für ein Computervirus, das sich immer neu verkleidet, um seinen Jägern zu entwischen. Ein durchaus teuflisches Instrumentarium. 

In seinen Beiträgen wurde klar, wie stolz Sperl auf sein Können ist. Er ist Teil einer Gruppe von Menschen, die denken und programmieren wie er. Die sich gegenseitig befeuern, sich für ihre Kreationen loben. Sie hätten ""Hightech"" geschaffen, sagt Sperl noch heute. Immer wieder schreibt er in seinen Texten, wie Antivirenfirmen daran gescheitert seien, seine Kreationen zu erkennen und unschädlich zu machen. Und dass IT-Sicherheitsexperten seine Arbeit ernst nähmen. 
"
KI,Zeit,2024-01-18,https://www.zeit.de/news/2024-01/17/samsung-will-mit-ki-offensive-gegen-apple-punkten,Smartphone-Rivalen: Samsung will mit KI-Offensive gegen Apple punkten | ZEIT ONLINE,"Samsung setzt auf Künstliche Intelligenz, um mit seinem nächsten Top-Smartphone gegen Apples iPhone anzutreten. In der Galaxy-S24-Serie soll KI-Software unter anderem Bilder verbessern, Texte zusammenfassen und Unterhaltungen übersetzen. 

Samsung inszenierte die Premiere mit Lichtshow auf großer Bühne im Silicon Valley, gerade einmal ein Dutzend Kilometer von der Apple-Zentrale entfernt. Markantes Detail der neuen Generation: Der kantige Rahmen der Basis-Varianten S24 und S24+ erinnert sehr stark an die Konturen jüngster iPhones.

Samsungs Smartphone-Chef TM Roh versprach in der Arena, in der sonst das Eishockey-Team San Jose Sharks spielt, KI-Innovationen, die das Leben einfacher machen sollen. Sein Anspruch: Samsung werde den «globalen Standard für mobile Künstliche Intelligenz setzen». Und das S24 sei «das erste KI-Telefon».

Die S24-Reihe bietet unter anderem Echtzeit-Übersetzungen von Telefonanrufen und Unterhaltungen in 13 Sprachen, darunter Deutsch. Die Software soll Spiegelungen in Fotos entfernen können. Herkömmliche Videos können nachträglich in Zeitlupenaufnahmen umgewandelt werden - die Software erstellt dafür künstlich zusätzliche Bilder.

Einige KI-Funktionen wie die Computer-Übersetzung bei Telefonanrufen basieren auf hauseigener Entwicklung bei Samsung. Für andere lieferte KI- und Suchmaschinen-Schwergewicht Google als Kooperationspartner viel seiner Technologie. Ein Beispiel ist eine Suchfunktion, für die es reicht, ein Wort, ein Satz oder ein Objekt auf dem Bildschirm des Smartphones zu umkreisen. Schon tauchen am unteren Ende des Displays Ergebnisse der Google-Suche auf. Der Internet-Konzern bringt die Funktion auch auf sein hauseigenes Smartphone Pixel 8.

TM Roh ließ zugleich durchblicken, dass in der Zukunft nicht mehr alle Funktionen auf Basis Künstlicher Intelligenz kostenlos bleiben dürften. Einige Kunden dürften bereit sein, für fortgeschrittenere Funktionalität Geld zu bezahlen, sagte der Samsung-Manager. Basis-Lösungen könnten dagegen weiter gratis angeboten werden. Wo die Trennlinie zwischen Gratis- und Bezahl-KI verlaufen wird, müsse auf Grundlage vieler verschiedener Faktoren entschieden werden.

Samsung stellt sich auf eine lange Lebenszeit der Geräte ein: Neue Smartphones sollen sieben Jahre lang mit Sicherheits-Updates versorgt werden.

Mit einem vernetzten Ring will Samsung zudem ein weiteres Gerät zum Sammeln von Gesundheitsdaten auf den Markt bringen. Der Ring werde voraussichtlich in der zweiten Jahreshälfte auf den Markt kommen, sagte der Chef von Samsungs Gesundheitsgeschäft, Hon Pak. Das Gerät sei unter anderem für diejenigen Kunden gedacht, die auch im Schlaf Informationen sammeln wollten, aber keine Lust hätten, dafür auch nachts eine Computer-Uhr zu tragen.

Bei Details wie konkreten Funktionen oder Preis hält sich Samsung noch bedeckt. Der Konzern zeigte am Rande der Vorstellung seines neuen Top-Smartphones Galaxy S24 aber einen funktionsfähigen Prototypen. Der extrem leichte Ring hat eine Hülle aus Titan. Ringe anderer Anbieter können unter anderem Puls und Aktivität messen. 

Samsung mit seinen hunderten Millionen Nutzern könnte der Produktkategorie einen stärkeren Schub geben als bisher in dem Markt aktive kleinere Anbieter. 

Samsung präsentierte sein Flaggschiff-Modell zum ersten Mal seit mehr als einem Jahrzehnt nicht als Smartphone-Marktführer: Apple erklomm im vergangenen Jahr nach Berechnungen der Analysefirma IDC erstmals den Thron. Ihr Experte Francisco Jeronimo fand das bemerkenswert, da ein iPhone 2,5 mal mehr koste als ein durchschnittliches Smartphone. Samsung hielt den Spitzenplatz beim Absatz durch das breite Angebot an Modellen in verschiedenen Preisklassen, während Apple stets die meisten Profite im Markt abschöpfte.

© dpa-infocom, dpa:240117-99-652090/3
"
Künstliche Intelligenz,Zeit,2024-01-17,https://www.zeit.de/digital/2024-01/kuenstliche-intelligenz-chatgpt-schule-experten,Künstliche Intelligenz: Experten raten von KI-Einsatz an Grundschulen ab | ZEIT ONLINE,"Ein Einsatz von künstlicher Intelligenz (KI) mit Programmen wie ChatGPT in Schulen hat aus Sicht von Bildungsexperten großes Potenzial. Es gebe allerdings viele Voraussetzungen und Bedingungen für eine lernförderliche und verantwortungsbewusste Nutzung dieser Instrumente, teilte die Ständige Wissenschaftliche Kommission (SWK) der Kultusministerkonferenz in einem Papier mit. 

Das Gremium empfahl kurzfristig eine Übergangsphase zur systematischen Erprobung solcher KI-Tools ""bei offener Fehlerkultur"". 

Auch auf Risiken und Hürden wies die SWK hin. ""KI kann und sollte Lehr-Lernprozess unterstützen, die finale Entscheidung beziehungsweise Bewertung und die Verantwortung für das Endprodukt muss bei Menschen liegen"". Lehrkräfte müssten dafür qualifiziert sein, Fortbildungsangebote rasch ausgebaut werden. 

KI-Chatbots wie ChatGPT können Texte auf dem sprachlichen Niveau eines Menschen formulieren. Das Prinzip dahinter ist, dass sie Wort für Wort abschätzen, wie ein Satz weitergehen sollte. Angelernt werden die Modelle mit gewaltigen Mengen an Informationen. Die Veröffentlichung von ChatGPT Ende vergangenen Jahres löste einen weltweiten Hype rund um künstliche Intelligenz aus. Auf ChatGPT greifen weltweit laut der Entwicklerfirma OpenAI pro Woche rund 100 Millionen Nutzer zu.

In der Grundschule sollte der Kommission zufolge auf texterstellende KI-Instrumente wie ChatGPT ganz und in den ersten Jahren der weiterführenden Schule weitgehend verzichtet werden. Hier müsse der Fokus auf dem Erwerb von Lese- und Schreibkompetenzen der Kinder liegen. Vom achten Jahrgang an könne ein regelmäßiger Einsatz als Schreibunterstützung erfolgen, während weiterhin auch Texte ohne diese Hilfsmittel erstellt werden sollten. Die Verwendung von KI müsse eng begleitet werden.

Die KI-Programme können nach SWK-Angaben vor allem dann unterstützen, ""wenn Lernende über hohe fachliche, Schreib-, Lese- und digitale Kompetenzen verfügen"". Sie sollten daher bei älteren Schülern wie auch in Hochschulen zum Einsatz kommen. Es gehe um eine ""produktive Nutzung"" dieser Technologie. Der Aufbau der Lese- und Schreibkompetenz in den ersten Schuljahren solle ohne sogenannte große Sprachmodelle (Large Language Models/LLM) wie ChatGPT und Co. erfolgen.

Aktuelle Schätzungen gehen laut Kommission davon aus, dass mindestens 20 Prozent der Schülerinnen und Schüler in Deutschland ChatGPT bereits als Infoquelle, für Textproduktion und -übersetzung verwenden. Für Lehrkräfte sehen die Bildungsexperten ebenfalls viele, oft noch unterschätzte Möglichkeiten: etwa für die Unterrichtsplanung, das Erstellen von Wissenstests mit unterschiedlichen Schwierigkeitsgraden oder auch die Entwicklung von Unterrichtsmaterial, differenziert nach Leistungsstärke der Schüler. KI könne aber die didaktischen Fachkenntnisse einer Lehrkraft nicht ersetzen.

Chatbots reagieren auf Spracheingaben und erstellen Texte, die auch erfundene Sachverhalte und Fehler enthalten, aber dennoch plausibel klingen. Die Schüler müssen in der Lage sein, Inhalte hinsichtlich Qualität, Korrektheit, Vertrauenswürdigkeit zu bewerten, die Steuerung im Prozess durch ihre Spracheingaben zu übernehmen, wie das Gremium schreibt. Kritisches, analytisches Denken, auch fachliches Wissen seien erforderlich. Gerade bei schwächeren Lernenden könnten diese Kompetenzen eher nicht vorausgesetzt werden.

Ein versierter Umgang der Schülerinnen und Schüler mit den KI-Instrumenten solle als neues Lernziel geübt und auch geprüft werden. Entsprechend müssten Lehrkräfte qualifiziert sein. ""Die dynamische Entwicklung der Tools fordert die Lehrkräfte besonders."" Die Verantwortung für eine Verwendung der KI – etwa zur Aufgabenerstellung oder Leistungsbeurteilung – soll laut Empfehlung bei den Lehrerinnen und Lehrern liegen.

Derzeit gebe es Unsicherheiten auch mit Blick auf Prüfungsformate, hier müsse die Prüfungskultur weiterentwickelt werden. Die Kommission rät in Prüfungen zur Unterscheidung zwischen hilfsmittelfreien Teilen und solchen, in denen KI-Tools genutzt werden dürfen. Kommen solche Instrumente zum Einsatz, ""sollte nicht nur der letztendliche Text, sondern auch die reflektierte Auseinandersetzung der Schülerinnen und Schüler mit der Erstellung und dem Ergebnis Gegenstand der Beurteilung sein"". Es sei davon auszugehen, dass eine gekonnte ""Koaktivität"" mit ChatGPT und Co. eine wichtige Zukunftskompetenz darstellen werde.

Das SWK-Papier verweist auch auf ""technologische, ethische und rechtliche Probleme"", die einen rechtmäßigen Einsatz im Schulbereich infrage stellten. Der Einsatz kommerzieller Tools sei marktwirtschaftlichen Interessen unterworfen, sie seien nicht für die Schulen gemacht worden. Der Bildungspolitik komme die Aufgabe zu, KI-Instrumente in geeignete Lernplattformen zu integrieren. ""Eine besonders große Herausforderung besteht derzeit noch darin, Tools für den Einsatz im Bildungskontext und in speziellen Fächern zu entwerfen"", schreibt die Direktorin des Leibniz-Instituts für Wissensmedien, Ulrike Cress.

Allen Lernenden und Lehrenden sollte dem Gremium zufolge ein kostenfreier oder günstiger Zugriff auf diese Tools ermöglicht werden. Die Präsidentin der Kultusministerkonferenz, Saarlands Bildungsministerin Christine Streichert-Clivot (SPD), sagte: ""Technologischer Fortschritt darf nicht zu stärkerer sozialer Ungleichheit führen, sondern die Chancen müssen für alle zugänglich sein.""
"
Künstliche Intelligenz,Zeit,2024-01-16,https://www.zeit.de/news/2024-01/16/microsoft-oeffnet-copilot-assistenten-fuer-verbraucher,"Künstliche Intelligenz: Microsoft öffnet ""Copilot""-Assistenten für Verbraucher | ZEIT ONLINE","Microsoft treibt seine KI-Offensive weiter voran und macht seinen «Copilot»-Assistenten in Produkten auch für Verbraucher verfügbar. 

Für 20 Dollar im Monat bekommen sie Zugang zur neuesten Version des Chatbots ChatGPT sowie Funktionen zum Erstellen von Bildern, wie der Konzern in der Nacht mitteilte. 

Verbraucher, die ein Abo für Microsofts Office-Software haben, werden auch dort KI-Funktionen nutzen können. In der Unternehmens-Version wird die Anforderung aufgehoben, mindestens 300 Abonnements abzuschließen.

Microsoft hatte einen milliardenschweren Pakt mit der ChatGPT-Entwicklerfirma OpenAI geschlossen und versucht, in alle seine Produkte Funktionen auf Basis Künstlicher Intelligenz unter dem «Copilot»-Namen zu integrieren. Der Konzern wird damit zu einer treibenden Kraft beim verstärkten Einsatz von KI in verschiedenen Bereichen. Nun führt Microsoft auch eine App für Android-Smartphones und iPhones mit «Copilot»-Funktionen ein.

© dpa-infocom, dpa:240116-99-628040/3
"
Künstliche Intelligenz,Zeit,2024-01-16,https://www.zeit.de/digital/internet/2024-01/kuenstliche-intelligenz-openai-werkzeuge-fake-news-wahlen-usa,OpenAI: OpenAI will ChatGPT gegen Desinformation sichern | ZEIT ONLINE,"Der KI-Entwickler OpenAI plant vor den großen Wahlen in diesem Jahr die Einführung von Hilfsmitteln zur Identifikation von Desinformation. ""Wir wollen sicherstellen, dass unsere Technologie nicht in einer Weise genutzt wird, die den demokratischen Prozess untergraben könnte"", teilte das Unternehmen mit. Entwickler arbeiteten derzeit an Werkzeugen, die es Nutzern ermöglichen sollen, vom Programm ChatGPT generierte Texte zuverlässig zuzuordnen und zu erkennen, ob ein Bild durch künstliche Intelligenz erstellt wurde. ""Wir arbeiten noch daran, zu verstehen, wie effektiv unsere Tools für 
die personalisierte Beeinflussung sein könnten"", hieß es in der Mitteilung.  

2024 ist ein sogenanntes Superwahljahr: In mehreren großen Ländern finden wichtige Wahlen statt, darunter in den USA, Indien und Großbritannien. Laut einer in der vergangenen Woche veröffentlichten Studie des Weltwirtschaftsforums (WEF) stellt die Beeinflussung von Wahlen durch KI-gesteuerte Desinformation kurzfristig eines der weltweit größten Risiken dar.

Bei Fragen zur US-Wahl leitet ChatGPT dem Unternehmen zufolge die Nutzer auf offizielle Webseiten. ""Die Lehren, die wir aus dieser Arbeit ziehen, werden die Grundlage unseres Ansatzes für andere Länder und Regionen sein"", teilte das Unternehmen. Zudem enthalte sein Bildgenerator Dall-E 3 ""Leitplanken"", die verhindern sollen, dass Nutzer Bilder von realen Personen – etwa von Präsidentschaftskandidaten – erzeugen können.

Auch andere US-Unternehmen wie Google und die Facebook-Muttergesellschaft Meta hatten im vergangenen Jahr bekannt gegeben, auf ihren Plattformen die Beeinflussung von Wahlen durch den Einsatz von KI begrenzen zu wollen.

ChatGPT gibt es seit gut einem Jahr. Die rasante Weiterentwicklung von künstlicher Intelligenz fasziniert Experten und Öffentlichkeit – sorgt mit Blick auf den Arbeitsmarkt, Datenschutz und mögliche Desinformation aber auch für große Sorge. Die EU will den Einsatz von KI mit dem sogenannten AI Act regulieren.
"
AI,Zeit,2024-01-17,https://www.zeit.de/news/2024-01/17/nokia-investiert-360-millionen-in-chip-design-in-deutschland,Technologie: Nokia investiert 360 Millionen in Chip-Design in Deutschland | ZEIT ONLINE,"Nokia will insgesamt 360 Millionen Euro an den Standorten Ulm und Nürnberg vor allem in Chip-Design investieren. Das kündigte der finnische Mobilfunkausrüster an. 

Die Großinvestition findet über einen Zeitraum von vier Jahren im Rahmen des europäischen IPCEI-Programms (Important Projects of Common European Interest) statt, das vom Bund sowie Baden-Württemberg und Bayern gefördert wird.

Bei dem Projekt sollen vor allem Chips für Funk- und Optikprodukte entwickelt werden, die in künftigen Mobilfunksystemen (5G-Advanced und 6G) zum Einsatz kommen sollen. Die neuen Mikroprozessoren sollen auch möglichst wenig Strom verbrauchen, um die europäischen Klimaziele einhalten zu können. Zu diesem Zweck arbeite man eng mit Forschungsinstituten und Universitäten zusammen, erklärte Nokia. Diese Zusammenarbeit werde durch die langfristige IPCEI-Investition und -Finanzierung gestärkt.

Deutschlandchef Eleftherios Papadopoulos sagte, das Projekt sei ein Meilenstein für Nokia und die Zukunft der Telekommunikationsbranche in Deutschland und Europa. «Es wird die Wettbewerbsfähigkeit und die Innovationskraft Europas speziell im Bereich Mikroelektronik für Zukunftstechnologien wie 6G und Künstliche Intelligenz (AI) stärken, komplexe Anwendungen etwa für das Metaverse ermöglichen und die Digitalisierung voranbringen.» Deutschland habe im Hinblick auf Forschung, Entwicklung sowie als Absatzmarkt für Nokia einen hohen Stellenwert.

Das Förderprogramm IPCEI wurde im Frühjahr 2023 von der EU-Kommission genehmigt und verteilt Beihilfen von bis zu 8,1 Milliarden Euro an Projekte mit Schwerpunkt auf Mikroelektronik und Kommunikationstechnologie von 14 EU-Staaten, darunter Deutschland. Alleine Deutschland erhält mit rund vier Milliarden etwa die Hälfte dieser Beihilfen.

Nokia ist neben den chinesischen Wettbewerbern Huawei und ZTE sowie dem europäischen Rivalen Ericsson einer der führenden Anbieter von Mobilfunk-Infrastruktur weltweit.

© dpa-infocom, dpa:240117-99-643460/2
"
AI,Zeit,2024-01-17,https://www.zeit.de/digital/mobil/2024-01/samsung-galaxy-s24-ki-apple,Samsung Galaxy S24: Das erste AI-Phone kommt nicht von Apple | ZEIT ONLINE,
AI,Zeit,2024-01-16,https://www.zeit.de/digital/internet/2024-01/kuenstliche-intelligenz-openai-werkzeuge-fake-news-wahlen-usa,OpenAI: OpenAI will ChatGPT gegen Desinformation sichern | ZEIT ONLINE,"Der KI-Entwickler OpenAI plant vor den großen Wahlen in diesem Jahr die Einführung von Hilfsmitteln zur Identifikation von Desinformation. ""Wir wollen sicherstellen, dass unsere Technologie nicht in einer Weise genutzt wird, die den demokratischen Prozess untergraben könnte"", teilte das Unternehmen mit. Entwickler arbeiteten derzeit an Werkzeugen, die es Nutzern ermöglichen sollen, vom Programm ChatGPT generierte Texte zuverlässig zuzuordnen und zu erkennen, ob ein Bild durch künstliche Intelligenz erstellt wurde. ""Wir arbeiten noch daran, zu verstehen, wie effektiv unsere Tools für 
die personalisierte Beeinflussung sein könnten"", hieß es in der Mitteilung.  

2024 ist ein sogenanntes Superwahljahr: In mehreren großen Ländern finden wichtige Wahlen statt, darunter in den USA, Indien und Großbritannien. Laut einer in der vergangenen Woche veröffentlichten Studie des Weltwirtschaftsforums (WEF) stellt die Beeinflussung von Wahlen durch KI-gesteuerte Desinformation kurzfristig eines der weltweit größten Risiken dar.

Bei Fragen zur US-Wahl leitet ChatGPT dem Unternehmen zufolge die Nutzer auf offizielle Webseiten. ""Die Lehren, die wir aus dieser Arbeit ziehen, werden die Grundlage unseres Ansatzes für andere Länder und Regionen sein"", teilte das Unternehmen. Zudem enthalte sein Bildgenerator Dall-E 3 ""Leitplanken"", die verhindern sollen, dass Nutzer Bilder von realen Personen – etwa von Präsidentschaftskandidaten – erzeugen können.

Auch andere US-Unternehmen wie Google und die Facebook-Muttergesellschaft Meta hatten im vergangenen Jahr bekannt gegeben, auf ihren Plattformen die Beeinflussung von Wahlen durch den Einsatz von KI begrenzen zu wollen.

ChatGPT gibt es seit gut einem Jahr. Die rasante Weiterentwicklung von künstlicher Intelligenz fasziniert Experten und Öffentlichkeit – sorgt mit Blick auf den Arbeitsmarkt, Datenschutz und mögliche Desinformation aber auch für große Sorge. Die EU will den Einsatz von KI mit dem sogenannten AI Act regulieren.
"
AI,Zeit,2024-01-15,https://www.zeit.de/wirtschaft/2024-01/weltwirtschaftsforum-davos-2024-krisen-politik,Weltwirtschaftsforum in Davos : Die Getriebenen von Davos | ZEIT ONLINE,"Je größer die Krisen, desto größer ist der Gesprächsbedarf, heißt es – so gesehen verwundert es nicht, dass das Weltwirtschaftsforum (WEF) in diesem Jahr wieder eine lange Teilnehmerliste präsentiert. Mehr als 60 Staats- und Regierungschefs haben sich angekündigt – deutlich mehr als in den vergangenen Jahren. Doch die Vorzeichen haben sich für die Wirtschafts- und Finanzelite dramatisch verändert. Wenn die dunklen Limousinen der Top-Manager ab diesem Montag in dem Schweizer Skiort Davos vorfahren, dann dient dies auch der Selbstvergewisserung der eigenen Macht. Wo vor Jahren noch über Freihandelszonen, Zölle und internationale Steuern diskutiert – und diese sicher auch mitgestaltet wurden –, geht es heute um Blockbildung, um Weltpolitik. Das große Geld hat an Einfluss verloren, es ist die Geopolitik, die die Manager weltweit vor sich hertreibt.  

Als Schwerpunkt für das diesjährige Treffen haben sich die Verantwortlichen das Motto ""Vertrauen wieder herstellen"" ausgedacht – was sich praktischerweise auf jede der aktuellen Herausforderungen anwenden lässt: die Kriege in der Ukraine und in Nahost, die Attacken auf Handelsschiffe im Roten Meer, die den Welthandel stören, der Klimawandel, der Einsatz künstlicher Intelligenz und die Folgen. Und dann stehen in diesem Jahr ja auch noch Wahlen in mehr als 50 Ländern an, darunter in Indien und in den USA. All das sind Risikofaktoren, denen Unternehmen weitestgehend ohnmächtig gegenüberstehen, die aber ihren Gestaltungsspielraum stark beeinflussen. Die geopolitischen Spannungen seien in diesem Jahr so groß wie seit Jahrzehnten nicht, sagt Forumspräsident Børge Brende. ""Der einzige Weg nach vorn ist: zusammenkommen und Lösungen finden.""

Bemerkenswert ist, dass mit Chinas Ministerpräsident Li Qiang und US-Außenminister Antony Blinken sowie US-Präsident Joe Bidens nationalem Sicherheitsberater, Jake Sullivan, erstmals seit Jahren wieder hochrangige chinesische und US-amerikanische Regierungsvertreter gleichzeitig teilnehmen werden. Auch der ukrainische Präsident Wolodymyr Selenskyj wird erstmals seit Beginn des russischen Angriffs auf die Ukraine wieder vor Ort sein und am Dienstag vor dem Plenum sprechen. In den vergangenen Jahren hatte er digital um Unterstützung für sein Land geworben. Bereits am Sonntag fand in Davos ein Ukraine-Treffen statt. Daran haben Sicherheitsberater und ranghohe Beamte aus rund 70 Ländern teilgenommen.  

Es dürfte ein wichtiger Auftritt werden für Selenskyj. Während Russlands Präsident Wladimir Putin die Ukraine zuletzt wieder großflächig mit Raketen angegriffen hat, gehen dem ukrainischen Militär zunehmend Waffen und Munition aus. Doch die internationale Unterstützung bröckelt. In den USA drängt Präsident Biden seit Wochen den Kongress, weitere 61 Milliarden Dollar an Militärhilfen für die Ukraine zu beschließen. Viele republikanische Kongressabgeordnete sind jedoch dagegen oder knüpfen sie an Bedingungen. Auch in der EU lässt die Solidarität mit der Ukraine nach. Es kommt weniger Munition als angekündigt in Kiew an, Hilfsgelder stecken in der Brüsseler Bürokratie fest. Es wird erwartet, dass sich EU-Kommissionspräsidentin Ursula von der Leyen dazu äußern wird, wenn sie ebenfalls am Dienstag in Davos spricht.  

Zu der Kriegsmüdigkeit im Westen kommt hinzu, dass sich die Ukraine die globale Aufmerksamkeit inzwischen mit dem Krieg in Gaza teilen muss, der nach den jüngsten Angriffen der USA und Großbritanniens auf Huthi-Stellungen immer weitere Kreise zieht. Das WEF hat auch das Potenzial, wichtige Interessenvertreter rund um den Gaza-Krieg zusammenzubringen. Neben den Ministerpräsidenten aus Katar, dem Irak, Jordanien und Libanon steht der israelische Präsident Izchak Herzog auf der Gästeliste. Nach Medienberichten wird auch der iranische Außenminister erwartet. Der Golfstaat Katar vermittelt zwischen Israel und der Hamas und machte so zuletzt auch den vorübergehenden Waffenstillstand möglich. Von palästinensischer Seite nimmt in Davos allerdings niemand teil.   

Anders als in den vergangenen Jahren ist Deutschland weniger prominent vertreten. Bundeskanzler Olaf Scholz (SPD) reist nicht nach Davos. Von deutscher Seite nehmen unter anderem Außenministerin Annalena Baerbock, Wirtschaftsminister Robert Habeck (beide Grüne) und Finanzminister Christian Lindner (FDP) teil. 

Wie ausgeprägt der Pessimismus weltweit ist, zeigt der Risikoreport, den das WEF vor wenigen Tagen veröffentlicht hat. Große Sorgen bereiten den knapp 1.500 Expertinnen und Experten vor allem der Klimawandel und Extremwetterereignisse. Fehl- und Desinformation werden in dem Report an zweiter Stelle bei den potenziellen Auslösern globaler Krisen aufgeführt. Sie stellen in den nächsten zwei Jahren demnach sogar das größte globale Risiko dar und könnten Wahlen weltweit beeinflussen, befürchten die Autoren. Knapp vier Milliarden Menschen, darunter die Bevölkerung in Indien, Russland, Mexiko, Südafrika, in den Vereinigten Staaten und die in den 27 EU-Ländern, bei den Wahlen zum Europäischen Parlament, werden 2024 zu den Urnen gehen. Insgesamt stehen sie für über 60 Prozent der weltweiten Wirtschaftsleistung. 

""Der weitverbreitete Einsatz von Fehlinformationen und Desinformationen sowie Instrumenten zu ihrer Verbreitung kann die Legitimität neu gewählter Regierungen untergraben"", warnen die vom WEF befragten Experten. Die daraus resultierenden sozialen Spannungen könnten von gewalttätigen Protesten und Hassverbrechen bis hin zu Extremismus reichen, heißt es in dem Bericht. Immerhin – es wäre ein Bereich, in dem die Unternehmen noch gestalten können, schließlich haben Konzerne wie Microsoft mit seinem Investment in Open AI großen Einfluss darauf, wie Unternehmen und Bürgerinnen und Bürger künftig künstliche Intelligenz nutzen. Es dürfte neben der Geopolitik das große Thema auf dem WEF werden  – Microsoft Chef Satya Nadella wird auch erwartet.  
"
Artificial Intelligence,Zeit,2024-01-17,https://www.zeit.de/digital/mobil/2024-01/samsung-galaxy-s24-ki-apple,Samsung Galaxy S24: Das erste AI-Phone kommt nicht von Apple | ZEIT ONLINE,
KI,Zeit,2024-01-17,https://www.zeit.de/digital/2024-01/kuenstliche-intelligenz-chatgpt-schule-experten,Künstliche Intelligenz: Experten raten von KI-Einsatz an Grundschulen ab | ZEIT ONLINE,"Ein Einsatz von künstlicher Intelligenz (KI) mit Programmen wie ChatGPT in Schulen hat aus Sicht von Bildungsexperten großes Potenzial. Es gebe allerdings viele Voraussetzungen und Bedingungen für eine lernförderliche und verantwortungsbewusste Nutzung dieser Instrumente, teilte die Ständige Wissenschaftliche Kommission (SWK) der Kultusministerkonferenz in einem Papier mit. 

Das Gremium empfahl kurzfristig eine Übergangsphase zur systematischen Erprobung solcher KI-Tools ""bei offener Fehlerkultur"". 

Auch auf Risiken und Hürden wies die SWK hin. ""KI kann und sollte Lehr-Lernprozess unterstützen, die finale Entscheidung beziehungsweise Bewertung und die Verantwortung für das Endprodukt muss bei Menschen liegen"". Lehrkräfte müssten dafür qualifiziert sein, Fortbildungsangebote rasch ausgebaut werden. 

KI-Chatbots wie ChatGPT können Texte auf dem sprachlichen Niveau eines Menschen formulieren. Das Prinzip dahinter ist, dass sie Wort für Wort abschätzen, wie ein Satz weitergehen sollte. Angelernt werden die Modelle mit gewaltigen Mengen an Informationen. Die Veröffentlichung von ChatGPT Ende vergangenen Jahres löste einen weltweiten Hype rund um künstliche Intelligenz aus. Auf ChatGPT greifen weltweit laut der Entwicklerfirma OpenAI pro Woche rund 100 Millionen Nutzer zu.

In der Grundschule sollte der Kommission zufolge auf texterstellende KI-Instrumente wie ChatGPT ganz und in den ersten Jahren der weiterführenden Schule weitgehend verzichtet werden. Hier müsse der Fokus auf dem Erwerb von Lese- und Schreibkompetenzen der Kinder liegen. Vom achten Jahrgang an könne ein regelmäßiger Einsatz als Schreibunterstützung erfolgen, während weiterhin auch Texte ohne diese Hilfsmittel erstellt werden sollten. Die Verwendung von KI müsse eng begleitet werden.

Die KI-Programme können nach SWK-Angaben vor allem dann unterstützen, ""wenn Lernende über hohe fachliche, Schreib-, Lese- und digitale Kompetenzen verfügen"". Sie sollten daher bei älteren Schülern wie auch in Hochschulen zum Einsatz kommen. Es gehe um eine ""produktive Nutzung"" dieser Technologie. Der Aufbau der Lese- und Schreibkompetenz in den ersten Schuljahren solle ohne sogenannte große Sprachmodelle (Large Language Models/LLM) wie ChatGPT und Co. erfolgen.

Aktuelle Schätzungen gehen laut Kommission davon aus, dass mindestens 20 Prozent der Schülerinnen und Schüler in Deutschland ChatGPT bereits als Infoquelle, für Textproduktion und -übersetzung verwenden. Für Lehrkräfte sehen die Bildungsexperten ebenfalls viele, oft noch unterschätzte Möglichkeiten: etwa für die Unterrichtsplanung, das Erstellen von Wissenstests mit unterschiedlichen Schwierigkeitsgraden oder auch die Entwicklung von Unterrichtsmaterial, differenziert nach Leistungsstärke der Schüler. KI könne aber die didaktischen Fachkenntnisse einer Lehrkraft nicht ersetzen.

Chatbots reagieren auf Spracheingaben und erstellen Texte, die auch erfundene Sachverhalte und Fehler enthalten, aber dennoch plausibel klingen. Die Schüler müssen in der Lage sein, Inhalte hinsichtlich Qualität, Korrektheit, Vertrauenswürdigkeit zu bewerten, die Steuerung im Prozess durch ihre Spracheingaben zu übernehmen, wie das Gremium schreibt. Kritisches, analytisches Denken, auch fachliches Wissen seien erforderlich. Gerade bei schwächeren Lernenden könnten diese Kompetenzen eher nicht vorausgesetzt werden.

Ein versierter Umgang der Schülerinnen und Schüler mit den KI-Instrumenten solle als neues Lernziel geübt und auch geprüft werden. Entsprechend müssten Lehrkräfte qualifiziert sein. ""Die dynamische Entwicklung der Tools fordert die Lehrkräfte besonders."" Die Verantwortung für eine Verwendung der KI – etwa zur Aufgabenerstellung oder Leistungsbeurteilung – soll laut Empfehlung bei den Lehrerinnen und Lehrern liegen.

Derzeit gebe es Unsicherheiten auch mit Blick auf Prüfungsformate, hier müsse die Prüfungskultur weiterentwickelt werden. Die Kommission rät in Prüfungen zur Unterscheidung zwischen hilfsmittelfreien Teilen und solchen, in denen KI-Tools genutzt werden dürfen. Kommen solche Instrumente zum Einsatz, ""sollte nicht nur der letztendliche Text, sondern auch die reflektierte Auseinandersetzung der Schülerinnen und Schüler mit der Erstellung und dem Ergebnis Gegenstand der Beurteilung sein"". Es sei davon auszugehen, dass eine gekonnte ""Koaktivität"" mit ChatGPT und Co. eine wichtige Zukunftskompetenz darstellen werde.

Das SWK-Papier verweist auch auf ""technologische, ethische und rechtliche Probleme"", die einen rechtmäßigen Einsatz im Schulbereich infrage stellten. Der Einsatz kommerzieller Tools sei marktwirtschaftlichen Interessen unterworfen, sie seien nicht für die Schulen gemacht worden. Der Bildungspolitik komme die Aufgabe zu, KI-Instrumente in geeignete Lernplattformen zu integrieren. ""Eine besonders große Herausforderung besteht derzeit noch darin, Tools für den Einsatz im Bildungskontext und in speziellen Fächern zu entwerfen"", schreibt die Direktorin des Leibniz-Instituts für Wissensmedien, Ulrike Cress.

Allen Lernenden und Lehrenden sollte dem Gremium zufolge ein kostenfreier oder günstiger Zugriff auf diese Tools ermöglicht werden. Die Präsidentin der Kultusministerkonferenz, Saarlands Bildungsministerin Christine Streichert-Clivot (SPD), sagte: ""Technologischer Fortschritt darf nicht zu stärkerer sozialer Ungleichheit führen, sondern die Chancen müssen für alle zugänglich sein.""
"
KI,Zeit,2024-01-16,https://www.zeit.de/digital/2024-01/ki-unterricht-chatgpt-hilfe,KI im Unterricht: Wenn die KI die Klassenarbeit korrigiert | ZEIT ONLINE,"Für Gert Mengel gibt es eine Zeit vor und eine nach ChatGPT. Im Januar 2023 war der
Leiter der Don-Bosco-Schule in Rostock auf einem Kongress für Schulleiterinnen
und Schulleiter. Von 50 kannten damals nur vier ChatGPT, erzählt er. Die verbleibenden
Kolleginnen und Kollegen erfuhren erst dort, dass der Bot in kürzester Zeit auch
ziemlich hausaufgabentaugliche Texte formulieren kann. Auf dem Kongress brach daraufhin
regelrecht Panik aus, so erinnert sich Mengel jedenfalls. ""Da hieß es, wir
müssen das sofort verbieten."" 

Ein Jahr später nutzen nicht nur Schülerinnen künstliche Intelligenz –
sondern auch ihre Lehrer. Die
Stimmung habe sich gedreht, sagt Mengel. Heute geht es längst nicht mehr darum,
künstliche Intelligenz aus den Schulnetzwerken zu verbannen, sondern vielmehr
darum, wie man sie klug einsetzt – im Unterricht, bei Prüfungen, aber auch beim
Korrigieren. 

Tanja Reinlein formuliert es so: Erst kam Corona, dann
kam ChatGPT. Reinlein kümmert sich um das Thema digitale Bildung, in
Nordrhein-Westfalen leitet sie die Abteilung Berufliche Bildung, Digitalisierung,
Prävention und Integration, Internationales des Schulministeriums.
NRW mit den deutschlandweit meisten
Schülern hatte knapp drei Monate nach Veröffentlichung von
ChatGPT als erstes Bundesland einen Handlungsleitfaden zum Umgang mit textgenerierenden
KI-Systemen (PDF) vorgelegt. Dort hieß es im Februar 2023 bereits, dass die
Auswirkungen auf das Bildungssystem ""immens"" sein werden und ein Verbot ""keine
tragfähige Reaktion"" sein könne. Seit einem Jahr gibt es deshalb auch eine Arbeitsgruppe, die sich,
angedockt an die Kultusministerkonferenz (KMK), um das Thema kümmern soll und von
NRW geleitet wird.  

In Reinleins KMK-Gruppe
ringen sie um nichts anderes als die ganz großen Fragen, die von Schülerinnen und Schülern gestellt werden:
""Müssen wir noch Fremdsprachen lernen, wenn KI doch simultan übersetzen kann? Muss
ich Texte noch flüssig schreiben können? Muss ich überhaupt lesen können, wenn
ChatGPT doch jeden Text verbalisieren kann?"", zählt Reinlein auf. Es geht um
Lehrpläne, die entrümpelt werden sollen. Es geht um neue Prüfungsformate, für
die Schülerinnen und Schüler KI nutzen sollen, und die Frage, wie bei alldem gleichzeitig die Daten von Millionen Schülerinnen und
Schüler in Deutschland ausreichend geschützt werden können. 

Bereits im
Mai, ein gutes halbes Jahr nach Veröffentlichung von ChatGPT, hatten mehr als
die Hälfte der Schülerinnen und Schüler laut einer Umfrage des Branchenverbands Bitkom den Bot
schon einmal genutzt. Selten wurde im Bildungsbereich so viel darüber debattiert,
wie im vergangenen Jahr: Auch, weil anfangs die Sorge so groß war, dass
Schülerinnen und Schüler ChatGPT zum Betuppen und Betrügen nutzen könnten, dass
sie gar nicht mehr lernen müssten, weil die KI ihnen alle Antworten servieren
würde. 

Ein Jahr
später kann man sagen: Ja, Schülerinnen nutzen KI als Hilfsmittel – aber längst
nicht für alles. Forscher der Hochschule Bielefeld (HSBI) befragten
rund 200 Schülerinnen,
die zwischen 15 und 19 Jahre alt waren. Wenig überraschend: Die Schüler verwendeten
KI vor allem, um Hausaufgaben zu machen, sich Texte schreiben oder übersetzen
zu lassen. Also genau das, was man erwarten würde. Wofür sie es weniger
nutzten: um sich Feedback zu ihren Texten von der KI einzuholen oder
Selbsttests zur Prüfungsvorbereitung generieren zu lassen. 

Was die
Forscher anhand der Daten vermuten: Der Bildungsabschluss und der Beruf der
Eltern spielen möglicherweise eine größere Rolle bei der Frage, ob eine
Schülerin KI einsetzt oder nicht. Die Studie zeigt, dass ein höheres Bildungsniveau der Eltern einhergehen kann mit einer selteneren Nutzung von
generativer KI, etwa um eigene Texte zu verbessern oder um sich Blogbeiträge zu
generieren. Schülerinnen und Schüler, deren schulische Leistungen im Mittelfeld liegen, neigten
eher dazu, generierte Ergebnisse einfach zu übernehmen, auch das deuten
die Daten der nicht repräsentativen Studie an. ""Wenn ich als Schülerin gut mit
einer Drei in Deutsch leben kann, reicht mir vielleicht die Gedichtanalyse, die
ChatGPT mir geschrieben hat – auch wenn sie inhaltlich nicht präzise ist"", sagt
Studienautor Thomas Süße, Professor für Personal und Organisation an der HSBI.

Wie groß das
Problem der Schummelei ist, lässt sich kaum beziffern. In einer Umfrage an 40 US-amerikanischen Schulen gaben
60 bis 70 Prozent der Schülerinnen und Schüler an, zuletzt geschummelt zu haben.
Die Gesamtzahl habe sich damit kaum erhöht, sagen die Forscherinnen und
Forscher. In den Jahren 2002 bis 2015 hatten 58 Prozent von 70.000 befragten
Schülern angegeben, schon einmal plagiiert zu haben. 

Stefan Düll
verwundert das kaum. ""Es geht immer noch schneller im Bus eben die Hausaufgaben
abzuschreiben, als sich die zu Hause von einer KI generieren zu lassen, wo ich
vorher noch den Arbeitsauftrag und möglicherweise auch noch einen Text eingeben
muss"", sagt der Präsident des Deutschen Lehrerverbands.
"
KI,Zeit,2024-01-15,https://www.zeit.de/politik/2024-01/weltwirtschaftsforum-davos-wef-2024-nachrichtenpodcast,"Weltwirtschaftsforum: Auf der Tagesordnung in Davos: Kriege, Krisen und KI | ZEIT ONLINE","Zum 54. Mal treffen sich Vertreterinnen und Vertreter großer globaler Unternehmen im Schweizer Skiort Davos. Sie treffen sich beim Weltwirtschaftsforum mit Menschen aus Politik, Wissenschaft und Gesellschaft, um über aktuelle Themen und Probleme zu diskutieren. Dieses Jahr werden
unter anderem EU-Kommissionspräsidentin Ursula von der Leyen,
UN-Generalsekretär António Guterres und der ukrainische Präsident Wolodymyr Selenskyj
erwartet. Das Motto des Treffens ist Rebuilding Trust, Vertrauen
wieder herstellen, vor dem Hintergrund globaler Krisen und Herausforderungen.
Dabei geht es vor allem um Kriege, Auswirkungen der
Klimakrise und die Gefahren von Desinformation mit Blick auf das Superwahljahr 2024. Einen
Großteil der Veranstaltungen kann man im Livestream verfolgen. ZEIT-ONLINE-Wirtschaftsredakteurin Anja Stehle begleitet das Treffen und spricht im Podcast darüber, was in den kommenden Tagen wichtig wird.

Im September hat der Bürgerrat zum Thema Ernährung seine Arbeit aufgenommen. 159 Menschen haben seitdem gemeinsam diskutiert, sich informiert und haben daran anschließend neun
Empfehlungen für die Ernährungspolitik erarbeitet. Gestern hat der Bürgerrat seine Vorschläge an Bundestagspräsidentin Bärbel Bas übergeben. Die Empfehlungen
sind nicht bindend, aber es soll eine Aussprache im Bundestagsplenum
darüber geben. 

Ziel eines Bürgerrats ist unter anderem
ein Stimmungsbild der Gesellschaft zu erarbeiten und die
Alltagserfahrungen der Bevölkerung einzubinden. Der Rat soll eine Art mini Deutschland sein, was die Zusammensetzung beispielsweise in Bezug auf Alter, Geschlecht oder Bildungsstand betrifft. Lenz Jacobsen,
Politikredakteur bei ZEIT ONLINE, hat die Arbeit des Bürgerrats verfolgt und ordnet im Podcast ein, ob dieses Gremium die Demokratie stärken kann.

Und sonst so: In
Schweden landen alte Weihnachtsbäume im Meer. Fische finden das super.

Moderation und Produktion: Simone Gaul

Mitarbeit:
Mathias Peer und
Lea Schüler

Alle
Folgen unseres Podcasts finden Sie hier.
Fragen, Kritik, Anregungen? Sie erreichen uns unter wasjetzt@zeit.de.
"
Künstliche Intelligenz,Zeit,2024-01-13,https://www.zeit.de/digital/2024-01/nina-george-ki-urheberrecht-kreative,"Autorin Nina George über KI: ""Wenig Chancen auf Nachzahlungen"" | ZEIT ONLINE","
In den USA hat die Tageszeitung ""New York Times"" die Entwickler der KI-Software ChatGPT wegen Urheberrechtsverletzungen verklagt. Auch andere wehren sich dagegen, dass ihre Texte zum KI-Training verwendet werden. In Deutschland gehört dazu die Autorin Nina George. Mit mehr als zwei Millionen verkauften Büchern zählt sie zu den derzeit erfolgreichsten deutschsprachigen
Autorinnen im Ausland. 

ZEIT ONLINE: Frau George, Sie schreiben unter Ihrem
eigenen Namen sowie mehreren Pseudonymen alles vom Kinderbuch bis zum
Kriminalroman, manchmal verfassen Sie im Jahr gleich mehrere Bücher. Haben Sie
schon einmal daran gedacht, sich den Plot für ihr nächstes Buch von einer
künstlichen Intelligenz generieren zu lassen?

Nina George: Das Beste am Schreiben ist die Freiheit, über jedes
Wort, jeden Twist, jede Figurenentwicklung selbst zu entscheiden – und die
tiefe Befriedigung, von Ereignissen und Charakterkonstellationen zu erzählen,
von denen so noch nie erzählt wurde. Diese Freiheit gebe ich doch nicht weiter,
um mir das Denken zu sparen. Abgesehen davon beherrscht so eine generative
Informatik ja weder einen originellen noch originalen Plot oder gar eine
stilistisch spannende Erzählform. Stattdessen greift sie auf Gespeichertes aus
einem zusammen geplünderten Bücherbestand zurück – und weiß dabei nicht mal,
was sie da tut. Es werden zudem immer mehr Worte und Kontexte werksseitig
verboten. Das sind etwa sexuell konnotierte Begriffe, politische Kontexte,
Schimpfwörter und laut Open AI ""andere unangemessene Inhalte"", wobei deren
Zensurvorgaben nicht öffentlich einsehbar sind. Was aus dem Textroboter herauskommt,
ist damit mediokre Einheitsbrause. So will ich nicht arbeiten.

ZEIT ONLINE: Welche konkreten Sorgen haben Sie als Autorin in Bezug
auf künstliche Intelligenz?

Nina George: Ich bevorzuge den Begriff künstliche oder simulierte
Kommunikation, wenn es um generative Informatik geht – denn ""intelligent"", und
was wir damit an kognitiven Kompetenzen verbinden, ist sie nicht und wird sie
auch nie werden. Meine Hauptsorge ist, dass Menschen sich selbst verzaubern und sogenannter KI mehr zutrauen, als sie kann. Daneben besorgt mich die
verspielte Blindheit vor allem politischer Entscheidungsträger für die
Herstellungsbedingungen und Auswirkungen solcher Systeme, seien es die
Rechtsbrüche bei der Besorgung von Kunst- und Kulturwerken, sei es die
Ausbeutung von linguistischen Arbeitern in Drittländern, seien es die absurd
enormen Energieverbräuche. Wenn ich lese, wie sorglos und stolz Politiker auf ihre
vermeintliche Technikaffinität verweisen und vom Einsatz generativer KI in Büro und
Alltag berichten, wird mir ein bisschen schlecht.

ZEIT ONLINE: In welchen Bereichen sehen Sie schon besorgniserregende
Auswirkungen?

Nina George: Illustratorinnen zum Beispiel vermerken seit einem Jahr
weniger Auftragseingänge, weil sich Agenturen oder Redaktionen was von Dall-E
zusammenplagiieren lassen. Gutachterinnen für fremdsprachige Bücher haben
weniger Aufträge, seit DeepL dem Lektorat verrät, was in dem koreanischen oder
litauischen Bestseller drinsteht. Für professionelle Sprecherinnen zieht sich
das Jobfeld zu, seitdem es synthetische Stimmen gibt. Mitunter werden bereits
Pressetexte, Klappentexte und Social-Media-Posts maschinell zusammengepanscht.
Im Textbereich sind es vermehrt KI-Fake-Bücher oder KI-Übersetzungen ohne
Lizenz, die als Selfpublishing-Werke in den Amazon-Markt
gedrückt werden – manche gar unter real existierenden Namen von
menschlichen Autoren, die dann zusätzlich mit einem Reputationsschaden und
Identitätsdiebstahl zu kämpfen haben.

ZEIT ONLINE: Sie sind in vielen
Verbänden engagiert, waren jahrelang Präsidentin des European Writers' Council
(EWC), haben das Netzwerk Autorenrechte mitgegründet – und in dieser Rolle die
Verhandlungen zum AI Act, ein Mammutgesetz zur Regulierung von künstlicher
Intelligenz auf EU-Ebene, sehr genau verfolgt. Was erhoffen Sie sich von diesem Gesetz?

Nina George: Sämtliche existierenden Sprachmodelle laufen nur
deshalb profitabel, weil sie auf der Basis illegal und
unbezahlt besorgter Kulturwerke entstanden sind. Ein Gesetz muss klarstellen,
dass das Persönlichkeitsrecht auf die eigene Stimme, Erscheinung oder Bewegung
ebenso bei maschinellem Lernen greift und nicht ungefragt oder unvergütet
benutzt werden kann. Wir Autorinnen und Autoren brauchen deshalb ein Gesetz,
das eine Autorisation, eine Vergütung und eine Transparenz garantiert und vor
allem durchsetzbar macht. Ich muss gefragt werden, ob meine Arbeit für die
Ausbildung der mit mir konkurrierenden Systeme verwendet werden darf, ich muss
einen Preis ansagen können und Lizenzbedingungen festlegen und sämtliche
Herstellungsprozesse als auch Kennzeichnung von KI-Produkten muss sicher sein. Jedes
generierte Produkt sollte gekennzeichnet sein, nicht nur um
Desinformationen vorzubeugen, sondern auch um Lesern die Chance zu geben, ""Nein,
Danke!"" zu einem Maschinenprodukt zu sagen.

ZEIT ONLINE: Hatten Sie schon einmal
das Gefühl, Ihr Werk ist ohne Ihre Zustimmung von einer KI verwendet worden?

Nina George: Ich weiß es sogar. Große
Sprachmodelle bedienen sich aus riesigen Zusammen­stellungen von Buchwerken
aller Sprachen, die häufig von Piraterieseiten zusammengesaugt wurden. Die
bekanntesten Korpora heißen The Pile und Books3. Bereits im März 2023
hat ein Rechercheteam aus den Niederlanden, das AI Safety Camp, 194.000 Titel identifizieren können,
die darin stecken und von den großen Sprachmodellen ohne zu fragen verwendet
wurden – so auch
zwei meiner US-Bücher, The Little Paris Bookshop und The Little
French Bistro, beides sind New-York-Times-Bestseller. Raus kommt man
aus dem System nicht mehr, und klagen müsste ich in den USA.
"
Künstliche Intelligenz,Zeit,2024-01-11,https://www.zeit.de/news/2024-01/11/experten-chancen-fuer-kultur-durch-kuenstliche-intelligenz,Landtag: Kultur setzt Hoffnungen in Künstliche Intelligenz | ZEIT ONLINE,"Künstliche Intelligenz ist für Experten des Kunst- und Kulturbetriebs kein Schreckgespenst, sondern inzwischen ein Hoffnungsträger, um etwa neue Zielgruppen anzusprechen. KI werde sich weiter entwickeln und erfordere auch im Kulturbereich entsprechende Kompetenzen der Akteure, stellt der Kulturrat NRW in einem schriftlichen Bericht für eine Sachverständigenanhörung am Donnerstag im Landtag fest.


Möglichkeiten für KI in der Kultur

Im Marketing etwa könne Künstliche Intelligenz Zielgruppen, Reichweiten und Akzeptanz berechnen, so der Kulturrat. Mit KI oder auch Virtual Reality-Technologien wie AR-Brillen könnten Museen, Theater und Künstler virtuelle Räume erschaffen, in denen Besucher Kunstwerke, historische Stätten und Galerien erkunden könnten. KI könne individualisierte Informationen zu Exponaten bereitstellen, neue Elemente in Rundgänge einbauen und dabei multisprachlich agieren. Solche interaktiven Ausstellungen und Installationen sprächen auch neue Zielgruppen an.

Der Kulturrat ermutigte Kunsthochschulen zu einem verstärkten Engagement im Bereich der Künstlichen Intelligenz und schlug die Schaffung eines fächerübergreifenden ""Kompetenzzentrums KI in Kunst und Kultur"" vor.


Künstliche Intelligenz in der Kunst

So erschafft der international gefeierte Künstler Refik Anadol mit Künstlicher Intelligenz überwältigende monumentale Datenbilder. In Theatern und Opern kommen immer öfter AR-Brillen zum Einsatz, die den Zuschauern zusätzliche Informationen, neue Kameraperspektiven oder auch animierte Bildwelten bieten. Bei der Restaurierung von Kunstwerken kann KI helfen, beschädigte oder verblasste Kunstwerke digital zu rekonstruieren.

Insgesamt erweitere KI die künstlerischen Ausdrucksmöglichkeiten und eröffne neue Wege auch für die Interaktion zwischen Kunstschaffenden und Publikum, so Marcus Lobbes, Direktor der Akademie für Theater und Digitalität Dortmund. Allerdings müsse ""eine ausgewogene Balance zwischen technologischer Innovation und der Bewahrung der menschlichen Kreativität und Authentizität"" gefunden werden.


Rechtsrahmen für Künstliche Intelligenz

Zugleich fordern der Kulturrat sowie weitere Sachverständige eine Kennzeichnungspflicht und einen Rechtsrahmen für KI-Inhalte. So sollen in der Europäischen Union (EU) künftig strengere Regeln für den Einsatz von Künstlicher Intelligenz gelten. Darauf hatten sich Unterhändler des Europaparlaments und der EU-Staaten im Dezember verständigt. Nach Angaben des EU-Parlaments handelt es sich um das weltweit erste KI-Gesetz, das auf den Weg gebracht werden soll.

Als eine zentrale Herausforderung sieht die Kulturpolitische Gesellschaft das Fehlen von Übereinkünften zur Bezahlung oder von Bezahlmodellen für geistiges Eigentum im Sinne des Urheberrechts, das für KI verwendet wird.

Künstliche Intelligenz ist nach einer Definition des Europaparlaments die Fähigkeit einer Maschine, menschliche Fähigkeiten wie logisches Denken, Lernen, Planen und Kreativität zu imitieren. KI-Systeme sind demnach in der Lage, ihr Handeln anzupassen, indem sie die Folgen früherer Aktionen analysieren und autonom arbeiten.


KI allgegenwärtig im Alltag

Das Generieren von täuschend echten ""Deepfakes"" - etwa des Bildes des Papstes mit hipper Balenciaga-Jacke - hat nach Darstellung der Professorin Pamela Scorzin vom Fachbereich Design der Fachhochschule Dortmund schon lange das Netz erobert. KI-Bildgeneratoren könnten in wenigen Sekunden Textanweisungen oder vorhandene Bilder in neuartige Werke oder Stile verwandeln. ""KI ist allgegenwärtig in unserem Alltag und ""unsichtbar"" in vielen Kommunikationswerkzeugen wie Smartphones eingebettet"", schreibt Scorzin. Dabei stellten sich Fragen zur Autorenschaft und zum Copyright. Scorzin plädierte dafür, nicht gleich die Rufe nach Verboten oder Restriktionen an erste Stelle treten zu lassen, sondern auch neue Geschäftsmodelle und Entwicklungspotenziale mit KI-basierten Werkzeugen aufzuzeigen.


Kultur fit machen für KI

Für Deutschland sehen Experten im Vergleich zu anderen Ländern beim Umgang mit Künstlicher Intelligenz noch Nachholbedarf. Die Kulturpolitische Gesellschaft forderte Fortbildungen zu Möglichkeiten und Techniken der KI, aber auch zum Urheberrecht sowie zu Persönlichkeits- und Nutzungsrechten. Kritisch begleitet werden müssten auch der Wegfall von Arbeitsplätzen sowie finanzielle Einbußen durch KI. Einig sind sich die Fachleute darin, dass die Medienkompetenz zum Erkennen von KI-erzeugten Inhalten in Text, Bild und Ton durch Bildungsangebote verstärkt werden muss.

© dpa-infocom, dpa:240111-99-564398/3
"
AI,Zeit,2024-01-13,https://www.zeit.de/digital/2024-01/nina-george-ki-urheberrecht-kreative,"Autorin Nina George über KI: ""Wenig Chancen auf Nachzahlungen"" | ZEIT ONLINE","
In den USA hat die Tageszeitung ""New York Times"" die Entwickler der KI-Software ChatGPT wegen Urheberrechtsverletzungen verklagt. Auch andere wehren sich dagegen, dass ihre Texte zum KI-Training verwendet werden. In Deutschland gehört dazu die Autorin Nina George. Mit mehr als zwei Millionen verkauften Büchern zählt sie zu den derzeit erfolgreichsten deutschsprachigen
Autorinnen im Ausland. 

ZEIT ONLINE: Frau George, Sie schreiben unter Ihrem
eigenen Namen sowie mehreren Pseudonymen alles vom Kinderbuch bis zum
Kriminalroman, manchmal verfassen Sie im Jahr gleich mehrere Bücher. Haben Sie
schon einmal daran gedacht, sich den Plot für ihr nächstes Buch von einer
künstlichen Intelligenz generieren zu lassen?

Nina George: Das Beste am Schreiben ist die Freiheit, über jedes
Wort, jeden Twist, jede Figurenentwicklung selbst zu entscheiden – und die
tiefe Befriedigung, von Ereignissen und Charakterkonstellationen zu erzählen,
von denen so noch nie erzählt wurde. Diese Freiheit gebe ich doch nicht weiter,
um mir das Denken zu sparen. Abgesehen davon beherrscht so eine generative
Informatik ja weder einen originellen noch originalen Plot oder gar eine
stilistisch spannende Erzählform. Stattdessen greift sie auf Gespeichertes aus
einem zusammen geplünderten Bücherbestand zurück – und weiß dabei nicht mal,
was sie da tut. Es werden zudem immer mehr Worte und Kontexte werksseitig
verboten. Das sind etwa sexuell konnotierte Begriffe, politische Kontexte,
Schimpfwörter und laut Open AI ""andere unangemessene Inhalte"", wobei deren
Zensurvorgaben nicht öffentlich einsehbar sind. Was aus dem Textroboter herauskommt,
ist damit mediokre Einheitsbrause. So will ich nicht arbeiten.

ZEIT ONLINE: Welche konkreten Sorgen haben Sie als Autorin in Bezug
auf künstliche Intelligenz?

Nina George: Ich bevorzuge den Begriff künstliche oder simulierte
Kommunikation, wenn es um generative Informatik geht – denn ""intelligent"", und
was wir damit an kognitiven Kompetenzen verbinden, ist sie nicht und wird sie
auch nie werden. Meine Hauptsorge ist, dass Menschen sich selbst verzaubern und sogenannter KI mehr zutrauen, als sie kann. Daneben besorgt mich die
verspielte Blindheit vor allem politischer Entscheidungsträger für die
Herstellungsbedingungen und Auswirkungen solcher Systeme, seien es die
Rechtsbrüche bei der Besorgung von Kunst- und Kulturwerken, sei es die
Ausbeutung von linguistischen Arbeitern in Drittländern, seien es die absurd
enormen Energieverbräuche. Wenn ich lese, wie sorglos und stolz Politiker auf ihre
vermeintliche Technikaffinität verweisen und vom Einsatz generativer KI in Büro und
Alltag berichten, wird mir ein bisschen schlecht.

ZEIT ONLINE: In welchen Bereichen sehen Sie schon besorgniserregende
Auswirkungen?

Nina George: Illustratorinnen zum Beispiel vermerken seit einem Jahr
weniger Auftragseingänge, weil sich Agenturen oder Redaktionen was von Dall-E
zusammenplagiieren lassen. Gutachterinnen für fremdsprachige Bücher haben
weniger Aufträge, seit DeepL dem Lektorat verrät, was in dem koreanischen oder
litauischen Bestseller drinsteht. Für professionelle Sprecherinnen zieht sich
das Jobfeld zu, seitdem es synthetische Stimmen gibt. Mitunter werden bereits
Pressetexte, Klappentexte und Social-Media-Posts maschinell zusammengepanscht.
Im Textbereich sind es vermehrt KI-Fake-Bücher oder KI-Übersetzungen ohne
Lizenz, die als Selfpublishing-Werke in den Amazon-Markt
gedrückt werden – manche gar unter real existierenden Namen von
menschlichen Autoren, die dann zusätzlich mit einem Reputationsschaden und
Identitätsdiebstahl zu kämpfen haben.

ZEIT ONLINE: Sie sind in vielen
Verbänden engagiert, waren jahrelang Präsidentin des European Writers' Council
(EWC), haben das Netzwerk Autorenrechte mitgegründet – und in dieser Rolle die
Verhandlungen zum AI Act, ein Mammutgesetz zur Regulierung von künstlicher
Intelligenz auf EU-Ebene, sehr genau verfolgt. Was erhoffen Sie sich von diesem Gesetz?

Nina George: Sämtliche existierenden Sprachmodelle laufen nur
deshalb profitabel, weil sie auf der Basis illegal und
unbezahlt besorgter Kulturwerke entstanden sind. Ein Gesetz muss klarstellen,
dass das Persönlichkeitsrecht auf die eigene Stimme, Erscheinung oder Bewegung
ebenso bei maschinellem Lernen greift und nicht ungefragt oder unvergütet
benutzt werden kann. Wir Autorinnen und Autoren brauchen deshalb ein Gesetz,
das eine Autorisation, eine Vergütung und eine Transparenz garantiert und vor
allem durchsetzbar macht. Ich muss gefragt werden, ob meine Arbeit für die
Ausbildung der mit mir konkurrierenden Systeme verwendet werden darf, ich muss
einen Preis ansagen können und Lizenzbedingungen festlegen und sämtliche
Herstellungsprozesse als auch Kennzeichnung von KI-Produkten muss sicher sein. Jedes
generierte Produkt sollte gekennzeichnet sein, nicht nur um
Desinformationen vorzubeugen, sondern auch um Lesern die Chance zu geben, ""Nein,
Danke!"" zu einem Maschinenprodukt zu sagen.

ZEIT ONLINE: Hatten Sie schon einmal
das Gefühl, Ihr Werk ist ohne Ihre Zustimmung von einer KI verwendet worden?

Nina George: Ich weiß es sogar. Große
Sprachmodelle bedienen sich aus riesigen Zusammen­stellungen von Buchwerken
aller Sprachen, die häufig von Piraterieseiten zusammengesaugt wurden. Die
bekanntesten Korpora heißen The Pile und Books3. Bereits im März 2023
hat ein Rechercheteam aus den Niederlanden, das AI Safety Camp, 194.000 Titel identifizieren können,
die darin stecken und von den großen Sprachmodellen ohne zu fragen verwendet
wurden – so auch
zwei meiner US-Bücher, The Little Paris Bookshop und The Little
French Bistro, beides sind New-York-Times-Bestseller. Raus kommt man
aus dem System nicht mehr, und klagen müsste ich in den USA.
"
AI,Zeit,2024-01-11,https://www.zeit.de/digital/2024-01/ces-2024-tech-messe-usa-innovationen,CES 2024: Handys bekommen wieder Tasten und Menschen einen Turbo | ZEIT ONLINE,"Kaum ein Ort steht so sehr für verrückte Erfindungen wie
die Consumer Electronics Show CES: Mehr als 4.000 Aussteller zeigen in den riesigen Messehallen von Las
Vegas derzeit ihre Geräte und Dienstleistungen. Darunter große Konzerne wie
Samsung, Siemens oder Sony, aber auch mehr als 1.000 Start-ups. ZEIT ONLINE ist
vor Ort und hat sieben Gadgets getestet, die besonders bemerkenswert sind. Ob alle
davon auch auf den Markt kommen, ist nicht klar: Die CES ist auch der Ort für sogenannte
Vaporware, für große Versprechen, die bald verpuffen. 

Es ist 2024, und Handys mit Tasten sind anscheinend wieder
cool. Wilde Zeiten. Clicks ist eine iPhone-Hülle mit einer Tastatur an der
Unterseite und womöglich das Gadget, das im Vorfeld der CES am meisten Social-Media-Aufregung
erzeugt hat. Sei es wegen grassierender
BlackBerry-Nostalgie, der knalligen Farbe oder weil eine physische Tastatur
wirklich nützlich für Menschen sein kann, die etwa durch eine
Sehbeeinträchtigung Probleme mit Touchscreen-Tastaturen haben. 

Bei einem Test auf der Messe fühlte sich ein normalgroßes iPhone
mit dem Case weniger riesig an als erwartet. Allerdings kann man die Tasten nur
zweihändig sinnvoll bedienen. Nette Gimmicks sind außerdem eine
Hintergrundbeleuchtung und Tastatur-Shortcuts. Die Tasten waren beim kurzen
Test allerdings eher steif und nicht so clicky, wie es schön gewesen wäre.
Ob das nur am Messegerät lag, lässt sich bald herausfinden: Clicky erscheint am
1. Februar für 139 US-Dollar (etwa 125 Euro) und wird auch international
versendet. Eine Android-Version soll später kommen.

In der Zukunft sind wir alle ein bisschen Cyborgs, zumindest
wenn es nach einigen der CES-Aussteller geht. Ein gutes Beispiel dafür sind die
Moonwalkers von Shift Robotics. Die sehen nach Rollschuhen mit
Starlight-Express-Flair aus, sind aber eher ein Turbo für den eigenen Gang. Man
steigt mit Straßenschuhen hinein, läuft normal, und gesteuert von künstlicher
Intelligenz – wie
es sich für die CES 2024 gehört – sorgen die Rollen für eine dreifach erhöhte
Gehgeschwindigkeit, ohne dass man Gefahr läuft vornüber zu kippen: Die Boots passen sich adaptiv dem eigenen Gang an.

Die jetzt auf der CES gezeigten Moonwalkers X sind leichter
als der 2022 präsentierte Vorgänger, und das merkt man beim Ausprobieren auch
sofort. Standardmäßig blockieren die Rollen – so soll man etwa Treppen steigen
können –, mit einer schnellen Drehung des Fußes wird der Booster aktiviert. Der
funktioniert beeindruckend gut, ein wenig fühlt es sich an, als würde man auf einem Laufband am
Flughafen gehen. Shift Robotics denkt aber weniger an
gehetzte Manager auf dem Weg zum Anschlussflug, sondern mehr an Unternehmen, die
ihre Lagermitarbeiter damit entlasten wollen. Oder sie noch mehr ausnutzen, das
liegt vielleicht im Auge des Betrachters. Ikea probiert das nach Angaben von Shift Robotics bereits aus. Einen
Preis gibt es noch nicht, der Vorgänger allerdings kostet 1.400 US-Dollar (etwa
1.275 Euro).

Noch ein Wiedergänger aus dem vergangenen Jahr ist das französische
Start-up Skyted. Was allerdings 2023 noch absurd groß
aussah, ist nun nicht viel größer als eine FFP2-Maske (Sie erinnern sich). Die
Idee ist simpel: Skyted blockiert alle Geräusche, die aus der Maske nach außen
dringen, ein eingebautes Mikrofon gibt sie weiter – so lässt es sich also nahezu
still telefonieren. Das geht mit Kabel- (249 US-Dollar) oder
Bluetoothverbindung (299 US-Dollar) voraussichtlich ab Dezember 2024. Derzeit sammelt
Skyted dafür noch Geld im Crowdfunding.

Beim Test in der zugegebenermaßen geräuschvollen Messehalle
war nach außen tatsächlich nichts mehr zu hören. Möglich macht das ein
Absorptionsmaterial aus der Luftfahrtindustrie. Für die hat Skyted auch einen
interessanten business case: Künftig möchte das Start-up mit Airlines
kooperieren, die ihr Flugzeug-WLAN beispielsweise nur für jene Voice-over-IP-Anrufe öffnen könnten,
die mit einer Skyted-Maske erledigt werden. Viel gewonnen für die Menschheit
wäre schon, wenn Skyted es schafft, plärrende Telefonierer im ICE-Ruheabteil
stummzuschalten. Ob die allerdings eine Notwendigkeit dafür sehen, ist eine
andere Frage. 

Keine CES ohne Roboter, das ist ein ungeschriebenes Gesetz. Neben
den hundeartigen Robotern
von Boston Dynamics, die (begleitet) zwischen den Messehallen umherliefern, machten in diesem Jahr in erster Linie zwei südkoreanische Konzerne
mit Robotern auf sich aufmerksam: einerseits Samsung mit dem knuffig aussehenden
und knuffig benannten Ballie – einem Fußball mit Rädern – und andererseits LG mit
dem ebenfalls sehr knuffigen, aber sehr basal benannten LG AI Agent, der
aussieht wie ein kleiner Hund mit Kopfhörern.

Beide Roboter kann man sich wie eine Alexa auf Rollen
vorstellen. Sie sollen smarte Geräte an- und abschalten können, sich per
Sprachbefehl steuern lassen und auch gucken, ob es den Haustieren aus Fleisch
und Blut gut geht. Ballie verfügt zudem noch über einen Beamer, der Filme oder
Rezeptvideos an die Wand werfen soll. Beide Roboter wurden auf der CES zwar
gezeigt, aber nur hinter Absperrungen. Ballie etwa schlug auf einer durchchoreografierten
Demo einem Schauspieler Sportübungen vor, informierte über die Luftqualität und rief
den lokalen Blumenhändler an. Ob, wann und zu welchem Preis die
Minihaushaltshilfen erscheinen, ist unklar.

Haben Sie sich schon einmal gefragt, wie Sie auch noch das
letzte bisschen Freizeit aus ihrem Leben wegoptimieren können? Dann könnte
Willcook etwas für Sie sein: eine Tasche, die im Innern so warm wird, dass
sie Lebensmittel nicht nur warmhalten, sondern sogar garen kann. Damit wird es
also möglich, sich Butterbrotschmieren und meal prep zu sparen und
stattdessen das Mittagessen direkt in der U-Bahn auf dem Weg zur Arbeit zu
kochen. ""Die Mikrowelle wird tragbar"" bewirbt das Start-up sein Produkt, das es
bisher nur in Japan verkauft. Es plant aber bereits zusätzlich einen Mikrowellen-Rucksack.

Mit einer vom Hersteller angegebenen Innentemperatur von 80
Grad Celcius betreibt Willcook eher Niedrigtemperaturgaren, aber das ist eh
seit ein paar Jahren in (wer anderer Meinung ist, werfe den ersten
Sous-vide-Stick), etwa für Rinderfilet oder Lammkeule. Auf der Messe fühlte
sich die Tasche innen wirklich sehr warm an – in Ermangelung eines Rindes,
geschweige denn seines Filets, konnten wir das Garen aber nicht ausprobieren. 

Kein auf der CES vorgestelltes Gerät hat vermutlich bei so
vielen Leuten ein ""Will ich haben!""-Gefühl ausgelöst wie das R1 von Rabbit.
Innerhalb von 24
Stunden nach der Präsentation hat das Start-up 10.000 Geräte verkauft. Dabei
ist es gar nicht einfach zu verstehen, was das R1 eigentlich ist: Stellen Sie
sich ein Gerät vor, halb so groß wie ein Smartphone, auf dem keine Apps laufen,
sondern bei dem Sie jeden Befehl per Sprache eingeben. Das Gerät interpretiert
das Gesagte mit künstlicher Intelligenz und führt es genau so aus, wie man
möchte. Feedback gibt es entweder über ein Display oder per Sprachausgabe. Quasi ein KI-Walkie-Talkie. ""It’s that simple"", sagt Rabbit-Gründer
Jesse Lyu.

Das Besondere am R1 (neben seinem coolen Design von Teenage
Engineering) ist, dass es nicht einfach ein Alexa-Siri-Assistent mit KI ist:
Über ein Webportal mit dem Namen the rabbit hole (wie auch sonst) lässt sich das R1 mit
verschiedenen Diensten verbinden. In der Unternehmenspräsentation
zeigte Lyu etwa, wie er mit dem R1 Pizza bestellt, nachdem er R1 einmalig mit
dem Lieferservice DoorDash verbunden hat. Ebenso kann das R1 Musikdienste wie
Spotify bedienen, Reisen planen und buchen oder mit der eingebauten Kamera
Fragen zur Umgebung beantworten und Rezepttipps für den Inhalt des Kühlschranks
geben. Das 199-Dollar-Gerät würde damit schaffen, woran
aktuelle KI-Agenten scheitern – ein großes Versprechen, das ab Ostern bewiesen
werden will, wenn das R1 ausgeliefert wird.

Wer jemals in einer deutschen Innenstadt einen Parkplatz
gesucht hat, kennt die Situation: Eigentlich gibt es da eine Parklücke – aber ein
Herr (es ist immer ein Herr) hat mit seiner Mercedes-B-Klasse die Lage der
Parklückenbegrenzung recht frei interpretiert. Also er steht mitten darauf. Und
nun bräuchte es das Talent eines Herzchirurgen, das eigene Auto lackschadenfrei
einzuparken. Also bleibt der halbblockierte Parkplatz leer.

Dieses Problem und überhaupt jeden Einparkstress soll Parkie
vom südkoreanischen Unternehmen HL Mando beseitigen. Es handelt sich dabei um
eine nur knapp einen Zentimeter hohe, fahrende Plattform. Zwei davon sollen
autonom unter ein Auto fahren, es anheben und dann perfekt und sorgenfrei
einparken. Das soll sogar bei bodennahen Sportwagen und schweren SUVs
funktionieren. So bräuchte es für die gleiche Anzahl Autos wesentlich weniger
Parkfläche in der Stadt – klingt fast zu schön, um wahr zu sein. Ab April soll
das System südlich von Seoul getestet werden.   
"
KI,Zeit,2024-01-13,https://www.zeit.de/digital/2024-01/nina-george-ki-urheberrecht-kreative,"Autorin Nina George über KI: ""Wenig Chancen auf Nachzahlungen"" | ZEIT ONLINE","
In den USA hat die Tageszeitung ""New York Times"" die Entwickler der KI-Software ChatGPT wegen Urheberrechtsverletzungen verklagt. Auch andere wehren sich dagegen, dass ihre Texte zum KI-Training verwendet werden. In Deutschland gehört dazu die Autorin Nina George. Mit mehr als zwei Millionen verkauften Büchern zählt sie zu den derzeit erfolgreichsten deutschsprachigen
Autorinnen im Ausland. 

ZEIT ONLINE: Frau George, Sie schreiben unter Ihrem
eigenen Namen sowie mehreren Pseudonymen alles vom Kinderbuch bis zum
Kriminalroman, manchmal verfassen Sie im Jahr gleich mehrere Bücher. Haben Sie
schon einmal daran gedacht, sich den Plot für ihr nächstes Buch von einer
künstlichen Intelligenz generieren zu lassen?

Nina George: Das Beste am Schreiben ist die Freiheit, über jedes
Wort, jeden Twist, jede Figurenentwicklung selbst zu entscheiden – und die
tiefe Befriedigung, von Ereignissen und Charakterkonstellationen zu erzählen,
von denen so noch nie erzählt wurde. Diese Freiheit gebe ich doch nicht weiter,
um mir das Denken zu sparen. Abgesehen davon beherrscht so eine generative
Informatik ja weder einen originellen noch originalen Plot oder gar eine
stilistisch spannende Erzählform. Stattdessen greift sie auf Gespeichertes aus
einem zusammen geplünderten Bücherbestand zurück – und weiß dabei nicht mal,
was sie da tut. Es werden zudem immer mehr Worte und Kontexte werksseitig
verboten. Das sind etwa sexuell konnotierte Begriffe, politische Kontexte,
Schimpfwörter und laut Open AI ""andere unangemessene Inhalte"", wobei deren
Zensurvorgaben nicht öffentlich einsehbar sind. Was aus dem Textroboter herauskommt,
ist damit mediokre Einheitsbrause. So will ich nicht arbeiten.

ZEIT ONLINE: Welche konkreten Sorgen haben Sie als Autorin in Bezug
auf künstliche Intelligenz?

Nina George: Ich bevorzuge den Begriff künstliche oder simulierte
Kommunikation, wenn es um generative Informatik geht – denn ""intelligent"", und
was wir damit an kognitiven Kompetenzen verbinden, ist sie nicht und wird sie
auch nie werden. Meine Hauptsorge ist, dass Menschen sich selbst verzaubern und sogenannter KI mehr zutrauen, als sie kann. Daneben besorgt mich die
verspielte Blindheit vor allem politischer Entscheidungsträger für die
Herstellungsbedingungen und Auswirkungen solcher Systeme, seien es die
Rechtsbrüche bei der Besorgung von Kunst- und Kulturwerken, sei es die
Ausbeutung von linguistischen Arbeitern in Drittländern, seien es die absurd
enormen Energieverbräuche. Wenn ich lese, wie sorglos und stolz Politiker auf ihre
vermeintliche Technikaffinität verweisen und vom Einsatz generativer KI in Büro und
Alltag berichten, wird mir ein bisschen schlecht.

ZEIT ONLINE: In welchen Bereichen sehen Sie schon besorgniserregende
Auswirkungen?

Nina George: Illustratorinnen zum Beispiel vermerken seit einem Jahr
weniger Auftragseingänge, weil sich Agenturen oder Redaktionen was von Dall-E
zusammenplagiieren lassen. Gutachterinnen für fremdsprachige Bücher haben
weniger Aufträge, seit DeepL dem Lektorat verrät, was in dem koreanischen oder
litauischen Bestseller drinsteht. Für professionelle Sprecherinnen zieht sich
das Jobfeld zu, seitdem es synthetische Stimmen gibt. Mitunter werden bereits
Pressetexte, Klappentexte und Social-Media-Posts maschinell zusammengepanscht.
Im Textbereich sind es vermehrt KI-Fake-Bücher oder KI-Übersetzungen ohne
Lizenz, die als Selfpublishing-Werke in den Amazon-Markt
gedrückt werden – manche gar unter real existierenden Namen von
menschlichen Autoren, die dann zusätzlich mit einem Reputationsschaden und
Identitätsdiebstahl zu kämpfen haben.

ZEIT ONLINE: Sie sind in vielen
Verbänden engagiert, waren jahrelang Präsidentin des European Writers' Council
(EWC), haben das Netzwerk Autorenrechte mitgegründet – und in dieser Rolle die
Verhandlungen zum AI Act, ein Mammutgesetz zur Regulierung von künstlicher
Intelligenz auf EU-Ebene, sehr genau verfolgt. Was erhoffen Sie sich von diesem Gesetz?

Nina George: Sämtliche existierenden Sprachmodelle laufen nur
deshalb profitabel, weil sie auf der Basis illegal und
unbezahlt besorgter Kulturwerke entstanden sind. Ein Gesetz muss klarstellen,
dass das Persönlichkeitsrecht auf die eigene Stimme, Erscheinung oder Bewegung
ebenso bei maschinellem Lernen greift und nicht ungefragt oder unvergütet
benutzt werden kann. Wir Autorinnen und Autoren brauchen deshalb ein Gesetz,
das eine Autorisation, eine Vergütung und eine Transparenz garantiert und vor
allem durchsetzbar macht. Ich muss gefragt werden, ob meine Arbeit für die
Ausbildung der mit mir konkurrierenden Systeme verwendet werden darf, ich muss
einen Preis ansagen können und Lizenzbedingungen festlegen und sämtliche
Herstellungsprozesse als auch Kennzeichnung von KI-Produkten muss sicher sein. Jedes
generierte Produkt sollte gekennzeichnet sein, nicht nur um
Desinformationen vorzubeugen, sondern auch um Lesern die Chance zu geben, ""Nein,
Danke!"" zu einem Maschinenprodukt zu sagen.

ZEIT ONLINE: Hatten Sie schon einmal
das Gefühl, Ihr Werk ist ohne Ihre Zustimmung von einer KI verwendet worden?

Nina George: Ich weiß es sogar. Große
Sprachmodelle bedienen sich aus riesigen Zusammen­stellungen von Buchwerken
aller Sprachen, die häufig von Piraterieseiten zusammengesaugt wurden. Die
bekanntesten Korpora heißen The Pile und Books3. Bereits im März 2023
hat ein Rechercheteam aus den Niederlanden, das AI Safety Camp, 194.000 Titel identifizieren können,
die darin stecken und von den großen Sprachmodellen ohne zu fragen verwendet
wurden – so auch
zwei meiner US-Bücher, The Little Paris Bookshop und The Little
French Bistro, beides sind New-York-Times-Bestseller. Raus kommt man
aus dem System nicht mehr, und klagen müsste ich in den USA.
"
Künstliche Intelligenz,Zeit,2024-01-09,https://www.zeit.de/digital/2024-01/ces-2024-las-vegas-kuenstliche-intelligenz-technik,CES 2024: Das sind die Techtrends des Jahres | ZEIT ONLINE,"Alt auszusehen, ist nie angenehm. Als Messe, die von sich
behauptet, eine ""Plattform für Innovatoren"" zu sein, ist es allerdings
besonders unangenehm. Genau das ist der CES im vergangenen Jahr passiert: Während
die ganze Welt über künstliche Intelligenz (KI) sprach, konnte die Techmesse
wenig dazu zeigen. Sie hatte schlicht Pech gehabt: Das Programm stand, die
Messestände waren geordert, in den wenigen Wochen zwischen der Veröffentlichung
von ChatGPT im November 2022 und dem CES-Start im Januar konnte keine ganze
Messe mehr umgeworfen werden. 

Dass die CES das nicht auf sich sitzen lassen will, zeigt
sich nun in Las Vegas. Schon im Vorfeld hatten die Veranstalter künstliche Intelligenz zum Topthema der Messe ausgerufen, das bestätigt sich auch vor
Ort: Bevor die Messe am Dienstag offiziell eröffnet wird, konnte sich ZEIT
ONLINE beim Vorabevent CES Unveiled und in diversen Pressekonferenzen bereits
einen Eindruck von den neuesten Techtrends machen. Und klar ist: Ohne KI geht
es nicht mehr. Was macht man aber mit dieser Technik? Auf diese Frage finden
die Techunternehmen ganz unterschiedliche Antworten. 

Die CES in Las Vegas gilt als eine der wichtigsten und größten Messen der Tech-Branche. In diesem Jahr werden 130.000 Besucher erwartet, denen mehr als 4.000 Aussteller ihre Produkte und Dienstleistungen zeigen: von Großkonzernen wie Amazon, Sony oder Samsung bis zu kleinen Start-ups aus aller Welt. Traditionell nehmen Geräte aus dem Audio-Video-Bereich und Haushaltselektronik einen großen Teil der Messehallen ein, aber auch Lebensmittel- und Gesundheitstechnologie, Gaminghardware oder Laptops sind hier zu finden. In den vergangenen Jahren wurde zudem das Thema Mobilität immer wichtiger, BMW, Mercedes oder Honda kündigen hier neue E-Autos und In-Car-Systeme an. 

Die CES (die nicht mehr Consumer Electronics Show genannt werden möchte) ist eine reine Fachmesse, Privatbesucher sind nicht zugelassen. Sie wird von der Consumer Technology Association (CTA) veranstaltet, einem Branchenverband der amerikanischen Tech-Branche. Der wird in diesem Jahr 100 Jahre alt – gegründet wurde er 1924 als Radio Manufacturers Association – damals war die heißeste Technik noch das Radio. Seit der ersten CES 1967 gab es auf der Messe immer wieder Premieren von Produkten, die bald darauf in jedem Haushalt standen, wie der erste Videorekorder oder der erste CD-Player.

Dass sich nun alles um KI dreht, machte Samsung am
deutlichsten. Der südkoreanische Konzern rief als Motto ""AI for All"" aus
(Bonuspunkte für homografe Wortspiele!). Heißt: Samsung will künstliche Intelligenz in alle seine Produkte bringen. Etwa in den neuen Kühlschrank Bespoke
4-Door French Door, bei dem eine Kamera mithilfe von KI erkennen soll, ob
noch genug Eier und Milch im Kühlschrank sind, um Pfannkuchen zu machen. Alle
Geräte im Haus sollen dann über Samsungs Smarthomesystem SmartThings miteinander
vernetzt sein und über den Sprachassistenten Bixby angesteuert werden können. Das
smarte Zuhause will Samsung dabei über die eigenen vier Wände hinaus erweitern:
Durch eine Partnerschaft mit Hyundai sollen Nutzerinnen künftig auch im Auto
Sprachbefehle wie ""Schalte die Heizung an"" nach Hause senden können. 

Bemerkenswert ist, wie einig sich Samsung und das ebenfalls
südkoreanische Unternehmen LG in der Vision eines KI-betriebenen, vollvernetzten
Zuhauses sind. Auch LG übt sich in Wortspielen und plant, KI in alle seine
Produkte zu bringen. Dabei möchte das Unternehmen aber AI statt als artificial intelligence als affectionate
intelligence, also liebevolle Intelligenz verstehen. Beide Unternehmen
sprechen davon, wie durch KI Haushaltsarbeit einfacher werden soll (LG nennt
das sehr optimistisch Zero Labor Home). Dazu sollen in beiden Fällen
auch Roboterassistenten beitragen: der Ballie (Optik: Star-Wars-BB-8-Droide)
von Samsung und der Smarthome AI Agent (Optik: Chihuahua mit Kopfhörern) von
LG. Sie sollen herumfahren, Smartgeräte kontrollieren, sowie reden (LG) oder
mit einem eingebauten Beamer Filme abspielen (Samsung). 

Die Unternehmen sind sich auch darin einig, dass der oft
totgesagte Fernseher alles andere als selbiges ist. Für LG, Samsung wie auch
für Panasonic wird der Fernseher künftig zur Zentrale der Steuerung des Smarthomes. Statt nur Netflix zu streamen, soll man hier künftig das Wetter checken,
die Überwachungskameras durchschalten und die Einkaufsliste verwalten können. LG
kooperiert dafür mit Google, Panasonic mit Amazon. Samsung und LG führten außerdem in
Las Vegas einen transparenten Fernseher vor –  im Falle von LG lässt sich zwischen
durchsichtig und undurchsichtig hin- und herschalten. Bei beiden ist noch
unklar, wann sie erscheinen und wie viele tausend Dollar sie kosten sollen – faszinierend
sah es aber allemal aus.

In noch einem Punkt herrscht bei den Unternehmen auf der CES
Einigkeit: Wer etwas auf sich hält, der stellt sich gut mit Microsoft. Das ist
einigermaßen überraschend, bedenkt man, dass Microsoft lange Zeit eher so etwas
wie das hässliche Entlein der Techbranche war – Windows war bisher vieles,
aber sicher nicht cool.
Seit Microsofts Milliardeninvestment in OpenAI sich aber in Form des Hypes um
ChatGPT auszahlt, lädt man Microsoft-Manager anscheinend wesentlich lieber zur
Pressekonferenz.

So kündigte Samsung an, dass in die demnächst startenden
Galaxy-Book-4-Laptops Microsofts KI-Assistent Copilot einziehen soll – ob sie auch
die neue Copilot-Taste erhalten (die erste Veränderung der Windows-Tastatur
seit fast 30 Jahren!), wurde nicht gesagt. Mit Siemens arbeitet Microsoft daran,
Copilot auch für Industrieanwendungen zu nutzen, für Anwendungen im Auto
kooperiert Microsoft mit Sony und Honda. Die zeigten erneut
ihren E-Auto-Prototypen Afeela. Der lässt sich anscheinend nicht nur mit
einem Playstation-Controller steuern, sondern soll auch einen von Microsoft
Copilot angetriebenen Sprachassistenten enthalten – wenn er denn jemals auf den
Markt kommt. Den direkteren Weg geht Volkswagen: Ab dem zweiten Quartal will
der deutsche Konzern ChatGPT in seine Autos einbauen, der Sprachassistent IDA
soll so natürlichere Antworten liefern.

Auffällig bei all diesen angekündigten Kooperationen: Nahezu
jeder große Techkonzern zeigt sich in Las Vegas – nur Apple macht nicht mit. Der
iPhone-Konzern ist wie immer nicht bei der Messe dabei (oder nur am
Rande), allerdings ist er doch oft schweigend präsent. In diesem Jahr
vielleicht so stark wie selten zuvor: Es ist sicher kein Zufall, dass Apple pünktlich
zur Eröffnung der großen CES-Pressekonferenzen am Montagmorgen Ortszeit den Verkaufsstart
für die Vision-Pro-Brille
verkündete – und so gleich Aufmerksamkeit von den Konkurrenten stahl.

Aber auch ohne diese Ankündigung war Apples
Mixed-Reality-Brille zwischen den Zeilen immer wieder ein Thema. So ging es in
Las Vegas auch um ""spatial computing"", wie Apple die Verbindung von
realer und digitaler Welt nennt (auch wenn
der Begriff älter ist). Sony und Siemens stellten gemeinsam ein Gerät zur ""spatial content creation"" vor,
dass es so womöglich ohne den Aufmerksamkeitsschub von Apple nicht geben würde.
Das Headset sieht der PS VR-2-Gamingbrille
ähnlich, statt zum Spielen ist sie aber dafür gedacht, 3D-Modelle anzusehen, zu
erstellen und zu bearbeiten. Ein Sprecher des Entwicklungsteams von Red Bull
Racing erklärte auf der CES-Bühne, wie mit der Brille ein neues
Formel-1-Auto entwickelt werden würde. 

Sony liefert die Hardware der Brille, Siemens die Software. Sie
soll noch in diesem Jahr erscheinen. Den größeren Rahmen dafür bildet eine
Idee, die Siemens CEO Roland Busch präsentierte: 2024 sei der Wendepunkt zum ""Industrial Metaverse"" gekommen, sagte
der Vorstandsvorsitzende. Siemens meint damit, dass inzwischen quasi alles – von
Flugzeugen über Fabriken bis zu menschlichen Herzen – realitätsgetreu digital
simuliert werden kann. An diesen ""digitalen Zwillingen"" könne man dann
experimentieren, Prozesse optimieren und so Innovation beschleunigen. Eine in
diesem Metaversum geplante Autofabrik sei etwa wesentlich produktiver und
energieeffizienter. Sagt zumindest Siemens.

Solche und ähnliche Energieeinsparungen behaupten alle Techkonzerne
einstimmig. Samsung und LG sprachen minutenlang über ihre
Nachhaltigkeitsinitiativen, Panasonic stellte gleich seine ganze Präsentation
unter das Motto ""Green Impact"", bei Bosch sprach man hauptsächlich über die
Energiewende im Verkehrssektor, für die Wasserstoff zentral sei (Volker Wissing
gefällt das). Wenn man die grünglitzernden Powerpoints so anschaut, könnte man
fast vergessen, dass es die gleichen Konzerne sind, die mit immer neuen
Produkten zu einem immer höheren Energiebedarf und zu immer höheren Elektroschrottbergen
beitragen. 

Die überzeugendsten Innovationen sind aber womöglich sowieso
abseits der großen Konferenzbühnen zu finden. Etwa bei der App des
niederländischen Start-ups Whispp. Mit
KI-gestützter Stimmerzeugung gibt sie Menschen die Stimme zurück, die sie etwa durch
Krankheiten wie Kehlkopfkrebs verloren haben. Auch wenn die Betroffenen nur
noch flüstern können, macht Whispp daraus wieder verständliche Anrufe und
Sprachnachrichten. Bei einer kleinen Vorführung am Rande der Messe
funktionierte das beeindruckend gut. Und das ist doch wesentlich innovativer
als ein sprechendes Auto oder eine Kühlschrank-KI. 

"
Künstliche Intelligenz,Zeit,2024-01-07,https://www.zeit.de/news/2024-01/07/so-soll-ki-bei-polizeiarbeit-helfen,Technologie: So soll KI bei der Polizeiarbeit helfen | ZEIT ONLINE,"Die Anruferin klingt wie die eigene Tochter - aber die Stimme ist geklaut. Künstliche Intelligenz (KI) macht betrügerische Schockanrufe noch tückischer, wenn Stimmen geklont werden und nicht mehr von den echten unterscheidbar sind. «Das ist schon ohne großes technisches Knowhow möglich», meint der KI-Koordinator beim Berliner Landeskriminalamt, Eugen Hofmann.

Deutsche Sicherheitsbehörden rufen nach Werkzeugen der KI, um digitaler Verbrechensausübung nicht länger hinterher zu laufen. «Die Polizei muss dem etwas entgegen setzen», meint LKA-Mann Hofmann. Vor allem auch die Bearbeitung riesiger Datenmengen soll künftig mit Algorithmen erleichtert werden etwa bei Ermittlungen wegen Kinderpornografie.

Auch Geldautomatensprengern oder Dokumentenfälschern lässt sich aus Forscher-Sicht mit Hilfe von KI auf die Spur kommen. Bislang aber fehlt oft - auch wegen hoher rechtlicher Hürden - der Praxiseinsatz bei der Polizei. Eine klare Ausrichtung für die Zukunft war bislang kaum erkennbar.

«Es wird Zeit, dass es anfängt», sagt der Informatiker Andreas Dengel. Er ist Geschäftsführender Direktor des Deutschen Forschungszentrums für Künstliche Intelligenz (DFKI) in Kaiserslautern, das mit dem Bundeskriminalamt (BKA) und dem LKA Rheinland-Pfalz kooperiert und KI-Systeme für Polizeiermittlungen testet. Bislang besteht aus Sicht des KI-Experten Dengel ein großer Flickenteppich bei den Polizeibehörden der Länder. «Es müsste einen nationalen Polizeibeauftragten geben, der eine KI-Strategie entwickelt.»

Einige Beispiele für Forschungsprojekte und KI-Tests für die Polizeiarbeit:

Wenn Geldautomaten-Sprenger, die 2023 in Deutschland mehr als 450 Geräte zerstörten, Schuhabdrücke hinterlassen, kann aus Sicht Dengels eine eigens dafür trainierte KI zum Einsatz kommen. Die Muster ließen sich dann bestimmten Schuhtypen zuordnen. Bislang seien bereits einige Tausend unterschiedliche Schuhabdrücke bundesweit gesammelt, so Dengel. Das Ziel: Täter am Schuhprofil zu identifizieren. Das System stehe kurz vor der Einführung, er hoffe noch 2024, sagt Dengel.

Mit neuen Verfahren kann die Polizei laut Deutschem Forschungszentrum in Kaiserslautern auch gefälschten Dokumenten auf die Spur kommen. Ein KI-System könne dabei die minimalen Unterschiede bei Druckern erkennen.

Getestet werden zudem beispielsweise im Süden Deutschlands und in Hamburg Videokameras, die mit Algorithmen und intelligenter Software verdächtige Bewegungsmuster finden sollen. Die KI muss dabei mit Daten und Bildern trainiert werden. Biometrische Daten werden nicht erfasst, auch Alter, Geschlecht oder Ethnie von Personen werden damit nicht bestimmt, wie die Hamburger Innenbehörde 2023 betont hatte.

Der Berliner KI-Koordindator beim Berliner LKA, Hofmann, nennt als ein mögliches Beispiel auch Fußballstadien, in denen mit Hilfe einer Gesichtserkennung etwa Randalierer ausgesiebt werden könnten. Eigene Datenmodelle dafür und andere KI-Systeme hat die Berliner Polizei aber nicht.

Im Zusammenhang mit Straftaten in der Silvesternacht hatte der stellvertretende Landesvorsitzende der Gewerkschaft der Polizei in Hamburg, Lars Osburg, vor kurzem gesagt: «Es ist nicht mehr zu vermitteln, warum wir massenhaft Kräfte zum Schutz von Veranstaltungen einsetzen müssen, aber auf die Chancen der KI bei der Fahndung nach bekannten Straftätern und Gefährdern verzichten.»

Die größte Gefahr in naher Zukunft sieht der Cybersicherheits-Experte Christian Dörr vom Hasso-Plattner-Institut in Potsdam aber darin, dass mit Hilfe Künstlicher Intelligenz zunehmend Deepfakes für Desinformationskampagnen eingesetzt werden. Dann spricht nicht der Kanzler im Video, sondern ein digitaler Zwilling.

Mit Hilfe von Manipulationen werde versucht, Wahlen zu beeinflussen und staatliche Systeme zu destabilisieren, meint Dörr. «Die großen Hackergruppen sind nationalstaatlich getrieben. Die haben natürlich auch sehr, sehr viel Interesse an KI.» Im Sommer 2022 hatte etwa die damalige Regierende Bürgermeisterin von Berlin, Franziska Giffey (SPD), per Video mit einer Person gesprochen, die wie der Kiewer Bürgermeister Vitali Klitschko aussah, aber nicht Klitschko war.

Für die Polizeiarbeit sind dem Einsatz von KI-Systemen aber auch enge Grenzen gesetzt. Bedenken gibt es, weil auch Risiken für unbescholtene Bürger gesehen werden und Grundrechte gefährdet sein können.

Der Bundesverfassungsschutz hatte bereits Regelungen in Hessen und Hamburg für verfassungswidrig erklärt. Es geht um eine Analyse-Software, mit der Polizisten mit einem Klick verschiedene Datenbanken durchsuchen, um in den riesigen Datenmengen Querverbindungen zu entdecken. Laut hessischer Polizei war im Zusammenhang mit einer Razzia gegen Reichsbürger so eine Festnahme gelungen. Im Dezember 2023 einigte sich nun die EU auf schärfere Regeln - das kann auch Folgen für KI-Instrumente bei der Polizei haben. «Wenn wir warten, bis Juristen einen wasserdichten Text haben, das dauert fünf Jahre», meint der Berliner LKA-Beamte Hofmann.

© dpa-infocom, dpa:240107-99-516369/3
"
AI,Zeit,2024-01-09,https://www.zeit.de/digital/2024-01/ces-2024-las-vegas-kuenstliche-intelligenz-technik,CES 2024: Das sind die Techtrends des Jahres | ZEIT ONLINE,"Alt auszusehen, ist nie angenehm. Als Messe, die von sich
behauptet, eine ""Plattform für Innovatoren"" zu sein, ist es allerdings
besonders unangenehm. Genau das ist der CES im vergangenen Jahr passiert: Während
die ganze Welt über künstliche Intelligenz (KI) sprach, konnte die Techmesse
wenig dazu zeigen. Sie hatte schlicht Pech gehabt: Das Programm stand, die
Messestände waren geordert, in den wenigen Wochen zwischen der Veröffentlichung
von ChatGPT im November 2022 und dem CES-Start im Januar konnte keine ganze
Messe mehr umgeworfen werden. 

Dass die CES das nicht auf sich sitzen lassen will, zeigt
sich nun in Las Vegas. Schon im Vorfeld hatten die Veranstalter künstliche Intelligenz zum Topthema der Messe ausgerufen, das bestätigt sich auch vor
Ort: Bevor die Messe am Dienstag offiziell eröffnet wird, konnte sich ZEIT
ONLINE beim Vorabevent CES Unveiled und in diversen Pressekonferenzen bereits
einen Eindruck von den neuesten Techtrends machen. Und klar ist: Ohne KI geht
es nicht mehr. Was macht man aber mit dieser Technik? Auf diese Frage finden
die Techunternehmen ganz unterschiedliche Antworten. 

Die CES in Las Vegas gilt als eine der wichtigsten und größten Messen der Tech-Branche. In diesem Jahr werden 130.000 Besucher erwartet, denen mehr als 4.000 Aussteller ihre Produkte und Dienstleistungen zeigen: von Großkonzernen wie Amazon, Sony oder Samsung bis zu kleinen Start-ups aus aller Welt. Traditionell nehmen Geräte aus dem Audio-Video-Bereich und Haushaltselektronik einen großen Teil der Messehallen ein, aber auch Lebensmittel- und Gesundheitstechnologie, Gaminghardware oder Laptops sind hier zu finden. In den vergangenen Jahren wurde zudem das Thema Mobilität immer wichtiger, BMW, Mercedes oder Honda kündigen hier neue E-Autos und In-Car-Systeme an. 

Die CES (die nicht mehr Consumer Electronics Show genannt werden möchte) ist eine reine Fachmesse, Privatbesucher sind nicht zugelassen. Sie wird von der Consumer Technology Association (CTA) veranstaltet, einem Branchenverband der amerikanischen Tech-Branche. Der wird in diesem Jahr 100 Jahre alt – gegründet wurde er 1924 als Radio Manufacturers Association – damals war die heißeste Technik noch das Radio. Seit der ersten CES 1967 gab es auf der Messe immer wieder Premieren von Produkten, die bald darauf in jedem Haushalt standen, wie der erste Videorekorder oder der erste CD-Player.

Dass sich nun alles um KI dreht, machte Samsung am
deutlichsten. Der südkoreanische Konzern rief als Motto ""AI for All"" aus
(Bonuspunkte für homografe Wortspiele!). Heißt: Samsung will künstliche Intelligenz in alle seine Produkte bringen. Etwa in den neuen Kühlschrank Bespoke
4-Door French Door, bei dem eine Kamera mithilfe von KI erkennen soll, ob
noch genug Eier und Milch im Kühlschrank sind, um Pfannkuchen zu machen. Alle
Geräte im Haus sollen dann über Samsungs Smarthomesystem SmartThings miteinander
vernetzt sein und über den Sprachassistenten Bixby angesteuert werden können. Das
smarte Zuhause will Samsung dabei über die eigenen vier Wände hinaus erweitern:
Durch eine Partnerschaft mit Hyundai sollen Nutzerinnen künftig auch im Auto
Sprachbefehle wie ""Schalte die Heizung an"" nach Hause senden können. 

Bemerkenswert ist, wie einig sich Samsung und das ebenfalls
südkoreanische Unternehmen LG in der Vision eines KI-betriebenen, vollvernetzten
Zuhauses sind. Auch LG übt sich in Wortspielen und plant, KI in alle seine
Produkte zu bringen. Dabei möchte das Unternehmen aber AI statt als artificial intelligence als affectionate
intelligence, also liebevolle Intelligenz verstehen. Beide Unternehmen
sprechen davon, wie durch KI Haushaltsarbeit einfacher werden soll (LG nennt
das sehr optimistisch Zero Labor Home). Dazu sollen in beiden Fällen
auch Roboterassistenten beitragen: der Ballie (Optik: Star-Wars-BB-8-Droide)
von Samsung und der Smarthome AI Agent (Optik: Chihuahua mit Kopfhörern) von
LG. Sie sollen herumfahren, Smartgeräte kontrollieren, sowie reden (LG) oder
mit einem eingebauten Beamer Filme abspielen (Samsung). 

Die Unternehmen sind sich auch darin einig, dass der oft
totgesagte Fernseher alles andere als selbiges ist. Für LG, Samsung wie auch
für Panasonic wird der Fernseher künftig zur Zentrale der Steuerung des Smarthomes. Statt nur Netflix zu streamen, soll man hier künftig das Wetter checken,
die Überwachungskameras durchschalten und die Einkaufsliste verwalten können. LG
kooperiert dafür mit Google, Panasonic mit Amazon. Samsung und LG führten außerdem in
Las Vegas einen transparenten Fernseher vor –  im Falle von LG lässt sich zwischen
durchsichtig und undurchsichtig hin- und herschalten. Bei beiden ist noch
unklar, wann sie erscheinen und wie viele tausend Dollar sie kosten sollen – faszinierend
sah es aber allemal aus.

In noch einem Punkt herrscht bei den Unternehmen auf der CES
Einigkeit: Wer etwas auf sich hält, der stellt sich gut mit Microsoft. Das ist
einigermaßen überraschend, bedenkt man, dass Microsoft lange Zeit eher so etwas
wie das hässliche Entlein der Techbranche war – Windows war bisher vieles,
aber sicher nicht cool.
Seit Microsofts Milliardeninvestment in OpenAI sich aber in Form des Hypes um
ChatGPT auszahlt, lädt man Microsoft-Manager anscheinend wesentlich lieber zur
Pressekonferenz.

So kündigte Samsung an, dass in die demnächst startenden
Galaxy-Book-4-Laptops Microsofts KI-Assistent Copilot einziehen soll – ob sie auch
die neue Copilot-Taste erhalten (die erste Veränderung der Windows-Tastatur
seit fast 30 Jahren!), wurde nicht gesagt. Mit Siemens arbeitet Microsoft daran,
Copilot auch für Industrieanwendungen zu nutzen, für Anwendungen im Auto
kooperiert Microsoft mit Sony und Honda. Die zeigten erneut
ihren E-Auto-Prototypen Afeela. Der lässt sich anscheinend nicht nur mit
einem Playstation-Controller steuern, sondern soll auch einen von Microsoft
Copilot angetriebenen Sprachassistenten enthalten – wenn er denn jemals auf den
Markt kommt. Den direkteren Weg geht Volkswagen: Ab dem zweiten Quartal will
der deutsche Konzern ChatGPT in seine Autos einbauen, der Sprachassistent IDA
soll so natürlichere Antworten liefern.

Auffällig bei all diesen angekündigten Kooperationen: Nahezu
jeder große Techkonzern zeigt sich in Las Vegas – nur Apple macht nicht mit. Der
iPhone-Konzern ist wie immer nicht bei der Messe dabei (oder nur am
Rande), allerdings ist er doch oft schweigend präsent. In diesem Jahr
vielleicht so stark wie selten zuvor: Es ist sicher kein Zufall, dass Apple pünktlich
zur Eröffnung der großen CES-Pressekonferenzen am Montagmorgen Ortszeit den Verkaufsstart
für die Vision-Pro-Brille
verkündete – und so gleich Aufmerksamkeit von den Konkurrenten stahl.

Aber auch ohne diese Ankündigung war Apples
Mixed-Reality-Brille zwischen den Zeilen immer wieder ein Thema. So ging es in
Las Vegas auch um ""spatial computing"", wie Apple die Verbindung von
realer und digitaler Welt nennt (auch wenn
der Begriff älter ist). Sony und Siemens stellten gemeinsam ein Gerät zur ""spatial content creation"" vor,
dass es so womöglich ohne den Aufmerksamkeitsschub von Apple nicht geben würde.
Das Headset sieht der PS VR-2-Gamingbrille
ähnlich, statt zum Spielen ist sie aber dafür gedacht, 3D-Modelle anzusehen, zu
erstellen und zu bearbeiten. Ein Sprecher des Entwicklungsteams von Red Bull
Racing erklärte auf der CES-Bühne, wie mit der Brille ein neues
Formel-1-Auto entwickelt werden würde. 

Sony liefert die Hardware der Brille, Siemens die Software. Sie
soll noch in diesem Jahr erscheinen. Den größeren Rahmen dafür bildet eine
Idee, die Siemens CEO Roland Busch präsentierte: 2024 sei der Wendepunkt zum ""Industrial Metaverse"" gekommen, sagte
der Vorstandsvorsitzende. Siemens meint damit, dass inzwischen quasi alles – von
Flugzeugen über Fabriken bis zu menschlichen Herzen – realitätsgetreu digital
simuliert werden kann. An diesen ""digitalen Zwillingen"" könne man dann
experimentieren, Prozesse optimieren und so Innovation beschleunigen. Eine in
diesem Metaversum geplante Autofabrik sei etwa wesentlich produktiver und
energieeffizienter. Sagt zumindest Siemens.

Solche und ähnliche Energieeinsparungen behaupten alle Techkonzerne
einstimmig. Samsung und LG sprachen minutenlang über ihre
Nachhaltigkeitsinitiativen, Panasonic stellte gleich seine ganze Präsentation
unter das Motto ""Green Impact"", bei Bosch sprach man hauptsächlich über die
Energiewende im Verkehrssektor, für die Wasserstoff zentral sei (Volker Wissing
gefällt das). Wenn man die grünglitzernden Powerpoints so anschaut, könnte man
fast vergessen, dass es die gleichen Konzerne sind, die mit immer neuen
Produkten zu einem immer höheren Energiebedarf und zu immer höheren Elektroschrottbergen
beitragen. 

Die überzeugendsten Innovationen sind aber womöglich sowieso
abseits der großen Konferenzbühnen zu finden. Etwa bei der App des
niederländischen Start-ups Whispp. Mit
KI-gestützter Stimmerzeugung gibt sie Menschen die Stimme zurück, die sie etwa durch
Krankheiten wie Kehlkopfkrebs verloren haben. Auch wenn die Betroffenen nur
noch flüstern können, macht Whispp daraus wieder verständliche Anrufe und
Sprachnachrichten. Bei einer kleinen Vorführung am Rande der Messe
funktionierte das beeindruckend gut. Und das ist doch wesentlich innovativer
als ein sprechendes Auto oder eine Kühlschrank-KI. 

"
Artificial Intelligence,Zeit,2024-01-09,https://www.zeit.de/digital/2024-01/ces-2024-las-vegas-kuenstliche-intelligenz-technik,CES 2024: Das sind die Techtrends des Jahres | ZEIT ONLINE,"Alt auszusehen, ist nie angenehm. Als Messe, die von sich
behauptet, eine ""Plattform für Innovatoren"" zu sein, ist es allerdings
besonders unangenehm. Genau das ist der CES im vergangenen Jahr passiert: Während
die ganze Welt über künstliche Intelligenz (KI) sprach, konnte die Techmesse
wenig dazu zeigen. Sie hatte schlicht Pech gehabt: Das Programm stand, die
Messestände waren geordert, in den wenigen Wochen zwischen der Veröffentlichung
von ChatGPT im November 2022 und dem CES-Start im Januar konnte keine ganze
Messe mehr umgeworfen werden. 

Dass die CES das nicht auf sich sitzen lassen will, zeigt
sich nun in Las Vegas. Schon im Vorfeld hatten die Veranstalter künstliche Intelligenz zum Topthema der Messe ausgerufen, das bestätigt sich auch vor
Ort: Bevor die Messe am Dienstag offiziell eröffnet wird, konnte sich ZEIT
ONLINE beim Vorabevent CES Unveiled und in diversen Pressekonferenzen bereits
einen Eindruck von den neuesten Techtrends machen. Und klar ist: Ohne KI geht
es nicht mehr. Was macht man aber mit dieser Technik? Auf diese Frage finden
die Techunternehmen ganz unterschiedliche Antworten. 

Die CES in Las Vegas gilt als eine der wichtigsten und größten Messen der Tech-Branche. In diesem Jahr werden 130.000 Besucher erwartet, denen mehr als 4.000 Aussteller ihre Produkte und Dienstleistungen zeigen: von Großkonzernen wie Amazon, Sony oder Samsung bis zu kleinen Start-ups aus aller Welt. Traditionell nehmen Geräte aus dem Audio-Video-Bereich und Haushaltselektronik einen großen Teil der Messehallen ein, aber auch Lebensmittel- und Gesundheitstechnologie, Gaminghardware oder Laptops sind hier zu finden. In den vergangenen Jahren wurde zudem das Thema Mobilität immer wichtiger, BMW, Mercedes oder Honda kündigen hier neue E-Autos und In-Car-Systeme an. 

Die CES (die nicht mehr Consumer Electronics Show genannt werden möchte) ist eine reine Fachmesse, Privatbesucher sind nicht zugelassen. Sie wird von der Consumer Technology Association (CTA) veranstaltet, einem Branchenverband der amerikanischen Tech-Branche. Der wird in diesem Jahr 100 Jahre alt – gegründet wurde er 1924 als Radio Manufacturers Association – damals war die heißeste Technik noch das Radio. Seit der ersten CES 1967 gab es auf der Messe immer wieder Premieren von Produkten, die bald darauf in jedem Haushalt standen, wie der erste Videorekorder oder der erste CD-Player.

Dass sich nun alles um KI dreht, machte Samsung am
deutlichsten. Der südkoreanische Konzern rief als Motto ""AI for All"" aus
(Bonuspunkte für homografe Wortspiele!). Heißt: Samsung will künstliche Intelligenz in alle seine Produkte bringen. Etwa in den neuen Kühlschrank Bespoke
4-Door French Door, bei dem eine Kamera mithilfe von KI erkennen soll, ob
noch genug Eier und Milch im Kühlschrank sind, um Pfannkuchen zu machen. Alle
Geräte im Haus sollen dann über Samsungs Smarthomesystem SmartThings miteinander
vernetzt sein und über den Sprachassistenten Bixby angesteuert werden können. Das
smarte Zuhause will Samsung dabei über die eigenen vier Wände hinaus erweitern:
Durch eine Partnerschaft mit Hyundai sollen Nutzerinnen künftig auch im Auto
Sprachbefehle wie ""Schalte die Heizung an"" nach Hause senden können. 

Bemerkenswert ist, wie einig sich Samsung und das ebenfalls
südkoreanische Unternehmen LG in der Vision eines KI-betriebenen, vollvernetzten
Zuhauses sind. Auch LG übt sich in Wortspielen und plant, KI in alle seine
Produkte zu bringen. Dabei möchte das Unternehmen aber AI statt als artificial intelligence als affectionate
intelligence, also liebevolle Intelligenz verstehen. Beide Unternehmen
sprechen davon, wie durch KI Haushaltsarbeit einfacher werden soll (LG nennt
das sehr optimistisch Zero Labor Home). Dazu sollen in beiden Fällen
auch Roboterassistenten beitragen: der Ballie (Optik: Star-Wars-BB-8-Droide)
von Samsung und der Smarthome AI Agent (Optik: Chihuahua mit Kopfhörern) von
LG. Sie sollen herumfahren, Smartgeräte kontrollieren, sowie reden (LG) oder
mit einem eingebauten Beamer Filme abspielen (Samsung). 

Die Unternehmen sind sich auch darin einig, dass der oft
totgesagte Fernseher alles andere als selbiges ist. Für LG, Samsung wie auch
für Panasonic wird der Fernseher künftig zur Zentrale der Steuerung des Smarthomes. Statt nur Netflix zu streamen, soll man hier künftig das Wetter checken,
die Überwachungskameras durchschalten und die Einkaufsliste verwalten können. LG
kooperiert dafür mit Google, Panasonic mit Amazon. Samsung und LG führten außerdem in
Las Vegas einen transparenten Fernseher vor –  im Falle von LG lässt sich zwischen
durchsichtig und undurchsichtig hin- und herschalten. Bei beiden ist noch
unklar, wann sie erscheinen und wie viele tausend Dollar sie kosten sollen – faszinierend
sah es aber allemal aus.

In noch einem Punkt herrscht bei den Unternehmen auf der CES
Einigkeit: Wer etwas auf sich hält, der stellt sich gut mit Microsoft. Das ist
einigermaßen überraschend, bedenkt man, dass Microsoft lange Zeit eher so etwas
wie das hässliche Entlein der Techbranche war – Windows war bisher vieles,
aber sicher nicht cool.
Seit Microsofts Milliardeninvestment in OpenAI sich aber in Form des Hypes um
ChatGPT auszahlt, lädt man Microsoft-Manager anscheinend wesentlich lieber zur
Pressekonferenz.

So kündigte Samsung an, dass in die demnächst startenden
Galaxy-Book-4-Laptops Microsofts KI-Assistent Copilot einziehen soll – ob sie auch
die neue Copilot-Taste erhalten (die erste Veränderung der Windows-Tastatur
seit fast 30 Jahren!), wurde nicht gesagt. Mit Siemens arbeitet Microsoft daran,
Copilot auch für Industrieanwendungen zu nutzen, für Anwendungen im Auto
kooperiert Microsoft mit Sony und Honda. Die zeigten erneut
ihren E-Auto-Prototypen Afeela. Der lässt sich anscheinend nicht nur mit
einem Playstation-Controller steuern, sondern soll auch einen von Microsoft
Copilot angetriebenen Sprachassistenten enthalten – wenn er denn jemals auf den
Markt kommt. Den direkteren Weg geht Volkswagen: Ab dem zweiten Quartal will
der deutsche Konzern ChatGPT in seine Autos einbauen, der Sprachassistent IDA
soll so natürlichere Antworten liefern.

Auffällig bei all diesen angekündigten Kooperationen: Nahezu
jeder große Techkonzern zeigt sich in Las Vegas – nur Apple macht nicht mit. Der
iPhone-Konzern ist wie immer nicht bei der Messe dabei (oder nur am
Rande), allerdings ist er doch oft schweigend präsent. In diesem Jahr
vielleicht so stark wie selten zuvor: Es ist sicher kein Zufall, dass Apple pünktlich
zur Eröffnung der großen CES-Pressekonferenzen am Montagmorgen Ortszeit den Verkaufsstart
für die Vision-Pro-Brille
verkündete – und so gleich Aufmerksamkeit von den Konkurrenten stahl.

Aber auch ohne diese Ankündigung war Apples
Mixed-Reality-Brille zwischen den Zeilen immer wieder ein Thema. So ging es in
Las Vegas auch um ""spatial computing"", wie Apple die Verbindung von
realer und digitaler Welt nennt (auch wenn
der Begriff älter ist). Sony und Siemens stellten gemeinsam ein Gerät zur ""spatial content creation"" vor,
dass es so womöglich ohne den Aufmerksamkeitsschub von Apple nicht geben würde.
Das Headset sieht der PS VR-2-Gamingbrille
ähnlich, statt zum Spielen ist sie aber dafür gedacht, 3D-Modelle anzusehen, zu
erstellen und zu bearbeiten. Ein Sprecher des Entwicklungsteams von Red Bull
Racing erklärte auf der CES-Bühne, wie mit der Brille ein neues
Formel-1-Auto entwickelt werden würde. 

Sony liefert die Hardware der Brille, Siemens die Software. Sie
soll noch in diesem Jahr erscheinen. Den größeren Rahmen dafür bildet eine
Idee, die Siemens CEO Roland Busch präsentierte: 2024 sei der Wendepunkt zum ""Industrial Metaverse"" gekommen, sagte
der Vorstandsvorsitzende. Siemens meint damit, dass inzwischen quasi alles – von
Flugzeugen über Fabriken bis zu menschlichen Herzen – realitätsgetreu digital
simuliert werden kann. An diesen ""digitalen Zwillingen"" könne man dann
experimentieren, Prozesse optimieren und so Innovation beschleunigen. Eine in
diesem Metaversum geplante Autofabrik sei etwa wesentlich produktiver und
energieeffizienter. Sagt zumindest Siemens.

Solche und ähnliche Energieeinsparungen behaupten alle Techkonzerne
einstimmig. Samsung und LG sprachen minutenlang über ihre
Nachhaltigkeitsinitiativen, Panasonic stellte gleich seine ganze Präsentation
unter das Motto ""Green Impact"", bei Bosch sprach man hauptsächlich über die
Energiewende im Verkehrssektor, für die Wasserstoff zentral sei (Volker Wissing
gefällt das). Wenn man die grünglitzernden Powerpoints so anschaut, könnte man
fast vergessen, dass es die gleichen Konzerne sind, die mit immer neuen
Produkten zu einem immer höheren Energiebedarf und zu immer höheren Elektroschrottbergen
beitragen. 

Die überzeugendsten Innovationen sind aber womöglich sowieso
abseits der großen Konferenzbühnen zu finden. Etwa bei der App des
niederländischen Start-ups Whispp. Mit
KI-gestützter Stimmerzeugung gibt sie Menschen die Stimme zurück, die sie etwa durch
Krankheiten wie Kehlkopfkrebs verloren haben. Auch wenn die Betroffenen nur
noch flüstern können, macht Whispp daraus wieder verständliche Anrufe und
Sprachnachrichten. Bei einer kleinen Vorführung am Rande der Messe
funktionierte das beeindruckend gut. Und das ist doch wesentlich innovativer
als ein sprechendes Auto oder eine Kühlschrank-KI. 

"
KI,Zeit,2024-01-09,https://www.zeit.de/digital/2024-01/ces-2024-las-vegas-kuenstliche-intelligenz-technik,CES 2024: Das sind die Techtrends des Jahres | ZEIT ONLINE,"Alt auszusehen, ist nie angenehm. Als Messe, die von sich
behauptet, eine ""Plattform für Innovatoren"" zu sein, ist es allerdings
besonders unangenehm. Genau das ist der CES im vergangenen Jahr passiert: Während
die ganze Welt über künstliche Intelligenz (KI) sprach, konnte die Techmesse
wenig dazu zeigen. Sie hatte schlicht Pech gehabt: Das Programm stand, die
Messestände waren geordert, in den wenigen Wochen zwischen der Veröffentlichung
von ChatGPT im November 2022 und dem CES-Start im Januar konnte keine ganze
Messe mehr umgeworfen werden. 

Dass die CES das nicht auf sich sitzen lassen will, zeigt
sich nun in Las Vegas. Schon im Vorfeld hatten die Veranstalter künstliche Intelligenz zum Topthema der Messe ausgerufen, das bestätigt sich auch vor
Ort: Bevor die Messe am Dienstag offiziell eröffnet wird, konnte sich ZEIT
ONLINE beim Vorabevent CES Unveiled und in diversen Pressekonferenzen bereits
einen Eindruck von den neuesten Techtrends machen. Und klar ist: Ohne KI geht
es nicht mehr. Was macht man aber mit dieser Technik? Auf diese Frage finden
die Techunternehmen ganz unterschiedliche Antworten. 

Die CES in Las Vegas gilt als eine der wichtigsten und größten Messen der Tech-Branche. In diesem Jahr werden 130.000 Besucher erwartet, denen mehr als 4.000 Aussteller ihre Produkte und Dienstleistungen zeigen: von Großkonzernen wie Amazon, Sony oder Samsung bis zu kleinen Start-ups aus aller Welt. Traditionell nehmen Geräte aus dem Audio-Video-Bereich und Haushaltselektronik einen großen Teil der Messehallen ein, aber auch Lebensmittel- und Gesundheitstechnologie, Gaminghardware oder Laptops sind hier zu finden. In den vergangenen Jahren wurde zudem das Thema Mobilität immer wichtiger, BMW, Mercedes oder Honda kündigen hier neue E-Autos und In-Car-Systeme an. 

Die CES (die nicht mehr Consumer Electronics Show genannt werden möchte) ist eine reine Fachmesse, Privatbesucher sind nicht zugelassen. Sie wird von der Consumer Technology Association (CTA) veranstaltet, einem Branchenverband der amerikanischen Tech-Branche. Der wird in diesem Jahr 100 Jahre alt – gegründet wurde er 1924 als Radio Manufacturers Association – damals war die heißeste Technik noch das Radio. Seit der ersten CES 1967 gab es auf der Messe immer wieder Premieren von Produkten, die bald darauf in jedem Haushalt standen, wie der erste Videorekorder oder der erste CD-Player.

Dass sich nun alles um KI dreht, machte Samsung am
deutlichsten. Der südkoreanische Konzern rief als Motto ""AI for All"" aus
(Bonuspunkte für homografe Wortspiele!). Heißt: Samsung will künstliche Intelligenz in alle seine Produkte bringen. Etwa in den neuen Kühlschrank Bespoke
4-Door French Door, bei dem eine Kamera mithilfe von KI erkennen soll, ob
noch genug Eier und Milch im Kühlschrank sind, um Pfannkuchen zu machen. Alle
Geräte im Haus sollen dann über Samsungs Smarthomesystem SmartThings miteinander
vernetzt sein und über den Sprachassistenten Bixby angesteuert werden können. Das
smarte Zuhause will Samsung dabei über die eigenen vier Wände hinaus erweitern:
Durch eine Partnerschaft mit Hyundai sollen Nutzerinnen künftig auch im Auto
Sprachbefehle wie ""Schalte die Heizung an"" nach Hause senden können. 

Bemerkenswert ist, wie einig sich Samsung und das ebenfalls
südkoreanische Unternehmen LG in der Vision eines KI-betriebenen, vollvernetzten
Zuhauses sind. Auch LG übt sich in Wortspielen und plant, KI in alle seine
Produkte zu bringen. Dabei möchte das Unternehmen aber AI statt als artificial intelligence als affectionate
intelligence, also liebevolle Intelligenz verstehen. Beide Unternehmen
sprechen davon, wie durch KI Haushaltsarbeit einfacher werden soll (LG nennt
das sehr optimistisch Zero Labor Home). Dazu sollen in beiden Fällen
auch Roboterassistenten beitragen: der Ballie (Optik: Star-Wars-BB-8-Droide)
von Samsung und der Smarthome AI Agent (Optik: Chihuahua mit Kopfhörern) von
LG. Sie sollen herumfahren, Smartgeräte kontrollieren, sowie reden (LG) oder
mit einem eingebauten Beamer Filme abspielen (Samsung). 

Die Unternehmen sind sich auch darin einig, dass der oft
totgesagte Fernseher alles andere als selbiges ist. Für LG, Samsung wie auch
für Panasonic wird der Fernseher künftig zur Zentrale der Steuerung des Smarthomes. Statt nur Netflix zu streamen, soll man hier künftig das Wetter checken,
die Überwachungskameras durchschalten und die Einkaufsliste verwalten können. LG
kooperiert dafür mit Google, Panasonic mit Amazon. Samsung und LG führten außerdem in
Las Vegas einen transparenten Fernseher vor –  im Falle von LG lässt sich zwischen
durchsichtig und undurchsichtig hin- und herschalten. Bei beiden ist noch
unklar, wann sie erscheinen und wie viele tausend Dollar sie kosten sollen – faszinierend
sah es aber allemal aus.

In noch einem Punkt herrscht bei den Unternehmen auf der CES
Einigkeit: Wer etwas auf sich hält, der stellt sich gut mit Microsoft. Das ist
einigermaßen überraschend, bedenkt man, dass Microsoft lange Zeit eher so etwas
wie das hässliche Entlein der Techbranche war – Windows war bisher vieles,
aber sicher nicht cool.
Seit Microsofts Milliardeninvestment in OpenAI sich aber in Form des Hypes um
ChatGPT auszahlt, lädt man Microsoft-Manager anscheinend wesentlich lieber zur
Pressekonferenz.

So kündigte Samsung an, dass in die demnächst startenden
Galaxy-Book-4-Laptops Microsofts KI-Assistent Copilot einziehen soll – ob sie auch
die neue Copilot-Taste erhalten (die erste Veränderung der Windows-Tastatur
seit fast 30 Jahren!), wurde nicht gesagt. Mit Siemens arbeitet Microsoft daran,
Copilot auch für Industrieanwendungen zu nutzen, für Anwendungen im Auto
kooperiert Microsoft mit Sony und Honda. Die zeigten erneut
ihren E-Auto-Prototypen Afeela. Der lässt sich anscheinend nicht nur mit
einem Playstation-Controller steuern, sondern soll auch einen von Microsoft
Copilot angetriebenen Sprachassistenten enthalten – wenn er denn jemals auf den
Markt kommt. Den direkteren Weg geht Volkswagen: Ab dem zweiten Quartal will
der deutsche Konzern ChatGPT in seine Autos einbauen, der Sprachassistent IDA
soll so natürlichere Antworten liefern.

Auffällig bei all diesen angekündigten Kooperationen: Nahezu
jeder große Techkonzern zeigt sich in Las Vegas – nur Apple macht nicht mit. Der
iPhone-Konzern ist wie immer nicht bei der Messe dabei (oder nur am
Rande), allerdings ist er doch oft schweigend präsent. In diesem Jahr
vielleicht so stark wie selten zuvor: Es ist sicher kein Zufall, dass Apple pünktlich
zur Eröffnung der großen CES-Pressekonferenzen am Montagmorgen Ortszeit den Verkaufsstart
für die Vision-Pro-Brille
verkündete – und so gleich Aufmerksamkeit von den Konkurrenten stahl.

Aber auch ohne diese Ankündigung war Apples
Mixed-Reality-Brille zwischen den Zeilen immer wieder ein Thema. So ging es in
Las Vegas auch um ""spatial computing"", wie Apple die Verbindung von
realer und digitaler Welt nennt (auch wenn
der Begriff älter ist). Sony und Siemens stellten gemeinsam ein Gerät zur ""spatial content creation"" vor,
dass es so womöglich ohne den Aufmerksamkeitsschub von Apple nicht geben würde.
Das Headset sieht der PS VR-2-Gamingbrille
ähnlich, statt zum Spielen ist sie aber dafür gedacht, 3D-Modelle anzusehen, zu
erstellen und zu bearbeiten. Ein Sprecher des Entwicklungsteams von Red Bull
Racing erklärte auf der CES-Bühne, wie mit der Brille ein neues
Formel-1-Auto entwickelt werden würde. 

Sony liefert die Hardware der Brille, Siemens die Software. Sie
soll noch in diesem Jahr erscheinen. Den größeren Rahmen dafür bildet eine
Idee, die Siemens CEO Roland Busch präsentierte: 2024 sei der Wendepunkt zum ""Industrial Metaverse"" gekommen, sagte
der Vorstandsvorsitzende. Siemens meint damit, dass inzwischen quasi alles – von
Flugzeugen über Fabriken bis zu menschlichen Herzen – realitätsgetreu digital
simuliert werden kann. An diesen ""digitalen Zwillingen"" könne man dann
experimentieren, Prozesse optimieren und so Innovation beschleunigen. Eine in
diesem Metaversum geplante Autofabrik sei etwa wesentlich produktiver und
energieeffizienter. Sagt zumindest Siemens.

Solche und ähnliche Energieeinsparungen behaupten alle Techkonzerne
einstimmig. Samsung und LG sprachen minutenlang über ihre
Nachhaltigkeitsinitiativen, Panasonic stellte gleich seine ganze Präsentation
unter das Motto ""Green Impact"", bei Bosch sprach man hauptsächlich über die
Energiewende im Verkehrssektor, für die Wasserstoff zentral sei (Volker Wissing
gefällt das). Wenn man die grünglitzernden Powerpoints so anschaut, könnte man
fast vergessen, dass es die gleichen Konzerne sind, die mit immer neuen
Produkten zu einem immer höheren Energiebedarf und zu immer höheren Elektroschrottbergen
beitragen. 

Die überzeugendsten Innovationen sind aber womöglich sowieso
abseits der großen Konferenzbühnen zu finden. Etwa bei der App des
niederländischen Start-ups Whispp. Mit
KI-gestützter Stimmerzeugung gibt sie Menschen die Stimme zurück, die sie etwa durch
Krankheiten wie Kehlkopfkrebs verloren haben. Auch wenn die Betroffenen nur
noch flüstern können, macht Whispp daraus wieder verständliche Anrufe und
Sprachnachrichten. Bei einer kleinen Vorführung am Rande der Messe
funktionierte das beeindruckend gut. Und das ist doch wesentlich innovativer
als ein sprechendes Auto oder eine Kühlschrank-KI. 

"
KI,Zeit,2024-01-07,https://www.zeit.de/news/2024-01/07/so-soll-ki-bei-polizeiarbeit-helfen,Technologie: So soll KI bei der Polizeiarbeit helfen | ZEIT ONLINE,"Die Anruferin klingt wie die eigene Tochter - aber die Stimme ist geklaut. Künstliche Intelligenz (KI) macht betrügerische Schockanrufe noch tückischer, wenn Stimmen geklont werden und nicht mehr von den echten unterscheidbar sind. «Das ist schon ohne großes technisches Knowhow möglich», meint der KI-Koordinator beim Berliner Landeskriminalamt, Eugen Hofmann.

Deutsche Sicherheitsbehörden rufen nach Werkzeugen der KI, um digitaler Verbrechensausübung nicht länger hinterher zu laufen. «Die Polizei muss dem etwas entgegen setzen», meint LKA-Mann Hofmann. Vor allem auch die Bearbeitung riesiger Datenmengen soll künftig mit Algorithmen erleichtert werden etwa bei Ermittlungen wegen Kinderpornografie.

Auch Geldautomatensprengern oder Dokumentenfälschern lässt sich aus Forscher-Sicht mit Hilfe von KI auf die Spur kommen. Bislang aber fehlt oft - auch wegen hoher rechtlicher Hürden - der Praxiseinsatz bei der Polizei. Eine klare Ausrichtung für die Zukunft war bislang kaum erkennbar.

«Es wird Zeit, dass es anfängt», sagt der Informatiker Andreas Dengel. Er ist Geschäftsführender Direktor des Deutschen Forschungszentrums für Künstliche Intelligenz (DFKI) in Kaiserslautern, das mit dem Bundeskriminalamt (BKA) und dem LKA Rheinland-Pfalz kooperiert und KI-Systeme für Polizeiermittlungen testet. Bislang besteht aus Sicht des KI-Experten Dengel ein großer Flickenteppich bei den Polizeibehörden der Länder. «Es müsste einen nationalen Polizeibeauftragten geben, der eine KI-Strategie entwickelt.»

Einige Beispiele für Forschungsprojekte und KI-Tests für die Polizeiarbeit:

Wenn Geldautomaten-Sprenger, die 2023 in Deutschland mehr als 450 Geräte zerstörten, Schuhabdrücke hinterlassen, kann aus Sicht Dengels eine eigens dafür trainierte KI zum Einsatz kommen. Die Muster ließen sich dann bestimmten Schuhtypen zuordnen. Bislang seien bereits einige Tausend unterschiedliche Schuhabdrücke bundesweit gesammelt, so Dengel. Das Ziel: Täter am Schuhprofil zu identifizieren. Das System stehe kurz vor der Einführung, er hoffe noch 2024, sagt Dengel.

Mit neuen Verfahren kann die Polizei laut Deutschem Forschungszentrum in Kaiserslautern auch gefälschten Dokumenten auf die Spur kommen. Ein KI-System könne dabei die minimalen Unterschiede bei Druckern erkennen.

Getestet werden zudem beispielsweise im Süden Deutschlands und in Hamburg Videokameras, die mit Algorithmen und intelligenter Software verdächtige Bewegungsmuster finden sollen. Die KI muss dabei mit Daten und Bildern trainiert werden. Biometrische Daten werden nicht erfasst, auch Alter, Geschlecht oder Ethnie von Personen werden damit nicht bestimmt, wie die Hamburger Innenbehörde 2023 betont hatte.

Der Berliner KI-Koordindator beim Berliner LKA, Hofmann, nennt als ein mögliches Beispiel auch Fußballstadien, in denen mit Hilfe einer Gesichtserkennung etwa Randalierer ausgesiebt werden könnten. Eigene Datenmodelle dafür und andere KI-Systeme hat die Berliner Polizei aber nicht.

Im Zusammenhang mit Straftaten in der Silvesternacht hatte der stellvertretende Landesvorsitzende der Gewerkschaft der Polizei in Hamburg, Lars Osburg, vor kurzem gesagt: «Es ist nicht mehr zu vermitteln, warum wir massenhaft Kräfte zum Schutz von Veranstaltungen einsetzen müssen, aber auf die Chancen der KI bei der Fahndung nach bekannten Straftätern und Gefährdern verzichten.»

Die größte Gefahr in naher Zukunft sieht der Cybersicherheits-Experte Christian Dörr vom Hasso-Plattner-Institut in Potsdam aber darin, dass mit Hilfe Künstlicher Intelligenz zunehmend Deepfakes für Desinformationskampagnen eingesetzt werden. Dann spricht nicht der Kanzler im Video, sondern ein digitaler Zwilling.

Mit Hilfe von Manipulationen werde versucht, Wahlen zu beeinflussen und staatliche Systeme zu destabilisieren, meint Dörr. «Die großen Hackergruppen sind nationalstaatlich getrieben. Die haben natürlich auch sehr, sehr viel Interesse an KI.» Im Sommer 2022 hatte etwa die damalige Regierende Bürgermeisterin von Berlin, Franziska Giffey (SPD), per Video mit einer Person gesprochen, die wie der Kiewer Bürgermeister Vitali Klitschko aussah, aber nicht Klitschko war.

Für die Polizeiarbeit sind dem Einsatz von KI-Systemen aber auch enge Grenzen gesetzt. Bedenken gibt es, weil auch Risiken für unbescholtene Bürger gesehen werden und Grundrechte gefährdet sein können.

Der Bundesverfassungsschutz hatte bereits Regelungen in Hessen und Hamburg für verfassungswidrig erklärt. Es geht um eine Analyse-Software, mit der Polizisten mit einem Klick verschiedene Datenbanken durchsuchen, um in den riesigen Datenmengen Querverbindungen zu entdecken. Laut hessischer Polizei war im Zusammenhang mit einer Razzia gegen Reichsbürger so eine Festnahme gelungen. Im Dezember 2023 einigte sich nun die EU auf schärfere Regeln - das kann auch Folgen für KI-Instrumente bei der Polizei haben. «Wenn wir warten, bis Juristen einen wasserdichten Text haben, das dauert fünf Jahre», meint der Berliner LKA-Beamte Hofmann.

© dpa-infocom, dpa:240107-99-516369/3
"
Künstliche Intelligenz,Zeit,2024-01-03,https://www.zeit.de/arbeit/2023-11/kuenstliche-intelligenz-jobs-beruf-automatisierung-technologie,"Künstliche Intelligenz: ""Bald fragen wir uns: Wieso wurden für diese Jobs Menschen gebraucht?"" | ZEIT Arbeit",
KI,Zeit,2024-01-05,https://www.zeit.de/news/2024-01/05/forscher-wollen-extremwetter-auswirkungen-mit-ki-vorhersagen,Katastrophenschutz: Forscher wollen Extremwetter-Auswirkungen mit KI vorhersagen | ZEIT ONLINE,"Mit Hilfe von Künstlicher Intelligenz (KI) wollen Jenaer Forscher die Auswirkungen von Klimaextremen besser vorhersagbar machen. Damit könnten den Wissenschaftlern zufolge künftig etwa Hilfseinsätze nach Fluten oder Trockenheiten früher geplant und die lokale Bevölkerung besser gewarnt werden. Schon heute gebe es Modelle, aus denen sich viele Informationen ableiten ließen.

«Wettervorhersagen hören oft bei der Frage auf, ob es regnet oder nicht», sagt der Direktor des Max-Planck-Instituts für Biochemie, Markus Reichstein. Beziehe man aber geografische Daten oder Bevölkerungsdaten mit ein, ließen sich so KI-basierte Frühwarnsysteme etablieren. Ziel sei, in ein bis zwei Jahren ein funktionierendes Vorhersage-Modell zu haben, das Hilfsorganisationen wie das Rote Kreuz nutzen könnten. Dazu gebe es bereits konkrete Gespräche.

Gemeinsam mit dem Lehrstuhlinhaber für Digitale Bildverarbeitung an der Friedrich-Schiller-Universität Jena, Joachim Denzler, leitet Reichstein die Ellis-Einheit Jena. Ellis ist ein europäisches Netzwerk zum Thema KI mit Forschungsgruppen in über einem Dutzend Länder. In Jena arbeiten nach dem Startschuss vor zwei Jahren nun etwa 40 Wissenschaftler in der Einheit. Die Idee sei gewesen, die Kompetenzen im Bereich maschinelles Lernen und in der Erforschung der Erdsysteme zusammenzubringen, so Reichstein.

Bei einer Trockenheit etwa griffen mehrere Konzepte: Auf Nordhängen mache sich Hitze weniger bemerkbar als auf Südhängen, in Mulden oder an einem Fluss sei die Vegetation weniger gestresst. «Da gibt es keine physikalischen Modelle, weil es so komplex ist und so viele Faktoren zusammenhängen», sagt Reichstein. Mit Hilfe der Daten könne man lernen, wie lokale Ökosysteme reagieren. Auch Bevölkerungsdaten bis hin zur Beschaffenheit von Häusern ließen sich abbilden und so Aussagen über die Verletzlichkeit der Bevölkerung treffen.

Darüber hinaus arbeiten die Forscher aber auch an Vorhersagen im medizinischen Bereich. «Ob man sich mit Fernerkundung die Entwaldung ansieht oder ob man sich Hautkrebs ansieht - viele Datensätze sind sehr ähnlich», sagt Reichstein. Denzler zufolge ließen sich aber zum Beispiel auch Voraussagen zu Depressionswellen treffen, wenn man verstehe, wie Krisen wie Corona oder der Krieg in der Ukraine auf die Bevölkerung wirke. Aber auch wenn die Datenstrukturen ähnlich sind: Im Bereich Datenschutz gibt es im medizinischen Bereich höhere Hürden, wie die beiden Wissenschaftler sagen.

© dpa-infocom, dpa:240105-99-497584/2
"
Künstliche Intelligenz,Zeit,2023-12-30,https://www.zeit.de/wirtschaft/2023-12/kuenstliche-intelligenz-bundeskartellamt-digitalkonzerne,Künstliche Intelligenz: Bundeskartellamt warnt vor Machtgewinn von Digitalkonzernen | ZEIT ONLINE,"Das Bundeskartellamt warnt vor einem Machtzuwachs der ohnehin bereits mächtigen Digitalkonzerne durch den Einsatz künstlicher Intelligenz (KI). ""Die großen Digitalkonzerne werden KI voraussichtlich nutzen, um ihre Marktmacht noch weiter auszudehnen"", sagte Behördenchef Andreas Mundt der Rheinischen Post. Genannt wurden Konzerne wie Google, Amazon, Apple, Microsoft und Facebook.

Diese Konzerne ""haben gigantische Datensätze mit Bezug zu Milliarden Menschen, sie haben riesige Finanzreserven, um Projekte voranzutreiben, sie haben die entsprechenden Serverkapazitäten"", sagte Mundt. ""Dies alles bedeutet, dass datengetriebene Netzwerkeffekte weiter zunehmen können.""

Mundt wies darauf hin, dass bereits mehr als die Hälfte des deutschen E-Commerce über Amazon abgewickelt werde. Die Macht des Unternehmens sei ""gigantisch in seiner Doppelfunktion als eigenständiger Händler und gleichzeitig Betreiber des Amazon-Marketplace, auf den ja viele Händler angewiesen sind"". Das Kartellamt überprüfe daher derzeit, ""ob Amazon die Preise von Händlern auf dem Marketplace überwacht und beeinflusst"".

Der Kartellamtschef sagte, es sei allen Unternehmen verboten, Preise mithilfe von KI zu koordinieren. Vielmehr müssten stets die Unternehmen selbst für das Handeln ihrer Computersysteme einstehen. Diese ""bleiben für ihre Algorithmen verantwortlich, auch wenn diese sich zu KI weiterentwickeln"".

Mundt äußerte sich skeptisch dazu, ob der Facebook-Konzern (Meta) der Vorgabe eines höheren Daten- und Kundenschutzes nachkommt, indem er neuerdings für 9,99 Euro Monatsgebühr verspricht, dass zahlende Kunden keine Werbung mehr eingeblendet bekommen. Mundt sagte: ""Ich gehe davon aus, dass sich die EU-Kommission dieses neue Modell genau anschauen wird. Das Ergebnis kann meines Erachtens jedenfalls nicht dazu führen, dass man Geld dafür bezahlen muss, dass der Datenschutz beachtet wird.""

Das Kartellamt hatte Facebook vor Jahren auferlegt, die Daten von Facebook, Whatsapp und anderen Diensten nicht mehr einfach zusammenzuführen. Diese Linie bestätigte der Europäische Gerichtshof. ""Es ist spätestens seitdem klar, dass die Macht über Daten eine Bedeutung für die wettbewerbliche Position eines Unternehmens hat"", sagte Mundt. Derzeit werde mit Facebook über die Art und Weise der Umsetzung des Beschlusses verhandelt.
"
AI,Zeit,2023-12-30,https://www.zeit.de/wirtschaft/2023-12/kuenstliche-intelligenz-bundeskartellamt-digitalkonzerne,Künstliche Intelligenz: Bundeskartellamt warnt vor Machtgewinn von Digitalkonzernen | ZEIT ONLINE,"Das Bundeskartellamt warnt vor einem Machtzuwachs der ohnehin bereits mächtigen Digitalkonzerne durch den Einsatz künstlicher Intelligenz (KI). ""Die großen Digitalkonzerne werden KI voraussichtlich nutzen, um ihre Marktmacht noch weiter auszudehnen"", sagte Behördenchef Andreas Mundt der Rheinischen Post. Genannt wurden Konzerne wie Google, Amazon, Apple, Microsoft und Facebook.

Diese Konzerne ""haben gigantische Datensätze mit Bezug zu Milliarden Menschen, sie haben riesige Finanzreserven, um Projekte voranzutreiben, sie haben die entsprechenden Serverkapazitäten"", sagte Mundt. ""Dies alles bedeutet, dass datengetriebene Netzwerkeffekte weiter zunehmen können.""

Mundt wies darauf hin, dass bereits mehr als die Hälfte des deutschen E-Commerce über Amazon abgewickelt werde. Die Macht des Unternehmens sei ""gigantisch in seiner Doppelfunktion als eigenständiger Händler und gleichzeitig Betreiber des Amazon-Marketplace, auf den ja viele Händler angewiesen sind"". Das Kartellamt überprüfe daher derzeit, ""ob Amazon die Preise von Händlern auf dem Marketplace überwacht und beeinflusst"".

Der Kartellamtschef sagte, es sei allen Unternehmen verboten, Preise mithilfe von KI zu koordinieren. Vielmehr müssten stets die Unternehmen selbst für das Handeln ihrer Computersysteme einstehen. Diese ""bleiben für ihre Algorithmen verantwortlich, auch wenn diese sich zu KI weiterentwickeln"".

Mundt äußerte sich skeptisch dazu, ob der Facebook-Konzern (Meta) der Vorgabe eines höheren Daten- und Kundenschutzes nachkommt, indem er neuerdings für 9,99 Euro Monatsgebühr verspricht, dass zahlende Kunden keine Werbung mehr eingeblendet bekommen. Mundt sagte: ""Ich gehe davon aus, dass sich die EU-Kommission dieses neue Modell genau anschauen wird. Das Ergebnis kann meines Erachtens jedenfalls nicht dazu führen, dass man Geld dafür bezahlen muss, dass der Datenschutz beachtet wird.""

Das Kartellamt hatte Facebook vor Jahren auferlegt, die Daten von Facebook, Whatsapp und anderen Diensten nicht mehr einfach zusammenzuführen. Diese Linie bestätigte der Europäische Gerichtshof. ""Es ist spätestens seitdem klar, dass die Macht über Daten eine Bedeutung für die wettbewerbliche Position eines Unternehmens hat"", sagte Mundt. Derzeit werde mit Facebook über die Art und Weise der Umsetzung des Beschlusses verhandelt.
"
Artificial Intelligence,Zeit,2023-12-30,https://www.zeit.de/wirtschaft/2023-12/kuenstliche-intelligenz-bundeskartellamt-digitalkonzerne,Künstliche Intelligenz: Bundeskartellamt warnt vor Machtgewinn von Digitalkonzernen | ZEIT ONLINE,"Das Bundeskartellamt warnt vor einem Machtzuwachs der ohnehin bereits mächtigen Digitalkonzerne durch den Einsatz künstlicher Intelligenz (KI). ""Die großen Digitalkonzerne werden KI voraussichtlich nutzen, um ihre Marktmacht noch weiter auszudehnen"", sagte Behördenchef Andreas Mundt der Rheinischen Post. Genannt wurden Konzerne wie Google, Amazon, Apple, Microsoft und Facebook.

Diese Konzerne ""haben gigantische Datensätze mit Bezug zu Milliarden Menschen, sie haben riesige Finanzreserven, um Projekte voranzutreiben, sie haben die entsprechenden Serverkapazitäten"", sagte Mundt. ""Dies alles bedeutet, dass datengetriebene Netzwerkeffekte weiter zunehmen können.""

Mundt wies darauf hin, dass bereits mehr als die Hälfte des deutschen E-Commerce über Amazon abgewickelt werde. Die Macht des Unternehmens sei ""gigantisch in seiner Doppelfunktion als eigenständiger Händler und gleichzeitig Betreiber des Amazon-Marketplace, auf den ja viele Händler angewiesen sind"". Das Kartellamt überprüfe daher derzeit, ""ob Amazon die Preise von Händlern auf dem Marketplace überwacht und beeinflusst"".

Der Kartellamtschef sagte, es sei allen Unternehmen verboten, Preise mithilfe von KI zu koordinieren. Vielmehr müssten stets die Unternehmen selbst für das Handeln ihrer Computersysteme einstehen. Diese ""bleiben für ihre Algorithmen verantwortlich, auch wenn diese sich zu KI weiterentwickeln"".

Mundt äußerte sich skeptisch dazu, ob der Facebook-Konzern (Meta) der Vorgabe eines höheren Daten- und Kundenschutzes nachkommt, indem er neuerdings für 9,99 Euro Monatsgebühr verspricht, dass zahlende Kunden keine Werbung mehr eingeblendet bekommen. Mundt sagte: ""Ich gehe davon aus, dass sich die EU-Kommission dieses neue Modell genau anschauen wird. Das Ergebnis kann meines Erachtens jedenfalls nicht dazu führen, dass man Geld dafür bezahlen muss, dass der Datenschutz beachtet wird.""

Das Kartellamt hatte Facebook vor Jahren auferlegt, die Daten von Facebook, Whatsapp und anderen Diensten nicht mehr einfach zusammenzuführen. Diese Linie bestätigte der Europäische Gerichtshof. ""Es ist spätestens seitdem klar, dass die Macht über Daten eine Bedeutung für die wettbewerbliche Position eines Unternehmens hat"", sagte Mundt. Derzeit werde mit Facebook über die Art und Weise der Umsetzung des Beschlusses verhandelt.
"
KI,Zeit,2023-12-30,https://www.zeit.de/wirtschaft/2023-12/kuenstliche-intelligenz-bundeskartellamt-digitalkonzerne,Künstliche Intelligenz: Bundeskartellamt warnt vor Machtgewinn von Digitalkonzernen | ZEIT ONLINE,"Das Bundeskartellamt warnt vor einem Machtzuwachs der ohnehin bereits mächtigen Digitalkonzerne durch den Einsatz künstlicher Intelligenz (KI). ""Die großen Digitalkonzerne werden KI voraussichtlich nutzen, um ihre Marktmacht noch weiter auszudehnen"", sagte Behördenchef Andreas Mundt der Rheinischen Post. Genannt wurden Konzerne wie Google, Amazon, Apple, Microsoft und Facebook.

Diese Konzerne ""haben gigantische Datensätze mit Bezug zu Milliarden Menschen, sie haben riesige Finanzreserven, um Projekte voranzutreiben, sie haben die entsprechenden Serverkapazitäten"", sagte Mundt. ""Dies alles bedeutet, dass datengetriebene Netzwerkeffekte weiter zunehmen können.""

Mundt wies darauf hin, dass bereits mehr als die Hälfte des deutschen E-Commerce über Amazon abgewickelt werde. Die Macht des Unternehmens sei ""gigantisch in seiner Doppelfunktion als eigenständiger Händler und gleichzeitig Betreiber des Amazon-Marketplace, auf den ja viele Händler angewiesen sind"". Das Kartellamt überprüfe daher derzeit, ""ob Amazon die Preise von Händlern auf dem Marketplace überwacht und beeinflusst"".

Der Kartellamtschef sagte, es sei allen Unternehmen verboten, Preise mithilfe von KI zu koordinieren. Vielmehr müssten stets die Unternehmen selbst für das Handeln ihrer Computersysteme einstehen. Diese ""bleiben für ihre Algorithmen verantwortlich, auch wenn diese sich zu KI weiterentwickeln"".

Mundt äußerte sich skeptisch dazu, ob der Facebook-Konzern (Meta) der Vorgabe eines höheren Daten- und Kundenschutzes nachkommt, indem er neuerdings für 9,99 Euro Monatsgebühr verspricht, dass zahlende Kunden keine Werbung mehr eingeblendet bekommen. Mundt sagte: ""Ich gehe davon aus, dass sich die EU-Kommission dieses neue Modell genau anschauen wird. Das Ergebnis kann meines Erachtens jedenfalls nicht dazu führen, dass man Geld dafür bezahlen muss, dass der Datenschutz beachtet wird.""

Das Kartellamt hatte Facebook vor Jahren auferlegt, die Daten von Facebook, Whatsapp und anderen Diensten nicht mehr einfach zusammenzuführen. Diese Linie bestätigte der Europäische Gerichtshof. ""Es ist spätestens seitdem klar, dass die Macht über Daten eine Bedeutung für die wettbewerbliche Position eines Unternehmens hat"", sagte Mundt. Derzeit werde mit Facebook über die Art und Weise der Umsetzung des Beschlusses verhandelt.
"
KI,Zeit,2023-12-30,https://www.zeit.de/2023/54/ki-filmindustrie-drehbuch-darsteller-oekonomie,KI in der Filmindustrie: Die menschenleere Traumfabrik | ZEIT ONLINE,
Künstliche Intelligenz,Zeit,2023-12-27,https://www.zeit.de/digital/2023-12/newyorktimes-openai-chatgpt-klage-urheberrecht,"Künstliche Intelligenz: ""New York Times"" reicht Klage gegen OpenAI und Microsoft ein | ZEIT ONLINE","Die US-Zeitung New York Times hat das Software-Unternehmen OpenAI und Microsoft wegen der Verletzung ihrer Urheberrechte verklagt. Die Firmen hätten die Daten von mehreren Millionen Artikeln der New York Times genutzt, um ihren Chatbot ChatGPT damit zu füttern und ein Geschäft aufzubauen, heißt es in der Anklage. Das Blatt ist die erste große amerikanische Zeitung, die OpenAI verklagt. 

In einem entsprechenden Artikel der New York Times heißt es weiter, die Anklage enthalte ""keine genaue Geldforderung"". Die Beklagten müssten jedoch für den faktisch und rechtlich entstandenen Schaden von geschätzt mehreren Milliarden Dollars ""verantwortlich gemacht werden"". Man fordere die Firmen außerdem dazu auf, die Nutzung der Inhalte einzustellen und die bereits gesammelten Daten zu vernichten.

Neben der Verletzung ihrer Urheberrechte sieht die Zeitung KI-Programme wie ChatGPT als potenziellen Konkurrenten in der Nachrichtenbranche. Wenn Nutzer Chatbots nach aktuellen Nachrichten fragen würden, könnten diese Antworten generieren, die auf vergangenen Artikeln der New York Times beruhen. Man sei besorgt, dass ChatGPT dadurch ""die Leserschaft abwerben"" und somit der Datenverkehr und Werbeeinnahmen auf der Website einbrechen könnten.

Mit der Klage geht die New York Times klar gegen die Verwendung ihrer journalistischen Arbeit durch Chatbots vor. Es ist jedoch nicht die erste Klage solcher Art: Aufstrebende KI-Firmen sehen sich derzeit mit einer Welle von Klagen konfrontiert, weil sie ohne Zustimmung Internetinhalte für den Aufbau ihrer generativen KI-Systeme nutzen. Unter anderem klagten bereits mehrere Autoren wie Jonathan Franzen, John Grisham oder George R. R. Martin gegen OpenAI. 

Andere große Medienkonzerne wie der deutsche Axel-Springer-Verlag hielten sich mit ähnlichen Klagen aber bisher zurück oder handeln Vereinbarungen mit den KI-Firmen aus. Es gilt nicht als ausgeschlossen, dass eine erfolgreiche Klage der New York Times nun weitere Klagen aus der Medienbranche nach sich ziehen könnte.
"
Künstliche Intelligenz,Zeit,2023-12-27,https://www.zeit.de/kultur/2023-11/kuenstliche-intelligenz-musik-wilsdorf-interview,"Künstliche Intelligenz in der Musik: ""Die Gefahr der KI kommt über Bande"" | ZEIT ONLINE","Ein neuer Beatles-Song nach 45 Jahren, ein Fake-Drake, der über den Beat rappt: Künstliche Intelligenz ist längst auch in der Musik angekommen, das hat allerspätestens das Jahr 2023 gezeigt. Jovanka von Wilsdorf, Artist Coach, Creative Consultant und Songwriterin, hat schon vor drei Jahren den Diana AI Songcontest mitgegründet und erzählt im Interview, wie das aussieht, wenn Musikerinnen und Musiker mit KI arbeiten – und ob sie sich dabei selbst abschaffen.

ZEIT Online:
Frau von Wilsdorf, ist Künstliche Intelligenz eine Chance oder eine Gefahr für
Musiker? 

Jovanka von
Wilsdorf: Beides.  

 ZEIT Online:
Dann erst mal zu den Chancen. Wie sieht das aus, wenn Musiker KI verwenden?

von Wilsdorf: KI ist zunächst einmal ein Buzzword. Nicht überall, wo KI draufsteht, ist auch KI drin.
Dazu kommt, dass die Werkzeuge komplett unterschiedliche Funktionen haben. Es
gibt die reinen Assistenten, die mir zum Beispiel Vorschläge für die finale
Abstimmung der Elemente eines Songs machen. Dann gibt es
Press-and-Play-Generatoren: Bei denen bestimme ich zwei, drei Parameter und
bekomme einen vollständigen Track ausgespielt. Und schließlich gibt es Tools, mit
denen ich intensiv im Dialog arbeite, die mir als Co-Writer oder Muse dienen
können. 

ZEIT Online:
Und wie klingt Musik, die dabei herauskommt?

von Wilsdorf: So
unterschiedlich wie die Arten, auf die wir die Tools einsetzen. Vor kurzem
fand der Diana AI Songcontest statt, den ich seit 2020 mit Ralph Christoph, dem
Direktor der c/o Pop Convention, veranstalte. Zwölf Teilnehmerinnen arbeiten für
zwei Tage in Dreiergruppen, jedes Team schreibt und produziert einen Song
und kreiert ein Musikvideo, mithilfe von KI-Tools. Am Ende gewinnt der beste
Song. Ich sage den Teilnehmerinnen immer: Ihr müsst mit einer klaren Vision da
reingehen und euch dann überraschen lassen. Von der futuristischen Indieballade
bis zum Rockbanger hatten wir schon alles. Das zeigt, wie enorm hoch der
menschliche Anteil bei dieser Art von Zusammenspiel mit KI ist. 

ZEIT Online:
Es hört sich aber an, als würden Musiker mit den eingangs erwähnten
Press-und-Play-Generatoren einen guten Teil ihres Musizierens
wegautomatisieren. 

von Wilsdorf: Das
geht natürlich. Es gibt aber auch andere Tools, und auch die spucken
mal mittelmäßige, mal groteske und manchmal großartige Vorschläge aus. Ich kann
mir einen Fitzel rausnehmen, ein musikalisches Thema schnappen und mit neuen
Sounds belegen oder nur einen Beat generieren, den ich dann variiere und
weiterverwende. Die KI-Tools können uns einen Teil der Grundarbeit abnehmen, sodass mehr Platz für die kreative Arbeit bleibt. Das ist in etwa, als würde
ich Mehl kaufen, um damit zu backen. Ich war nicht auf dem Feld, ich habe nicht
geerntet, aber niemand würde sagen, ich hätte den Kuchen nicht selbst gebacken.
Auch mit KI bleibe ich die Musikerin. Und das heißt auch: Wenn ich mit KI-Tools
arbeite und das Ergebnis ist mittelmäßig, dann war ich das, denn ich habe
mittelmäßige Entscheidungen getroffen. 

ZEIT Online:
Aber besteht nicht das Risiko, dass Musiker zunehmend bequem werden und sich
zu immer größeren Teilen auf diese Systeme verlassen?

von Wilsdorf: Der
Mensch neigt dazu, Abkürzungen zu bevorzugen. Daran ist aber nicht die KI schuld. Das war schon
immer so. Je besser allerdings die KI-Tools werden, desto verführerischer werden
sie, gerade für Nicht- oder Amateurmusiker. Und es gibt Grenzen: Wenn ich Pizza
bestelle und mir aussuche, ob da Mozzarella oder Sardellen draufliegen, macht
mich das noch nicht zum Bäcker, um wieder eine Essensmetapher zu nutzen. Aber Musizieren
ist dem Menschen ein Urbedürfnis. Eine Mutter singt für ihr Baby, ein Kind
trommelt auf dem Tisch oder zupft am Eierschneider, weil da Töne rauskommen. Das
Musizieren werden sich die Menschen nicht nehmen lassen. Warum auch?
Gefährlicher wird es im Bereich der Gebrauchsmusik.
"
Künstliche Intelligenz,Zeit,2023-12-27,https://www.zeit.de/news/2023-12/27/new-york-times-verklagt-openai-und-microsoft-wegen-chatgpt,"Künstliche Intelligenz: ""New York Times"" verklagt OpenAI und Microsoft wegen ChatGPT | ZEIT ONLINE","Als erste große amerikanische Zeitung hat die «New York Times» die Software-Unternehmen OpenAI und Microsoft wegen ihres KI-Chatbots ChatGPT verklagt. Das Blatt wirft den Firmen vor, dass sie Wissen aus Millionen Artikeln benutzt haben, um ChatGPT zu füttern und damit auf Kosten der «New York Times» ein Geschäft aufbauen.

«Ziel dieser Klage ist es, jene für die gesetzlichen und tatsächlichen Schadenersatzforderungen in Milliardenhöhe haftbar zu machen, die sie der Times für das rechtswidrige Kopieren und Verwenden der einzigartig wertvollen Werke schulden», heißt es in der Klageschrift.

Die Abkürzung KI steht für Künstliche Intelligenz, gemeint sind damit Methoden, menschliche Denkvorgänge auf Computer zu übertragen. Ein Chatbot ist ein Text-Dialogsystem auf Basis eines Computerprogramms.

Mit seinem KI-Chatbot hatte die Softwareschmiede OpenAI, die maßgeblich von Microsoft unterstützt wird, vor etwas mehr als einem Jahr für Furore gesorgt. ChatGPT schürte den Hype um Künstliche Intelligenz mit Erwartungen an ein digitales Schlaraffenland für alle bis hin zur Angst vor einem Auslöschen der Menschheit. Entsprechend wurde OpenAI zum wichtigsten Start-up der Welt mit einem geschätzten Wert von 80 Milliarden Dollar - zu einer Firma, die die Welt verändern könnte und Tech-Schwergewichte wie Google und den Facebook-Konzern Meta in Zugzwang brachte.

Nutzer können mit ChatGPT ganz einfach frei kommunizieren und zum Beispiel Aufgaben verteilen oder Wissen abfragen - sie bekommen dann Antworten, die sich von menschlichen oft kaum mehr unterscheiden. Dafür hat OpenAI ChatGPT fast mit dem gesamten Wissen des Internets gefüttert. Von Foreneinträgen, Firmenwebsites, Drehbüchern bis hin zu journalistischen Artikeln. Die «New York Times» hofft deswegen nun auf Schadenersatz. Es ist nicht ausgeschlossen, dass eine erfolgreiche Klage viele Nachahmer in der Medienbranche finden könnte.

© dpa-infocom, dpa:231227-99-420552/2
"
AI,Zeit,2023-12-27,https://www.zeit.de/digital/2023-12/newyorktimes-openai-chatgpt-klage-urheberrecht,"Künstliche Intelligenz: ""New York Times"" reicht Klage gegen OpenAI und Microsoft ein | ZEIT ONLINE","Die US-Zeitung New York Times hat das Software-Unternehmen OpenAI und Microsoft wegen der Verletzung ihrer Urheberrechte verklagt. Die Firmen hätten die Daten von mehreren Millionen Artikeln der New York Times genutzt, um ihren Chatbot ChatGPT damit zu füttern und ein Geschäft aufzubauen, heißt es in der Anklage. Das Blatt ist die erste große amerikanische Zeitung, die OpenAI verklagt. 

In einem entsprechenden Artikel der New York Times heißt es weiter, die Anklage enthalte ""keine genaue Geldforderung"". Die Beklagten müssten jedoch für den faktisch und rechtlich entstandenen Schaden von geschätzt mehreren Milliarden Dollars ""verantwortlich gemacht werden"". Man fordere die Firmen außerdem dazu auf, die Nutzung der Inhalte einzustellen und die bereits gesammelten Daten zu vernichten.

Neben der Verletzung ihrer Urheberrechte sieht die Zeitung KI-Programme wie ChatGPT als potenziellen Konkurrenten in der Nachrichtenbranche. Wenn Nutzer Chatbots nach aktuellen Nachrichten fragen würden, könnten diese Antworten generieren, die auf vergangenen Artikeln der New York Times beruhen. Man sei besorgt, dass ChatGPT dadurch ""die Leserschaft abwerben"" und somit der Datenverkehr und Werbeeinnahmen auf der Website einbrechen könnten.

Mit der Klage geht die New York Times klar gegen die Verwendung ihrer journalistischen Arbeit durch Chatbots vor. Es ist jedoch nicht die erste Klage solcher Art: Aufstrebende KI-Firmen sehen sich derzeit mit einer Welle von Klagen konfrontiert, weil sie ohne Zustimmung Internetinhalte für den Aufbau ihrer generativen KI-Systeme nutzen. Unter anderem klagten bereits mehrere Autoren wie Jonathan Franzen, John Grisham oder George R. R. Martin gegen OpenAI. 

Andere große Medienkonzerne wie der deutsche Axel-Springer-Verlag hielten sich mit ähnlichen Klagen aber bisher zurück oder handeln Vereinbarungen mit den KI-Firmen aus. Es gilt nicht als ausgeschlossen, dass eine erfolgreiche Klage der New York Times nun weitere Klagen aus der Medienbranche nach sich ziehen könnte.
"
AI,Zeit,2023-12-27,https://www.zeit.de/kultur/2023-11/kuenstliche-intelligenz-musik-wilsdorf-interview,"Künstliche Intelligenz in der Musik: ""Die Gefahr der KI kommt über Bande"" | ZEIT ONLINE","Ein neuer Beatles-Song nach 45 Jahren, ein Fake-Drake, der über den Beat rappt: Künstliche Intelligenz ist längst auch in der Musik angekommen, das hat allerspätestens das Jahr 2023 gezeigt. Jovanka von Wilsdorf, Artist Coach, Creative Consultant und Songwriterin, hat schon vor drei Jahren den Diana AI Songcontest mitgegründet und erzählt im Interview, wie das aussieht, wenn Musikerinnen und Musiker mit KI arbeiten – und ob sie sich dabei selbst abschaffen.

ZEIT Online:
Frau von Wilsdorf, ist Künstliche Intelligenz eine Chance oder eine Gefahr für
Musiker? 

Jovanka von
Wilsdorf: Beides.  

 ZEIT Online:
Dann erst mal zu den Chancen. Wie sieht das aus, wenn Musiker KI verwenden?

von Wilsdorf: KI ist zunächst einmal ein Buzzword. Nicht überall, wo KI draufsteht, ist auch KI drin.
Dazu kommt, dass die Werkzeuge komplett unterschiedliche Funktionen haben. Es
gibt die reinen Assistenten, die mir zum Beispiel Vorschläge für die finale
Abstimmung der Elemente eines Songs machen. Dann gibt es
Press-and-Play-Generatoren: Bei denen bestimme ich zwei, drei Parameter und
bekomme einen vollständigen Track ausgespielt. Und schließlich gibt es Tools, mit
denen ich intensiv im Dialog arbeite, die mir als Co-Writer oder Muse dienen
können. 

ZEIT Online:
Und wie klingt Musik, die dabei herauskommt?

von Wilsdorf: So
unterschiedlich wie die Arten, auf die wir die Tools einsetzen. Vor kurzem
fand der Diana AI Songcontest statt, den ich seit 2020 mit Ralph Christoph, dem
Direktor der c/o Pop Convention, veranstalte. Zwölf Teilnehmerinnen arbeiten für
zwei Tage in Dreiergruppen, jedes Team schreibt und produziert einen Song
und kreiert ein Musikvideo, mithilfe von KI-Tools. Am Ende gewinnt der beste
Song. Ich sage den Teilnehmerinnen immer: Ihr müsst mit einer klaren Vision da
reingehen und euch dann überraschen lassen. Von der futuristischen Indieballade
bis zum Rockbanger hatten wir schon alles. Das zeigt, wie enorm hoch der
menschliche Anteil bei dieser Art von Zusammenspiel mit KI ist. 

ZEIT Online:
Es hört sich aber an, als würden Musiker mit den eingangs erwähnten
Press-und-Play-Generatoren einen guten Teil ihres Musizierens
wegautomatisieren. 

von Wilsdorf: Das
geht natürlich. Es gibt aber auch andere Tools, und auch die spucken
mal mittelmäßige, mal groteske und manchmal großartige Vorschläge aus. Ich kann
mir einen Fitzel rausnehmen, ein musikalisches Thema schnappen und mit neuen
Sounds belegen oder nur einen Beat generieren, den ich dann variiere und
weiterverwende. Die KI-Tools können uns einen Teil der Grundarbeit abnehmen, sodass mehr Platz für die kreative Arbeit bleibt. Das ist in etwa, als würde
ich Mehl kaufen, um damit zu backen. Ich war nicht auf dem Feld, ich habe nicht
geerntet, aber niemand würde sagen, ich hätte den Kuchen nicht selbst gebacken.
Auch mit KI bleibe ich die Musikerin. Und das heißt auch: Wenn ich mit KI-Tools
arbeite und das Ergebnis ist mittelmäßig, dann war ich das, denn ich habe
mittelmäßige Entscheidungen getroffen. 

ZEIT Online:
Aber besteht nicht das Risiko, dass Musiker zunehmend bequem werden und sich
zu immer größeren Teilen auf diese Systeme verlassen?

von Wilsdorf: Der
Mensch neigt dazu, Abkürzungen zu bevorzugen. Daran ist aber nicht die KI schuld. Das war schon
immer so. Je besser allerdings die KI-Tools werden, desto verführerischer werden
sie, gerade für Nicht- oder Amateurmusiker. Und es gibt Grenzen: Wenn ich Pizza
bestelle und mir aussuche, ob da Mozzarella oder Sardellen draufliegen, macht
mich das noch nicht zum Bäcker, um wieder eine Essensmetapher zu nutzen. Aber Musizieren
ist dem Menschen ein Urbedürfnis. Eine Mutter singt für ihr Baby, ein Kind
trommelt auf dem Tisch oder zupft am Eierschneider, weil da Töne rauskommen. Das
Musizieren werden sich die Menschen nicht nehmen lassen. Warum auch?
Gefährlicher wird es im Bereich der Gebrauchsmusik.
"
AI,Zeit,2023-12-27,https://www.zeit.de/digital/2023-12/chaos-communication-congress-tesla-hacking-autopilot-sicherheit,Chaos Communication Congress: Die Fahrt des Vorbesitzers | ZEIT ONLINE,"Ein Screen
zeigt Videoclips von sieben Tesla-Kameras. Im Detail und aus allen Perspektiven
kann man sehen, wie der Fahrer an Ampeln hält und wieder anfährt, was um
sein Auto herum passiert. Unter anderem sind andere Fahrzeuge zu sehen,
Fahrbahnmarkierungen und Gebäude. Nach Rekonstruktionen von ZEIT ONLINE handelt
es sich um eine Straße in San José südlich von San Francisco. Das Auto fährt
vorbei an einer Tankstelle, einer Werkstatt, einem Imbiss. 

Diese
Kamerabilder sind Aufzeichnungen, die drei Forscher der TU Berlin von einer
gebrauchten Tesla-Platine rekonstruiert haben – Videomaterial, das eigentlich
schon gelöscht, aber noch nicht überschrieben wurde. Wie es ihnen gelungen
ist, diese durchaus teils privaten Einblicke in das Vorleben der eigentlich
streng gesicherten Platine zu gewinnen, die sie zu Forschungszwecken erlangt
und ausgewertet hatten, demonstrierten sie am Mittwoch auf dem CCC-Kongress in
Hamburg. 

Woher genau
die Platine aus dem Auto des Vorbesitzers stammt, ist unklar. Man habe sie von
einem befreundeten Hardware-Hacker aus den USA bekommen, so die Forscher.
Solche Teile würden häufig auf Ebay verkauft. Hatte der Besitzer des Teslas
womöglich einen Unfall, sodass das Autowrack ausgeschlachtet wurde, um die
noch verwendbaren Teile secondhand zu verkaufen? Das wird man wohl nie
herausfinden können.

Doch in dem
Vorgang steckt noch etwas Spektakuläreres als die Einblicke in das Vorleben der
Platine und die Routine des Vorbesitzers, erklärt Christian Werling im
Interview mit ZEIT ONLINE: Die Auswertung und Analyse der Platine erlaubt
Einblicke in die Mechanismen und die Architektur von Teslas Autopilot. 

Werling, der
Doktorand an der TU Berlin ist, hat gemeinsam mit seinen Kollegen Niclas
Kühnapfel und Hans Niklas Jacob eine Sicherheitslücke entdeckt, die wertvolle
Geschäftsgeheimnisse von Tesla freisetzt: Einblicke in die Architektur des Systems
hinter dem automatisierten Fahren von Tesla. Schließlich gilt ein System, das
unter anderem mithilfe künstlicher Intelligenz Millionen Kamerabilder und
Sensordaten auswertet und entscheidet, wie ein Fahrzeug im Verkehr reagieren
soll, als eine der wichtigsten Zutaten für den Erfolg des Autokonzerns. Die
rekonstruierten Daten zeigen, wie Tesla die Vielzahl aufgezeichneter Fahrdaten
aufbereitet und auswertet.

Das alles
durch eine Auswertung einer mutmaßlich aus einem Autowrack ausgebauten Platine?
""Wir waren schon auch ein bisschen überrascht, dass Tesla sich dagegen noch
nicht schützt"", sagt Werling gegenüber ZEIT ONLINE. Schließlich sei deren
Softwaresicherheit ""schon sehr gut"".

Vielleicht
hat Tesla aber auch einfach nicht mit der Ausdauer der Berliner Hardware-Hacker
gerechnet. Denn der Begriff
""ausgewertet"" ist freilich untertrieben – schließlich haben die drei Forscher
die Ergebnisse der Arbeit der drei vergangenen Monate am Mittwoch auf dem
Kongress des Chaos Computer Club präsentiert. Man braucht zweifelsohne eine
gehörige Ausdauer und zudem Erfahrung im sogenannten Hardware-Hacking, um über
eine solche Platine an entsprechende Daten zu kommen. Dabei manipulierten die
Forscher mit verschiedenen Methoden die Hardware – in diesem Fall die Platine –
eines Systems, bis sie es schaffen, in sie eindringen zu können und sich
schließlich alle Rechte zu verschaffen, unter anderem, um auf gespeicherte und
sogar gelöschte Daten zugreifen zu können. Sie erlangten Root-Zugriff, wie es
in der Fachsprache heißt. 

Besonders
spannend sei es gewesen zu sehen, wie Tesla mit der Fülle an Daten umgeht,
berichtet Werling im Gespräch. Vor dieser Herausforderung dürften alle Anbieter
entsprechender Systeme für automatisiertes und autonomes Fahren stehen: Zwar
können unzählige Sensoren und Kameras viele Terabyte an Daten aufzeichnen, aber
die Frage ist, wie mit diesen so verfahren wird, dass mit einem realistischen
Aufwand Entscheidungen in Echtzeit getroffen werden können. Denn sie können nicht
alle zur Auswertung auf Teslas Server geladen werden – die Funktion eines
autonom fahrenden Autos kann schließlich nicht von der Netzabdeckung auf der
Straße abhängen.

Bedeutet:
Tesla selektiert – nach einem aufwendig selbst entwickelten Mechanismus. ""Der
Mechanismus ist dafür zuständig, Entscheidungen in Echtzeit abzuleiten"",
erklärt Werling – und das sei alles andere als einfach. ""Tesla steckt da viel
Entwicklungskosten rein."" Und genau diese Kosten könnten sich Wettbewerber
sparen, wenn sie den gleichen Weg gehen wie die drei Berliner Forscher.
Freilich schlachten Werling und seine Kollegen die Platine aber nicht aus, um
ihre Erkenntnisse nach China zu verkaufen. ""Unsere Motivation ist, im Detail zu
schauen, wie die Schwachstelle ausgenutzt und vor allem verhindert werden
kann."" Eigenen Angaben zufolge hatten die Berliner Sicherheitsforscher Tesla
über ihre Ergebnisse im Vorfeld des Vortrags informiert. 
"
Artificial Intelligence,Zeit,2023-12-27,https://www.zeit.de/digital/2023-12/newyorktimes-openai-chatgpt-klage-urheberrecht,"Künstliche Intelligenz: ""New York Times"" reicht Klage gegen OpenAI und Microsoft ein | ZEIT ONLINE","Die US-Zeitung New York Times hat das Software-Unternehmen OpenAI und Microsoft wegen der Verletzung ihrer Urheberrechte verklagt. Die Firmen hätten die Daten von mehreren Millionen Artikeln der New York Times genutzt, um ihren Chatbot ChatGPT damit zu füttern und ein Geschäft aufzubauen, heißt es in der Anklage. Das Blatt ist die erste große amerikanische Zeitung, die OpenAI verklagt. 

In einem entsprechenden Artikel der New York Times heißt es weiter, die Anklage enthalte ""keine genaue Geldforderung"". Die Beklagten müssten jedoch für den faktisch und rechtlich entstandenen Schaden von geschätzt mehreren Milliarden Dollars ""verantwortlich gemacht werden"". Man fordere die Firmen außerdem dazu auf, die Nutzung der Inhalte einzustellen und die bereits gesammelten Daten zu vernichten.

Neben der Verletzung ihrer Urheberrechte sieht die Zeitung KI-Programme wie ChatGPT als potenziellen Konkurrenten in der Nachrichtenbranche. Wenn Nutzer Chatbots nach aktuellen Nachrichten fragen würden, könnten diese Antworten generieren, die auf vergangenen Artikeln der New York Times beruhen. Man sei besorgt, dass ChatGPT dadurch ""die Leserschaft abwerben"" und somit der Datenverkehr und Werbeeinnahmen auf der Website einbrechen könnten.

Mit der Klage geht die New York Times klar gegen die Verwendung ihrer journalistischen Arbeit durch Chatbots vor. Es ist jedoch nicht die erste Klage solcher Art: Aufstrebende KI-Firmen sehen sich derzeit mit einer Welle von Klagen konfrontiert, weil sie ohne Zustimmung Internetinhalte für den Aufbau ihrer generativen KI-Systeme nutzen. Unter anderem klagten bereits mehrere Autoren wie Jonathan Franzen, John Grisham oder George R. R. Martin gegen OpenAI. 

Andere große Medienkonzerne wie der deutsche Axel-Springer-Verlag hielten sich mit ähnlichen Klagen aber bisher zurück oder handeln Vereinbarungen mit den KI-Firmen aus. Es gilt nicht als ausgeschlossen, dass eine erfolgreiche Klage der New York Times nun weitere Klagen aus der Medienbranche nach sich ziehen könnte.
"
KI,Zeit,2023-12-27,https://www.zeit.de/digital/2023-12/newyorktimes-openai-chatgpt-klage-urheberrecht,"Künstliche Intelligenz: ""New York Times"" reicht Klage gegen OpenAI und Microsoft ein | ZEIT ONLINE","Die US-Zeitung New York Times hat das Software-Unternehmen OpenAI und Microsoft wegen der Verletzung ihrer Urheberrechte verklagt. Die Firmen hätten die Daten von mehreren Millionen Artikeln der New York Times genutzt, um ihren Chatbot ChatGPT damit zu füttern und ein Geschäft aufzubauen, heißt es in der Anklage. Das Blatt ist die erste große amerikanische Zeitung, die OpenAI verklagt. 

In einem entsprechenden Artikel der New York Times heißt es weiter, die Anklage enthalte ""keine genaue Geldforderung"". Die Beklagten müssten jedoch für den faktisch und rechtlich entstandenen Schaden von geschätzt mehreren Milliarden Dollars ""verantwortlich gemacht werden"". Man fordere die Firmen außerdem dazu auf, die Nutzung der Inhalte einzustellen und die bereits gesammelten Daten zu vernichten.

Neben der Verletzung ihrer Urheberrechte sieht die Zeitung KI-Programme wie ChatGPT als potenziellen Konkurrenten in der Nachrichtenbranche. Wenn Nutzer Chatbots nach aktuellen Nachrichten fragen würden, könnten diese Antworten generieren, die auf vergangenen Artikeln der New York Times beruhen. Man sei besorgt, dass ChatGPT dadurch ""die Leserschaft abwerben"" und somit der Datenverkehr und Werbeeinnahmen auf der Website einbrechen könnten.

Mit der Klage geht die New York Times klar gegen die Verwendung ihrer journalistischen Arbeit durch Chatbots vor. Es ist jedoch nicht die erste Klage solcher Art: Aufstrebende KI-Firmen sehen sich derzeit mit einer Welle von Klagen konfrontiert, weil sie ohne Zustimmung Internetinhalte für den Aufbau ihrer generativen KI-Systeme nutzen. Unter anderem klagten bereits mehrere Autoren wie Jonathan Franzen, John Grisham oder George R. R. Martin gegen OpenAI. 

Andere große Medienkonzerne wie der deutsche Axel-Springer-Verlag hielten sich mit ähnlichen Klagen aber bisher zurück oder handeln Vereinbarungen mit den KI-Firmen aus. Es gilt nicht als ausgeschlossen, dass eine erfolgreiche Klage der New York Times nun weitere Klagen aus der Medienbranche nach sich ziehen könnte.
"
KI,Zeit,2023-12-27,https://www.zeit.de/kultur/2023-11/kuenstliche-intelligenz-musik-wilsdorf-interview,"Künstliche Intelligenz in der Musik: ""Die Gefahr der KI kommt über Bande"" | ZEIT ONLINE","Ein neuer Beatles-Song nach 45 Jahren, ein Fake-Drake, der über den Beat rappt: Künstliche Intelligenz ist längst auch in der Musik angekommen, das hat allerspätestens das Jahr 2023 gezeigt. Jovanka von Wilsdorf, Artist Coach, Creative Consultant und Songwriterin, hat schon vor drei Jahren den Diana AI Songcontest mitgegründet und erzählt im Interview, wie das aussieht, wenn Musikerinnen und Musiker mit KI arbeiten – und ob sie sich dabei selbst abschaffen.

ZEIT Online:
Frau von Wilsdorf, ist Künstliche Intelligenz eine Chance oder eine Gefahr für
Musiker? 

Jovanka von
Wilsdorf: Beides.  

 ZEIT Online:
Dann erst mal zu den Chancen. Wie sieht das aus, wenn Musiker KI verwenden?

von Wilsdorf: KI ist zunächst einmal ein Buzzword. Nicht überall, wo KI draufsteht, ist auch KI drin.
Dazu kommt, dass die Werkzeuge komplett unterschiedliche Funktionen haben. Es
gibt die reinen Assistenten, die mir zum Beispiel Vorschläge für die finale
Abstimmung der Elemente eines Songs machen. Dann gibt es
Press-and-Play-Generatoren: Bei denen bestimme ich zwei, drei Parameter und
bekomme einen vollständigen Track ausgespielt. Und schließlich gibt es Tools, mit
denen ich intensiv im Dialog arbeite, die mir als Co-Writer oder Muse dienen
können. 

ZEIT Online:
Und wie klingt Musik, die dabei herauskommt?

von Wilsdorf: So
unterschiedlich wie die Arten, auf die wir die Tools einsetzen. Vor kurzem
fand der Diana AI Songcontest statt, den ich seit 2020 mit Ralph Christoph, dem
Direktor der c/o Pop Convention, veranstalte. Zwölf Teilnehmerinnen arbeiten für
zwei Tage in Dreiergruppen, jedes Team schreibt und produziert einen Song
und kreiert ein Musikvideo, mithilfe von KI-Tools. Am Ende gewinnt der beste
Song. Ich sage den Teilnehmerinnen immer: Ihr müsst mit einer klaren Vision da
reingehen und euch dann überraschen lassen. Von der futuristischen Indieballade
bis zum Rockbanger hatten wir schon alles. Das zeigt, wie enorm hoch der
menschliche Anteil bei dieser Art von Zusammenspiel mit KI ist. 

ZEIT Online:
Es hört sich aber an, als würden Musiker mit den eingangs erwähnten
Press-und-Play-Generatoren einen guten Teil ihres Musizierens
wegautomatisieren. 

von Wilsdorf: Das
geht natürlich. Es gibt aber auch andere Tools, und auch die spucken
mal mittelmäßige, mal groteske und manchmal großartige Vorschläge aus. Ich kann
mir einen Fitzel rausnehmen, ein musikalisches Thema schnappen und mit neuen
Sounds belegen oder nur einen Beat generieren, den ich dann variiere und
weiterverwende. Die KI-Tools können uns einen Teil der Grundarbeit abnehmen, sodass mehr Platz für die kreative Arbeit bleibt. Das ist in etwa, als würde
ich Mehl kaufen, um damit zu backen. Ich war nicht auf dem Feld, ich habe nicht
geerntet, aber niemand würde sagen, ich hätte den Kuchen nicht selbst gebacken.
Auch mit KI bleibe ich die Musikerin. Und das heißt auch: Wenn ich mit KI-Tools
arbeite und das Ergebnis ist mittelmäßig, dann war ich das, denn ich habe
mittelmäßige Entscheidungen getroffen. 

ZEIT Online:
Aber besteht nicht das Risiko, dass Musiker zunehmend bequem werden und sich
zu immer größeren Teilen auf diese Systeme verlassen?

von Wilsdorf: Der
Mensch neigt dazu, Abkürzungen zu bevorzugen. Daran ist aber nicht die KI schuld. Das war schon
immer so. Je besser allerdings die KI-Tools werden, desto verführerischer werden
sie, gerade für Nicht- oder Amateurmusiker. Und es gibt Grenzen: Wenn ich Pizza
bestelle und mir aussuche, ob da Mozzarella oder Sardellen draufliegen, macht
mich das noch nicht zum Bäcker, um wieder eine Essensmetapher zu nutzen. Aber Musizieren
ist dem Menschen ein Urbedürfnis. Eine Mutter singt für ihr Baby, ein Kind
trommelt auf dem Tisch oder zupft am Eierschneider, weil da Töne rauskommen. Das
Musizieren werden sich die Menschen nicht nehmen lassen. Warum auch?
Gefährlicher wird es im Bereich der Gebrauchsmusik.
"
KI,Zeit,2023-12-27,https://www.zeit.de/news/2023-12/27/new-york-times-verklagt-openai-und-microsoft-wegen-chatgpt,"Künstliche Intelligenz: ""New York Times"" verklagt OpenAI und Microsoft wegen ChatGPT | ZEIT ONLINE","Als erste große amerikanische Zeitung hat die «New York Times» die Software-Unternehmen OpenAI und Microsoft wegen ihres KI-Chatbots ChatGPT verklagt. Das Blatt wirft den Firmen vor, dass sie Wissen aus Millionen Artikeln benutzt haben, um ChatGPT zu füttern und damit auf Kosten der «New York Times» ein Geschäft aufbauen.

«Ziel dieser Klage ist es, jene für die gesetzlichen und tatsächlichen Schadenersatzforderungen in Milliardenhöhe haftbar zu machen, die sie der Times für das rechtswidrige Kopieren und Verwenden der einzigartig wertvollen Werke schulden», heißt es in der Klageschrift.

Die Abkürzung KI steht für Künstliche Intelligenz, gemeint sind damit Methoden, menschliche Denkvorgänge auf Computer zu übertragen. Ein Chatbot ist ein Text-Dialogsystem auf Basis eines Computerprogramms.

Mit seinem KI-Chatbot hatte die Softwareschmiede OpenAI, die maßgeblich von Microsoft unterstützt wird, vor etwas mehr als einem Jahr für Furore gesorgt. ChatGPT schürte den Hype um Künstliche Intelligenz mit Erwartungen an ein digitales Schlaraffenland für alle bis hin zur Angst vor einem Auslöschen der Menschheit. Entsprechend wurde OpenAI zum wichtigsten Start-up der Welt mit einem geschätzten Wert von 80 Milliarden Dollar - zu einer Firma, die die Welt verändern könnte und Tech-Schwergewichte wie Google und den Facebook-Konzern Meta in Zugzwang brachte.

Nutzer können mit ChatGPT ganz einfach frei kommunizieren und zum Beispiel Aufgaben verteilen oder Wissen abfragen - sie bekommen dann Antworten, die sich von menschlichen oft kaum mehr unterscheiden. Dafür hat OpenAI ChatGPT fast mit dem gesamten Wissen des Internets gefüttert. Von Foreneinträgen, Firmenwebsites, Drehbüchern bis hin zu journalistischen Artikeln. Die «New York Times» hofft deswegen nun auf Schadenersatz. Es ist nicht ausgeschlossen, dass eine erfolgreiche Klage viele Nachahmer in der Medienbranche finden könnte.

© dpa-infocom, dpa:231227-99-420552/2
"
KI,Zeit,2023-12-25,https://www.zeit.de/news/2023-12/25/influencerin-ki-kann-gebaerdensprache-noch-nicht-erkennen,Musik: Influencerin: KI kann Gebärdensprache noch nicht erkennen | ZEIT ONLINE,"Künstliche Intelligenz (KI) kann nach Angaben der tauben Musikperformerin Cindy Klink Gebärdensprache noch nicht gut erkennen. «Das ist in Arbeit, aber sehr schwierig», sagte die 26 Jahre alte Influencerin im Interview der Deutschen Presse-Agentur. 

In den Gebärdensprachen seien nicht nur die Hände wichtig, sondern auch die Mimik und das Mundbild. «Wenn es nur nach den Händen gehen würde, gäbe es schon längst eine KI, die das machen könnte», so ihre Einschätzung. «Aber dadurch, dass das Gesicht so extrem wichtig ist, ist das noch eine Baustelle.» Derzeit läuft in Deutschland etwa das Forschungsprojekt BIGEKO, das einmal Gebärdensprache mit Hilfe von KI übersetzen soll.

Cindy Klink aus der Nähe von Koblenz performt Lieder in Gebärdensprache, um die Musik auch gehörlosen und schwerhörigen Menschen zugänglich zu machen. Die Jurastudentin veröffentlicht Videos ihrer Performances auf Instagram und Tiktok, wo ihr Hunderttausende folgen. Sie tritt auch live mit Stars wie Wincent Weiss und den Fantastischen Vier auf der Bühne auf. Zuletzt nahm sie ein Musikvideo mit dem Social-Media-Star Levent Geiger auf. Dabei nutzt sie mittlerweile auch eine Hörprothese.

Die 26-Jährige erklärt, dass sie ihre Performance an die Gefühle anpasse, die das Lied transportiere. «Ist es ein Liebeslied, sieht man das, dann wirke ich freundlicher. Ist es ein ironischer Song, sieht man das auch an der Mimik, dass ich da versteckt denke: Das ist purer Quatsch.» Ähnliches gelte für aggressive Songs. «Nicht nur die Mimik ist anders, auch die Gebärden sind je nachdem stärker und wilder oder sanfter.»

© dpa-infocom, dpa:231225-99-403537/2
"
KI,Zeit,2023-12-25,https://www.zeit.de/news/2023-12/25/ministerin-hubig-ki-kann-lehrer-entlasten,Digitalisierung: Ministerin Hubig: KI kann Lehrer entlasten | ZEIT ONLINE,"Künstliche Intelligenz (KI) kann Lehrer nach Ansicht der rheinland-pfälzischen Bildungsministerin Stefanie Hubig (SPD) bei ihrer Arbeit unterstützen. «Ich glaube, KI ist auch eine Chance zur Entlastung von Lehrkräften bei der Unterrichtsvorbereitung», sagte Hubig der Deutschen Presse-Agentur in Mainz. Klar sei, KI werde nicht mehr verschwinden. «Das geht nicht mehr weg, das wird Teil unseres Lebens.» Entsprechend müsse geschaut werden, wie diese Technologie sinnvoll genutzt werden könne.

Auch deswegen stelle Rheinland-Pfalz Lehrerinnen und Lehrern sowie Schülerinnen und Schülern von Februar an die Plattform «fobizz» zur Verfügung, sagte Hubig. Die Plattform bietet einen kostenfreien Zugang zu KI-Tools und gibt Tipps zum Umgang damit. Rheinland-Pfalz tue das als zweites Bundesland nach Mecklenburg-Vorpommern.

Der Umgang mit digitalen Medien müsse an den Schulen im Land zur Selbstverständlichkeit werden, sagte Hubig. Es genüge nicht, ein im Rahmen des Digitalpakts finanziertes Gerät in der Schule zu haben, es müsse genutzt werden können. «Es muss dann auch im Unterricht sinnvoll verwendet werden», sagte Hubig. «Das ist ein Prozess.»

Fortbildungen des Pädagogischen Landesinstituts würden gut nachgefragt, sagte Hubig. Laut dem Ministerium gab es allein im vergangenen Jahr 667 Fortbildungen zum Schwerpunkt «Digitalisierung und Medien» mit mehr als 19.700 Teilnehmern. 2021 waren es sogar mehr als 22.000 Teilnehmer bei 722 Veranstaltungen.

© dpa-infocom, dpa:231225-99-404590/2
"
Künstliche Intelligenz,Zeit,2023-12-22,https://www.zeit.de/news/2023-12/22/kuenstliche-intelligenz-haelt-einzug-in-tourismusbranche,Arbeitsalltag: Künstliche Intelligenz hält Einzug in Tourismusbranche | ZEIT ONLINE,"Die Nutzung von Künstlicher Intelligenz (KI) wird in der rheinland-pfälzischen Tourismusbranche immer bedeutender. «KI-Anwendungen halten Einzug in unsere tägliche Arbeit», sagte der Geschäftsführer der Rheinland-Pfalz Tourismus GmbH, Stefan Zindler, der Deutschen Presse-Agentur in Mainz. «KI wird ein Riesenthema auch im Tourismus.»

Potenzial sieht Zindler aktuell schon vor allem für die internationale Vermarktung der Branche bei Übersetzungen. Dabei gehe es um die Beschreibung von Betrieben und Anwendungen in den für Rheinland-Pfalz relevanten Sprachen. Neben Englisch seien das Französisch und Flämisch für die Niederlande und Belgien.

«Das wird mehr und mehr über automatische Prozesse erledigt», berichtete Zindler. Auf diesem Weg müssten nicht mehr Übersetzungsbüros extra für diese Aufgaben beauftragt werden.

Künstlicher Intelligenz bezeichnet meist Anwendungen auf Basis maschinellen Lernens, bei denen eine Software Datenmengen nach Übereinstimmungen durchforstet und Schlussfolgerungen zieht. Die EU hatte sich Anfang Dezember nach zähen Verhandlungen auf schärfere Regeln für Künstliche Intelligenz geeinigt.

© dpa-infocom, dpa:231222-99-381768/2
"
AI,Zeit,2023-12-21,https://www.zeit.de/digital/internet/2023-12/xai-elon-musk-chatbot-kuenstliche-intelligenz,xAI: Sie wollen die Mathematik lösen | ZEIT ONLINE,
AI,Zeit,2023-12-19,https://www.zeit.de/arbeit/2023-12/ki-kinderbuecher-illustrationen-geschichten-kosten,Kinderbücher mit KI: Nur mal kurz ein Kinderbuch gestalten | ZEIT Arbeit,
KI,Zeit,2023-12-19,https://www.zeit.de/arbeit/2023-12/ki-kinderbuecher-illustrationen-geschichten-kosten,Kinderbücher mit KI: Nur mal kurz ein Kinderbuch gestalten | ZEIT Arbeit,
KI,Zeit,2023-12-19,https://www.zeit.de/arbeit/2023-12/ki-kinderbuecher-illustrationen-geschichten-kosten/seite-3,"Kinderbücher mit KI: Die Geschichte, die KI mich schreiben ließ | ZEIT Arbeit",
Künstliche Intelligenz,Zeit,2023-12-16,https://www.zeit.de/kultur/2023-12/chatgpt-kuenstliche-intelligenz-faul,Künstliche Intelligenz: ChatGPT hat keine Lust mehr | ZEIT ONLINE,
Künstliche Intelligenz,Zeit,2023-12-15,https://www.zeit.de/news/2023-12/15/ki-forschung-startup-erhaelt-grossauftrag,Technik: KI-Forschung: Startup erhält Großauftrag | ZEIT ONLINE,"Das Cottbuser Startup Zander Laboratories GmbH kann mit einem Forschungsauftrag im Umfang von 30 Millionen Euro die Künstliche Intelligenz (KI) voranbringen. Das Ziel: Mensch und Maschine sollen besser interagieren können. Einen entsprechenden Vertrag hat das Unternehmen am Freitag in Cottbus mit der Agentur für Innovation in der Cybersicherheit GmbH (Cyberagentur) unterzeichnet. In einem Projekt soll das Startup - eine Ausgründung der Cottbuser Uni - in vier Jahren Prototypen entwickeln und damit die Interaktion zwischen Mensch und Maschine revolutionieren.

Die Cyberagentur mit Sitz in Halle ist dem Verteidigungs- sowie dem Bundesinnenministerium zugeordnet. Sie hat vom Bundestag ein Gesamtbudget in Höhe von 240 Millionen Euro bewilligt bekommen. Aus diesen Mitteln werden Forschungsprojekte finanziert.

Das Startup-Projekt «Neuroadaptivität für autonome Systeme» (NAFAS) will Maschinen erschaffen, die die Hirnaktivität des Menschen auslesen und interpretieren können. Das sagte Thorsten Zander, Geschäftsführer von Zander Labs. Mit dem Auftrag vergibt die Cyberagentur nach eigenen Angaben die größte Einzelfinanzierung eines Forschungsprojektes in der EU. Es geht um reine Forschung.

Das Startup hatte an einem Wettbewerb teilgenommen, der innovative Ideen zur Erforschung von Neurotechnologie im Zusammenhang mit der Mensch-Maschine-Interaktion suchte - und den Zuschlag erhalten. Es gab vier weitere Bewerber. Die konzeptionelle Stärke und innovative Herangehensweise hätten überzeugt, erklärte Andreas Schönau, stellvertretender Projektleiter bei der Cyberagentur. Das Projekt habe das Potenzial, «neue wissenschaftliche Standards in den Neurowissenschaften zu setzen.»

Hintergrund der Forschung ist den Angaben nach, dass Maschinen weiterhin in ihrer Fähigkeit begrenzt bleiben, menschliche Emotionen und ihre Entscheidungsfindung zu verstehen. Diese Einschränkung schränke das Potenzial der Mensch-Computer-Interaktion ein.

Zander hat 20 Jahre dazu geforscht. Die Brandenburgische-Technische Universität Cottbus-Senftenberg (BTU) gebe ihm die Möglichkeit, diese Forschungen umzusetzen, sagte der Professor für Neuroadaptive Mensch-Technik-Interaktion. Diese Technologie solle das Labor nun verlassen und in der realen Welt Anwendung finden. Dazu werden neue Hard- und Software entwickelt. «Wir werden viel zu erledigen haben, aber ich glaube auch, dass wir das Potenzial haben, dass wir die Welt verändern können», sagte Zander.

Wissenschaftsministerin Manja Schüle zeigte sich begeistert: «Nein, das ist kein neues Filmprojekt von Steven Spielberg oder Ridley Scott - das ist Science Fiction «Made in Cottbus».» Sie sieht einen großen Schub für Wissenschaft und Forschung in der Lausitz.

Das Projekt vereint mehrere Institutionnen mit spezialisiertem Fachwissen. Hierzu zählen die Fraunhofer Institute für Photonische Mikrosysteme (IPMS) und für Digitale Medientechnologie (IDMT), die niederländische TNO, Brain Products GmbH in München, Eaglescience Software B.V. in Haarlem sowie akademische Einrichtungen der BTU, der Universität Wien und der Julius-Maximilians-Universität Würzburg.

© dpa-infocom, dpa:231215-99-310962/3
"
Künstliche Intelligenz,Zeit,2023-12-13,https://www.zeit.de/news/2023-12/13/google-ki-technologie-gemini-offen-fuer-externe-entwickler,Künstliche Intelligenz: Google: KI-Technologie Gemini offen für externe Entwickler | ZEIT ONLINE,"Im Konkurrenzkampf bei Künstlicher Intelligenz macht Google Tempo und senkt die Preise. Der Konzern öffnete auf mehreren Ebenen den Zugriff auf seine neueste KI-Technik für externe Entwickler, damit diese auf Basis der neuen Technologie konkret Programme schreiben können.

Eine Woche zuvor hatte der Konzern überraschend das Modell mit dem Namen Gemini vorgestellt. Es soll nicht nur schneller sein als das Modell GPT-4 des Microsoft-Partners OpenAI, sondern auch deutlich vielseitiger. Gemini kann nicht nur in einem Chatbot Texte generieren, sondern auch bestimmte Probleme lösen und situationsabhängige Entscheidungen treffen. Dabei kann es auch Informationen aus Fotos und Videos aufnehmen. 

Schneller als erwartet gibt Google nun externen Entwicklern die Möglichkeit, selbst eigene Anwendungen zu schreiben, die auf Gemini aufsetzen. Das betrifft zum einen die Entwicklungsumgebung Google AI Studio, mit der kleinere KI-Anwendungen gebaut werden können. 

Als Beispiel demonstrierte Google die Programmierung einer App für Immobilienmakler, die auf Basis einer Videoaufnahme einer Führung durch ein Haus eine ausführliche Beschreibung des Wohnobjektes schreiben kann. Demonstriert wurde außerdem Gemini-Anwendungen in Vertex AI, einem KI-Service aus der Google Cloud für Unternehmen, sowie in Duet AI, einer KI-Lösung für die Office-Anwendungen von Google.

Der Internet-Konzern verlässt sich im Wettlauf gegen Microsoft und OpenAI aber nicht nur auf die Leistungsfähigkeit der Gemini-Technologie, sondern startete auch einen Preiswettkampf. Thomas Kurian, der Chef von Google Cloud, kündigte eine umfassende Preissenkung an. Die Technik kann bis zu einem Limit, das für viele Anwendungen ausreiche, kostenlos genutzt werden. Für die Unternehmenslösung Vertex AI würden nach einer kostenlosen Probephase die Gebühren für Eingaben auf 25 Prozent des ursprünglichen Preise gesenkt (0,00025 US-Dollar pro 1000 Buchstaben oder 0,0025 US-Dollar pro Bild). Bei den Textausgaben werde man den Preis auf 0,0005 Dollar pro 1000 Zeichen halbieren.

Google galt lange als führend bei Künstlicher Intelligenz. Doch dann geriet der Internet-Pionier ins Hintertreffen. Vor einem Jahr setzte sich das Start-up OpenAI mit der Veröffentlichung seines Chat-Bots ChatGPT überraschend an die Spitze. Die Software sorgte für viel Aufsehen, weil sie Sätze wie ein Mensch bilden kann. Sie wird mit gewaltigen Datenmengen trainiert und schätzt Wort für Wort ab, wie ein Satz weitergehen könnte. Das bringt allerdings das Risiko mit sich, dass sie völlig falsche Informationen ausgeben kann. Derzeit arbeiten alle großen KI-Player daran, ihren Systemen mehr Faktengenauigkeit beizubringen.

© dpa-infocom, dpa:231213-99-285804/2
"
Künstliche Intelligenz,Zeit,2023-12-13,https://www.zeit.de/2023/53/chatgpt-rueckblick-jaron-lanier-kuenstliche-intelligenz,"ChatGPT: ""Etwas größenwahnsinnig"" | ZEIT ONLINE",
AI,Zeit,2023-12-15,https://www.zeit.de/digital/internet/2023-12/x-elon-musk-chatbot-soziale-medien,X: Rechte Kaffeekränzchen und ein aufmuckender Chatbot | ZEIT ONLINE,"
2023 war das Jahr, in dem Twitter starb. So titelt das US-Technikmedium The Verge in einer Artikelserie und irgendwie stimmt es ja auch. Zum einen heißt Twitter jetzt X, zum anderen ist in den vergangenen zwölf Monaten für viele langjährige Nutzerinnen und Nutzer etwas verloren gegangen an der Platform
formerly known as Twitter. 

Symptomatisch dafür stehen vier Entwicklungen der vergangenen Wochen. An ihnen lässt sich ableiten, wie es um die Plattform steht und welche Herausforderungen in den kommenden
Monaten warten. 

Seit Sonntag darf
Alex Jones wieder Inhalte auf X veröffentlichen. Der rechte
US-Verschwörungstheoretiker und seine Website InfoWars wurden
2018 verbannt. Dort hatte er in der Vergangenheit unter anderem
verbreitet, dass die US-Regierung an den Anschlägen am 11. September
2001 in New York beteiligt gewesen sei und dass der Amoklauf an der
Sandy Hook Elementary School im Jahr 2012 von Schauspielern
inszeniert worden sei. Den Angehörigen der Betroffenen schuldet
Jones 85
Millionen US-Dollar an Schadensersatz, die er aber nicht
begleichen kann, da er pleite ist.

Elon Musk hatte
zuvor in
einer Umfrage auf X gefragt, ob Alex Jones seinen Account
zurückbekommen sollte. 70 Prozent stimmten mit Ja, wodurch Jones zu
einer illustren Runde an Ultrarechten, Verschwörungstheoretikern und
Antisemiten gehört, die inzwischen wieder zurück sind, darunter
Ex-Präsident Donald Trump, Rapper Kanye West, die US-Politikerin
Marjorie Taylor Greene, Verschwörungstheoretiker Ali Alexander und
der Gründer der Neonaziwebsite Daily Stormer, Andrew Anglin.

Passend zum Comeback
hat Musk Jones gleich eine Art Willkommensparty in Form eines
dreistündigen Gesprächs gegeben, an dem auch Vivek
Ramaswamy teilnahm, Bewerber für die republikanische
Präsidentschaftskandidatur, der gegen Abtreibung ist und den
Klimawandel kleinredet, sowie der Influencer Andrew Tate (ein weiterer Rückkehrer), der wegen
mutmaßlichen Menschenhandels in
Rumänien angeklagt ist. Eine bemerkenswert
unsympathische Runde also.

Das passt zur
Entwicklung von X. Nach der Übernahme durch Elon Musk nahm die Zahl
antisemitischer
und rassistischer
Inhalte zu, gleichzeitig gewannen viele
rechte Stimmen an Followern. Eine Studie
der Universität von Südkalifornien kam im April zu dem Fazit,
dass ""hasserfüllte Nutzer hasserfüllter geworden sind"".
Während der Ton spürbar rauer und die Meinungen extremer werden,
verlassen
viele Wissenschaftler und Journalisten die Plattform oder posten weniger. Statt wie versprochen für ein breiteres Spektrum
an Meinungen zu sorgen, hat Elon Musk dafür gesorgt, dass manche Stimmen lauter werden und andere zunehmend verstummen.

Während die
extremen Stimmen zurückkommen, flüchten, nicht ganz überraschend,
wichtige Werbepartner. Wie das Finanzmagazin Bloomberg
am
Mittwoch schrieb, werden die Werbeeinnahmen in diesem Jahr mit
etwa 2,5 Milliarden US-Dollar nur noch halb so hoch ausfallen wie
vor der Übernahme durch Elon Musk.

Obwohl schon im
vergangenen Jahr mehrere große Unternehmen Werbeanzeigen auf Twitter
ausgesetzt hatten, hat sich die Situation zuletzt noch einmal
verschärft. Nach einem antisemitischen Tweet von Musk persönlich
pausierten
unter anderem Apple, IBM, Sony und Disney die Kooperation, da sie
nicht in einem hasserfüllten Umfeld werben wollen. An ihrer Stelle
bekommen die Nutzerinnen und Nutzer vermehrt qualitativ minderwertige
Werbung zu sehen. Zum Beispiel von dubiosen Firmen, die
Sets anbieten, um heimlich Sperma von Männern zur künstlichen
Befruchtung zu stehlen.

Elon Musk sieht
derweil eine Verschwörung von Organisationen wie der jüdischen
Anti-Defamation League und dem Center for Countering Digital Hate,
gegen die X sogar klagt.
Sie würden die Werbepartner vergraulen, äußerte er sinngemäß. Gleichzeitig
schießt Musk aber auch gegen die ehemaligen Werbepartner: ""Go
fuck yourself!"", sagte
er Ende November auf einer Konferenz in Richtung des (nicht
persönlich anwesenden) Disney-Chefs Bob Iger. X werde sich nicht von
Unternehmen erpressen lassen. ""Wir hatten einfach das Gefühl,
dass die Verbindung mit Elon Musk und X nicht unbedingt positiv für
uns war"", hatte Iger zuvor verlauten lassen.

Für X ist das ein
Problem, da Werbeeinnahmen immer noch zwischen 70 und 75 Prozent des Umsatzes ausmachen, wie Bloomberg schreibt. Das Abomodell kann
die Verluste nicht ausgleichen, weshalb sich die Plattform trotz
harter Sparmaßnahmen in einer prekären Lage befindet, wie Musk
selbst immer wieder betont.
"
AI,Zeit,2023-12-13,https://www.zeit.de/news/2023-12/13/google-ki-technologie-gemini-offen-fuer-externe-entwickler,Künstliche Intelligenz: Google: KI-Technologie Gemini offen für externe Entwickler | ZEIT ONLINE,"Im Konkurrenzkampf bei Künstlicher Intelligenz macht Google Tempo und senkt die Preise. Der Konzern öffnete auf mehreren Ebenen den Zugriff auf seine neueste KI-Technik für externe Entwickler, damit diese auf Basis der neuen Technologie konkret Programme schreiben können.

Eine Woche zuvor hatte der Konzern überraschend das Modell mit dem Namen Gemini vorgestellt. Es soll nicht nur schneller sein als das Modell GPT-4 des Microsoft-Partners OpenAI, sondern auch deutlich vielseitiger. Gemini kann nicht nur in einem Chatbot Texte generieren, sondern auch bestimmte Probleme lösen und situationsabhängige Entscheidungen treffen. Dabei kann es auch Informationen aus Fotos und Videos aufnehmen. 

Schneller als erwartet gibt Google nun externen Entwicklern die Möglichkeit, selbst eigene Anwendungen zu schreiben, die auf Gemini aufsetzen. Das betrifft zum einen die Entwicklungsumgebung Google AI Studio, mit der kleinere KI-Anwendungen gebaut werden können. 

Als Beispiel demonstrierte Google die Programmierung einer App für Immobilienmakler, die auf Basis einer Videoaufnahme einer Führung durch ein Haus eine ausführliche Beschreibung des Wohnobjektes schreiben kann. Demonstriert wurde außerdem Gemini-Anwendungen in Vertex AI, einem KI-Service aus der Google Cloud für Unternehmen, sowie in Duet AI, einer KI-Lösung für die Office-Anwendungen von Google.

Der Internet-Konzern verlässt sich im Wettlauf gegen Microsoft und OpenAI aber nicht nur auf die Leistungsfähigkeit der Gemini-Technologie, sondern startete auch einen Preiswettkampf. Thomas Kurian, der Chef von Google Cloud, kündigte eine umfassende Preissenkung an. Die Technik kann bis zu einem Limit, das für viele Anwendungen ausreiche, kostenlos genutzt werden. Für die Unternehmenslösung Vertex AI würden nach einer kostenlosen Probephase die Gebühren für Eingaben auf 25 Prozent des ursprünglichen Preise gesenkt (0,00025 US-Dollar pro 1000 Buchstaben oder 0,0025 US-Dollar pro Bild). Bei den Textausgaben werde man den Preis auf 0,0005 Dollar pro 1000 Zeichen halbieren.

Google galt lange als führend bei Künstlicher Intelligenz. Doch dann geriet der Internet-Pionier ins Hintertreffen. Vor einem Jahr setzte sich das Start-up OpenAI mit der Veröffentlichung seines Chat-Bots ChatGPT überraschend an die Spitze. Die Software sorgte für viel Aufsehen, weil sie Sätze wie ein Mensch bilden kann. Sie wird mit gewaltigen Datenmengen trainiert und schätzt Wort für Wort ab, wie ein Satz weitergehen könnte. Das bringt allerdings das Risiko mit sich, dass sie völlig falsche Informationen ausgeben kann. Derzeit arbeiten alle großen KI-Player daran, ihren Systemen mehr Faktengenauigkeit beizubringen.

© dpa-infocom, dpa:231213-99-285804/2
"
AI,Zeit,2023-12-13,https://www.zeit.de/news/2023-12/13/worldcoin-projekt-dockt-an-telegram-und-reddit-an,Internet: Worldcoin-Projekt dockt an Telegram und Reddit an | ZEIT ONLINE,"Der Digital-Ausweis World ID aus dem umstrittenen Worldcoin-Projekt von OpenAI-Chef Sam Altman soll künftig verschiedenen Online-Plattformen dabei helfen, menschliche Nutzer von Software-Robotern zu unterscheiden. Ein erweitertes Protokoll werde unter anderem in die Anmeldeprozesse bei der Gaming-Plattform Minecraft sowie bei den sozialen Netzwerken Reddit und Telegram integriert. Das kündigte das Unternehmen Tools for Humanity (TFH), das die World-App entwickelt hat und betreibt, in San Francisco an.

Auch bei den Online-Handelshäusern Shopify aus Kanada sowie Mercado Libre, einem Handels-Marktführer aus Argentinien, werde die World ID eingebettet. Bislang wurde die World ID vor allem auf der Gaming-Plattform Discord dazu verwendet, um den Nachweis zu führen, dass ein Account-Inhaber tatsächlich ein menschliches Wesen ist. Künftig kann man damit auch beispielsweise verfolgen, wie viele Accounts ein Inhaber bereits auf einer Plattform angelegt hat.

Aktuell verfügen nach Angaben von TFH rund fünf Millionen Menschen weltweit über eine World ID. Mehr als die Hälfte der Anwender habe sich auch den Augapfel scannen lassen, um die World ID voll nutzen zu können. Obwohl TFH keine weiteren persönlichen Daten wie Name, Geburtstag, Adresse oder Bankverbindung sammelt, war das Konzept in der Politik und bei Behörden auf großes Misstrauen gestoßen. Das Bayerische Landesamt für Datenschutzaufsicht und die Bankenaufsicht Bafin kündigten Untersuchungen an. Ergebnisse liegen noch nicht vor.

Die neuen Version 2.0 der World ID biete nun vier abgestufte Verifizierungsstufen, um Entwicklern die Wahl zu lassen, wie streng die Anforderungen sein sollen, um sich als Mensch auszuweisen. Die Variante World ID Pro verlangt zusätzlich einen Gesichtsscan, ähnlich wie bei den Entsperrmethoden bei modernen Smartphones. Die strengste Variante World ID Max erfordert einen nochmaligen Iris-Scan. 

In Deutschland bietet Worldcoin derzeit an fünf Standorten In Berlin, Köln, Nürnberg und Stuttgart die Möglichkeit, seine World ID mit einem Iris-Scan validieren zu lassen. Mit der neuen Version der World-App sei es auch möglich, sein World-ID-Konto wieder löschen zu lassen, versprach TFH-Manager Tiago Sada. «Wie bei allem, was mit Krypto zu tun hat, war das sehr schwierig, weil Dinge in Krypto dazu neigen, für immer und ewig angelegt zu sein.» Man habe aber eine technische Lösung dafür gefunden.

Die TFH GmbH, die von Sam Altman und dem deutschen Informatiker Alex Blania gegründet wurde, unterhält Büros in Erlangen, San Francisco und Berlin. Sada sagte der Deutschen Presse-Agentur, Altman sei bei Worldcoin «so engagiert wie eh und je». Altman nehme an den wichtigen Diskussionen teil und habe aktiv zu vielen Neuerungen beigetragen. «Natürlich ist seine Hauptaufgabe die des CEO von OpenAI. Aber er ist Teil des Teams.» 

© dpa-infocom, dpa:231213-99-286663/3
"
KI,Zeit,2023-12-15,https://www.zeit.de/news/2023-12/15/ki-forschung-startup-erhaelt-grossauftrag,Technik: KI-Forschung: Startup erhält Großauftrag | ZEIT ONLINE,"Das Cottbuser Startup Zander Laboratories GmbH kann mit einem Forschungsauftrag im Umfang von 30 Millionen Euro die Künstliche Intelligenz (KI) voranbringen. Das Ziel: Mensch und Maschine sollen besser interagieren können. Einen entsprechenden Vertrag hat das Unternehmen am Freitag in Cottbus mit der Agentur für Innovation in der Cybersicherheit GmbH (Cyberagentur) unterzeichnet. In einem Projekt soll das Startup - eine Ausgründung der Cottbuser Uni - in vier Jahren Prototypen entwickeln und damit die Interaktion zwischen Mensch und Maschine revolutionieren.

Die Cyberagentur mit Sitz in Halle ist dem Verteidigungs- sowie dem Bundesinnenministerium zugeordnet. Sie hat vom Bundestag ein Gesamtbudget in Höhe von 240 Millionen Euro bewilligt bekommen. Aus diesen Mitteln werden Forschungsprojekte finanziert.

Das Startup-Projekt «Neuroadaptivität für autonome Systeme» (NAFAS) will Maschinen erschaffen, die die Hirnaktivität des Menschen auslesen und interpretieren können. Das sagte Thorsten Zander, Geschäftsführer von Zander Labs. Mit dem Auftrag vergibt die Cyberagentur nach eigenen Angaben die größte Einzelfinanzierung eines Forschungsprojektes in der EU. Es geht um reine Forschung.

Das Startup hatte an einem Wettbewerb teilgenommen, der innovative Ideen zur Erforschung von Neurotechnologie im Zusammenhang mit der Mensch-Maschine-Interaktion suchte - und den Zuschlag erhalten. Es gab vier weitere Bewerber. Die konzeptionelle Stärke und innovative Herangehensweise hätten überzeugt, erklärte Andreas Schönau, stellvertretender Projektleiter bei der Cyberagentur. Das Projekt habe das Potenzial, «neue wissenschaftliche Standards in den Neurowissenschaften zu setzen.»

Hintergrund der Forschung ist den Angaben nach, dass Maschinen weiterhin in ihrer Fähigkeit begrenzt bleiben, menschliche Emotionen und ihre Entscheidungsfindung zu verstehen. Diese Einschränkung schränke das Potenzial der Mensch-Computer-Interaktion ein.

Zander hat 20 Jahre dazu geforscht. Die Brandenburgische-Technische Universität Cottbus-Senftenberg (BTU) gebe ihm die Möglichkeit, diese Forschungen umzusetzen, sagte der Professor für Neuroadaptive Mensch-Technik-Interaktion. Diese Technologie solle das Labor nun verlassen und in der realen Welt Anwendung finden. Dazu werden neue Hard- und Software entwickelt. «Wir werden viel zu erledigen haben, aber ich glaube auch, dass wir das Potenzial haben, dass wir die Welt verändern können», sagte Zander.

Wissenschaftsministerin Manja Schüle zeigte sich begeistert: «Nein, das ist kein neues Filmprojekt von Steven Spielberg oder Ridley Scott - das ist Science Fiction «Made in Cottbus».» Sie sieht einen großen Schub für Wissenschaft und Forschung in der Lausitz.

Das Projekt vereint mehrere Institutionnen mit spezialisiertem Fachwissen. Hierzu zählen die Fraunhofer Institute für Photonische Mikrosysteme (IPMS) und für Digitale Medientechnologie (IDMT), die niederländische TNO, Brain Products GmbH in München, Eaglescience Software B.V. in Haarlem sowie akademische Einrichtungen der BTU, der Universität Wien und der Julius-Maximilians-Universität Würzburg.

© dpa-infocom, dpa:231215-99-310962/3
"
KI,Zeit,2023-12-13,https://www.zeit.de/news/2023-12/13/google-ki-technologie-gemini-offen-fuer-externe-entwickler,Künstliche Intelligenz: Google: KI-Technologie Gemini offen für externe Entwickler | ZEIT ONLINE,"Im Konkurrenzkampf bei Künstlicher Intelligenz macht Google Tempo und senkt die Preise. Der Konzern öffnete auf mehreren Ebenen den Zugriff auf seine neueste KI-Technik für externe Entwickler, damit diese auf Basis der neuen Technologie konkret Programme schreiben können.

Eine Woche zuvor hatte der Konzern überraschend das Modell mit dem Namen Gemini vorgestellt. Es soll nicht nur schneller sein als das Modell GPT-4 des Microsoft-Partners OpenAI, sondern auch deutlich vielseitiger. Gemini kann nicht nur in einem Chatbot Texte generieren, sondern auch bestimmte Probleme lösen und situationsabhängige Entscheidungen treffen. Dabei kann es auch Informationen aus Fotos und Videos aufnehmen. 

Schneller als erwartet gibt Google nun externen Entwicklern die Möglichkeit, selbst eigene Anwendungen zu schreiben, die auf Gemini aufsetzen. Das betrifft zum einen die Entwicklungsumgebung Google AI Studio, mit der kleinere KI-Anwendungen gebaut werden können. 

Als Beispiel demonstrierte Google die Programmierung einer App für Immobilienmakler, die auf Basis einer Videoaufnahme einer Führung durch ein Haus eine ausführliche Beschreibung des Wohnobjektes schreiben kann. Demonstriert wurde außerdem Gemini-Anwendungen in Vertex AI, einem KI-Service aus der Google Cloud für Unternehmen, sowie in Duet AI, einer KI-Lösung für die Office-Anwendungen von Google.

Der Internet-Konzern verlässt sich im Wettlauf gegen Microsoft und OpenAI aber nicht nur auf die Leistungsfähigkeit der Gemini-Technologie, sondern startete auch einen Preiswettkampf. Thomas Kurian, der Chef von Google Cloud, kündigte eine umfassende Preissenkung an. Die Technik kann bis zu einem Limit, das für viele Anwendungen ausreiche, kostenlos genutzt werden. Für die Unternehmenslösung Vertex AI würden nach einer kostenlosen Probephase die Gebühren für Eingaben auf 25 Prozent des ursprünglichen Preise gesenkt (0,00025 US-Dollar pro 1000 Buchstaben oder 0,0025 US-Dollar pro Bild). Bei den Textausgaben werde man den Preis auf 0,0005 Dollar pro 1000 Zeichen halbieren.

Google galt lange als führend bei Künstlicher Intelligenz. Doch dann geriet der Internet-Pionier ins Hintertreffen. Vor einem Jahr setzte sich das Start-up OpenAI mit der Veröffentlichung seines Chat-Bots ChatGPT überraschend an die Spitze. Die Software sorgte für viel Aufsehen, weil sie Sätze wie ein Mensch bilden kann. Sie wird mit gewaltigen Datenmengen trainiert und schätzt Wort für Wort ab, wie ein Satz weitergehen könnte. Das bringt allerdings das Risiko mit sich, dass sie völlig falsche Informationen ausgeben kann. Derzeit arbeiten alle großen KI-Player daran, ihren Systemen mehr Faktengenauigkeit beizubringen.

© dpa-infocom, dpa:231213-99-285804/2
"
KI,Zeit,2023-12-13,https://www.zeit.de/2023/53/chatgpt-rueckblick-jaron-lanier-kuenstliche-intelligenz,"ChatGPT: ""Etwas größenwahnsinnig"" | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2023-12-12,https://www.zeit.de/news/2023-12/12/volker-tuerk-ki-gefahr-fuer-grundlagen-des-menschseins,Menschenrechte: Volker Türk: KI Gefahr für Grundlagen des Menschseins | ZEIT ONLINE,"Künstliche Intelligenz (KI) und neue Technologien können nach Angaben des UN-Hochkommissars für Menschenrechte, Volker Türk, zur Gefahr für die Menschenwürde werden. «Die digitale Transformation hat riesiges Potenzial», sagte Türk in Genf. «Aber seien wir ehrlich: Von künstlicher Intelligenz bis hin zu Neurotechnologie, Cyberkriminalität, Überwachung und Biowaffen sind wir mit einer Welt konfrontiert, in der die Grundlagen des Menschseins - die Menschenwürde und das menschliche Handeln - in Gefahr sind». 

Türk sprach am zweiten Tag eines Forums zur Würdigung der Allgemeinen Menschenrechtserklärung. Sie wurde vor 75 Jahren, am 10. Dezember 1948, angenommen. Sie gilt als Meilenstein der Menschheitsgeschichte, weil sie erstmals weltweit unter anderem das Recht jedes Menschen auf Freiheit, Sicherheit und Schutz vor Staatswillkür festschreibt. 

«Wie können wir unsere Menschlichkeit und unsere Rechte in diesem neuen Universum schützen?», sagte Türk unter Bezug auf die digitale Transformation. «Das ist die Frage, die wir uns stellen müssen.» 

Türk rief Regierungen auf, die Menschenrechte in einer dunkeln Stunde der Geschichte mit Kriegen und Konflikten in vielen Weltregionen erneut zu bekräftigen und für die Einhaltung zu sorgen. «Wir sind hier, um die Grundlage für Hoffnung wieder aufzubauen», sagte er. An den Podiumsdiskussionen nahmen Staats- und Regierungsspitzen teil, darunter die Präsidenten Polens und Senegals, Andrzej Duda und Macky Sall, und die Präsidentin Griechenlands, Katerina Sakellaropoulou.

© dpa-infocom, dpa:231212-99-267280/2
"
Künstliche Intelligenz,Zeit,2023-12-10,https://www.zeit.de/news/2023-12/10/tu-nuernberg-soll-sich-auf-kuenstliche-intelligenz-fokussieren,Hochschulen: TU Nürnberg soll sich auf Künstliche Intelligenz fokussieren | ZEIT ONLINE,"Ein bundesweites Novum: In Nürnberg soll Deutschlands erste rein auf Künstliche Intelligenz (KI) spezialisierte Universität entstehen. Als «Franconian University of Artificial Intelligence» bezeichnete Ministerpräsident Markus Söder (CSU) das Vorhaben in seiner Heimatstadt. Doch was steckt dahinter?

Die Pläne der Staatsregierung sehen vor, dass sich die noch junge Technische Universität Nürnberg künftig voll auf das Thema KI fokussiert. KI-Universität bedeute, dass es künftig keinen einzigen Fachbereich geben werde, in dem KI keine Rolle spielen werde, teilte ein Sprecher des Wissenschaftsministeriums mit. Da der Lehr- und Forschungsbetrieb an der TU Nürnberg noch am Anfang stehe, werde der weitere Aufbau ab sofort unter dem Fokus auf KI erfolgen.

2021 gegründet, hat die Hochschule zu diesem Wintersemester die ersten acht Studierenden aufgenommen. Titel des ersten Masterstudiengangs: KI und Robotics. Ein zweites Masterprogramm zum Wintersemester 2025 soll sich mit menschlicher und künstlicher Intelligenz beschäftigen, wie ein Sprecher der TU mitteilte. Ein Bachelorstudiengang soll erstmals 2027 an den Start gehen.

Die großen gesellschaftlichen Zukunftsfragen in den Blick zu nehmen und auf neue Entwicklungen schnell zu reagieren, liege schon im Gründungsauftrag der TU Nürnberg, teilte Wissenschaftsminister Markus Blume (CSU) mit. Bei der Weiterentwicklung mit Fokus auf KI gehe es nun darum, wie KI bestmöglich genutzt und gleichzeitig die Risiken begrenzt werden könnten. «Auch die Auswirkungen auf die Gesellschaft, auf unsere Interaktionen und Emotionen müssen erforscht werden. Wir wollen KI nach unseren Werten gestalten», fügte Blume hinzu.

Zu den angedachten Themenfeldern in Forschung und Lehre gehören nach Ministeriumsangaben etwa autonome Systeme, die in der Lage sind, flexibel auf eine veränderte Umgebung zu reagieren und durch Erfahrungen zu lernen. Anwendungsbereiche könnten autonomes Fahren bis hin zu moderner Robotik in Industrie, Medizin und Pflege sein.

Auch im Bereich ziviler Sicherheit sollen die Einsatzmöglichkeiten Künstlicher Intelligenz an der TU demnach künftig erforscht werden. Ein weiteres Feld ist den Angaben zufolge das Thema Arbeit und wie sich Beschäftigungsmöglichkeiten durch KI in Zukunft ändern werden.

Mit der TU hat der Freistaat neben der Friedrich-Alexander-Universität Erlangen-Nürnberg und der Technischen Hochschule Nürnberg bereits die dritte Hochschule mit technischem Schwerpunkt in der Region geschaffen - und viel Geld nach Franken fließen lassen.

Zur Frage, ob die TU für die Spezialisierung auf KI mit mehr Geld ausgestattet werde, teilte der Sprecher mit: «Die Universität in Nürnberg wächst gerade auf.» Von Jahr zu Jahr werde bis zum Endausbau kontinuierlich mehr Geld an die TU Nürnberg fließen. Eine konkrete Summe nannte das Ministerium nicht.

© dpa-infocom, dpa:231210-99-242449/2
"
Künstliche Intelligenz,Zeit,2023-12-10,https://www.zeit.de/news/2023-12/08/kuenstliche-intelligenz-soll-in-eu-staerker-geregelt-werden,Künstliche Intelligenz: Zu streng oder zu schwach? EU-Regelwerk für KI spaltet | ZEIT ONLINE,"Die Europäische Union (EU) hat sich kurz vor dem Wochenende auf Regeln für die Nutzung Künstlicher Intelligenz (KI) verständigt. Das Europaparlament und die EU-Staaten stellten heraus: Es sind weltweit die ersten Regeln für KI. Jedoch kommt von zwei Seiten Kritik. Die einen halten die Regeln für zu streng, die anderen für zu lasch. Die Wirtschaft befürchtet, die künftige EU-Verordnung werde Innovationen hemmen. Aus Sicht von Verbraucherschützern werden die Risiken mancher Anwendungen nicht ernst genug genommen.

Europa droht nach Ansicht des Bundesverbands der Deutschen Industrie (BDI) bei der Schlüsseltechnologie KI nun ins Hintertreffen zu geraten. «Mit der umfassenden Regulierung von KI-Basismodellen und KI-Anwendungen gefährdet der AI Act die Wettbewerbs- und Innovationsfähigkeit sowohl auf Hersteller- als auch auf Anwenderseite», sagte BDI-Geschäftsführungsmitglied Iris Plöger. Die Regulierung basiere auf unausgereiften Kriterien, die den Unternehmen weniger statt mehr Rechtssicherheit brächten.

Der Technikbranchenverband Bitkom sprach von einem «politischen Schaufenster-Erfolg zu Lasten von Wirtschaft und Gesellschaft». Der erzielte Kompromiss greife tief in die Technologie ein.«Die EU bindet damit den Unternehmen einen regulatorischen Klotz ans Bein. Das Risiko ist groß, dass europäische Unternehmen durch nicht praxistaugliche Vorhaben der rasanten technologischen Entwicklung künftig nicht folgen können», sagte Bitkom-Hauptgeschäftsführer Bernhard Rohleder.

Die europäische Verbraucherschutzorganisation Beuc kritisierte dagegen, dass sich die EU zu sehr auf den guten Willen der Unternehmen zur Selbstregulierung verlasse. «So werden beispielsweise virtuelle Assistenten oder KI-gesteuerte Spielzeuge nicht ausreichend reguliert, da sie nicht als Hochrisikosysteme gelten. Auch Systeme wie ChatGPT oder Bard werden nicht die notwendigen Leitplanken erhalten, damit die Verbraucher ihnen vertrauen können», hieß es.

Die deutsche Verbraucherschutzministerin Steffi Lemke (Grüne) sieht in der KI-Verordnung hingegen einen Schutz für Verbraucher vor den Risiken der neuen Technologie. «In den Verhandlungen haben wir uns besonders dafür eingesetzt, dass KI-Systeme transparent, nachvollziehbar und überprüfbar gestaltet werden. So müssen nun künftig Unternehmen, die den Einsatz von KI-Technologien anbieten, Informationen über die Funktionsweise ihrer Systeme bereitstellen und KI-gestützte Entscheidungen erläutern», berichtete Lemke am Samstag.  Bei Verstößen könnten Verbraucherverbände mit Verbandsklagen gerichtlich dagegen vorgehen. 

Die vorgelegten Vorschriften legen Verpflichtungen für KI auf Grundlage ihrer potenziellen Risiken und Auswirkungen fest. Als besonders riskant werden KI eingestuft, die ein erhebliches Schadenspotenzial etwa für Gesundheit, Demokratie, Umwelt oder Sicherheit haben. 

Bestimmte Anwendungen werden komplett verboten, etwa biometrische Kategorisierungssysteme, die sensible Merkmale wie zum Beispiel die sexuelle Orientierung oder religiöse Überzeugungen verwenden. Auch das ungezielte Auslesen von Bildern aus dem Internet oder aus Überwachungsaufnahmen für Gesichtserkennungsdatenbanken soll nicht erlaubt sein. Allerdings wird es Ausnahmen für biometrische Identifizierungen im öffentlichen Raum in Echtzeit geben, etwa bei der Gefahr eines Terroranschlags oder bei der gezielten Suche von Opfern von Menschenhandel. Um diesen Punkt wurde intensiv gerungen, das EU-Parlament wollte eigentlich ein komplettes Verbot.

Ein weiterer Streitpunkt war die Regulierung von sogenannten Basismodellen. Das sind sehr leistungsfähige KI-Modelle, die mit einem breiten Satz an Daten trainiert wurden. Sie können die Grundlage für viele andere Anwendungen sein. Dazu zählt etwa GPT. Deutschland, Frankreich und Italien hatten zuvor gefordert, dass nur konkrete Anwendungen von KI reguliert werden sollten, nicht aber die Basis-Technologie an sich. Nun einigten sich die Unterhändler auf bestimmte Transparenzpflichten für diese Modelle. 

Der Bundesminister für Digitales, Volker Wissing (FDP), enthielt sich einer endgültigen Bewertung. Einerseits sei verhindert worden, dass bestimmte Systeme in den Hochrisikobereich fallen, andererseits müsse das Regelwerk Innovationen ermöglichen und verhältnismäßig sein, sagte er am Samstag. «Ob dies gelungen ist, werden wir uns in den nächsten Tagen sehr genau anschauen.»

© dpa-infocom, dpa:231208-99-232521/5
"
Künstliche Intelligenz,Zeit,2023-12-10,https://www.zeit.de/2023/52/kuenstliche-intelligenz-test-wissenschaft-menschen,Künstliche Intelligenz: War’s das mit meiner Intelligenz? | ZEIT ONLINE,"Falls die künstliche Intelligenz den Menschen überflügelt, wird es sich wohl ähnlich anfühlen wie jener Moment im Spiel des Programms AlphaGo im März 2016: Damals trat die Maschine gegen den stärksten Go-Spieler der Welt an. In der zweiten Partie machte sie einen seltsamen Zug, der gegen jede Regel zu verstoßen schien. Fachleute vermuteten einen Fehler des Programms. Erst im Verlauf des Spieles zeigte sich die Genialität dieses 37. Zuges: Er erwies sich als weit vorausschauender Siegeszug, den es in der Geschichte des Go-Spiels so noch nie gegeben hatte. Am Ende hatte der Mensch keine Chance, und AlphaGo bekam vom südkoreanischen Go-Verband den höchsten Meisterrang verliehen, mit der Begründung, es habe ""fast göttliche Bereiche"" des Spiels erreicht.

Für Ilya Sutskever zeigt dieses Beispiel, wozu die künstliche Intelligenz (KI) in der Lage ist: Sie habe eine Strategie gefunden, die besser war ""als all das, was die Menschheit gemeinsam über Tausende von Jahren entwickelt hat"", erklärte Sutskever kürzlich der Technology Review. Wenn die KI diese Überlegenheit auch auf anderen Feldern erreiche, sei sie ""superintelligent"". Dann gute Nacht, Menschheit.

Nun ist Ilya Sutskever nicht irgendwer. Er gilt als einer der besten Informatiker der Welt und ist Chief Scientist der Firma OpenAI, bekannt für KI-Produkte wie ChatGPT oder das Illustrationsprogramm Dall-E. Derzeit aber treibt ihn die Sorge um, die KI könnte die Schwelle zur Superintelligenz überschreiten. Auch viele andere KI-Forscher fragen aktuell: Können die Programme stets nur einzelne Facetten menschlicher Fähigkeiten simulieren – wie etwa Go-Spielen –, oder sind sie irgendwann zu eigener Denkfähigkeit in der Lage? Entwickeln sie das, was Fachleute ArtificialGeneral Intelligence nennen, künstliche allgemeine Intelligenz – und falls ja: Wann ist es so weit?

Der Auslöser für die neue Debatte ist jenes Drama, das OpenAI der Welt Mitte November vorführte und in dem Sutskever eine Hauptrolle spielte. Zuerst wurde da OpenAI-Chef Sam Altman mit großem Trara entlassen, nur um wenige Tage später wieder eingestellt zu werden – worauf ein Teil des Verwaltungsrats von OpenAI gehen musste, darunter auch Sutskever. Der Hintergrund dieser Personalrochade sind offenbar unterschiedliche Ansichten über die Risiken der KI: Während Altman die Kommerzialisierung von OpenAI massiv vorantreibt, will Sutskever vorsichtiger vorgehen. Ausschlaggebend für die Entlassung Altmans – so berichtet die Nachrichtenagentur Reuters unter Berufung auf Insider – sei ein Projekt namens Q* gewesen, das möglicherweise bereits die Bedingungen einer menschenähnlichen Intelligenz erfülle. Aus Furcht davor habe Sutskever die Notbremse gezogen – und am Ende gegen Altman verloren.

Das Problem ist: Kaum jemand weiß, was hinter dem geheimnisvollen Q* wirklich steckt. Manche raunen von der Superintelligenz, andere vermuten nur einen Marketingtrick von OpenAI. So oder so haben die Gerüchte die Debatte um die Artificial General Intelligence (AGI) neu entfacht und die Frage virulent werden lassen: Wie würde man eigentlich erkennen, dass die Maschinen so etwas wie menschenähnliche Denkfähigkeit entwickeln?

Wer sich dieser Frage nähert, stellt schnell fest, dass das Konzept der allgemeinen Intelligenz alles andere als klar ist. ""Wenn man 100 KI-Experten bitten würde, zu definieren, was sie mit AGI meinen, würde man wahrscheinlich 100 unterschiedliche Definitionen bekommen"", schrieb kürzlich ein Forscherteam von Google Deep Mind, das versuchte, die Debatte zu sortieren. So glauben einige Experten, dass in den großen Sprachmodellen wie ChatGPT bereits ""Funken"" von allgemeiner Intelligenz aufscheinen; andere sagen voraus, dass es noch ein bis mehrere Jahrzehnte dauere, bis die KI menschenähnliche Intelligenz erreiche, während wieder andere die ganze Diskussion für Panikmache halten und der Überzeugung sind, Maschinen könnten dem Menschen nie das Wasser reichen – weil sie keinen Körper, kein Empfinden und keine Eigeninteressen hätten.

Da könnte man sich nun in philosophische Grundsatzdebatten verstricken. Man kann aber auch pragmatisch fragen: Wie lässt sich maschinelle Intelligenz überhaupt testen – und wann billigen wir ihr Denkfähigkeit zu? Anhand dieser Frage wird schnell deutlich, was die KI kann und was nicht.

Die berühmteste Antwort auf die Test-Frage lieferte 1950 der Informatiker Alan Turing: Ein KI-System sei dann intelligent zu nennen, wenn es im Gespräch nicht mehr von einem Menschen zu unterscheiden ist. Nun erleben wir diese Art von Turing-Test heute ständig im Alltag, wenn wir uns fragen: Chatte ich in der Hotline oder im Servicecenter eines Unternehmens mit einem Menschen oder einem Chatbot? Oft ist das kaum mehr zu beantworten. Auch ein Experiment mit 1,5 Millionen Probanden im Mai ergab, dass viele Menschen den Unterschied zwischen Mensch und Maschine nicht merken. Anders ist es bei Forschenden, die sich professionell mit den Stärken und Schwächen der Sprachmodelle auskennen: Diese lassen sich von der KI nicht so leicht in die Irre führen. 
"
Künstliche Intelligenz,Zeit,2023-12-09,https://www.zeit.de/digital/datenschutz/2023-12/kuenstliche-intelligenz-regulierung-eu-bruessel-gesetzentwurf,"Regulierung von KI: Gesichtserkennung im Terrorfall, Datenfilter und Transparenzpflicht | ZEIT ONLINE","
Nach langen Verhandlungen hat sich die EU auf strengere Regeln für künstliche Intelligenz (KI) geeinigt. Es seien weltweit die ersten Regeln für KI, teilten Europaparlament und die EU-Staaten mit. Was umfassen die Vorgaben und wie fallen die Reaktionen aus? Hier die wichtigsten Fragen und Antworten:

Die auf EU-Ebene entworfenen
Vorschriften legen fest, wozu sich die Mitgliedsstaaten verpflichten sollen, um künstliche Intelligenz hinsichtlich ihrer 
potenziellen Risiken und Auswirkungen im Griff zu behalten. Als besonders riskant werden
 KI eingestuft, die Gesundheit, Demokratie, Umwelt oder Sicherheit erheblich beschädigen können. Bestimmte 
Anwendungen werden daher verboten, etwa 
Kategorisierungssysteme, die sensible Merkmale wie zum Beispiel die 
sexuelle Orientierung oder religiöse Überzeugungen verwenden.

Auch das 
ungezielte Auslesen von Bildern aus dem Internet oder aus 
Überwachungsaufnahmen für Gesichtserkennungsdatenbanken soll nicht 
erlaubt sein. Allerdings wird es Ausnahmen für biometrische 
Identifizierungen im öffentlichen Raum in Echtzeit geben, etwa bei der 
Gefahr eines Terroranschlags oder bei der gezielten Suche von Opfern von
 Menschenhandel. Um diesen Punkt wurde intensiv gerungen, das 
EU-Parlament wollte eigentlich ein komplettes Verbot.

Ein weiterer
 Streitpunkt war die Regulierung von sogenannten Basismodellen. Das sind
 sehr leistungsfähige KI-Modelle, die mit einem breiten Satz an Daten 
trainiert wurden. Sie können die Grundlage für viele andere Anwendungen 
sein. Dazu zählt etwa GPT, das Basismodell für den Text-Roboter ChatGPT. Deutschland, Frankreich und Italien hatten 
zuvor gefordert, dass nur konkrete Anwendungen von KI reguliert werden 
sollten, nicht aber die Basistechnologie an sich. Nun einigten sich die
 Unterhändler auf bestimmte Transparenzpflichten für diese Modelle.

Unter künstlicher Intelligenz (KI) versteht man den Versuch, menschliches Lernen und Denken auf den Computer zu übertragen. Ziel ist es, komplexe Aufgaben erledigen zu lassen, die normalerweise menschliche Intelligenz erfordern. Generelle Problemlösungsmaschinen (Artificial General Intelligence) sind trotz aller Fortschritte noch nicht in Sicht. Allerdings finden enger definierte KI-Anwendungen in unserer heutigen Welt bereits breite Verwendung: Dazu zählen etwa automatische Übersetzungen, personalisierte Empfehlungen beim Online-Shopping, Gesichtserkennung am Handy, aber auch intelligente Thermostate oder Navis. Auch die Anwendungen der generativen KI wie der Text-Roboter ChatGPT gehören zu den enger definierten KI-Anwendungen. Ihr simples und zugleich kompliziertes Prinzip: Sie leiten aus Texten ab, mit denen man sie gefüttert hat, welches Wort statistisch auf das vorhergehende folgen könnte und generieren so ihre Antworten.  

KI ist eine Zukunftstechnologie. Experten vermuten, dass die Technologie künftig praktisch alle Aspekte in der Wirtschaft, aber auch im Alltag betreffen könnte und sich zum Beispiel der Arbeitsmarkt dadurch massiv wandeln wird: Manche Jobs werden sich verändern, andere werden vielleicht ganz verschwinden. KI gilt aber auch als Technologie, die Gefahren birgt. So warnte etwa der Chef des ChatGPT-Erfinders OpenAI, Sam Altman, vor Falschinformationen mithilfe künstlicher Intelligenz und sprach sich deshalb für eine Regulierung aus. Fotos oder Videos können durch KI leicht manipuliert und neu generiert werden. Als Problem gilt auch, dass KI teilweise mit verzerrten Datensätzen trainiert wurden und damit Personen diskriminiert werden. Auch der Einsatz in der Kriegsführung wird für möglich gehalten.

EU-Kommissionspräsidentin Ursula von der Leyen begrüßte die Einigung und bezeichnete das Gesetz als ""weltweites Novum"". Svenja Hahn von der FDP zieht eine gemischte Bilanz: ""In 38 Stunden Verhandlung über drei Tage konnten wir eine massive Überregulierung von KI-Innovation verhindern und rechtsstaatliche Prinzipien beim Einsatz von KI in der Strafverfolgung verankern"", sagte die Europaabgeordnete. ""Ich hätte mir mehr Freude an Innovation und noch stärkere Bekenntnisse zu Bürgerrechten gewünscht."" Der rechtspolitische Sprecher der CDU, Axel Voss, teilte mit, er sei nicht davon überzeugt, dass dies der richtige Weg sei, um Europa im Bereich der KI wettbewerbsfähig zu machen. ""Innovation wird immer noch anderswo stattfinden, hier haben wir als Europäische Union unsere Chance verpasst.""

Die europäische Verbraucherschutzorganisation Beuc kritisierte, dass sich die EU zu sehr auf den guten Willen der Unternehmen zur Selbstregulierung verlasse. ""So werden beispielsweise virtuelle Assistenten oder KI-gesteuerte Spielzeuge nicht ausreichend reguliert, da sie nicht als Hochrisikosysteme gelten. Auch Systeme wie ChatGPT oder Bard werden nicht die notwendigen Leitplanken erhalten, damit die Verbraucher ihnen vertrauen können"", hieß es.

Zunächst erfordert das Vorhaben, dass die EU-Staaten und das Europaparlament noch zustimmen. Das gilt aber als Formsache.
"
Künstliche Intelligenz,Zeit,2023-12-09,https://www.zeit.de/digital/2023-12/eu-kuenstliche-intelligenz-gesetz-regeln-einigung,KI-Gesetz: EU einigt sich auf Regeln für künstliche Intelligenz | ZEIT ONLINE,"Für den Einsatz von künstlicher Intelligenz (KI) sollen in der EU
künftig enge Regeln gelten. Unterhändler von Europaparlament und
EU-Staaten verständigten sich in Brüssel nach langen
Verhandlungen auf entsprechende Vorgaben. Nach Angaben des EU-Parlaments
handelt es sich um das weltweit erste KI-Gesetz. Im nächsten Schritt müssen das
EU-Parlament und der EU-Rat den Gesetzentwurf verabschieden. 

Außerdem müssen noch technische Details ausgearbeitet
werden. Die Regeln sollen unter anderem die Qualität der für die Entwicklung
der Algorithmen verwendeten Daten gewährleisten und sicherstellen, dass bei der
KI-Entwicklung keine Urheberrechte verletzt werden. Außerdem müssen Entwicklerinnen
und Entwickler sicherstellen, dass Texte, Bilder und Töne, die durch KI
entstanden sind, als solche zu erkennen sind.  

Verschärfte Vorgaben soll es für ""risikoreiche""
Anwendungen geben, etwa bei kritischer Infrastruktur, Sicherheitsbehörden und
Personalverwaltung. Dort sollen eine Kontrolle durch den Menschen über KI, eine
technische Dokumentation und ein System zum Risikomanagement festgeschrieben
werden.

KI-Anwendungen wie
eine automatisierte Gesichtserkennung werden verboten. Es soll aber Ausnahmen geben, etwa zum Schutz der nationalen Sicherheit, zur
Verteidigung und für andere militärische Zwecke. Der Gesetzentwurf beschränkt zudem die Nutzung biometrischer
Identifizierung durch die Strafverfolgungsbehörden.   

Zuletzt sind die Verhandlungen an der Frage der Regulierung
von sogenannten Basismodellen fast gescheitert. Das sind sehr leistungsfähige
KI-Modelle, die mit einem breiten Satz an Daten trainiert wurden. Sie können
die Grundlage für viele andere Anwendungen sein. Dazu zählen etwa Sprachmodelle
wie ChatGPT. 

Deutschland, Frankreich und Italien hatten gefordert, dass
nur konkrete Anwendungen von KI reguliert werden sollten, nicht aber die
Basistechnologie an sich. Sie hatten Befürchtungen geäußert, dass schärfere
Regeln lokale KI-Start-ups wie Aleph Alpha oder Mistral AI in ihrer Entwicklung
behindern. 

Die EU-Regeln sehen unter anderem vor, dass Firmen, die
Grundlagenmodelle entwickeln, bestimmte Informationen veröffentlichen müssen.
Sie sollen beispielsweise dokumentieren, wie das Modell trainiert wurde, interne
Tests durchführen und deren Ergebnisse veröffentlichen.   

Bei besonders leistungsfähigen Modellen sollen noch schärfere
Regeln gelten. Dann müssen Firmen auch externe Tests durchführen, um
Schwachstellen zu ermitteln. Außerdem müssen sie die Risiken ermitteln, die von
ihren Modellen ausgehen könnten. Start-ups wie Aleph Alpha wären von dieser
Regelung aber wohl nicht betroffen. 
"
AI,Zeit,2023-12-10,https://www.zeit.de/news/2023-12/10/tu-nuernberg-soll-sich-auf-kuenstliche-intelligenz-fokussieren,Hochschulen: TU Nürnberg soll sich auf Künstliche Intelligenz fokussieren | ZEIT ONLINE,"Ein bundesweites Novum: In Nürnberg soll Deutschlands erste rein auf Künstliche Intelligenz (KI) spezialisierte Universität entstehen. Als «Franconian University of Artificial Intelligence» bezeichnete Ministerpräsident Markus Söder (CSU) das Vorhaben in seiner Heimatstadt. Doch was steckt dahinter?

Die Pläne der Staatsregierung sehen vor, dass sich die noch junge Technische Universität Nürnberg künftig voll auf das Thema KI fokussiert. KI-Universität bedeute, dass es künftig keinen einzigen Fachbereich geben werde, in dem KI keine Rolle spielen werde, teilte ein Sprecher des Wissenschaftsministeriums mit. Da der Lehr- und Forschungsbetrieb an der TU Nürnberg noch am Anfang stehe, werde der weitere Aufbau ab sofort unter dem Fokus auf KI erfolgen.

2021 gegründet, hat die Hochschule zu diesem Wintersemester die ersten acht Studierenden aufgenommen. Titel des ersten Masterstudiengangs: KI und Robotics. Ein zweites Masterprogramm zum Wintersemester 2025 soll sich mit menschlicher und künstlicher Intelligenz beschäftigen, wie ein Sprecher der TU mitteilte. Ein Bachelorstudiengang soll erstmals 2027 an den Start gehen.

Die großen gesellschaftlichen Zukunftsfragen in den Blick zu nehmen und auf neue Entwicklungen schnell zu reagieren, liege schon im Gründungsauftrag der TU Nürnberg, teilte Wissenschaftsminister Markus Blume (CSU) mit. Bei der Weiterentwicklung mit Fokus auf KI gehe es nun darum, wie KI bestmöglich genutzt und gleichzeitig die Risiken begrenzt werden könnten. «Auch die Auswirkungen auf die Gesellschaft, auf unsere Interaktionen und Emotionen müssen erforscht werden. Wir wollen KI nach unseren Werten gestalten», fügte Blume hinzu.

Zu den angedachten Themenfeldern in Forschung und Lehre gehören nach Ministeriumsangaben etwa autonome Systeme, die in der Lage sind, flexibel auf eine veränderte Umgebung zu reagieren und durch Erfahrungen zu lernen. Anwendungsbereiche könnten autonomes Fahren bis hin zu moderner Robotik in Industrie, Medizin und Pflege sein.

Auch im Bereich ziviler Sicherheit sollen die Einsatzmöglichkeiten Künstlicher Intelligenz an der TU demnach künftig erforscht werden. Ein weiteres Feld ist den Angaben zufolge das Thema Arbeit und wie sich Beschäftigungsmöglichkeiten durch KI in Zukunft ändern werden.

Mit der TU hat der Freistaat neben der Friedrich-Alexander-Universität Erlangen-Nürnberg und der Technischen Hochschule Nürnberg bereits die dritte Hochschule mit technischem Schwerpunkt in der Region geschaffen - und viel Geld nach Franken fließen lassen.

Zur Frage, ob die TU für die Spezialisierung auf KI mit mehr Geld ausgestattet werde, teilte der Sprecher mit: «Die Universität in Nürnberg wächst gerade auf.» Von Jahr zu Jahr werde bis zum Endausbau kontinuierlich mehr Geld an die TU Nürnberg fließen. Eine konkrete Summe nannte das Ministerium nicht.

© dpa-infocom, dpa:231210-99-242449/2
"
AI,Zeit,2023-12-10,https://www.zeit.de/news/2023-12/08/kuenstliche-intelligenz-soll-in-eu-staerker-geregelt-werden,Künstliche Intelligenz: Zu streng oder zu schwach? EU-Regelwerk für KI spaltet | ZEIT ONLINE,"Die Europäische Union (EU) hat sich kurz vor dem Wochenende auf Regeln für die Nutzung Künstlicher Intelligenz (KI) verständigt. Das Europaparlament und die EU-Staaten stellten heraus: Es sind weltweit die ersten Regeln für KI. Jedoch kommt von zwei Seiten Kritik. Die einen halten die Regeln für zu streng, die anderen für zu lasch. Die Wirtschaft befürchtet, die künftige EU-Verordnung werde Innovationen hemmen. Aus Sicht von Verbraucherschützern werden die Risiken mancher Anwendungen nicht ernst genug genommen.

Europa droht nach Ansicht des Bundesverbands der Deutschen Industrie (BDI) bei der Schlüsseltechnologie KI nun ins Hintertreffen zu geraten. «Mit der umfassenden Regulierung von KI-Basismodellen und KI-Anwendungen gefährdet der AI Act die Wettbewerbs- und Innovationsfähigkeit sowohl auf Hersteller- als auch auf Anwenderseite», sagte BDI-Geschäftsführungsmitglied Iris Plöger. Die Regulierung basiere auf unausgereiften Kriterien, die den Unternehmen weniger statt mehr Rechtssicherheit brächten.

Der Technikbranchenverband Bitkom sprach von einem «politischen Schaufenster-Erfolg zu Lasten von Wirtschaft und Gesellschaft». Der erzielte Kompromiss greife tief in die Technologie ein.«Die EU bindet damit den Unternehmen einen regulatorischen Klotz ans Bein. Das Risiko ist groß, dass europäische Unternehmen durch nicht praxistaugliche Vorhaben der rasanten technologischen Entwicklung künftig nicht folgen können», sagte Bitkom-Hauptgeschäftsführer Bernhard Rohleder.

Die europäische Verbraucherschutzorganisation Beuc kritisierte dagegen, dass sich die EU zu sehr auf den guten Willen der Unternehmen zur Selbstregulierung verlasse. «So werden beispielsweise virtuelle Assistenten oder KI-gesteuerte Spielzeuge nicht ausreichend reguliert, da sie nicht als Hochrisikosysteme gelten. Auch Systeme wie ChatGPT oder Bard werden nicht die notwendigen Leitplanken erhalten, damit die Verbraucher ihnen vertrauen können», hieß es.

Die deutsche Verbraucherschutzministerin Steffi Lemke (Grüne) sieht in der KI-Verordnung hingegen einen Schutz für Verbraucher vor den Risiken der neuen Technologie. «In den Verhandlungen haben wir uns besonders dafür eingesetzt, dass KI-Systeme transparent, nachvollziehbar und überprüfbar gestaltet werden. So müssen nun künftig Unternehmen, die den Einsatz von KI-Technologien anbieten, Informationen über die Funktionsweise ihrer Systeme bereitstellen und KI-gestützte Entscheidungen erläutern», berichtete Lemke am Samstag.  Bei Verstößen könnten Verbraucherverbände mit Verbandsklagen gerichtlich dagegen vorgehen. 

Die vorgelegten Vorschriften legen Verpflichtungen für KI auf Grundlage ihrer potenziellen Risiken und Auswirkungen fest. Als besonders riskant werden KI eingestuft, die ein erhebliches Schadenspotenzial etwa für Gesundheit, Demokratie, Umwelt oder Sicherheit haben. 

Bestimmte Anwendungen werden komplett verboten, etwa biometrische Kategorisierungssysteme, die sensible Merkmale wie zum Beispiel die sexuelle Orientierung oder religiöse Überzeugungen verwenden. Auch das ungezielte Auslesen von Bildern aus dem Internet oder aus Überwachungsaufnahmen für Gesichtserkennungsdatenbanken soll nicht erlaubt sein. Allerdings wird es Ausnahmen für biometrische Identifizierungen im öffentlichen Raum in Echtzeit geben, etwa bei der Gefahr eines Terroranschlags oder bei der gezielten Suche von Opfern von Menschenhandel. Um diesen Punkt wurde intensiv gerungen, das EU-Parlament wollte eigentlich ein komplettes Verbot.

Ein weiterer Streitpunkt war die Regulierung von sogenannten Basismodellen. Das sind sehr leistungsfähige KI-Modelle, die mit einem breiten Satz an Daten trainiert wurden. Sie können die Grundlage für viele andere Anwendungen sein. Dazu zählt etwa GPT. Deutschland, Frankreich und Italien hatten zuvor gefordert, dass nur konkrete Anwendungen von KI reguliert werden sollten, nicht aber die Basis-Technologie an sich. Nun einigten sich die Unterhändler auf bestimmte Transparenzpflichten für diese Modelle. 

Der Bundesminister für Digitales, Volker Wissing (FDP), enthielt sich einer endgültigen Bewertung. Einerseits sei verhindert worden, dass bestimmte Systeme in den Hochrisikobereich fallen, andererseits müsse das Regelwerk Innovationen ermöglichen und verhältnismäßig sein, sagte er am Samstag. «Ob dies gelungen ist, werden wir uns in den nächsten Tagen sehr genau anschauen.»

© dpa-infocom, dpa:231208-99-232521/5
"
AI,Zeit,2023-12-10,https://www.zeit.de/2023/52/kuenstliche-intelligenz-test-wissenschaft-menschen,Künstliche Intelligenz: War’s das mit meiner Intelligenz? | ZEIT ONLINE,"Falls die künstliche Intelligenz den Menschen überflügelt, wird es sich wohl ähnlich anfühlen wie jener Moment im Spiel des Programms AlphaGo im März 2016: Damals trat die Maschine gegen den stärksten Go-Spieler der Welt an. In der zweiten Partie machte sie einen seltsamen Zug, der gegen jede Regel zu verstoßen schien. Fachleute vermuteten einen Fehler des Programms. Erst im Verlauf des Spieles zeigte sich die Genialität dieses 37. Zuges: Er erwies sich als weit vorausschauender Siegeszug, den es in der Geschichte des Go-Spiels so noch nie gegeben hatte. Am Ende hatte der Mensch keine Chance, und AlphaGo bekam vom südkoreanischen Go-Verband den höchsten Meisterrang verliehen, mit der Begründung, es habe ""fast göttliche Bereiche"" des Spiels erreicht.

Für Ilya Sutskever zeigt dieses Beispiel, wozu die künstliche Intelligenz (KI) in der Lage ist: Sie habe eine Strategie gefunden, die besser war ""als all das, was die Menschheit gemeinsam über Tausende von Jahren entwickelt hat"", erklärte Sutskever kürzlich der Technology Review. Wenn die KI diese Überlegenheit auch auf anderen Feldern erreiche, sei sie ""superintelligent"". Dann gute Nacht, Menschheit.

Nun ist Ilya Sutskever nicht irgendwer. Er gilt als einer der besten Informatiker der Welt und ist Chief Scientist der Firma OpenAI, bekannt für KI-Produkte wie ChatGPT oder das Illustrationsprogramm Dall-E. Derzeit aber treibt ihn die Sorge um, die KI könnte die Schwelle zur Superintelligenz überschreiten. Auch viele andere KI-Forscher fragen aktuell: Können die Programme stets nur einzelne Facetten menschlicher Fähigkeiten simulieren – wie etwa Go-Spielen –, oder sind sie irgendwann zu eigener Denkfähigkeit in der Lage? Entwickeln sie das, was Fachleute ArtificialGeneral Intelligence nennen, künstliche allgemeine Intelligenz – und falls ja: Wann ist es so weit?

Der Auslöser für die neue Debatte ist jenes Drama, das OpenAI der Welt Mitte November vorführte und in dem Sutskever eine Hauptrolle spielte. Zuerst wurde da OpenAI-Chef Sam Altman mit großem Trara entlassen, nur um wenige Tage später wieder eingestellt zu werden – worauf ein Teil des Verwaltungsrats von OpenAI gehen musste, darunter auch Sutskever. Der Hintergrund dieser Personalrochade sind offenbar unterschiedliche Ansichten über die Risiken der KI: Während Altman die Kommerzialisierung von OpenAI massiv vorantreibt, will Sutskever vorsichtiger vorgehen. Ausschlaggebend für die Entlassung Altmans – so berichtet die Nachrichtenagentur Reuters unter Berufung auf Insider – sei ein Projekt namens Q* gewesen, das möglicherweise bereits die Bedingungen einer menschenähnlichen Intelligenz erfülle. Aus Furcht davor habe Sutskever die Notbremse gezogen – und am Ende gegen Altman verloren.

Das Problem ist: Kaum jemand weiß, was hinter dem geheimnisvollen Q* wirklich steckt. Manche raunen von der Superintelligenz, andere vermuten nur einen Marketingtrick von OpenAI. So oder so haben die Gerüchte die Debatte um die Artificial General Intelligence (AGI) neu entfacht und die Frage virulent werden lassen: Wie würde man eigentlich erkennen, dass die Maschinen so etwas wie menschenähnliche Denkfähigkeit entwickeln?

Wer sich dieser Frage nähert, stellt schnell fest, dass das Konzept der allgemeinen Intelligenz alles andere als klar ist. ""Wenn man 100 KI-Experten bitten würde, zu definieren, was sie mit AGI meinen, würde man wahrscheinlich 100 unterschiedliche Definitionen bekommen"", schrieb kürzlich ein Forscherteam von Google Deep Mind, das versuchte, die Debatte zu sortieren. So glauben einige Experten, dass in den großen Sprachmodellen wie ChatGPT bereits ""Funken"" von allgemeiner Intelligenz aufscheinen; andere sagen voraus, dass es noch ein bis mehrere Jahrzehnte dauere, bis die KI menschenähnliche Intelligenz erreiche, während wieder andere die ganze Diskussion für Panikmache halten und der Überzeugung sind, Maschinen könnten dem Menschen nie das Wasser reichen – weil sie keinen Körper, kein Empfinden und keine Eigeninteressen hätten.

Da könnte man sich nun in philosophische Grundsatzdebatten verstricken. Man kann aber auch pragmatisch fragen: Wie lässt sich maschinelle Intelligenz überhaupt testen – und wann billigen wir ihr Denkfähigkeit zu? Anhand dieser Frage wird schnell deutlich, was die KI kann und was nicht.

Die berühmteste Antwort auf die Test-Frage lieferte 1950 der Informatiker Alan Turing: Ein KI-System sei dann intelligent zu nennen, wenn es im Gespräch nicht mehr von einem Menschen zu unterscheiden ist. Nun erleben wir diese Art von Turing-Test heute ständig im Alltag, wenn wir uns fragen: Chatte ich in der Hotline oder im Servicecenter eines Unternehmens mit einem Menschen oder einem Chatbot? Oft ist das kaum mehr zu beantworten. Auch ein Experiment mit 1,5 Millionen Probanden im Mai ergab, dass viele Menschen den Unterschied zwischen Mensch und Maschine nicht merken. Anders ist es bei Forschenden, die sich professionell mit den Stärken und Schwächen der Sprachmodelle auskennen: Diese lassen sich von der KI nicht so leicht in die Irre führen. 
"
AI,Zeit,2023-12-09,https://www.zeit.de/digital/2023-12/ai-act-eu-gesetz-kuenstliche-intelligenz,AI Act: So will die EU ihre Bürger vor KI schützen | ZEIT ONLINE,
AI,Zeit,2023-12-09,https://www.zeit.de/digital/2023-12/eu-kuenstliche-intelligenz-gesetz-regeln-einigung,KI-Gesetz: EU einigt sich auf Regeln für künstliche Intelligenz | ZEIT ONLINE,"Für den Einsatz von künstlicher Intelligenz (KI) sollen in der EU
künftig enge Regeln gelten. Unterhändler von Europaparlament und
EU-Staaten verständigten sich in Brüssel nach langen
Verhandlungen auf entsprechende Vorgaben. Nach Angaben des EU-Parlaments
handelt es sich um das weltweit erste KI-Gesetz. Im nächsten Schritt müssen das
EU-Parlament und der EU-Rat den Gesetzentwurf verabschieden. 

Außerdem müssen noch technische Details ausgearbeitet
werden. Die Regeln sollen unter anderem die Qualität der für die Entwicklung
der Algorithmen verwendeten Daten gewährleisten und sicherstellen, dass bei der
KI-Entwicklung keine Urheberrechte verletzt werden. Außerdem müssen Entwicklerinnen
und Entwickler sicherstellen, dass Texte, Bilder und Töne, die durch KI
entstanden sind, als solche zu erkennen sind.  

Verschärfte Vorgaben soll es für ""risikoreiche""
Anwendungen geben, etwa bei kritischer Infrastruktur, Sicherheitsbehörden und
Personalverwaltung. Dort sollen eine Kontrolle durch den Menschen über KI, eine
technische Dokumentation und ein System zum Risikomanagement festgeschrieben
werden.

KI-Anwendungen wie
eine automatisierte Gesichtserkennung werden verboten. Es soll aber Ausnahmen geben, etwa zum Schutz der nationalen Sicherheit, zur
Verteidigung und für andere militärische Zwecke. Der Gesetzentwurf beschränkt zudem die Nutzung biometrischer
Identifizierung durch die Strafverfolgungsbehörden.   

Zuletzt sind die Verhandlungen an der Frage der Regulierung
von sogenannten Basismodellen fast gescheitert. Das sind sehr leistungsfähige
KI-Modelle, die mit einem breiten Satz an Daten trainiert wurden. Sie können
die Grundlage für viele andere Anwendungen sein. Dazu zählen etwa Sprachmodelle
wie ChatGPT. 

Deutschland, Frankreich und Italien hatten gefordert, dass
nur konkrete Anwendungen von KI reguliert werden sollten, nicht aber die
Basistechnologie an sich. Sie hatten Befürchtungen geäußert, dass schärfere
Regeln lokale KI-Start-ups wie Aleph Alpha oder Mistral AI in ihrer Entwicklung
behindern. 

Die EU-Regeln sehen unter anderem vor, dass Firmen, die
Grundlagenmodelle entwickeln, bestimmte Informationen veröffentlichen müssen.
Sie sollen beispielsweise dokumentieren, wie das Modell trainiert wurde, interne
Tests durchführen und deren Ergebnisse veröffentlichen.   

Bei besonders leistungsfähigen Modellen sollen noch schärfere
Regeln gelten. Dann müssen Firmen auch externe Tests durchführen, um
Schwachstellen zu ermitteln. Außerdem müssen sie die Risiken ermitteln, die von
ihren Modellen ausgehen könnten. Start-ups wie Aleph Alpha wären von dieser
Regelung aber wohl nicht betroffen. 
"
Artificial Intelligence,Zeit,2023-12-10,https://www.zeit.de/news/2023-12/10/tu-nuernberg-soll-sich-auf-kuenstliche-intelligenz-fokussieren,Hochschulen: TU Nürnberg soll sich auf Künstliche Intelligenz fokussieren | ZEIT ONLINE,"Ein bundesweites Novum: In Nürnberg soll Deutschlands erste rein auf Künstliche Intelligenz (KI) spezialisierte Universität entstehen. Als «Franconian University of Artificial Intelligence» bezeichnete Ministerpräsident Markus Söder (CSU) das Vorhaben in seiner Heimatstadt. Doch was steckt dahinter?

Die Pläne der Staatsregierung sehen vor, dass sich die noch junge Technische Universität Nürnberg künftig voll auf das Thema KI fokussiert. KI-Universität bedeute, dass es künftig keinen einzigen Fachbereich geben werde, in dem KI keine Rolle spielen werde, teilte ein Sprecher des Wissenschaftsministeriums mit. Da der Lehr- und Forschungsbetrieb an der TU Nürnberg noch am Anfang stehe, werde der weitere Aufbau ab sofort unter dem Fokus auf KI erfolgen.

2021 gegründet, hat die Hochschule zu diesem Wintersemester die ersten acht Studierenden aufgenommen. Titel des ersten Masterstudiengangs: KI und Robotics. Ein zweites Masterprogramm zum Wintersemester 2025 soll sich mit menschlicher und künstlicher Intelligenz beschäftigen, wie ein Sprecher der TU mitteilte. Ein Bachelorstudiengang soll erstmals 2027 an den Start gehen.

Die großen gesellschaftlichen Zukunftsfragen in den Blick zu nehmen und auf neue Entwicklungen schnell zu reagieren, liege schon im Gründungsauftrag der TU Nürnberg, teilte Wissenschaftsminister Markus Blume (CSU) mit. Bei der Weiterentwicklung mit Fokus auf KI gehe es nun darum, wie KI bestmöglich genutzt und gleichzeitig die Risiken begrenzt werden könnten. «Auch die Auswirkungen auf die Gesellschaft, auf unsere Interaktionen und Emotionen müssen erforscht werden. Wir wollen KI nach unseren Werten gestalten», fügte Blume hinzu.

Zu den angedachten Themenfeldern in Forschung und Lehre gehören nach Ministeriumsangaben etwa autonome Systeme, die in der Lage sind, flexibel auf eine veränderte Umgebung zu reagieren und durch Erfahrungen zu lernen. Anwendungsbereiche könnten autonomes Fahren bis hin zu moderner Robotik in Industrie, Medizin und Pflege sein.

Auch im Bereich ziviler Sicherheit sollen die Einsatzmöglichkeiten Künstlicher Intelligenz an der TU demnach künftig erforscht werden. Ein weiteres Feld ist den Angaben zufolge das Thema Arbeit und wie sich Beschäftigungsmöglichkeiten durch KI in Zukunft ändern werden.

Mit der TU hat der Freistaat neben der Friedrich-Alexander-Universität Erlangen-Nürnberg und der Technischen Hochschule Nürnberg bereits die dritte Hochschule mit technischem Schwerpunkt in der Region geschaffen - und viel Geld nach Franken fließen lassen.

Zur Frage, ob die TU für die Spezialisierung auf KI mit mehr Geld ausgestattet werde, teilte der Sprecher mit: «Die Universität in Nürnberg wächst gerade auf.» Von Jahr zu Jahr werde bis zum Endausbau kontinuierlich mehr Geld an die TU Nürnberg fließen. Eine konkrete Summe nannte das Ministerium nicht.

© dpa-infocom, dpa:231210-99-242449/2
"
Artificial Intelligence,Zeit,2023-12-10,https://www.zeit.de/2023/52/kuenstliche-intelligenz-test-wissenschaft-menschen,Künstliche Intelligenz: War’s das mit meiner Intelligenz? | ZEIT ONLINE,"Falls die künstliche Intelligenz den Menschen überflügelt, wird es sich wohl ähnlich anfühlen wie jener Moment im Spiel des Programms AlphaGo im März 2016: Damals trat die Maschine gegen den stärksten Go-Spieler der Welt an. In der zweiten Partie machte sie einen seltsamen Zug, der gegen jede Regel zu verstoßen schien. Fachleute vermuteten einen Fehler des Programms. Erst im Verlauf des Spieles zeigte sich die Genialität dieses 37. Zuges: Er erwies sich als weit vorausschauender Siegeszug, den es in der Geschichte des Go-Spiels so noch nie gegeben hatte. Am Ende hatte der Mensch keine Chance, und AlphaGo bekam vom südkoreanischen Go-Verband den höchsten Meisterrang verliehen, mit der Begründung, es habe ""fast göttliche Bereiche"" des Spiels erreicht.

Für Ilya Sutskever zeigt dieses Beispiel, wozu die künstliche Intelligenz (KI) in der Lage ist: Sie habe eine Strategie gefunden, die besser war ""als all das, was die Menschheit gemeinsam über Tausende von Jahren entwickelt hat"", erklärte Sutskever kürzlich der Technology Review. Wenn die KI diese Überlegenheit auch auf anderen Feldern erreiche, sei sie ""superintelligent"". Dann gute Nacht, Menschheit.

Nun ist Ilya Sutskever nicht irgendwer. Er gilt als einer der besten Informatiker der Welt und ist Chief Scientist der Firma OpenAI, bekannt für KI-Produkte wie ChatGPT oder das Illustrationsprogramm Dall-E. Derzeit aber treibt ihn die Sorge um, die KI könnte die Schwelle zur Superintelligenz überschreiten. Auch viele andere KI-Forscher fragen aktuell: Können die Programme stets nur einzelne Facetten menschlicher Fähigkeiten simulieren – wie etwa Go-Spielen –, oder sind sie irgendwann zu eigener Denkfähigkeit in der Lage? Entwickeln sie das, was Fachleute ArtificialGeneral Intelligence nennen, künstliche allgemeine Intelligenz – und falls ja: Wann ist es so weit?

Der Auslöser für die neue Debatte ist jenes Drama, das OpenAI der Welt Mitte November vorführte und in dem Sutskever eine Hauptrolle spielte. Zuerst wurde da OpenAI-Chef Sam Altman mit großem Trara entlassen, nur um wenige Tage später wieder eingestellt zu werden – worauf ein Teil des Verwaltungsrats von OpenAI gehen musste, darunter auch Sutskever. Der Hintergrund dieser Personalrochade sind offenbar unterschiedliche Ansichten über die Risiken der KI: Während Altman die Kommerzialisierung von OpenAI massiv vorantreibt, will Sutskever vorsichtiger vorgehen. Ausschlaggebend für die Entlassung Altmans – so berichtet die Nachrichtenagentur Reuters unter Berufung auf Insider – sei ein Projekt namens Q* gewesen, das möglicherweise bereits die Bedingungen einer menschenähnlichen Intelligenz erfülle. Aus Furcht davor habe Sutskever die Notbremse gezogen – und am Ende gegen Altman verloren.

Das Problem ist: Kaum jemand weiß, was hinter dem geheimnisvollen Q* wirklich steckt. Manche raunen von der Superintelligenz, andere vermuten nur einen Marketingtrick von OpenAI. So oder so haben die Gerüchte die Debatte um die Artificial General Intelligence (AGI) neu entfacht und die Frage virulent werden lassen: Wie würde man eigentlich erkennen, dass die Maschinen so etwas wie menschenähnliche Denkfähigkeit entwickeln?

Wer sich dieser Frage nähert, stellt schnell fest, dass das Konzept der allgemeinen Intelligenz alles andere als klar ist. ""Wenn man 100 KI-Experten bitten würde, zu definieren, was sie mit AGI meinen, würde man wahrscheinlich 100 unterschiedliche Definitionen bekommen"", schrieb kürzlich ein Forscherteam von Google Deep Mind, das versuchte, die Debatte zu sortieren. So glauben einige Experten, dass in den großen Sprachmodellen wie ChatGPT bereits ""Funken"" von allgemeiner Intelligenz aufscheinen; andere sagen voraus, dass es noch ein bis mehrere Jahrzehnte dauere, bis die KI menschenähnliche Intelligenz erreiche, während wieder andere die ganze Diskussion für Panikmache halten und der Überzeugung sind, Maschinen könnten dem Menschen nie das Wasser reichen – weil sie keinen Körper, kein Empfinden und keine Eigeninteressen hätten.

Da könnte man sich nun in philosophische Grundsatzdebatten verstricken. Man kann aber auch pragmatisch fragen: Wie lässt sich maschinelle Intelligenz überhaupt testen – und wann billigen wir ihr Denkfähigkeit zu? Anhand dieser Frage wird schnell deutlich, was die KI kann und was nicht.

Die berühmteste Antwort auf die Test-Frage lieferte 1950 der Informatiker Alan Turing: Ein KI-System sei dann intelligent zu nennen, wenn es im Gespräch nicht mehr von einem Menschen zu unterscheiden ist. Nun erleben wir diese Art von Turing-Test heute ständig im Alltag, wenn wir uns fragen: Chatte ich in der Hotline oder im Servicecenter eines Unternehmens mit einem Menschen oder einem Chatbot? Oft ist das kaum mehr zu beantworten. Auch ein Experiment mit 1,5 Millionen Probanden im Mai ergab, dass viele Menschen den Unterschied zwischen Mensch und Maschine nicht merken. Anders ist es bei Forschenden, die sich professionell mit den Stärken und Schwächen der Sprachmodelle auskennen: Diese lassen sich von der KI nicht so leicht in die Irre führen. 
"
Artificial Intelligence,Zeit,2023-12-09,https://www.zeit.de/digital/2023-12/ai-act-eu-gesetz-kuenstliche-intelligenz,AI Act: So will die EU ihre Bürger vor KI schützen | ZEIT ONLINE,
Artificial Intelligence,Zeit,2023-12-09,https://www.zeit.de/digital/datenschutz/2023-12/kuenstliche-intelligenz-regulierung-eu-bruessel-gesetzentwurf,"Regulierung von KI: Gesichtserkennung im Terrorfall, Datenfilter und Transparenzpflicht | ZEIT ONLINE","
Nach langen Verhandlungen hat sich die EU auf strengere Regeln für künstliche Intelligenz (KI) geeinigt. Es seien weltweit die ersten Regeln für KI, teilten Europaparlament und die EU-Staaten mit. Was umfassen die Vorgaben und wie fallen die Reaktionen aus? Hier die wichtigsten Fragen und Antworten:

Die auf EU-Ebene entworfenen
Vorschriften legen fest, wozu sich die Mitgliedsstaaten verpflichten sollen, um künstliche Intelligenz hinsichtlich ihrer 
potenziellen Risiken und Auswirkungen im Griff zu behalten. Als besonders riskant werden
 KI eingestuft, die Gesundheit, Demokratie, Umwelt oder Sicherheit erheblich beschädigen können. Bestimmte 
Anwendungen werden daher verboten, etwa 
Kategorisierungssysteme, die sensible Merkmale wie zum Beispiel die 
sexuelle Orientierung oder religiöse Überzeugungen verwenden.

Auch das 
ungezielte Auslesen von Bildern aus dem Internet oder aus 
Überwachungsaufnahmen für Gesichtserkennungsdatenbanken soll nicht 
erlaubt sein. Allerdings wird es Ausnahmen für biometrische 
Identifizierungen im öffentlichen Raum in Echtzeit geben, etwa bei der 
Gefahr eines Terroranschlags oder bei der gezielten Suche von Opfern von
 Menschenhandel. Um diesen Punkt wurde intensiv gerungen, das 
EU-Parlament wollte eigentlich ein komplettes Verbot.

Ein weiterer
 Streitpunkt war die Regulierung von sogenannten Basismodellen. Das sind
 sehr leistungsfähige KI-Modelle, die mit einem breiten Satz an Daten 
trainiert wurden. Sie können die Grundlage für viele andere Anwendungen 
sein. Dazu zählt etwa GPT, das Basismodell für den Text-Roboter ChatGPT. Deutschland, Frankreich und Italien hatten 
zuvor gefordert, dass nur konkrete Anwendungen von KI reguliert werden 
sollten, nicht aber die Basistechnologie an sich. Nun einigten sich die
 Unterhändler auf bestimmte Transparenzpflichten für diese Modelle.

Unter künstlicher Intelligenz (KI) versteht man den Versuch, menschliches Lernen und Denken auf den Computer zu übertragen. Ziel ist es, komplexe Aufgaben erledigen zu lassen, die normalerweise menschliche Intelligenz erfordern. Generelle Problemlösungsmaschinen (Artificial General Intelligence) sind trotz aller Fortschritte noch nicht in Sicht. Allerdings finden enger definierte KI-Anwendungen in unserer heutigen Welt bereits breite Verwendung: Dazu zählen etwa automatische Übersetzungen, personalisierte Empfehlungen beim Online-Shopping, Gesichtserkennung am Handy, aber auch intelligente Thermostate oder Navis. Auch die Anwendungen der generativen KI wie der Text-Roboter ChatGPT gehören zu den enger definierten KI-Anwendungen. Ihr simples und zugleich kompliziertes Prinzip: Sie leiten aus Texten ab, mit denen man sie gefüttert hat, welches Wort statistisch auf das vorhergehende folgen könnte und generieren so ihre Antworten.  

KI ist eine Zukunftstechnologie. Experten vermuten, dass die Technologie künftig praktisch alle Aspekte in der Wirtschaft, aber auch im Alltag betreffen könnte und sich zum Beispiel der Arbeitsmarkt dadurch massiv wandeln wird: Manche Jobs werden sich verändern, andere werden vielleicht ganz verschwinden. KI gilt aber auch als Technologie, die Gefahren birgt. So warnte etwa der Chef des ChatGPT-Erfinders OpenAI, Sam Altman, vor Falschinformationen mithilfe künstlicher Intelligenz und sprach sich deshalb für eine Regulierung aus. Fotos oder Videos können durch KI leicht manipuliert und neu generiert werden. Als Problem gilt auch, dass KI teilweise mit verzerrten Datensätzen trainiert wurden und damit Personen diskriminiert werden. Auch der Einsatz in der Kriegsführung wird für möglich gehalten.

EU-Kommissionspräsidentin Ursula von der Leyen begrüßte die Einigung und bezeichnete das Gesetz als ""weltweites Novum"". Svenja Hahn von der FDP zieht eine gemischte Bilanz: ""In 38 Stunden Verhandlung über drei Tage konnten wir eine massive Überregulierung von KI-Innovation verhindern und rechtsstaatliche Prinzipien beim Einsatz von KI in der Strafverfolgung verankern"", sagte die Europaabgeordnete. ""Ich hätte mir mehr Freude an Innovation und noch stärkere Bekenntnisse zu Bürgerrechten gewünscht."" Der rechtspolitische Sprecher der CDU, Axel Voss, teilte mit, er sei nicht davon überzeugt, dass dies der richtige Weg sei, um Europa im Bereich der KI wettbewerbsfähig zu machen. ""Innovation wird immer noch anderswo stattfinden, hier haben wir als Europäische Union unsere Chance verpasst.""

Die europäische Verbraucherschutzorganisation Beuc kritisierte, dass sich die EU zu sehr auf den guten Willen der Unternehmen zur Selbstregulierung verlasse. ""So werden beispielsweise virtuelle Assistenten oder KI-gesteuerte Spielzeuge nicht ausreichend reguliert, da sie nicht als Hochrisikosysteme gelten. Auch Systeme wie ChatGPT oder Bard werden nicht die notwendigen Leitplanken erhalten, damit die Verbraucher ihnen vertrauen können"", hieß es.

Zunächst erfordert das Vorhaben, dass die EU-Staaten und das Europaparlament noch zustimmen. Das gilt aber als Formsache.
"
Artificial Intelligence,Zeit,2023-12-09,https://www.zeit.de/digital/2023-12/eu-kuenstliche-intelligenz-gesetz-regeln-einigung,KI-Gesetz: EU einigt sich auf Regeln für künstliche Intelligenz | ZEIT ONLINE,"Für den Einsatz von künstlicher Intelligenz (KI) sollen in der EU
künftig enge Regeln gelten. Unterhändler von Europaparlament und
EU-Staaten verständigten sich in Brüssel nach langen
Verhandlungen auf entsprechende Vorgaben. Nach Angaben des EU-Parlaments
handelt es sich um das weltweit erste KI-Gesetz. Im nächsten Schritt müssen das
EU-Parlament und der EU-Rat den Gesetzentwurf verabschieden. 

Außerdem müssen noch technische Details ausgearbeitet
werden. Die Regeln sollen unter anderem die Qualität der für die Entwicklung
der Algorithmen verwendeten Daten gewährleisten und sicherstellen, dass bei der
KI-Entwicklung keine Urheberrechte verletzt werden. Außerdem müssen Entwicklerinnen
und Entwickler sicherstellen, dass Texte, Bilder und Töne, die durch KI
entstanden sind, als solche zu erkennen sind.  

Verschärfte Vorgaben soll es für ""risikoreiche""
Anwendungen geben, etwa bei kritischer Infrastruktur, Sicherheitsbehörden und
Personalverwaltung. Dort sollen eine Kontrolle durch den Menschen über KI, eine
technische Dokumentation und ein System zum Risikomanagement festgeschrieben
werden.

KI-Anwendungen wie
eine automatisierte Gesichtserkennung werden verboten. Es soll aber Ausnahmen geben, etwa zum Schutz der nationalen Sicherheit, zur
Verteidigung und für andere militärische Zwecke. Der Gesetzentwurf beschränkt zudem die Nutzung biometrischer
Identifizierung durch die Strafverfolgungsbehörden.   

Zuletzt sind die Verhandlungen an der Frage der Regulierung
von sogenannten Basismodellen fast gescheitert. Das sind sehr leistungsfähige
KI-Modelle, die mit einem breiten Satz an Daten trainiert wurden. Sie können
die Grundlage für viele andere Anwendungen sein. Dazu zählen etwa Sprachmodelle
wie ChatGPT. 

Deutschland, Frankreich und Italien hatten gefordert, dass
nur konkrete Anwendungen von KI reguliert werden sollten, nicht aber die
Basistechnologie an sich. Sie hatten Befürchtungen geäußert, dass schärfere
Regeln lokale KI-Start-ups wie Aleph Alpha oder Mistral AI in ihrer Entwicklung
behindern. 

Die EU-Regeln sehen unter anderem vor, dass Firmen, die
Grundlagenmodelle entwickeln, bestimmte Informationen veröffentlichen müssen.
Sie sollen beispielsweise dokumentieren, wie das Modell trainiert wurde, interne
Tests durchführen und deren Ergebnisse veröffentlichen.   

Bei besonders leistungsfähigen Modellen sollen noch schärfere
Regeln gelten. Dann müssen Firmen auch externe Tests durchführen, um
Schwachstellen zu ermitteln. Außerdem müssen sie die Risiken ermitteln, die von
ihren Modellen ausgehen könnten. Start-ups wie Aleph Alpha wären von dieser
Regelung aber wohl nicht betroffen. 
"
KI,Zeit,2023-12-12,https://www.zeit.de/news/2023-12/12/volker-tuerk-ki-gefahr-fuer-grundlagen-des-menschseins,Menschenrechte: Volker Türk: KI Gefahr für Grundlagen des Menschseins | ZEIT ONLINE,"Künstliche Intelligenz (KI) und neue Technologien können nach Angaben des UN-Hochkommissars für Menschenrechte, Volker Türk, zur Gefahr für die Menschenwürde werden. «Die digitale Transformation hat riesiges Potenzial», sagte Türk in Genf. «Aber seien wir ehrlich: Von künstlicher Intelligenz bis hin zu Neurotechnologie, Cyberkriminalität, Überwachung und Biowaffen sind wir mit einer Welt konfrontiert, in der die Grundlagen des Menschseins - die Menschenwürde und das menschliche Handeln - in Gefahr sind». 

Türk sprach am zweiten Tag eines Forums zur Würdigung der Allgemeinen Menschenrechtserklärung. Sie wurde vor 75 Jahren, am 10. Dezember 1948, angenommen. Sie gilt als Meilenstein der Menschheitsgeschichte, weil sie erstmals weltweit unter anderem das Recht jedes Menschen auf Freiheit, Sicherheit und Schutz vor Staatswillkür festschreibt. 

«Wie können wir unsere Menschlichkeit und unsere Rechte in diesem neuen Universum schützen?», sagte Türk unter Bezug auf die digitale Transformation. «Das ist die Frage, die wir uns stellen müssen.» 

Türk rief Regierungen auf, die Menschenrechte in einer dunkeln Stunde der Geschichte mit Kriegen und Konflikten in vielen Weltregionen erneut zu bekräftigen und für die Einhaltung zu sorgen. «Wir sind hier, um die Grundlage für Hoffnung wieder aufzubauen», sagte er. An den Podiumsdiskussionen nahmen Staats- und Regierungsspitzen teil, darunter die Präsidenten Polens und Senegals, Andrzej Duda und Macky Sall, und die Präsidentin Griechenlands, Katerina Sakellaropoulou.

© dpa-infocom, dpa:231212-99-267280/2
"
KI,Zeit,2023-12-10,https://www.zeit.de/news/2023-12/10/tu-nuernberg-soll-sich-auf-kuenstliche-intelligenz-fokussieren,Hochschulen: TU Nürnberg soll sich auf Künstliche Intelligenz fokussieren | ZEIT ONLINE,"Ein bundesweites Novum: In Nürnberg soll Deutschlands erste rein auf Künstliche Intelligenz (KI) spezialisierte Universität entstehen. Als «Franconian University of Artificial Intelligence» bezeichnete Ministerpräsident Markus Söder (CSU) das Vorhaben in seiner Heimatstadt. Doch was steckt dahinter?

Die Pläne der Staatsregierung sehen vor, dass sich die noch junge Technische Universität Nürnberg künftig voll auf das Thema KI fokussiert. KI-Universität bedeute, dass es künftig keinen einzigen Fachbereich geben werde, in dem KI keine Rolle spielen werde, teilte ein Sprecher des Wissenschaftsministeriums mit. Da der Lehr- und Forschungsbetrieb an der TU Nürnberg noch am Anfang stehe, werde der weitere Aufbau ab sofort unter dem Fokus auf KI erfolgen.

2021 gegründet, hat die Hochschule zu diesem Wintersemester die ersten acht Studierenden aufgenommen. Titel des ersten Masterstudiengangs: KI und Robotics. Ein zweites Masterprogramm zum Wintersemester 2025 soll sich mit menschlicher und künstlicher Intelligenz beschäftigen, wie ein Sprecher der TU mitteilte. Ein Bachelorstudiengang soll erstmals 2027 an den Start gehen.

Die großen gesellschaftlichen Zukunftsfragen in den Blick zu nehmen und auf neue Entwicklungen schnell zu reagieren, liege schon im Gründungsauftrag der TU Nürnberg, teilte Wissenschaftsminister Markus Blume (CSU) mit. Bei der Weiterentwicklung mit Fokus auf KI gehe es nun darum, wie KI bestmöglich genutzt und gleichzeitig die Risiken begrenzt werden könnten. «Auch die Auswirkungen auf die Gesellschaft, auf unsere Interaktionen und Emotionen müssen erforscht werden. Wir wollen KI nach unseren Werten gestalten», fügte Blume hinzu.

Zu den angedachten Themenfeldern in Forschung und Lehre gehören nach Ministeriumsangaben etwa autonome Systeme, die in der Lage sind, flexibel auf eine veränderte Umgebung zu reagieren und durch Erfahrungen zu lernen. Anwendungsbereiche könnten autonomes Fahren bis hin zu moderner Robotik in Industrie, Medizin und Pflege sein.

Auch im Bereich ziviler Sicherheit sollen die Einsatzmöglichkeiten Künstlicher Intelligenz an der TU demnach künftig erforscht werden. Ein weiteres Feld ist den Angaben zufolge das Thema Arbeit und wie sich Beschäftigungsmöglichkeiten durch KI in Zukunft ändern werden.

Mit der TU hat der Freistaat neben der Friedrich-Alexander-Universität Erlangen-Nürnberg und der Technischen Hochschule Nürnberg bereits die dritte Hochschule mit technischem Schwerpunkt in der Region geschaffen - und viel Geld nach Franken fließen lassen.

Zur Frage, ob die TU für die Spezialisierung auf KI mit mehr Geld ausgestattet werde, teilte der Sprecher mit: «Die Universität in Nürnberg wächst gerade auf.» Von Jahr zu Jahr werde bis zum Endausbau kontinuierlich mehr Geld an die TU Nürnberg fließen. Eine konkrete Summe nannte das Ministerium nicht.

© dpa-infocom, dpa:231210-99-242449/2
"
KI,Zeit,2023-12-10,https://www.zeit.de/news/2023-12/08/kuenstliche-intelligenz-soll-in-eu-staerker-geregelt-werden,Künstliche Intelligenz: Zu streng oder zu schwach? EU-Regelwerk für KI spaltet | ZEIT ONLINE,"Die Europäische Union (EU) hat sich kurz vor dem Wochenende auf Regeln für die Nutzung Künstlicher Intelligenz (KI) verständigt. Das Europaparlament und die EU-Staaten stellten heraus: Es sind weltweit die ersten Regeln für KI. Jedoch kommt von zwei Seiten Kritik. Die einen halten die Regeln für zu streng, die anderen für zu lasch. Die Wirtschaft befürchtet, die künftige EU-Verordnung werde Innovationen hemmen. Aus Sicht von Verbraucherschützern werden die Risiken mancher Anwendungen nicht ernst genug genommen.

Europa droht nach Ansicht des Bundesverbands der Deutschen Industrie (BDI) bei der Schlüsseltechnologie KI nun ins Hintertreffen zu geraten. «Mit der umfassenden Regulierung von KI-Basismodellen und KI-Anwendungen gefährdet der AI Act die Wettbewerbs- und Innovationsfähigkeit sowohl auf Hersteller- als auch auf Anwenderseite», sagte BDI-Geschäftsführungsmitglied Iris Plöger. Die Regulierung basiere auf unausgereiften Kriterien, die den Unternehmen weniger statt mehr Rechtssicherheit brächten.

Der Technikbranchenverband Bitkom sprach von einem «politischen Schaufenster-Erfolg zu Lasten von Wirtschaft und Gesellschaft». Der erzielte Kompromiss greife tief in die Technologie ein.«Die EU bindet damit den Unternehmen einen regulatorischen Klotz ans Bein. Das Risiko ist groß, dass europäische Unternehmen durch nicht praxistaugliche Vorhaben der rasanten technologischen Entwicklung künftig nicht folgen können», sagte Bitkom-Hauptgeschäftsführer Bernhard Rohleder.

Die europäische Verbraucherschutzorganisation Beuc kritisierte dagegen, dass sich die EU zu sehr auf den guten Willen der Unternehmen zur Selbstregulierung verlasse. «So werden beispielsweise virtuelle Assistenten oder KI-gesteuerte Spielzeuge nicht ausreichend reguliert, da sie nicht als Hochrisikosysteme gelten. Auch Systeme wie ChatGPT oder Bard werden nicht die notwendigen Leitplanken erhalten, damit die Verbraucher ihnen vertrauen können», hieß es.

Die deutsche Verbraucherschutzministerin Steffi Lemke (Grüne) sieht in der KI-Verordnung hingegen einen Schutz für Verbraucher vor den Risiken der neuen Technologie. «In den Verhandlungen haben wir uns besonders dafür eingesetzt, dass KI-Systeme transparent, nachvollziehbar und überprüfbar gestaltet werden. So müssen nun künftig Unternehmen, die den Einsatz von KI-Technologien anbieten, Informationen über die Funktionsweise ihrer Systeme bereitstellen und KI-gestützte Entscheidungen erläutern», berichtete Lemke am Samstag.  Bei Verstößen könnten Verbraucherverbände mit Verbandsklagen gerichtlich dagegen vorgehen. 

Die vorgelegten Vorschriften legen Verpflichtungen für KI auf Grundlage ihrer potenziellen Risiken und Auswirkungen fest. Als besonders riskant werden KI eingestuft, die ein erhebliches Schadenspotenzial etwa für Gesundheit, Demokratie, Umwelt oder Sicherheit haben. 

Bestimmte Anwendungen werden komplett verboten, etwa biometrische Kategorisierungssysteme, die sensible Merkmale wie zum Beispiel die sexuelle Orientierung oder religiöse Überzeugungen verwenden. Auch das ungezielte Auslesen von Bildern aus dem Internet oder aus Überwachungsaufnahmen für Gesichtserkennungsdatenbanken soll nicht erlaubt sein. Allerdings wird es Ausnahmen für biometrische Identifizierungen im öffentlichen Raum in Echtzeit geben, etwa bei der Gefahr eines Terroranschlags oder bei der gezielten Suche von Opfern von Menschenhandel. Um diesen Punkt wurde intensiv gerungen, das EU-Parlament wollte eigentlich ein komplettes Verbot.

Ein weiterer Streitpunkt war die Regulierung von sogenannten Basismodellen. Das sind sehr leistungsfähige KI-Modelle, die mit einem breiten Satz an Daten trainiert wurden. Sie können die Grundlage für viele andere Anwendungen sein. Dazu zählt etwa GPT. Deutschland, Frankreich und Italien hatten zuvor gefordert, dass nur konkrete Anwendungen von KI reguliert werden sollten, nicht aber die Basis-Technologie an sich. Nun einigten sich die Unterhändler auf bestimmte Transparenzpflichten für diese Modelle. 

Der Bundesminister für Digitales, Volker Wissing (FDP), enthielt sich einer endgültigen Bewertung. Einerseits sei verhindert worden, dass bestimmte Systeme in den Hochrisikobereich fallen, andererseits müsse das Regelwerk Innovationen ermöglichen und verhältnismäßig sein, sagte er am Samstag. «Ob dies gelungen ist, werden wir uns in den nächsten Tagen sehr genau anschauen.»

© dpa-infocom, dpa:231208-99-232521/5
"
KI,Zeit,2023-12-10,https://www.zeit.de/2023/52/kuenstliche-intelligenz-test-wissenschaft-menschen,Künstliche Intelligenz: War’s das mit meiner Intelligenz? | ZEIT ONLINE,"Falls die künstliche Intelligenz den Menschen überflügelt, wird es sich wohl ähnlich anfühlen wie jener Moment im Spiel des Programms AlphaGo im März 2016: Damals trat die Maschine gegen den stärksten Go-Spieler der Welt an. In der zweiten Partie machte sie einen seltsamen Zug, der gegen jede Regel zu verstoßen schien. Fachleute vermuteten einen Fehler des Programms. Erst im Verlauf des Spieles zeigte sich die Genialität dieses 37. Zuges: Er erwies sich als weit vorausschauender Siegeszug, den es in der Geschichte des Go-Spiels so noch nie gegeben hatte. Am Ende hatte der Mensch keine Chance, und AlphaGo bekam vom südkoreanischen Go-Verband den höchsten Meisterrang verliehen, mit der Begründung, es habe ""fast göttliche Bereiche"" des Spiels erreicht.

Für Ilya Sutskever zeigt dieses Beispiel, wozu die künstliche Intelligenz (KI) in der Lage ist: Sie habe eine Strategie gefunden, die besser war ""als all das, was die Menschheit gemeinsam über Tausende von Jahren entwickelt hat"", erklärte Sutskever kürzlich der Technology Review. Wenn die KI diese Überlegenheit auch auf anderen Feldern erreiche, sei sie ""superintelligent"". Dann gute Nacht, Menschheit.

Nun ist Ilya Sutskever nicht irgendwer. Er gilt als einer der besten Informatiker der Welt und ist Chief Scientist der Firma OpenAI, bekannt für KI-Produkte wie ChatGPT oder das Illustrationsprogramm Dall-E. Derzeit aber treibt ihn die Sorge um, die KI könnte die Schwelle zur Superintelligenz überschreiten. Auch viele andere KI-Forscher fragen aktuell: Können die Programme stets nur einzelne Facetten menschlicher Fähigkeiten simulieren – wie etwa Go-Spielen –, oder sind sie irgendwann zu eigener Denkfähigkeit in der Lage? Entwickeln sie das, was Fachleute ArtificialGeneral Intelligence nennen, künstliche allgemeine Intelligenz – und falls ja: Wann ist es so weit?

Der Auslöser für die neue Debatte ist jenes Drama, das OpenAI der Welt Mitte November vorführte und in dem Sutskever eine Hauptrolle spielte. Zuerst wurde da OpenAI-Chef Sam Altman mit großem Trara entlassen, nur um wenige Tage später wieder eingestellt zu werden – worauf ein Teil des Verwaltungsrats von OpenAI gehen musste, darunter auch Sutskever. Der Hintergrund dieser Personalrochade sind offenbar unterschiedliche Ansichten über die Risiken der KI: Während Altman die Kommerzialisierung von OpenAI massiv vorantreibt, will Sutskever vorsichtiger vorgehen. Ausschlaggebend für die Entlassung Altmans – so berichtet die Nachrichtenagentur Reuters unter Berufung auf Insider – sei ein Projekt namens Q* gewesen, das möglicherweise bereits die Bedingungen einer menschenähnlichen Intelligenz erfülle. Aus Furcht davor habe Sutskever die Notbremse gezogen – und am Ende gegen Altman verloren.

Das Problem ist: Kaum jemand weiß, was hinter dem geheimnisvollen Q* wirklich steckt. Manche raunen von der Superintelligenz, andere vermuten nur einen Marketingtrick von OpenAI. So oder so haben die Gerüchte die Debatte um die Artificial General Intelligence (AGI) neu entfacht und die Frage virulent werden lassen: Wie würde man eigentlich erkennen, dass die Maschinen so etwas wie menschenähnliche Denkfähigkeit entwickeln?

Wer sich dieser Frage nähert, stellt schnell fest, dass das Konzept der allgemeinen Intelligenz alles andere als klar ist. ""Wenn man 100 KI-Experten bitten würde, zu definieren, was sie mit AGI meinen, würde man wahrscheinlich 100 unterschiedliche Definitionen bekommen"", schrieb kürzlich ein Forscherteam von Google Deep Mind, das versuchte, die Debatte zu sortieren. So glauben einige Experten, dass in den großen Sprachmodellen wie ChatGPT bereits ""Funken"" von allgemeiner Intelligenz aufscheinen; andere sagen voraus, dass es noch ein bis mehrere Jahrzehnte dauere, bis die KI menschenähnliche Intelligenz erreiche, während wieder andere die ganze Diskussion für Panikmache halten und der Überzeugung sind, Maschinen könnten dem Menschen nie das Wasser reichen – weil sie keinen Körper, kein Empfinden und keine Eigeninteressen hätten.

Da könnte man sich nun in philosophische Grundsatzdebatten verstricken. Man kann aber auch pragmatisch fragen: Wie lässt sich maschinelle Intelligenz überhaupt testen – und wann billigen wir ihr Denkfähigkeit zu? Anhand dieser Frage wird schnell deutlich, was die KI kann und was nicht.

Die berühmteste Antwort auf die Test-Frage lieferte 1950 der Informatiker Alan Turing: Ein KI-System sei dann intelligent zu nennen, wenn es im Gespräch nicht mehr von einem Menschen zu unterscheiden ist. Nun erleben wir diese Art von Turing-Test heute ständig im Alltag, wenn wir uns fragen: Chatte ich in der Hotline oder im Servicecenter eines Unternehmens mit einem Menschen oder einem Chatbot? Oft ist das kaum mehr zu beantworten. Auch ein Experiment mit 1,5 Millionen Probanden im Mai ergab, dass viele Menschen den Unterschied zwischen Mensch und Maschine nicht merken. Anders ist es bei Forschenden, die sich professionell mit den Stärken und Schwächen der Sprachmodelle auskennen: Diese lassen sich von der KI nicht so leicht in die Irre führen. 
"
KI,Zeit,2023-12-09,https://www.zeit.de/digital/2023-12/ai-act-eu-gesetz-kuenstliche-intelligenz,AI Act: So will die EU ihre Bürger vor KI schützen | ZEIT ONLINE,
KI,Zeit,2023-12-09,https://www.zeit.de/digital/datenschutz/2023-12/kuenstliche-intelligenz-regulierung-eu-bruessel-gesetzentwurf,"Regulierung von KI: Gesichtserkennung im Terrorfall, Datenfilter und Transparenzpflicht | ZEIT ONLINE","
Nach langen Verhandlungen hat sich die EU auf strengere Regeln für künstliche Intelligenz (KI) geeinigt. Es seien weltweit die ersten Regeln für KI, teilten Europaparlament und die EU-Staaten mit. Was umfassen die Vorgaben und wie fallen die Reaktionen aus? Hier die wichtigsten Fragen und Antworten:

Die auf EU-Ebene entworfenen
Vorschriften legen fest, wozu sich die Mitgliedsstaaten verpflichten sollen, um künstliche Intelligenz hinsichtlich ihrer 
potenziellen Risiken und Auswirkungen im Griff zu behalten. Als besonders riskant werden
 KI eingestuft, die Gesundheit, Demokratie, Umwelt oder Sicherheit erheblich beschädigen können. Bestimmte 
Anwendungen werden daher verboten, etwa 
Kategorisierungssysteme, die sensible Merkmale wie zum Beispiel die 
sexuelle Orientierung oder religiöse Überzeugungen verwenden.

Auch das 
ungezielte Auslesen von Bildern aus dem Internet oder aus 
Überwachungsaufnahmen für Gesichtserkennungsdatenbanken soll nicht 
erlaubt sein. Allerdings wird es Ausnahmen für biometrische 
Identifizierungen im öffentlichen Raum in Echtzeit geben, etwa bei der 
Gefahr eines Terroranschlags oder bei der gezielten Suche von Opfern von
 Menschenhandel. Um diesen Punkt wurde intensiv gerungen, das 
EU-Parlament wollte eigentlich ein komplettes Verbot.

Ein weiterer
 Streitpunkt war die Regulierung von sogenannten Basismodellen. Das sind
 sehr leistungsfähige KI-Modelle, die mit einem breiten Satz an Daten 
trainiert wurden. Sie können die Grundlage für viele andere Anwendungen 
sein. Dazu zählt etwa GPT, das Basismodell für den Text-Roboter ChatGPT. Deutschland, Frankreich und Italien hatten 
zuvor gefordert, dass nur konkrete Anwendungen von KI reguliert werden 
sollten, nicht aber die Basistechnologie an sich. Nun einigten sich die
 Unterhändler auf bestimmte Transparenzpflichten für diese Modelle.

Unter künstlicher Intelligenz (KI) versteht man den Versuch, menschliches Lernen und Denken auf den Computer zu übertragen. Ziel ist es, komplexe Aufgaben erledigen zu lassen, die normalerweise menschliche Intelligenz erfordern. Generelle Problemlösungsmaschinen (Artificial General Intelligence) sind trotz aller Fortschritte noch nicht in Sicht. Allerdings finden enger definierte KI-Anwendungen in unserer heutigen Welt bereits breite Verwendung: Dazu zählen etwa automatische Übersetzungen, personalisierte Empfehlungen beim Online-Shopping, Gesichtserkennung am Handy, aber auch intelligente Thermostate oder Navis. Auch die Anwendungen der generativen KI wie der Text-Roboter ChatGPT gehören zu den enger definierten KI-Anwendungen. Ihr simples und zugleich kompliziertes Prinzip: Sie leiten aus Texten ab, mit denen man sie gefüttert hat, welches Wort statistisch auf das vorhergehende folgen könnte und generieren so ihre Antworten.  

KI ist eine Zukunftstechnologie. Experten vermuten, dass die Technologie künftig praktisch alle Aspekte in der Wirtschaft, aber auch im Alltag betreffen könnte und sich zum Beispiel der Arbeitsmarkt dadurch massiv wandeln wird: Manche Jobs werden sich verändern, andere werden vielleicht ganz verschwinden. KI gilt aber auch als Technologie, die Gefahren birgt. So warnte etwa der Chef des ChatGPT-Erfinders OpenAI, Sam Altman, vor Falschinformationen mithilfe künstlicher Intelligenz und sprach sich deshalb für eine Regulierung aus. Fotos oder Videos können durch KI leicht manipuliert und neu generiert werden. Als Problem gilt auch, dass KI teilweise mit verzerrten Datensätzen trainiert wurden und damit Personen diskriminiert werden. Auch der Einsatz in der Kriegsführung wird für möglich gehalten.

EU-Kommissionspräsidentin Ursula von der Leyen begrüßte die Einigung und bezeichnete das Gesetz als ""weltweites Novum"". Svenja Hahn von der FDP zieht eine gemischte Bilanz: ""In 38 Stunden Verhandlung über drei Tage konnten wir eine massive Überregulierung von KI-Innovation verhindern und rechtsstaatliche Prinzipien beim Einsatz von KI in der Strafverfolgung verankern"", sagte die Europaabgeordnete. ""Ich hätte mir mehr Freude an Innovation und noch stärkere Bekenntnisse zu Bürgerrechten gewünscht."" Der rechtspolitische Sprecher der CDU, Axel Voss, teilte mit, er sei nicht davon überzeugt, dass dies der richtige Weg sei, um Europa im Bereich der KI wettbewerbsfähig zu machen. ""Innovation wird immer noch anderswo stattfinden, hier haben wir als Europäische Union unsere Chance verpasst.""

Die europäische Verbraucherschutzorganisation Beuc kritisierte, dass sich die EU zu sehr auf den guten Willen der Unternehmen zur Selbstregulierung verlasse. ""So werden beispielsweise virtuelle Assistenten oder KI-gesteuerte Spielzeuge nicht ausreichend reguliert, da sie nicht als Hochrisikosysteme gelten. Auch Systeme wie ChatGPT oder Bard werden nicht die notwendigen Leitplanken erhalten, damit die Verbraucher ihnen vertrauen können"", hieß es.

Zunächst erfordert das Vorhaben, dass die EU-Staaten und das Europaparlament noch zustimmen. Das gilt aber als Formsache.
"
KI,Zeit,2023-12-09,https://www.zeit.de/digital/2023-12/eu-kuenstliche-intelligenz-gesetz-regeln-einigung,KI-Gesetz: EU einigt sich auf Regeln für künstliche Intelligenz | ZEIT ONLINE,"Für den Einsatz von künstlicher Intelligenz (KI) sollen in der EU
künftig enge Regeln gelten. Unterhändler von Europaparlament und
EU-Staaten verständigten sich in Brüssel nach langen
Verhandlungen auf entsprechende Vorgaben. Nach Angaben des EU-Parlaments
handelt es sich um das weltweit erste KI-Gesetz. Im nächsten Schritt müssen das
EU-Parlament und der EU-Rat den Gesetzentwurf verabschieden. 

Außerdem müssen noch technische Details ausgearbeitet
werden. Die Regeln sollen unter anderem die Qualität der für die Entwicklung
der Algorithmen verwendeten Daten gewährleisten und sicherstellen, dass bei der
KI-Entwicklung keine Urheberrechte verletzt werden. Außerdem müssen Entwicklerinnen
und Entwickler sicherstellen, dass Texte, Bilder und Töne, die durch KI
entstanden sind, als solche zu erkennen sind.  

Verschärfte Vorgaben soll es für ""risikoreiche""
Anwendungen geben, etwa bei kritischer Infrastruktur, Sicherheitsbehörden und
Personalverwaltung. Dort sollen eine Kontrolle durch den Menschen über KI, eine
technische Dokumentation und ein System zum Risikomanagement festgeschrieben
werden.

KI-Anwendungen wie
eine automatisierte Gesichtserkennung werden verboten. Es soll aber Ausnahmen geben, etwa zum Schutz der nationalen Sicherheit, zur
Verteidigung und für andere militärische Zwecke. Der Gesetzentwurf beschränkt zudem die Nutzung biometrischer
Identifizierung durch die Strafverfolgungsbehörden.   

Zuletzt sind die Verhandlungen an der Frage der Regulierung
von sogenannten Basismodellen fast gescheitert. Das sind sehr leistungsfähige
KI-Modelle, die mit einem breiten Satz an Daten trainiert wurden. Sie können
die Grundlage für viele andere Anwendungen sein. Dazu zählen etwa Sprachmodelle
wie ChatGPT. 

Deutschland, Frankreich und Italien hatten gefordert, dass
nur konkrete Anwendungen von KI reguliert werden sollten, nicht aber die
Basistechnologie an sich. Sie hatten Befürchtungen geäußert, dass schärfere
Regeln lokale KI-Start-ups wie Aleph Alpha oder Mistral AI in ihrer Entwicklung
behindern. 

Die EU-Regeln sehen unter anderem vor, dass Firmen, die
Grundlagenmodelle entwickeln, bestimmte Informationen veröffentlichen müssen.
Sie sollen beispielsweise dokumentieren, wie das Modell trainiert wurde, interne
Tests durchführen und deren Ergebnisse veröffentlichen.   

Bei besonders leistungsfähigen Modellen sollen noch schärfere
Regeln gelten. Dann müssen Firmen auch externe Tests durchführen, um
Schwachstellen zu ermitteln. Außerdem müssen sie die Risiken ermitteln, die von
ihren Modellen ausgehen könnten. Start-ups wie Aleph Alpha wären von dieser
Regelung aber wohl nicht betroffen. 
"
Künstliche Intelligenz,Zeit,2023-12-06,https://www.zeit.de/news/2023-12/06/un-hochkommissar-besorgt-ueber-kuenstliche-intelligenz,Internet: UN-Hochkommissar besorgt über Künstliche Intelligenz | ZEIT ONLINE,"Der UN-Hochkommissar für Menschenrechte, Volker Türk, hat Regierungen in aller Welt im Zeitalter Künstlicher Intelligenz (KI) zu besonderer Wachsamkeit vor Wahlen aufgerufen.

Im kommenden Jahr dürften in mehr als 70 Ländern Wahlen stattfinden, in denen mehr als die Hälfte der Weltbevölkerung wohne, sagte Türk bei einer Pressekonferenz in Genf. Sie seien unter den ersten, die im Zeitalter von KI-Anwendungen für die breite Masse stattfänden. Das berge neue Risiken für eine neue Art von Propaganda und Desinformation. Regierungen und Technologieunternehmen müssten in der Lage sein, auf gefährliche Online-Inhalte zu reagieren.

Er rief Regierungen auf, im Wahlkampf das Menschenrecht auf freie Meinungsäußerung und friedliche Versammlungen zu respektieren. Wahlkämpfe seien fruchtbarer Boden für Extremismus und das Schüren von Angst, oft mit Hassreden gegen Gegner. «Ich rufe Politiker und andere Führungsfiguren auf, keine Ängste gegen «die anderen» zu schüren, keine Spaltungen zu verursachen und Unterschiede nicht zu instrumentalisieren, um Stimmen zu gewinnen.»

© dpa-infocom, dpa:231206-99-201060/3
"
Künstliche Intelligenz,Zeit,2023-12-06,https://www.zeit.de/digital/2023-12/google-gemini-ki-openai-chatgpt-konkurrenz,Google Gemini: Ist die neue Google-KI besser als ChatGPT? | ZEIT ONLINE,"Der Computer erkennt, wie aus ein paar Strichen ein gezeichneter Vogel wird. Eine gewellte Linie kommt dazu, aha, Wasser, der Vogel könnte eine Ente sein. Sind Enten blau? Normalerweise nicht. Ganz schön schlau, diese neue künstliche Intelligenz, die Google da in einem Demovideo präsentiert. Nur: Der Marktführer ChatGPT kann das auch – und zwar seit Monaten.

Google hat am Mittwoch eine Reihe von Produkten vorgestellt, mit denen der Software-Konzern im Rennen um die beste künstliche Intelligenz aufholen will. Von einer der ""größten wissenschaftlichen und technischen Anstrengungen, die wir als Unternehmen unternommen haben"", schreibt der Google-CEO Sundar Pichai.

Das leistungsfähigste der neuen Modelle, Gemini Ultra, soll dem bisher als am fortschrittlichsten geltenden Sprachmodell GPT-4 der Firma OpenAI in einigen Bereichen überlegen sein. Allzu weit ist der Vorsprung, wenn überhaupt, aber offenbar nicht.

Insgesamt präsentierte Google drei Sprachmodelle, die alle Gemini im Namen tragen, wie das Sternzeichen Zwillinge auf Englisch: Gemini Pro ist laut Google ab sofort verfügbar und treibt nun den Chatbot Bard an. Gemini Nano ist eine Variante, die auf Mobilgeräten laufen soll. Auf dem Google Pixel 8 soll die Technik zum Beispiel kurze Zusammenfassungen von Sprachaufnahmen schreiben können. Das dritte und beste Modell, Gemini Ultra, ist noch nicht öffentlich verfügbar. Es soll Anfang kommenden Jahres folgen, in einer erweiterten Version des Chatbots Bard.

Produktseitig erinnert viel an den Chatbot ChatGPT, mit dem die Firma OpenAI vor gut einem Jahr einen KI-Hype ausgelöst hat. Auch OpenAI bietet eine Basisversion seines Chatbots an und eine erweiterte Version. Bei ChatGPT kostet diese Pro-Variante knapp 24 Dollar im Monat. Ob Googles ""Bard Advanced"" Geld kostet, steht nicht in den Ankündigungen. Die schiere Existenz einer solchen Trennung legt es aber nahe – denn wenn beide Varianten kostenlos wären, wer sollte sich für die Basisversion entscheiden?

Dass Google sich im KI-Wettrennen nicht abhängen lassen will, wurde spätestens klar, als der Konzern im März den Chatbot Bard vorstellte. Zuvor hatte der Konzern Microsoft sich durch eine Milliardeninvestition die Technologie des Unternehmens OpenAI gesichert. Dessen Sprachmodell GPT-4 galt bisher als führend im Bereich KI.

Für die Frage, ob das beste Gemini mit dem besten GPT mithalten kann, muss man sich derzeit noch auf Angaben von Google verlassen. Auf seiner Website zeigt Google, was Gemini Ultra können soll. Es ist ein multimodales Modell, das heißt, es kann nicht nur Text verarbeiten, sondern etwa auch Sprache und Bilder. Deshalb erkennt die KI in einigen geschwungenen Filzstiftstrichen eine Ente, und deshalb kann das System kurz darauf die Aussprache des Wortes für Ente auf Chinesisch korrigieren.

Solche Funktionen, Bilder zu verarbeiten oder Sprache auszugeben, hat auch ChatGPT. Seit einigen Wochen können alle Nutzerinnen und Nutzer des Bezahlangebots darauf zugreifen. 

Im Test von ZEIT ONLINE hat ChatGPT die Aufgabe mit der blauen Ente ähnlich gut bewältigt wie Gemini im Video. Das ist natürlich nur eine Stichprobe einer einzelnen Aufgabe. Doch es deckt sich ungefähr mit dem, was auch Google selbst an Tests veröffentlicht. In 30 von 32 Tests ""übertrifft die Leistung von Gemini Ultra den aktuellen Stand der Technik"", heißt es in der Produktpräsentation. In einer Tabelle ist ersichtlich, dass die Ergebnisse in den Tests meist um wenige Prozentpunkte besser sind als die von GPT-4.
"
Künstliche Intelligenz,Zeit,2023-12-06,https://www.zeit.de/digital/2023-12/google-bard-upgrade-gemini-sprachmodell-ki,Gemini: Google stellt neues KI-Sprachmodell vor | ZEIT ONLINE,"Google will mit seinem neuen KI-Sprachmodell Gemini den Konkurrenten OpenAI und dessen Produkt ChatGPT-4 übertreffen. ""Wir bringen Gemini über die Google-Produkte zu Milliarden von Menschen"", kündigte der Chef von Google DeepMind, Demis Hassabis, an. Google DeepMind kam im April aus der Google-internen KI-Abteilung und dem 2014 vom Konzern gekauften britischen Start-up DeepMind hervor.

Gemini kann demnach nicht nur in einem Chatbot Texte generieren, sondern auch bestimmte Probleme lösen und situationsabhängige Entscheidungen treffen. Außerdem kann das Modell Informationen aus Fotos und Videos aufnehmen. Google demonstrierte das System mit einem Videochat, bei dem Gemini Zeichnungen und Handgesten eines menschlichen Gegenübers sofort erkannte und richtig einordnete.

Google wird das neue System in drei unterschiedlichen Varianten einführen: Gemini Ultra, Gemini Pro und Gemini Nano. Gemini Ultra ist das größte und leistungsfähigste Modell für hochkomplexe Aufgaben – diese Variante richtet sich vor allem an Unternehmenskunden.

Gemini Pro soll sich an ein breites Publikum wenden und beispielsweise dem Chatbot Google Bard ""fortgeschrittenes Denken, Planen, Verstehen und mehr"" beibringen. ""Dies ist das größte Upgrade für Bard seit seiner Einführung"", sagte Hassabis. Bard werde ab sofort in mehr als 170 Ländern und Gebieten auf Englisch verfügbar sein. Google plane aber, in naher Zukunft neue Sprachen und Standorte zu unterstützen.

Die dritte Variante Nano soll das System für Smartphones nutzbar machen. Zunächst soll es auf der Google-eigenen Smartphonebaureihe Pixel laufen, sagte Hassabis. Die Smartphones sollen dadurch künftig beispielsweise in der Lage sein, nicht nur Gespräche oder Vorträge aufzuzeichnen und in Echtzeit in schriftliche Sprache umzuwandeln – sondern auch, kompakte Zusammenfassungen ohne Zeitverzögerung zu erstellen. Auch soll Gemini künftig in weiteren Google-Produkten und -Diensten wie bei der Suche, in Werbeanzeigen und im Browser Chrome eingebaut werden.   

Der Wandel, den KIs mit sich brächten, sei der ""tiefgreifendste in unserem Leben"", sagte der Google-Chef Sundar Pichai, ""weitaus größer als die Umstellung auf das Mobiltelefon oder auf das Internet davor"". Das US-Unternehmen arbeitet seit Jahren an Anwendungen auf KI-Basis, wurde aber von OpenAI und dessen ChatGPT-4 überholt. 

Der Erfolg des Chatbots zwang Google und weitere Konzerne wie Microsoft, ihre eigenen KI-Produkte auf den Markt zu bringen und führte zu einem KI-Boom in der Digitalindustrie. Dieser wird wiederum von Sorgen um nicht erwartete Nebenwirkungen der Nutzung von künstlicher Intelligenz begleitet, etwa durch die Verbreitung von Falschinformationen. Zudem könnten durch KI-Programme auch anspruchsvolle Jobs wegfallen. Befürworter von KI argumentieren hingegen, dass die meisten solchen Jobs durch KI lediglich verändert würden. 
"
Künstliche Intelligenz,Zeit,2023-12-06,https://www.zeit.de/kultur/film/2023-12/hollywood-streik-beendet-vertrag-studios-schauspielergewerkschaft,Hollywood-Streik: US-Schauspieler stimmen mit großer Mehrheit Vertrag mit Studios zu | ZEIT ONLINE,"Nach einem monatelangen Kampf um bessere Löhne und Arbeitsbedingungen haben die Schauspieler in Hollywood mit großer Mehrheit einem neuen Vertrag mit den Filmstudios zugestimmt. Ihren Streik beendeten die Schauspieler damit endgültig.  

78 Prozent der Abstimmungsteilnehmenden hätten sich für den bis Ende Juni 2026 geltenden Vertrag ausgesprochen, teilte die Schauspielergewerkschaft SAG-AFTRA mit. 22 Prozent stimmten demnach dagegen. Für die Abstimmung war eine einfache Mehrheit der Mitglieder erforderlich. Nach Angaben der Gewerkschaft gaben 38 Prozent der Mitglieder ihre Stimme ab.

Es sei ein ""goldenes Zeitalter"" für die SAG-AFTRA, sagte deren Präsidentin, die Schauspielerin und Produzentin Fran Drescher. ""Unsere Gewerkschaft war noch nie so stark."" Die SAG-AFTRA vertritt die Interessen von etwa 160.000 Schauspielerinnen, Stuntleuten, Tänzern und anderen Darstellern im Filmgeschäft. Nur wenige von ihnen sind Stars mit Millioneneinnahmen.

Die Einigung umfasst nach Angaben mehr als eine Milliarde Dollar (rund 930 Millionen Euro) an neuen Vergütungen und Leistungen sowie den Schutz der Schauspieler vor dem Einsatz künstlicher Intelligenz durch die Studios. 

Bereits vor rund einem Monat hatten die Schauspieler ihren monatelangen Ausstand beendet, nachdem sie sich mit Unterhaltungskonzernen wie Disney, Universal und Netflix auf eine Grundsatzvereinbarung geeinigt hatten. Das Abkommen musste allerdings noch vom Vorstand der Gewerkschaft und den Mitgliedern angenommen werden.

Der Branchenverband AMPTP, der die Filmstudios vertritt, begrüßte die Ratifizierung des Vertrags. Mit dieser Abstimmung könne die Branche ""mit voller Kraft zurückkehren"", teilte der Verband mit. Der insgesamt 118-tägige Streik hatte zahlreiche Film- und Serienproduktionen verzögert und viele Darsteller in Hollywood in Existenznot gebracht.

Es war der erste Streik der US-Schauspieler seit 1980. Weil bereits vor diesem Arbeitskampf die Drehbuchautoren die Arbeit niedergelegt hatten, erlebte Hollywood erstmals seit mehr als 60 Jahren einen Doppelstreik. Die Drehbuchautoren beendeten ihren Arbeitskampf bereits im Oktober. Experten schätzen die Verluste durch den Doppelstreik auf mindestens sechs Milliarden Dollar.
"
Künstliche Intelligenz,Zeit,2023-12-06,https://www.zeit.de/digital/2023-11/ki-hollywood-visuelle-effekte-film-evan-halleck,"KI in Hollywood: ""Es liegt eine Schönheit im menschlichen Gehirn, das etwas erschafft"" | ZEIT ONLINE","Egal ob man eine Explosion nachträglich aufhübschen oder eine Szene vor einem anderen Hintergrund zeigen will: Wann immer es darum geht, Filmszenen mit digitalen Effekten nachträglich zu verändern, kommen Spezialistinnen und Spezialisten für visuelle Effekte (VFX) zum Einsatz. Evan Halleck ist einer von ihnen, er hat unter anderem an Hollywoodproduktionen mitgearbeitet. Auf der MTH Mediatech Hub Conference hat er uns erklärt, warum er mit generativer KI wenig anfangen kann. 

ZEIT ONLINE: Evan Halleck, Sie haben beim
oscargekrönten Film Everything Everywhere All at Once im Team für visuelle
Effekte mitgearbeitet – also für Effekte, die erst in der Postproduktion
digital entstanden sind. Dieses Team war ziemlich klein für einen Film,
der voller visueller Effekte ist. Wie war das möglich?  

Evan Halleck: An den
visuellen Effekten haben fünf Leute in Vollzeit gearbeitet – und ich in
Teilzeit. Das feste VFX-Team bestand aus Leuten, die schon seit zehn Jahren
miteinander arbeiten und an verschiedenen Projekten, diese Chemie hat das
möglich gemacht. Eine Rolle hat aber auch gespielt, dass wir von zu Hause aus
arbeiten konnten und uns Software für Effekte zur Verfügung stand, die uns mehr
Flexibilität ermöglicht hat.  

ZEIT ONLINE: Sie haben dafür ein Tool der Firma Runway genutzt, ein
KI-Unternehmen, dessen Dienste online inzwischen vielen Nutzern offenstehen und
mit der man zum Beispiel aus Texteingaben Videos generieren kann. Sie haben
Runway-Tools als hilfreich für Ihre Arbeit bezeichnet. Waren
sie das Killer-Feature, das den Film mit so vielen Effekten und einem so
kleinen Team erst möglich gemacht hat?  

Halleck: Ich habe
Runway nur für einen winzigen Teil von Everything Everywhere All at Once
genutzt. Eigentlich habe ich nur ein sogenanntes Green-Screen-Tool verwendet:
Für einige Szenen haben die Regisseure Steine mit Schaufeln bewegt – und ich
musste sie später digital ausschneiden. Rotoscoping heißt das in der Branche.
Das mit Runway-Tools zu machen, hat mir einige Stunden Zeit gespart.  

ZEIT ONLINE: Künstliche Intelligenz
hat also für den Film gar keine so große Rolle gespielt?  

Halleck: Nein. 99,9
Prozent des Films wurden ohne KI gemacht. Es ist im Nachhinein manchmal ein
bisschen verzerrt dargestellt worden. Viele Menschen halten diese Technologie
im Bereich von visuellen Effekten für weiter fortgeschritten, als sie es
tatsächlich ist. An der traditionellen Vorgehensweise eines VFX-Spezialisten
hat sich eigentlich gar nicht so schrecklich viel geändert.  

ZEIT ONLINE: Das überrascht mich. Im
Netz findet man Texte, in denen Sie sich sehr positiv über den Einsatz von
KI-Tools und die damit verbundene Zeitersparnis äußern.  

Halleck: Ich habe
meine Meinung über einige Dinge in den vergangenen Jahren geändert.  

ZEIT ONLINE: Weil Sie von einigen
Tools enttäuscht waren?  
"
Künstliche Intelligenz,Zeit,2023-12-05,https://www.zeit.de/news/2023-12/05/wissing-zu-regulierung-von-ki-praezision-vor-geschwindigkeit,Künstliche Intelligenz: Wissing zu Regulierung von KI: Präzision vor Geschwindigkeit | ZEIT ONLINE,"Digitalminister Volker Wissing hat gefordert, sich für eine EU-weite Regulierung von Künstlicher Intelligenz (KI) die nötige Zeit zu nehmen. «Präzision und internationale Abstimmung gehen vor Geschwindigkeit», sagte der FDP-Politiker in Brüssel vor einem Treffen mit seinen EU-Amtskolleginnen und -kollegen. KI werde große Auswirkungen auf die Wettbewerbsfähigkeit europäischer Volkswirtschaften haben.

Ein Fehler bei der Regulierung könne bewirken, «dass Technologie einen Bogen um Europa macht, dass wir kein innovationsfreundlicher Standort bleiben und am Ende wir diese Schlüsseltechnologie nicht selbst beherrschen, sondern importieren müssen», so Wissing.

Auf EU-Ebene wird derzeit eine Regulierung von Künstlicher Intelligenz erarbeitet. KI bezeichnet meist Anwendungen auf Basis maschinellen Lernens, bei denen eine Software Datenmengen nach Übereinstimmungen durchforstet und Schlussfolgerungen zieht.

© dpa-infocom, dpa:231205-99-187982/2
"
AI,Zeit,2023-12-06,https://www.zeit.de/kultur/film/2023-12/hollywood-streik-beendet-vertrag-studios-schauspielergewerkschaft,Hollywood-Streik: US-Schauspieler stimmen mit großer Mehrheit Vertrag mit Studios zu | ZEIT ONLINE,"Nach einem monatelangen Kampf um bessere Löhne und Arbeitsbedingungen haben die Schauspieler in Hollywood mit großer Mehrheit einem neuen Vertrag mit den Filmstudios zugestimmt. Ihren Streik beendeten die Schauspieler damit endgültig.  

78 Prozent der Abstimmungsteilnehmenden hätten sich für den bis Ende Juni 2026 geltenden Vertrag ausgesprochen, teilte die Schauspielergewerkschaft SAG-AFTRA mit. 22 Prozent stimmten demnach dagegen. Für die Abstimmung war eine einfache Mehrheit der Mitglieder erforderlich. Nach Angaben der Gewerkschaft gaben 38 Prozent der Mitglieder ihre Stimme ab.

Es sei ein ""goldenes Zeitalter"" für die SAG-AFTRA, sagte deren Präsidentin, die Schauspielerin und Produzentin Fran Drescher. ""Unsere Gewerkschaft war noch nie so stark."" Die SAG-AFTRA vertritt die Interessen von etwa 160.000 Schauspielerinnen, Stuntleuten, Tänzern und anderen Darstellern im Filmgeschäft. Nur wenige von ihnen sind Stars mit Millioneneinnahmen.

Die Einigung umfasst nach Angaben mehr als eine Milliarde Dollar (rund 930 Millionen Euro) an neuen Vergütungen und Leistungen sowie den Schutz der Schauspieler vor dem Einsatz künstlicher Intelligenz durch die Studios. 

Bereits vor rund einem Monat hatten die Schauspieler ihren monatelangen Ausstand beendet, nachdem sie sich mit Unterhaltungskonzernen wie Disney, Universal und Netflix auf eine Grundsatzvereinbarung geeinigt hatten. Das Abkommen musste allerdings noch vom Vorstand der Gewerkschaft und den Mitgliedern angenommen werden.

Der Branchenverband AMPTP, der die Filmstudios vertritt, begrüßte die Ratifizierung des Vertrags. Mit dieser Abstimmung könne die Branche ""mit voller Kraft zurückkehren"", teilte der Verband mit. Der insgesamt 118-tägige Streik hatte zahlreiche Film- und Serienproduktionen verzögert und viele Darsteller in Hollywood in Existenznot gebracht.

Es war der erste Streik der US-Schauspieler seit 1980. Weil bereits vor diesem Arbeitskampf die Drehbuchautoren die Arbeit niedergelegt hatten, erlebte Hollywood erstmals seit mehr als 60 Jahren einen Doppelstreik. Die Drehbuchautoren beendeten ihren Arbeitskampf bereits im Oktober. Experten schätzen die Verluste durch den Doppelstreik auf mindestens sechs Milliarden Dollar.
"
AI,Zeit,2023-12-05,https://www.zeit.de/news/2023-12/05/raketen-magnetschwebebahn-ki-uni-soeder-setzt-auf-hightech,"Landtag: Raketen, Magnetschwebebahn, KI-Uni: Söder setzt auf Hightech | ZEIT ONLINE","Ein Testzentrum für Raketenantriebe, eine Teststrecke für eine Magnetschwebebahn in Nürnberg und eine Universität allein zum Thema Künstliche Intelligenz: Bayerns Ministerpräsident Markus Söder (CSU) will in der neuen Wahlperiode deutliche Schwerpunkte auf Zukunftsforschung und -technologien setzen. 

«Wir sind das Silicon Valley Europas und wollen Bayern zum führenden Hightech-Standort des europäischen Kontinents weiterentwickeln», sagte Söder am Dienstag in seiner ersten Regierungserklärung in der neuen Legislaturperiode im Landtag. Deshalb werde die Staatsregierung die Hightech Agenda wie angekündigt weiterführen und «noch toppen». Der Freistaat stehe nicht im Wettbewerb mit anderen Bundesländern, sondern «mit unseren Freunden aus den USA und Partnern aus Asien».

Um der Hightech Agenda zusätzlichen Schub zu verleihen, kündigte Söder ab 2024 einen neuen Zukunftspreis an - den «Hightech Oscar» für die schlauesten Köpfe und Start-ups in Bayern.  

«Wir werden zudem Deutschlands erste KI-Uni in Bayern errichten, und zwar in Nürnberg», kündigte Söder an. «Wir machen die TU Nürnberg zu Deutschlands erster rein auf KI spezialisierten Universität: die Franconian University of Artificial Intelligence.»

Das Deutsches Raumfahrtkontrollzentrum in Oberpfaffenhofen will Söder nach eigenen Worten zum «Houston Deutschlands» entwickeln - Houston ist der Sitz des US-Raumfahrtkontrollzentrums. Es sei «sehr wahrscheinlich», das europäische Mondkontrollzentrum in Oberpfaffenhofen anzusiedeln. Diese Idee hatte Söder bereits in der Vergangenheit bereits geäußert. Zudem strebe man in Bayern nun auch ein Testzentrum für zukunftsweisende Raketenantriebe an. «Möge die Macht mit uns sein.» 

Söder will aber auch auf der Erde neue Verkehrsprojekte prüfen. «Ähnlich wie Berlin wollen wir eine Magnetschwebebahn untersuchen. Sie ist günstiger als eine U-Bahn, geräuschlos und klimaneutral», sagte Söder. «Dazu haben wir eine mögliche Teststrecke in Nürnberg zwischen Universität, Messe und Klinikum ins Auge gefasst.»

Zu konkreten Zeitplänen äußerte sich Söder jeweils zunächst nicht.

© dpa-infocom, dpa:231205-99-189988/2
"
Artificial Intelligence,Zeit,2023-12-05,https://www.zeit.de/news/2023-12/05/raketen-magnetschwebebahn-ki-uni-soeder-setzt-auf-hightech,"Landtag: Raketen, Magnetschwebebahn, KI-Uni: Söder setzt auf Hightech | ZEIT ONLINE","Ein Testzentrum für Raketenantriebe, eine Teststrecke für eine Magnetschwebebahn in Nürnberg und eine Universität allein zum Thema Künstliche Intelligenz: Bayerns Ministerpräsident Markus Söder (CSU) will in der neuen Wahlperiode deutliche Schwerpunkte auf Zukunftsforschung und -technologien setzen. 

«Wir sind das Silicon Valley Europas und wollen Bayern zum führenden Hightech-Standort des europäischen Kontinents weiterentwickeln», sagte Söder am Dienstag in seiner ersten Regierungserklärung in der neuen Legislaturperiode im Landtag. Deshalb werde die Staatsregierung die Hightech Agenda wie angekündigt weiterführen und «noch toppen». Der Freistaat stehe nicht im Wettbewerb mit anderen Bundesländern, sondern «mit unseren Freunden aus den USA und Partnern aus Asien».

Um der Hightech Agenda zusätzlichen Schub zu verleihen, kündigte Söder ab 2024 einen neuen Zukunftspreis an - den «Hightech Oscar» für die schlauesten Köpfe und Start-ups in Bayern.  

«Wir werden zudem Deutschlands erste KI-Uni in Bayern errichten, und zwar in Nürnberg», kündigte Söder an. «Wir machen die TU Nürnberg zu Deutschlands erster rein auf KI spezialisierten Universität: die Franconian University of Artificial Intelligence.»

Das Deutsches Raumfahrtkontrollzentrum in Oberpfaffenhofen will Söder nach eigenen Worten zum «Houston Deutschlands» entwickeln - Houston ist der Sitz des US-Raumfahrtkontrollzentrums. Es sei «sehr wahrscheinlich», das europäische Mondkontrollzentrum in Oberpfaffenhofen anzusiedeln. Diese Idee hatte Söder bereits in der Vergangenheit bereits geäußert. Zudem strebe man in Bayern nun auch ein Testzentrum für zukunftsweisende Raketenantriebe an. «Möge die Macht mit uns sein.» 

Söder will aber auch auf der Erde neue Verkehrsprojekte prüfen. «Ähnlich wie Berlin wollen wir eine Magnetschwebebahn untersuchen. Sie ist günstiger als eine U-Bahn, geräuschlos und klimaneutral», sagte Söder. «Dazu haben wir eine mögliche Teststrecke in Nürnberg zwischen Universität, Messe und Klinikum ins Auge gefasst.»

Zu konkreten Zeitplänen äußerte sich Söder jeweils zunächst nicht.

© dpa-infocom, dpa:231205-99-189988/2
"
Artificial Intelligence,Zeit,2023-12-05,https://www.zeit.de/news/2023-12/05/soeder-kuendigt-gender-verbot-fuer-bayerns-schulen-und-behoerden-an,Regierungserklärung: Ampel-Kritik und Gender-Verbot: Söders neue Kursbestimmung | ZEIT ONLINE,"In einer rund 75-minütigen Kursbestimmung hat Ministerpräsident Markus Söder die Regierungsziele der kommenden Jahre in Bayern als Gegenentwurf zur Bundesregierung präsentiert. ""Unsere Koalition aus CSU und FW ist keine Liebesheirat, aber sie ist mehr als eine Zweckehe"", sagte der CSU-Chef am Dienstag in seiner ersten Regierungserklärung in der neuen Legislaturperiode im Landtag. Neben viel Kritik an der Bundesregierung kündigte Söder dabei auch einige Neuerungen an. Von der Opposition kam viel und laute Kritik.

Was hat Söder in der Rede angesprochen?

Einen Großteil und den Beginn seiner Rede widmete Söder der Wirtschaft und kritisierte dabei die Bundespolitik. Probleme seien hausgemacht und von der Ampel-Regierung zu verantworten. ""Wäre Bayern allein, würde ich mir keine Sorgen um die Wirtschaft machen. Aber wir sind in Deutschland und leiden unter den schlechten Standortbedingungen unserer Nation"", sagte der CSU-Vorsitzende.

Der Glaube, nur mit Staatsverschuldung die Wirtschaft zu stärken, sei ein Trugschluss, sagte Söder. Deutschland sei kein ""Land des Staatsdirigismus oder Staatskapitalismus, sondern des innovativen Mittelstandes"".

Erneut forderte Söder auch eine grundlegende Wende in der Migrationspolitik. Die stationären Kontrollen an den bayerischen Grenzen seien wirkungsvoll, ""Pull-Faktoren"" müssten reduziert werden.

Was hat Söder Neues angekündigt?

Die wahrscheinlich größte Reaktion löste Söder mit seiner Ankündigung zum Gendern aus: ""Für Bayern kann ich sagen: mit uns wird es kein verpflichtendes Gendern geben. Im Gegenteil: wir werden das Gendern in Schule und Verwaltung sogar untersagen"", sagte der Ministerpräsident.

Daneben hatte Söder einige Ziele für seine Heimatstadt Nürnberg parat: ""Wir werden zudem Deutschlands erste KI-Uni in Bayern errichten, und zwar in Nürnberg. Wir machen die TU Nürnberg zu Deutschlands erster rein auf KI spezialisierten Universität: die Franconian University of Artificial Intelligence.""

Zudem solle geprüft werden, ob es in Nürnberg eine Teststrecke für eine Magnetschwebebahn zwischen Universität, Messe und Klinikum geben könne. ""Sie ist günstiger als eine U-Bahn, geräuschlos und klimaneutral"", sagte er. Für Augsburg kündigte Söder an, eine Wiedereröffnung des Römer-Museums prüfen zu lassen.

Das Deutsches Raumfahrtkontrollzentrum in Oberpfaffenhofen will Söder nach eigenen Worten zum ""Houston Deutschlands"" entwickeln - Houston ist der Sitz des US-Raumfahrtkontrollzentrums. Es sei ""sehr wahrscheinlich"", das europäische Mondkontrollzentrum in Oberpfaffenhofen anzusiedeln. Diese Idee hatte Söder bereits in der Vergangenheit geäußert. Bayern strebe zudem ein Testzentrum für zukunftsweisende Raketenantriebe an. ""Möge die Macht mit uns sein."" Zu konkreten Zeitplänen äußerte sich Söder jeweils zunächst nicht.

Was sagt die Opposition?

AfD-Landtagsfraktionschefin Katrin Ebner-Steiner durfte als erste erwidern - die AfD war bei der Landtagswahl als stärkste Oppositionspartei auf Platz drei gelandet. Sie attackierte Söder in scharfen Worten, warf ihm und der Staatsregierung aus CSU und Freien Wählern eine Politik zum Schaden Bayerns und gegen die eigene Bevölkerung vor. ""Ihr Amtseid war nichts anderes als ein Meineid"", sagte Ebner-Steiner. ""Ihre Politik ist der größte Schaden für Bayern seit Ende des Zweiten Weltkriegs."" Am Vortag des Nikolaustages habe Söder eine Märchenstunde abgeliefert. Vieles sei aus dem Wahlprogramm der AfD übernommen.

Grünen-Fraktionschefin Katharina Schulze warf Söder ""katastrophale Fehler"" in der Wirtschafts- und Energiepolitik vor. ""Sie haben in Bayern versagt beim Ausbau der erneuerbaren Energien - das ist Fakt"", sagte Schulze. Bayern hinke hier massiv hinterher, die Staatsregierung befinde sich auf einer ""energiepolitischen Irrfahrt"".

""Ihre Mondlandungen in allen Ehren - aber für den Anfang würde mir ein Zug, der fährt, schon reichen"", sagte Schulze mit Blick auf Söders Hightech-Politik - und vor dem Hintergrund der Tatsache, dass mehrere Tage nach dem massiven Schneefall in Bayern immer noch viele Züge und S-Bahnen stillstehen.

SPD-Fraktionschef Florian von Brunn warf Söder vor, einmal mehr eine Wahlkampfrede mit vielen Ankündigungen gehalten zu haben. In der Praxis fehle es aber an seriösen Zielen für die Herausforderungen - etwa beim Arbeits- und Fachkräftemangel.

Welche Botschaft sendet Söder nach Berlin?

Der Bundesregierung stellte Söder ein denkbar schlechtes Zeugnis aus. Die Kindergrundsicherung etwa kritisierte er als ""Bürokratiemonster"". ""Kein Cent kommt am Ende bei den Kindern an. Das ist doch keine gute Familienpolitik"", sagte er. Im Gegensatz zur ""komplett überforderten Ampel in Berlin"" setze die Staatsregierung in den kommenden fünf Jahren auf Stabilität, Erneuerung und Kontinuität. ""Unser Ziel ist es, dass es Land und Leuten in fünf Jahren mindestens genauso gut geht wie heute. Bayern soll Bayern bleiben, auch wenn die halbe Welt verrückt spielt.""

© dpa-infocom, dpa:231205-99-189978/6
"
KI,Zeit,2023-12-06,https://www.zeit.de/digital/2023-12/google-gemini-ki-openai-chatgpt-konkurrenz,Google Gemini: Ist die neue Google-KI besser als ChatGPT? | ZEIT ONLINE,"Der Computer erkennt, wie aus ein paar Strichen ein gezeichneter Vogel wird. Eine gewellte Linie kommt dazu, aha, Wasser, der Vogel könnte eine Ente sein. Sind Enten blau? Normalerweise nicht. Ganz schön schlau, diese neue künstliche Intelligenz, die Google da in einem Demovideo präsentiert. Nur: Der Marktführer ChatGPT kann das auch – und zwar seit Monaten.

Google hat am Mittwoch eine Reihe von Produkten vorgestellt, mit denen der Software-Konzern im Rennen um die beste künstliche Intelligenz aufholen will. Von einer der ""größten wissenschaftlichen und technischen Anstrengungen, die wir als Unternehmen unternommen haben"", schreibt der Google-CEO Sundar Pichai.

Das leistungsfähigste der neuen Modelle, Gemini Ultra, soll dem bisher als am fortschrittlichsten geltenden Sprachmodell GPT-4 der Firma OpenAI in einigen Bereichen überlegen sein. Allzu weit ist der Vorsprung, wenn überhaupt, aber offenbar nicht.

Insgesamt präsentierte Google drei Sprachmodelle, die alle Gemini im Namen tragen, wie das Sternzeichen Zwillinge auf Englisch: Gemini Pro ist laut Google ab sofort verfügbar und treibt nun den Chatbot Bard an. Gemini Nano ist eine Variante, die auf Mobilgeräten laufen soll. Auf dem Google Pixel 8 soll die Technik zum Beispiel kurze Zusammenfassungen von Sprachaufnahmen schreiben können. Das dritte und beste Modell, Gemini Ultra, ist noch nicht öffentlich verfügbar. Es soll Anfang kommenden Jahres folgen, in einer erweiterten Version des Chatbots Bard.

Produktseitig erinnert viel an den Chatbot ChatGPT, mit dem die Firma OpenAI vor gut einem Jahr einen KI-Hype ausgelöst hat. Auch OpenAI bietet eine Basisversion seines Chatbots an und eine erweiterte Version. Bei ChatGPT kostet diese Pro-Variante knapp 24 Dollar im Monat. Ob Googles ""Bard Advanced"" Geld kostet, steht nicht in den Ankündigungen. Die schiere Existenz einer solchen Trennung legt es aber nahe – denn wenn beide Varianten kostenlos wären, wer sollte sich für die Basisversion entscheiden?

Dass Google sich im KI-Wettrennen nicht abhängen lassen will, wurde spätestens klar, als der Konzern im März den Chatbot Bard vorstellte. Zuvor hatte der Konzern Microsoft sich durch eine Milliardeninvestition die Technologie des Unternehmens OpenAI gesichert. Dessen Sprachmodell GPT-4 galt bisher als führend im Bereich KI.

Für die Frage, ob das beste Gemini mit dem besten GPT mithalten kann, muss man sich derzeit noch auf Angaben von Google verlassen. Auf seiner Website zeigt Google, was Gemini Ultra können soll. Es ist ein multimodales Modell, das heißt, es kann nicht nur Text verarbeiten, sondern etwa auch Sprache und Bilder. Deshalb erkennt die KI in einigen geschwungenen Filzstiftstrichen eine Ente, und deshalb kann das System kurz darauf die Aussprache des Wortes für Ente auf Chinesisch korrigieren.

Solche Funktionen, Bilder zu verarbeiten oder Sprache auszugeben, hat auch ChatGPT. Seit einigen Wochen können alle Nutzerinnen und Nutzer des Bezahlangebots darauf zugreifen. 

Im Test von ZEIT ONLINE hat ChatGPT die Aufgabe mit der blauen Ente ähnlich gut bewältigt wie Gemini im Video. Das ist natürlich nur eine Stichprobe einer einzelnen Aufgabe. Doch es deckt sich ungefähr mit dem, was auch Google selbst an Tests veröffentlicht. In 30 von 32 Tests ""übertrifft die Leistung von Gemini Ultra den aktuellen Stand der Technik"", heißt es in der Produktpräsentation. In einer Tabelle ist ersichtlich, dass die Ergebnisse in den Tests meist um wenige Prozentpunkte besser sind als die von GPT-4.
"
KI,Zeit,2023-12-06,https://www.zeit.de/digital/2023-12/google-bard-upgrade-gemini-sprachmodell-ki,Gemini: Google stellt neues KI-Sprachmodell vor | ZEIT ONLINE,"Google will mit seinem neuen KI-Sprachmodell Gemini den Konkurrenten OpenAI und dessen Produkt ChatGPT-4 übertreffen. ""Wir bringen Gemini über die Google-Produkte zu Milliarden von Menschen"", kündigte der Chef von Google DeepMind, Demis Hassabis, an. Google DeepMind kam im April aus der Google-internen KI-Abteilung und dem 2014 vom Konzern gekauften britischen Start-up DeepMind hervor.

Gemini kann demnach nicht nur in einem Chatbot Texte generieren, sondern auch bestimmte Probleme lösen und situationsabhängige Entscheidungen treffen. Außerdem kann das Modell Informationen aus Fotos und Videos aufnehmen. Google demonstrierte das System mit einem Videochat, bei dem Gemini Zeichnungen und Handgesten eines menschlichen Gegenübers sofort erkannte und richtig einordnete.

Google wird das neue System in drei unterschiedlichen Varianten einführen: Gemini Ultra, Gemini Pro und Gemini Nano. Gemini Ultra ist das größte und leistungsfähigste Modell für hochkomplexe Aufgaben – diese Variante richtet sich vor allem an Unternehmenskunden.

Gemini Pro soll sich an ein breites Publikum wenden und beispielsweise dem Chatbot Google Bard ""fortgeschrittenes Denken, Planen, Verstehen und mehr"" beibringen. ""Dies ist das größte Upgrade für Bard seit seiner Einführung"", sagte Hassabis. Bard werde ab sofort in mehr als 170 Ländern und Gebieten auf Englisch verfügbar sein. Google plane aber, in naher Zukunft neue Sprachen und Standorte zu unterstützen.

Die dritte Variante Nano soll das System für Smartphones nutzbar machen. Zunächst soll es auf der Google-eigenen Smartphonebaureihe Pixel laufen, sagte Hassabis. Die Smartphones sollen dadurch künftig beispielsweise in der Lage sein, nicht nur Gespräche oder Vorträge aufzuzeichnen und in Echtzeit in schriftliche Sprache umzuwandeln – sondern auch, kompakte Zusammenfassungen ohne Zeitverzögerung zu erstellen. Auch soll Gemini künftig in weiteren Google-Produkten und -Diensten wie bei der Suche, in Werbeanzeigen und im Browser Chrome eingebaut werden.   

Der Wandel, den KIs mit sich brächten, sei der ""tiefgreifendste in unserem Leben"", sagte der Google-Chef Sundar Pichai, ""weitaus größer als die Umstellung auf das Mobiltelefon oder auf das Internet davor"". Das US-Unternehmen arbeitet seit Jahren an Anwendungen auf KI-Basis, wurde aber von OpenAI und dessen ChatGPT-4 überholt. 

Der Erfolg des Chatbots zwang Google und weitere Konzerne wie Microsoft, ihre eigenen KI-Produkte auf den Markt zu bringen und führte zu einem KI-Boom in der Digitalindustrie. Dieser wird wiederum von Sorgen um nicht erwartete Nebenwirkungen der Nutzung von künstlicher Intelligenz begleitet, etwa durch die Verbreitung von Falschinformationen. Zudem könnten durch KI-Programme auch anspruchsvolle Jobs wegfallen. Befürworter von KI argumentieren hingegen, dass die meisten solchen Jobs durch KI lediglich verändert würden. 
"
KI,Zeit,2023-12-06,https://www.zeit.de/digital/2023-11/ki-hollywood-visuelle-effekte-film-evan-halleck,"KI in Hollywood: ""Es liegt eine Schönheit im menschlichen Gehirn, das etwas erschafft"" | ZEIT ONLINE","Egal ob man eine Explosion nachträglich aufhübschen oder eine Szene vor einem anderen Hintergrund zeigen will: Wann immer es darum geht, Filmszenen mit digitalen Effekten nachträglich zu verändern, kommen Spezialistinnen und Spezialisten für visuelle Effekte (VFX) zum Einsatz. Evan Halleck ist einer von ihnen, er hat unter anderem an Hollywoodproduktionen mitgearbeitet. Auf der MTH Mediatech Hub Conference hat er uns erklärt, warum er mit generativer KI wenig anfangen kann. 

ZEIT ONLINE: Evan Halleck, Sie haben beim
oscargekrönten Film Everything Everywhere All at Once im Team für visuelle
Effekte mitgearbeitet – also für Effekte, die erst in der Postproduktion
digital entstanden sind. Dieses Team war ziemlich klein für einen Film,
der voller visueller Effekte ist. Wie war das möglich?  

Evan Halleck: An den
visuellen Effekten haben fünf Leute in Vollzeit gearbeitet – und ich in
Teilzeit. Das feste VFX-Team bestand aus Leuten, die schon seit zehn Jahren
miteinander arbeiten und an verschiedenen Projekten, diese Chemie hat das
möglich gemacht. Eine Rolle hat aber auch gespielt, dass wir von zu Hause aus
arbeiten konnten und uns Software für Effekte zur Verfügung stand, die uns mehr
Flexibilität ermöglicht hat.  

ZEIT ONLINE: Sie haben dafür ein Tool der Firma Runway genutzt, ein
KI-Unternehmen, dessen Dienste online inzwischen vielen Nutzern offenstehen und
mit der man zum Beispiel aus Texteingaben Videos generieren kann. Sie haben
Runway-Tools als hilfreich für Ihre Arbeit bezeichnet. Waren
sie das Killer-Feature, das den Film mit so vielen Effekten und einem so
kleinen Team erst möglich gemacht hat?  

Halleck: Ich habe
Runway nur für einen winzigen Teil von Everything Everywhere All at Once
genutzt. Eigentlich habe ich nur ein sogenanntes Green-Screen-Tool verwendet:
Für einige Szenen haben die Regisseure Steine mit Schaufeln bewegt – und ich
musste sie später digital ausschneiden. Rotoscoping heißt das in der Branche.
Das mit Runway-Tools zu machen, hat mir einige Stunden Zeit gespart.  

ZEIT ONLINE: Künstliche Intelligenz
hat also für den Film gar keine so große Rolle gespielt?  

Halleck: Nein. 99,9
Prozent des Films wurden ohne KI gemacht. Es ist im Nachhinein manchmal ein
bisschen verzerrt dargestellt worden. Viele Menschen halten diese Technologie
im Bereich von visuellen Effekten für weiter fortgeschritten, als sie es
tatsächlich ist. An der traditionellen Vorgehensweise eines VFX-Spezialisten
hat sich eigentlich gar nicht so schrecklich viel geändert.  

ZEIT ONLINE: Das überrascht mich. Im
Netz findet man Texte, in denen Sie sich sehr positiv über den Einsatz von
KI-Tools und die damit verbundene Zeitersparnis äußern.  

Halleck: Ich habe
meine Meinung über einige Dinge in den vergangenen Jahren geändert.  

ZEIT ONLINE: Weil Sie von einigen
Tools enttäuscht waren?  
"
KI,Zeit,2023-12-06,https://www.zeit.de/news/2023-12/06/google-will-mit-neuem-ki-modell-konkurrenz-abhaengen,Sprachmodell Gemini: Google will mit neuem KI-Modell Konkurrenz abhängen | ZEIT ONLINE,"Im Wettlauf bei Künstlicher Intelligenz will sich Google mit dem neuen Sprachmodell Gemini an die Spitze setzen. Das KI-System soll nicht nur mit dem Sprachmodell GPT4 des Konkurrenten OpenAI mithalten, sondern es auch übertreffen. 

Google-Chef Sundar Pichai sagte zur Ankündigung von Gemini, dass der Wandel, den man gerade mit der KI erlebe, der «tiefgreifendste in unserem Leben sein wird, weitaus größer als die Umstellung auf das Mobiltelefon oder auf das Internet davor».

Gemini kann nicht nur in einem Chatbot Texte generieren, sondern auch bestimmte Probleme lösen und situationsabhängige Entscheidungen treffen. Es kann auch Informationen aus Fotos und Videos aufnehmen. Google demonstrierte das System mit einem Videochat, bei dem Gemini Zeichnungen und Handgesten des menschlichen Gegenübers sofort erkannte und richtig einordnete.

«Wir bringen Gemini über die Google-Produkte zu Milliarden von Menschen», kündigte der Chef von Google DeepMind, Demis Hassabis, an. Die Google-Tochter war im April 2023 aus dem 2014 zugekauften britischen Start-up DeepMind und der Google-internen KI-Abteilung hervorgegangen. Mit der Integration des bis zu diesem Zeitpunkt weitgehend unabhängig agierenden Start-ups wollte Pichai die KI-Anstrengungen im Google-Konzern bündeln, um entschiedener gegen OpenAI antreten zu können.

Google wird das neue System in drei unterschiedlichen Dimensionen einführen: Gemini Ultra, Gemini Pro und Gemini Nano. Gemini Ultra ist das größte und leistungsfähigste Modell für hochkomplexe Aufgaben. Diese Variante richtet sich vor allem an Unternehmenskunden.

Gemini Pro wird sich an ein breites Publikum wenden und beispielsweise dem Chatbot Google Bard «fortgeschrittenes Denken, Planen, Verstehen und mehr» beibringen. «Dies ist das größte Upgrade für Bard seit seiner Einführung», sagte Hassabis. Bard werde von sofort an in mehr als 170 Ländern und Gebieten auf Englisch verfügbar sein. Google plane aber, in naher Zukunft neue Sprachen und Standorte zu unterstützen.

Die dritte Gemini-Variante Nano bringt das System auf das Topmodell der Google-Smartphones Pixel. «Das Pixel 8 Pro ist das erste Smartphone, auf dem Gemini Nano läuft», kündigte Hassabis an. Damit lässt sich beispielsweise mit der Recorder App nicht nur gesprochene Sprache aus einem längeren Meeting, einer Vorlesung oder einem Interview aufzeichnen und in Echtzeit in schriftliche Sprache umwandeln. Mit Hilfe von Gemini Nano kann das Pixel dann außerdem ohne Zeitverzögerung eine kompakte Zusammenfassung erstellen. In den kommenden Monaten wird Gemini in weiteren Google-Produkten und Diensten wie Suche, Werbeanzeigen oder dem Chrome-Browser verfügbar sein.

Google arbeitet schon seit Jahren an Anwendungen auf Basis Künstlicher Intelligenz, steht aktuell aber unter Zugzwang, mehr davon preiszugeben. Vor einem Jahr löste das Start-up OpenAI einen neuen Wettstreit bei Künstlicher Intelligenz aus, als es seinen Chat-Bot ChatGPT öffentlich machte. Die Software sorgte für viel Aufsehen, weil sie Sätze wie ein Mensch bilden kann. Sie wird mit gewaltigen Datenmengen trainiert und schätzt Wort für Wort ab, wie ein Satz weitergehen könnte. Das bringt das Risiko mit sich, dass sie völlig falsche Informationen ausgeben kann.

© dpa-infocom, dpa:231206-99-203173/2
"
KI,Zeit,2023-12-05,https://www.zeit.de/news/2023-12/05/wissing-zu-regulierung-von-ki-praezision-vor-geschwindigkeit,Künstliche Intelligenz: Wissing zu Regulierung von KI: Präzision vor Geschwindigkeit | ZEIT ONLINE,"Digitalminister Volker Wissing hat gefordert, sich für eine EU-weite Regulierung von Künstlicher Intelligenz (KI) die nötige Zeit zu nehmen. «Präzision und internationale Abstimmung gehen vor Geschwindigkeit», sagte der FDP-Politiker in Brüssel vor einem Treffen mit seinen EU-Amtskolleginnen und -kollegen. KI werde große Auswirkungen auf die Wettbewerbsfähigkeit europäischer Volkswirtschaften haben.

Ein Fehler bei der Regulierung könne bewirken, «dass Technologie einen Bogen um Europa macht, dass wir kein innovationsfreundlicher Standort bleiben und am Ende wir diese Schlüsseltechnologie nicht selbst beherrschen, sondern importieren müssen», so Wissing.

Auf EU-Ebene wird derzeit eine Regulierung von Künstlicher Intelligenz erarbeitet. KI bezeichnet meist Anwendungen auf Basis maschinellen Lernens, bei denen eine Software Datenmengen nach Übereinstimmungen durchforstet und Schlussfolgerungen zieht.

© dpa-infocom, dpa:231205-99-187982/2
"
KI,Zeit,2023-12-05,https://www.zeit.de/news/2023-12/05/raketen-magnetschwebebahn-ki-uni-soeder-setzt-auf-hightech,"Landtag: Raketen, Magnetschwebebahn, KI-Uni: Söder setzt auf Hightech | ZEIT ONLINE","Ein Testzentrum für Raketenantriebe, eine Teststrecke für eine Magnetschwebebahn in Nürnberg und eine Universität allein zum Thema Künstliche Intelligenz: Bayerns Ministerpräsident Markus Söder (CSU) will in der neuen Wahlperiode deutliche Schwerpunkte auf Zukunftsforschung und -technologien setzen. 

«Wir sind das Silicon Valley Europas und wollen Bayern zum führenden Hightech-Standort des europäischen Kontinents weiterentwickeln», sagte Söder am Dienstag in seiner ersten Regierungserklärung in der neuen Legislaturperiode im Landtag. Deshalb werde die Staatsregierung die Hightech Agenda wie angekündigt weiterführen und «noch toppen». Der Freistaat stehe nicht im Wettbewerb mit anderen Bundesländern, sondern «mit unseren Freunden aus den USA und Partnern aus Asien».

Um der Hightech Agenda zusätzlichen Schub zu verleihen, kündigte Söder ab 2024 einen neuen Zukunftspreis an - den «Hightech Oscar» für die schlauesten Köpfe und Start-ups in Bayern.  

«Wir werden zudem Deutschlands erste KI-Uni in Bayern errichten, und zwar in Nürnberg», kündigte Söder an. «Wir machen die TU Nürnberg zu Deutschlands erster rein auf KI spezialisierten Universität: die Franconian University of Artificial Intelligence.»

Das Deutsches Raumfahrtkontrollzentrum in Oberpfaffenhofen will Söder nach eigenen Worten zum «Houston Deutschlands» entwickeln - Houston ist der Sitz des US-Raumfahrtkontrollzentrums. Es sei «sehr wahrscheinlich», das europäische Mondkontrollzentrum in Oberpfaffenhofen anzusiedeln. Diese Idee hatte Söder bereits in der Vergangenheit bereits geäußert. Zudem strebe man in Bayern nun auch ein Testzentrum für zukunftsweisende Raketenantriebe an. «Möge die Macht mit uns sein.» 

Söder will aber auch auf der Erde neue Verkehrsprojekte prüfen. «Ähnlich wie Berlin wollen wir eine Magnetschwebebahn untersuchen. Sie ist günstiger als eine U-Bahn, geräuschlos und klimaneutral», sagte Söder. «Dazu haben wir eine mögliche Teststrecke in Nürnberg zwischen Universität, Messe und Klinikum ins Auge gefasst.»

Zu konkreten Zeitplänen äußerte sich Söder jeweils zunächst nicht.

© dpa-infocom, dpa:231205-99-189988/2
"
KI,Zeit,2023-12-05,https://www.zeit.de/news/2023-12/05/helge-schneider-ueber-ki-musik-und-nervige-grammatik,"Musik: Helge Schneider über KI, Musik und nervige Grammatik | ZEIT ONLINE","Der Künstler und Musiker Helge Schneider will sich auf gar keinen Fall von einer Künstlichen Intelligenz vertreten lassen. «Ich finde es spooky. Ich finde es komisch. Aber noch schlimmer wäre, solche Avatare auf die Bühne zu schicken wie bei Abba», sagte der Musiker und Komponist in München in Anspielung auf die schwedische Popband, die in ihrer Konzertshow «Abba Voyage» virtuell auf der Bühne steht. Schneider versprach: Bei seinen Konzerten bleibe alles live und echt, seine Band bestehe ausschließlich aus lebenden Menschen.

Schneider zeigt keine Anzeichen von Müdigkeit: Kürzlich erschien sein Album «Live aus Graz» und in Zürich startet seine Tournee «Katzeklo auf Rädern». Mit über 70 Auftritten sei es «eine Welttournee durch Deutschland, Österreich und die Schweiz», sagte der 68-Jährige. Schneider komponierte das Lied «Katzeklo» 1993, als die Menschen sich noch nicht vorstellen konnten, dass es in Zukunft möglich sein würde, sich am Telefon zu sehen. Jetzt ist auch sein Publikum bei Konzerten mit Handys unterwegs. «Mich stört es, aber sobald ich das thematisiere oder so, dann stört es mich noch mehr», sagte er.

Im Oktober erschien sein neuer Krimi «Stepptanz». In München witzelte der Autor über grammatikalische Regeln. Es sei manchmal sogar besser, so zu schreiben, wie man spreche. «Mir ist auch langsam egal, wie meine Grammatik ist - Hauptsache, man versteht es», sagte Schneider.

Ob er ein weiteres Buch schreiben wird, ließ der 68-Jährige offen. Lieber widmet er sich der Musik. «Schreiben ist nicht so imaginär wie Klavierspielen - Musik ist eine richtige Kunst, die ist so himmlisch. Und Schreiben ist mehr auf der Erde», so Schneider.

© dpa-infocom, dpa:231205-99-190971/6
"
Künstliche Intelligenz,Zeit,2023-12-04,https://www.zeit.de/digital/2023-11/ki-gesetz-eu-parlament-regulierung-bundesregierung,KI-Gesetz der EU: Regulierung oder Innovation? Beides! | ZEIT ONLINE,"Wenn der Verkehrsminister, der auch Digitalminister ist, über die Technologie der Zukunft redet, landet er am Ende doch beim Auto. Kurz nach der Erfindung des Autos, sagte Volker Wissing vergangene Woche auf dem Digitalgipfel in Jena, sei der Menschheit aufgefallen, dass damit Unfälle passieren können. Also habe man Sicherheitsgurte und Führerscheine eingeführt. Aber: ""Wir haben Autos nicht verboten"", sagte er. ""Genauso müssen wir auch mit KI umgehen.""  

Das ist ein erstaunlicher Vergleich. Was Wissing sagt, klingt, als stünde ernsthaft zur Diskussion, künstliche Intelligenz zu verbieten. Das ist aber überhaupt nicht der Fall. Es ist ein alter Trick: Wissing deutet ein Extremszenario an, das gar nicht zur Debatte steht, um sich dann als Kämpfer dagegen zu inszenieren. In Wahrheit wehren sich Wissing und die Bundesregierung gar nicht gegen ein Verbot von KI, sondern gegen eine kleine Handvoll eher bescheidener Regeln.    

Es wird gerade darüber gestritten, welche Regeln für die Anbieter von künstlicher Intelligenz in Europa gelten sollen. Der AI Act, das KI-Gesetz der Europäischen Union, wird derzeit verhandelt und kurz vor dem Ziel gibt es noch eine große offene Frage. Eine, an der im Extremfall sogar das ganze Gesetz nach jahrelangen Verhandlungen noch scheitern könnte.   

Die Frage lautet: Soll es für besonders leistungsfähige Systeme künstlicher Intelligenz besondere Regeln geben? Es geht dabei um sogenannte Basismodelle oder Foundation Models. Darunter fallen unter anderem große Sprachmodelle wie GPT-4. Solche Modelle können Chatbots antreiben, aber sie fließen auch in viele andere Anwendungen ein. So könnten sie etwa in die Software von Krankenhäusern, von Rechtsanwaltskanzleien oder Personalabteilungen eingebaut werden und dort an wichtigen Entscheidungen beteiligt sein.  

Ein Mensch bekommt einen Job nicht, den er verdient hätte, oder eine Person wird nicht so behandelt, wie ihre Krankheit es verlangt: Wenn ein KI-System in einer Personalabteilung oder gar in einem Krankenhaus eine falsche Entscheidung trifft, kann das schwerwiegende Folgen haben. Eine wichtige Frage in der aktuellen Diskussion ist: Wer ist dafür zuständig, das zu verhindern?   

Die Grundlagenmodelle selbst seien nicht gefährlich, reguliert werden müsse allein die Anwendung, also zum Beispiel die Software in der Personalabteilung. So lässt sich die Sichtweise der Bundesregierung zusammenfassen. Die Regierungen von Deutschland, Frankreich und Italien wollen, dass es keine gesetzlichen Vorschriften für die Anbieter von Grundlagenmodellen gibt. Die drei Länder einigten sich vergangene Woche auf eine gemeinsame Position. Geht es nach ihnen, soll es lediglich eine Art Selbstverpflichtung geben.   

Viele KI-Fachleute aber sagen, es sind genau die Grundlagenmodelle selbst, die gefährlich werden könnten: Desinformation, falsche Entscheidungen, Cyberangriffe. Die Liste der möglichen negativen Folgen von KI ist lang. Wenn sie in der Zukunft tatsächlich eintreten, schreibt der KI-Experte Gary Marcus, dann werde man sich vielleicht einmal fragen: ""Hätte das verhindert werden können, wenn Macron und Scholz nicht den besten Versuch des Planeten blockiert hätten, Techfirmen in die Schranken zu weisen?"" 
"
Künstliche Intelligenz,Zeit,2023-12-03,https://www.zeit.de/politik/ausland/2023-12/israel-gaza-hamas-zielauswahl-ki-kollateralschaden,"Krieg in Gaza: Die ""Zielfabrik"" der israelischen Armee | ZEIT ONLINE","Seit dem Überfall der Hamas auf Israel fliegt die israelische Armee (Israel Defense Forces, kurz IDF) zahllose Luftangriffe auf den Gazastreifen. Wie Satellitenbilder zeigen, werden dabei auch ganze Wohnsiedlungen zerstört. Mehreren Medienberichten zufolge sind die enormen Zerstörungen Folge einer veränderten Strategie der israelischen Militärführung. Um Hamas-Mitglieder zu töten, würden inzwischen sehr viel mehr zivile Opfer in Kauf genommen als bei früheren Angriffen, schreiben das israelische Magazin +972 und die Nachrichtenseite Local Call. Außerdem nutzten die IDF für die Auswahl der Bombardierungsziele künstliche Intelligenz, berichten +972, Local Call und der britische Guardian unter Berufung auf Quellen im Militär. Dadurch habe sich die Zahl der potenziellen Ziele vervielfacht.

Israel macht keinen Hehl daraus, dass man versucht, die Hamas vollständig zu zerstören. Raketenstellungen, Bunker, Tunnel und Kommandozentralen werden ebenso angegriffen wie die Wohnhäuser einzelner Hamas-Mitglieder. Letztere werden von der israelischen Luftwaffe auch dann beschossen, wenn sich darin unbeteiligte Zivilisten befinden. In der Sprache des Militärs heißen solche Opfer euphemistisch Kollateralschaden, weil sie ungewollt, aber unvermeidbar seien, um ein Ziel zu erreichen. 

Die Regeln, wie viele zivile Tote dabei in Kauf genommen werden, seien bei dem derzeitigen Krieg gelockert worden, schreibt das Magazin +972. In einem Fall habe die Militärführung die Tötung Hunderter palästinensischer Zivilisten genehmigt, um einen einzigen hochrangigen Hamas-Kommandeur zu treffen. ""Die Zahlen stiegen von Dutzenden von zivilen Todesfällen, die als Kollateralschaden im Rahmen eines Angriffs auf einen hochrangigen Beamten bei früheren Einsätzen zugelassen wurden, auf Hunderte von zivilen Todesfällen als Kollateralschaden"", zitiert das Magazin eine anonyme Quelle.

Die für die Auswahl der Ziele zuständige israelische Einheit nutzt dazu inzwischen auch künstliche Intelligenz. Die seit 2019 existierende Direktion für Ziele, wie die IDF sie nennen, arbeite mit allen Geheimdiensten des Landes zusammen, um Hamas-Strukturen ausfindig zu machen. Dabei würden auch ""automatische Werkzeuge"" und ""künstliche Intelligenzsysteme"" genutzt, heißt es in einer Mitteilung der IDF. Die Armee nennt das System ""Verkündigung"" oder ""Botschaft"" und teilt mit, es habe die Identifizierung von Zielen erheblich beschleunigt und ausgeweitet. Die IDF bezeichnet diese Einheit darin als regelrechte ""Zielfabrik"".

Laut Aviv Kochavi, der bis Januar 2023 Generalstabschef der israelischen Streitkräfte war, wurde das System bereits im elftägigen Krieg Israels mit der Hamas im Mai 2021 eingesetzt. Es habe bereits damals 100 Ziele pro Tag identifiziert, sagte er in einem Interview mit YNet vor Beginn der derzeitigen Kämpfe. ""Um das ins Verhältnis zu setzen: Früher ermittelten wir in Gaza 50 Ziele pro Jahr. Jetzt hat diese Maschine an einem einzigen Tag 100 Ziele identifiziert, von denen 50 Prozent anschließend angegriffen wurden.""

Seit es maschinelle Systeme gibt, die große Datenmengen filtern, darin Muster suchen und aufgrund dessen Entscheidungen treffen, existiert die Sorge, dass sie auch in Kriegen eingesetzt werden. Das ist längst der Fall. Aus Sicht des Militärs ist diese Entwicklung unvermeidbar. Vernetzte Waffen und Überwachungssysteme auf modernen Gefechtsfeldern erzeugen Unmengen von Daten, die für einzelne Menschen nicht mehr durchschaubar sind. Technik soll dabei helfen, in diesen Daten Sinn zu finden. Auch im Krieg in der Ukraine sind KI-Systeme im Einsatz, um Taktiken zu identifizieren und Ziele auszuwählen. 

Das aber erzeugt technische und vor allem ethische Probleme. Denn die Entscheidungen solcher Systeme sind nicht mehr einfach nachvollziehbar. Dafür sind die Datenmengen zu groß und die Verfahren zu komplex. Noch schwieriger wird es, wenn der gesamte Prozess so wie in Israel geheim ist. Eine Beurteilung, ob eine Entscheidung ethisch gerechtfertigt ist oder nicht, ist damit kaum möglich. Auch Fehler der Systeme lassen sich kaum erkennen, im Zweifel auch nicht für jene, die sie bedienen. 

""Andere Staaten schauen zu und lernen"", zitiert der Guardian einen ehemaligen Sicherheitsbeamten des Weißen Hauses, der mit dem Einsatz autonomer Systeme durch das US-Militär vertraut ist. Wenn die IDF künstliche Intelligenz in großem Umfang nutzten, ""um gezielte Entscheidungen zu treffen, die Folgen für Leben und Tod haben"", sei das in den Augen anderer Länder sicher ein wichtiger Moment. ""Von allen technologischen Revolutionen dürfte die künstliche Intelligenz die radikalste sein, im Guten wie im Schlechten"", sagte der 
ehemalige IDF-Chef Kochavi.

Sichtbar ist für externe Beobachter nur das Ergebnis. Die IDF teilten Anfang November mit, ihre Zielsuchabteilung habe ""mehr als 12.000"" Ziele in Gaza identifiziert. Grund dafür scheint neben der KI aber auch die veränderte Strategie der IDF zu sein. 

Das Magazin +972 und Local Call zitieren dazu einen nicht näher identifizierten Menschen, der bei früheren Einsätzen im Gazastreifen an Zielentscheidungen mitgearbeitet habe. Bisher seien Häuser von Nachwuchskräften der Hamas nicht angegriffen worden. Das habe sich nun offenbar geändert und sie würden bombardiert, unabhängig vom Rang der Hamas-Mitglieder, die dort wohnten. Außerdem würden nun auch Hochhäuser angegriffen. Satellitenbilder bestätigen das. Auf aktuellen Bildern des Anbieters Maxar ist beispielsweise zu erkennen, dass in den vergangenen Wochen ganze Hochhausblocks zerstört wurden.

Die genaue Zahl der Opfer dieser Strategie ist unbekannt. Nach Angaben des Gesundheitsministeriums im von der Hamas kontrollierten Gazastreifen wurden bei den Angriffen der IDF bisher mehr als 15.000 Menschen getötet. Überprüft werden kann das derzeit nicht.
"
Künstliche Intelligenz,Zeit,2023-12-03,https://www.zeit.de/digital/internet/2023-12/kuenstliche-intelligenz-arbeitswelt-automatisierung-carl-frey,"Künstliche Intelligenz in der Arbeitswelt: ""Wir werden Bürojobs sehen, die einfach verschwinden"" | ZEIT ONLINE",
Künstliche Intelligenz,Zeit,2023-12-02,https://www.zeit.de/2023/51/kuenstliche-intelligenz-wissenschaftliches-schreiben-routinearbeit-chatgpt-ersatz,Künstliche Intelligenz: KI kann wissenschaftliches Schreiben nicht ersetzen | ZEIT Arbeit,"Tina Kretschmer ist Professorin für Erziehungswissenschaften an der Universität Groningen, Niederlande.

Anwendungen der künstlichen Intelligenz, die auf sogenannten Large Language Models – großen Sprachmodellen – basieren, werden seit etwa einem Jahr an Universitäten, von Herausgeberinnen von Fachzeitschriften und bei Forschungsförderorganisationen wie der Deutschen Forschungsgemeinschaft kontrovers diskutiert. Dürfen Studierende ChatGPT nutzen, um Essays oder Laborberichte zu schreiben? Sollen Wissenschaftlerinnen sich bald nicht mehr die Nächte um die Ohren schlagen müssen, um Förderanträge zu überarbeiten, sondern die Arbeit einer KI übergeben können? Viele sind überzeugt, dass die verbalen Fähigkeiten von ChatGPT und Co. die von jenen übersteigen, die Englisch nicht als Muttersprache sprechen – so etwa Juan M. Lavista Ferres, Chief Data Scientist bei Microsoft, der im Gastkommentar in der ZEIT dafür plädiert hat, die KI schreiben zu lassen.

Natürlich gibt es KI-Anwendungen, die die Wissenschaft vereinfachen, weil sie stupide, aber zeitintensive Arbeiten übernehmen. Übersetzungsprogramme sind nützlich, wenn einem ein Wort nicht einfällt oder der Satzbau holpert. Doch wenn es darum geht, was wir als Wissenschaftlerinnen akzeptieren sollten und was nicht, ist es wichtig zu wissen, ob die KI wissenschaftliche Texte produziert oder nur verbessert. Diese Unterscheidung wird von Juan M. Lavista Ferres nicht gemacht.

Programme wie ChatGPT tragen nämlich mitnichten dazu bei, dass Nicht-Muttersprachler besseres Englisch lernen. Die Erfahrung zeigt, dass der künstlich geschaffene Text kaum mehr reflektiert wird. ChatGPT erschafft scheinbar flüssige Texte, die jedoch von Worthülsen wie ""Konstrukt"" oder ""Konzept"" nur so wimmeln. Es greift in der ohne Bezahlung zugänglichen Version weder auf neueste Veröffentlichungen zurück noch auf Publikationen hinter Paywalls. Sicher werden diese Schwächen in zukünftigen Versionen verschwinden. Aber die Annahme, Wissenschaft werde besser, wenn man die Kommunikation an Anwendungen der künstlichen Intelligenz auslagert, ist ein gewaltiger Irrtum.

Als Doktormutter und Betreuerin von Bachelor- und Masterarbeiten im sozialwissenschaftlichen Bereich unterrichte ich wissenschaftlichen Nachwuchs darin, Texte gezielt zu rezipieren, eine Synthese des Feldes zu erstellen, Hypothesen abzuleiten und Ergebnisse im Kontext bestehender Forschung zu diskutieren. Die intensive Beschäftigung mit Text, mit Argumenten und Interpretationen ist wesentlicher Bestandteil der wissenschaftlichen Ausbildung und der Wissenschaft selbst. Es mag sein, dass KI in manchen Naturwissenschaften einen Teil der Kommunikation übernehmen kann, in den Geistes- und Sozialwissenschaften würde man damit einen wichtigen Teil der forschenden Tätigkeit an den Computer übergeben.

Wenn ich Texte von Nachwuchswissenschaftlerinnen korrigiere, kommentiere ich sie, mache Vorschläge für Veränderungen, erläutere, wo und warum eine andere Struktur, ein anderes Wort überzeugender oder schlicht deutlicher ist. Bei ChatGPT ist der Lerneffekt für den wissenschaftlichen Nachwuchs gering. Das Argument Juan M. Lavista Ferres’, dass KI die Sprache verbessere, geht nicht auf: Vielleicht ist der Text auf dem Papier lesbarer, aber die Lernenden machen sicher keine Sprachfortschritte.

Die Gefahr ist groß, dass innovative Texte bald nur noch von jenen kommen, die Englisch auf Muttersprachniveau sprechen, und dass andere der falschen Überzeugung erliegen, die künstliche Intelligenz mache die Kommunikation durch den Wissenschaftler selbst unnötig.

Wissenschaftliches Schreiben lernen wir durch Übung, wissenschaftliches Englisch durch Praxis. KI wird in der Zukunft Zeit sparen, viele Routinearbeiten übernehmen und unser Englisch verfeinern. Wer hofft oder gar spekuliert, dass wir der KI das Schaffen, Schärfen und Präzisieren von Ideen überlassen können, vergisst, was gute, innovative Wissenschaft bedeutet: in neuen, ungedachten Bahnen zu denken. 
"
AI,Zeit,2023-12-04,https://www.zeit.de/digital/2023-11/ki-gesetz-eu-parlament-regulierung-bundesregierung,KI-Gesetz der EU: Regulierung oder Innovation? Beides! | ZEIT ONLINE,"Wenn der Verkehrsminister, der auch Digitalminister ist, über die Technologie der Zukunft redet, landet er am Ende doch beim Auto. Kurz nach der Erfindung des Autos, sagte Volker Wissing vergangene Woche auf dem Digitalgipfel in Jena, sei der Menschheit aufgefallen, dass damit Unfälle passieren können. Also habe man Sicherheitsgurte und Führerscheine eingeführt. Aber: ""Wir haben Autos nicht verboten"", sagte er. ""Genauso müssen wir auch mit KI umgehen.""  

Das ist ein erstaunlicher Vergleich. Was Wissing sagt, klingt, als stünde ernsthaft zur Diskussion, künstliche Intelligenz zu verbieten. Das ist aber überhaupt nicht der Fall. Es ist ein alter Trick: Wissing deutet ein Extremszenario an, das gar nicht zur Debatte steht, um sich dann als Kämpfer dagegen zu inszenieren. In Wahrheit wehren sich Wissing und die Bundesregierung gar nicht gegen ein Verbot von KI, sondern gegen eine kleine Handvoll eher bescheidener Regeln.    

Es wird gerade darüber gestritten, welche Regeln für die Anbieter von künstlicher Intelligenz in Europa gelten sollen. Der AI Act, das KI-Gesetz der Europäischen Union, wird derzeit verhandelt und kurz vor dem Ziel gibt es noch eine große offene Frage. Eine, an der im Extremfall sogar das ganze Gesetz nach jahrelangen Verhandlungen noch scheitern könnte.   

Die Frage lautet: Soll es für besonders leistungsfähige Systeme künstlicher Intelligenz besondere Regeln geben? Es geht dabei um sogenannte Basismodelle oder Foundation Models. Darunter fallen unter anderem große Sprachmodelle wie GPT-4. Solche Modelle können Chatbots antreiben, aber sie fließen auch in viele andere Anwendungen ein. So könnten sie etwa in die Software von Krankenhäusern, von Rechtsanwaltskanzleien oder Personalabteilungen eingebaut werden und dort an wichtigen Entscheidungen beteiligt sein.  

Ein Mensch bekommt einen Job nicht, den er verdient hätte, oder eine Person wird nicht so behandelt, wie ihre Krankheit es verlangt: Wenn ein KI-System in einer Personalabteilung oder gar in einem Krankenhaus eine falsche Entscheidung trifft, kann das schwerwiegende Folgen haben. Eine wichtige Frage in der aktuellen Diskussion ist: Wer ist dafür zuständig, das zu verhindern?   

Die Grundlagenmodelle selbst seien nicht gefährlich, reguliert werden müsse allein die Anwendung, also zum Beispiel die Software in der Personalabteilung. So lässt sich die Sichtweise der Bundesregierung zusammenfassen. Die Regierungen von Deutschland, Frankreich und Italien wollen, dass es keine gesetzlichen Vorschriften für die Anbieter von Grundlagenmodellen gibt. Die drei Länder einigten sich vergangene Woche auf eine gemeinsame Position. Geht es nach ihnen, soll es lediglich eine Art Selbstverpflichtung geben.   

Viele KI-Fachleute aber sagen, es sind genau die Grundlagenmodelle selbst, die gefährlich werden könnten: Desinformation, falsche Entscheidungen, Cyberangriffe. Die Liste der möglichen negativen Folgen von KI ist lang. Wenn sie in der Zukunft tatsächlich eintreten, schreibt der KI-Experte Gary Marcus, dann werde man sich vielleicht einmal fragen: ""Hätte das verhindert werden können, wenn Macron und Scholz nicht den besten Versuch des Planeten blockiert hätten, Techfirmen in die Schranken zu weisen?"" 
"
AI,Zeit,2023-12-03,https://www.zeit.de/politik/ausland/2023-12/israel-gaza-hamas-zielauswahl-ki-kollateralschaden,"Krieg in Gaza: Die ""Zielfabrik"" der israelischen Armee | ZEIT ONLINE","Seit dem Überfall der Hamas auf Israel fliegt die israelische Armee (Israel Defense Forces, kurz IDF) zahllose Luftangriffe auf den Gazastreifen. Wie Satellitenbilder zeigen, werden dabei auch ganze Wohnsiedlungen zerstört. Mehreren Medienberichten zufolge sind die enormen Zerstörungen Folge einer veränderten Strategie der israelischen Militärführung. Um Hamas-Mitglieder zu töten, würden inzwischen sehr viel mehr zivile Opfer in Kauf genommen als bei früheren Angriffen, schreiben das israelische Magazin +972 und die Nachrichtenseite Local Call. Außerdem nutzten die IDF für die Auswahl der Bombardierungsziele künstliche Intelligenz, berichten +972, Local Call und der britische Guardian unter Berufung auf Quellen im Militär. Dadurch habe sich die Zahl der potenziellen Ziele vervielfacht.

Israel macht keinen Hehl daraus, dass man versucht, die Hamas vollständig zu zerstören. Raketenstellungen, Bunker, Tunnel und Kommandozentralen werden ebenso angegriffen wie die Wohnhäuser einzelner Hamas-Mitglieder. Letztere werden von der israelischen Luftwaffe auch dann beschossen, wenn sich darin unbeteiligte Zivilisten befinden. In der Sprache des Militärs heißen solche Opfer euphemistisch Kollateralschaden, weil sie ungewollt, aber unvermeidbar seien, um ein Ziel zu erreichen. 

Die Regeln, wie viele zivile Tote dabei in Kauf genommen werden, seien bei dem derzeitigen Krieg gelockert worden, schreibt das Magazin +972. In einem Fall habe die Militärführung die Tötung Hunderter palästinensischer Zivilisten genehmigt, um einen einzigen hochrangigen Hamas-Kommandeur zu treffen. ""Die Zahlen stiegen von Dutzenden von zivilen Todesfällen, die als Kollateralschaden im Rahmen eines Angriffs auf einen hochrangigen Beamten bei früheren Einsätzen zugelassen wurden, auf Hunderte von zivilen Todesfällen als Kollateralschaden"", zitiert das Magazin eine anonyme Quelle.

Die für die Auswahl der Ziele zuständige israelische Einheit nutzt dazu inzwischen auch künstliche Intelligenz. Die seit 2019 existierende Direktion für Ziele, wie die IDF sie nennen, arbeite mit allen Geheimdiensten des Landes zusammen, um Hamas-Strukturen ausfindig zu machen. Dabei würden auch ""automatische Werkzeuge"" und ""künstliche Intelligenzsysteme"" genutzt, heißt es in einer Mitteilung der IDF. Die Armee nennt das System ""Verkündigung"" oder ""Botschaft"" und teilt mit, es habe die Identifizierung von Zielen erheblich beschleunigt und ausgeweitet. Die IDF bezeichnet diese Einheit darin als regelrechte ""Zielfabrik"".

Laut Aviv Kochavi, der bis Januar 2023 Generalstabschef der israelischen Streitkräfte war, wurde das System bereits im elftägigen Krieg Israels mit der Hamas im Mai 2021 eingesetzt. Es habe bereits damals 100 Ziele pro Tag identifiziert, sagte er in einem Interview mit YNet vor Beginn der derzeitigen Kämpfe. ""Um das ins Verhältnis zu setzen: Früher ermittelten wir in Gaza 50 Ziele pro Jahr. Jetzt hat diese Maschine an einem einzigen Tag 100 Ziele identifiziert, von denen 50 Prozent anschließend angegriffen wurden.""

Seit es maschinelle Systeme gibt, die große Datenmengen filtern, darin Muster suchen und aufgrund dessen Entscheidungen treffen, existiert die Sorge, dass sie auch in Kriegen eingesetzt werden. Das ist längst der Fall. Aus Sicht des Militärs ist diese Entwicklung unvermeidbar. Vernetzte Waffen und Überwachungssysteme auf modernen Gefechtsfeldern erzeugen Unmengen von Daten, die für einzelne Menschen nicht mehr durchschaubar sind. Technik soll dabei helfen, in diesen Daten Sinn zu finden. Auch im Krieg in der Ukraine sind KI-Systeme im Einsatz, um Taktiken zu identifizieren und Ziele auszuwählen. 

Das aber erzeugt technische und vor allem ethische Probleme. Denn die Entscheidungen solcher Systeme sind nicht mehr einfach nachvollziehbar. Dafür sind die Datenmengen zu groß und die Verfahren zu komplex. Noch schwieriger wird es, wenn der gesamte Prozess so wie in Israel geheim ist. Eine Beurteilung, ob eine Entscheidung ethisch gerechtfertigt ist oder nicht, ist damit kaum möglich. Auch Fehler der Systeme lassen sich kaum erkennen, im Zweifel auch nicht für jene, die sie bedienen. 

""Andere Staaten schauen zu und lernen"", zitiert der Guardian einen ehemaligen Sicherheitsbeamten des Weißen Hauses, der mit dem Einsatz autonomer Systeme durch das US-Militär vertraut ist. Wenn die IDF künstliche Intelligenz in großem Umfang nutzten, ""um gezielte Entscheidungen zu treffen, die Folgen für Leben und Tod haben"", sei das in den Augen anderer Länder sicher ein wichtiger Moment. ""Von allen technologischen Revolutionen dürfte die künstliche Intelligenz die radikalste sein, im Guten wie im Schlechten"", sagte der 
ehemalige IDF-Chef Kochavi.

Sichtbar ist für externe Beobachter nur das Ergebnis. Die IDF teilten Anfang November mit, ihre Zielsuchabteilung habe ""mehr als 12.000"" Ziele in Gaza identifiziert. Grund dafür scheint neben der KI aber auch die veränderte Strategie der IDF zu sein. 

Das Magazin +972 und Local Call zitieren dazu einen nicht näher identifizierten Menschen, der bei früheren Einsätzen im Gazastreifen an Zielentscheidungen mitgearbeitet habe. Bisher seien Häuser von Nachwuchskräften der Hamas nicht angegriffen worden. Das habe sich nun offenbar geändert und sie würden bombardiert, unabhängig vom Rang der Hamas-Mitglieder, die dort wohnten. Außerdem würden nun auch Hochhäuser angegriffen. Satellitenbilder bestätigen das. Auf aktuellen Bildern des Anbieters Maxar ist beispielsweise zu erkennen, dass in den vergangenen Wochen ganze Hochhausblocks zerstört wurden.

Die genaue Zahl der Opfer dieser Strategie ist unbekannt. Nach Angaben des Gesundheitsministeriums im von der Hamas kontrollierten Gazastreifen wurden bei den Angriffen der IDF bisher mehr als 15.000 Menschen getötet. Überprüft werden kann das derzeit nicht.
"
AI,Zeit,2023-12-02,https://www.zeit.de/2023/51/kuenstliche-intelligenz-wissenschaftliches-schreiben-routinearbeit-chatgpt-ersatz,Künstliche Intelligenz: KI kann wissenschaftliches Schreiben nicht ersetzen | ZEIT Arbeit,"Tina Kretschmer ist Professorin für Erziehungswissenschaften an der Universität Groningen, Niederlande.

Anwendungen der künstlichen Intelligenz, die auf sogenannten Large Language Models – großen Sprachmodellen – basieren, werden seit etwa einem Jahr an Universitäten, von Herausgeberinnen von Fachzeitschriften und bei Forschungsförderorganisationen wie der Deutschen Forschungsgemeinschaft kontrovers diskutiert. Dürfen Studierende ChatGPT nutzen, um Essays oder Laborberichte zu schreiben? Sollen Wissenschaftlerinnen sich bald nicht mehr die Nächte um die Ohren schlagen müssen, um Förderanträge zu überarbeiten, sondern die Arbeit einer KI übergeben können? Viele sind überzeugt, dass die verbalen Fähigkeiten von ChatGPT und Co. die von jenen übersteigen, die Englisch nicht als Muttersprache sprechen – so etwa Juan M. Lavista Ferres, Chief Data Scientist bei Microsoft, der im Gastkommentar in der ZEIT dafür plädiert hat, die KI schreiben zu lassen.

Natürlich gibt es KI-Anwendungen, die die Wissenschaft vereinfachen, weil sie stupide, aber zeitintensive Arbeiten übernehmen. Übersetzungsprogramme sind nützlich, wenn einem ein Wort nicht einfällt oder der Satzbau holpert. Doch wenn es darum geht, was wir als Wissenschaftlerinnen akzeptieren sollten und was nicht, ist es wichtig zu wissen, ob die KI wissenschaftliche Texte produziert oder nur verbessert. Diese Unterscheidung wird von Juan M. Lavista Ferres nicht gemacht.

Programme wie ChatGPT tragen nämlich mitnichten dazu bei, dass Nicht-Muttersprachler besseres Englisch lernen. Die Erfahrung zeigt, dass der künstlich geschaffene Text kaum mehr reflektiert wird. ChatGPT erschafft scheinbar flüssige Texte, die jedoch von Worthülsen wie ""Konstrukt"" oder ""Konzept"" nur so wimmeln. Es greift in der ohne Bezahlung zugänglichen Version weder auf neueste Veröffentlichungen zurück noch auf Publikationen hinter Paywalls. Sicher werden diese Schwächen in zukünftigen Versionen verschwinden. Aber die Annahme, Wissenschaft werde besser, wenn man die Kommunikation an Anwendungen der künstlichen Intelligenz auslagert, ist ein gewaltiger Irrtum.

Als Doktormutter und Betreuerin von Bachelor- und Masterarbeiten im sozialwissenschaftlichen Bereich unterrichte ich wissenschaftlichen Nachwuchs darin, Texte gezielt zu rezipieren, eine Synthese des Feldes zu erstellen, Hypothesen abzuleiten und Ergebnisse im Kontext bestehender Forschung zu diskutieren. Die intensive Beschäftigung mit Text, mit Argumenten und Interpretationen ist wesentlicher Bestandteil der wissenschaftlichen Ausbildung und der Wissenschaft selbst. Es mag sein, dass KI in manchen Naturwissenschaften einen Teil der Kommunikation übernehmen kann, in den Geistes- und Sozialwissenschaften würde man damit einen wichtigen Teil der forschenden Tätigkeit an den Computer übergeben.

Wenn ich Texte von Nachwuchswissenschaftlerinnen korrigiere, kommentiere ich sie, mache Vorschläge für Veränderungen, erläutere, wo und warum eine andere Struktur, ein anderes Wort überzeugender oder schlicht deutlicher ist. Bei ChatGPT ist der Lerneffekt für den wissenschaftlichen Nachwuchs gering. Das Argument Juan M. Lavista Ferres’, dass KI die Sprache verbessere, geht nicht auf: Vielleicht ist der Text auf dem Papier lesbarer, aber die Lernenden machen sicher keine Sprachfortschritte.

Die Gefahr ist groß, dass innovative Texte bald nur noch von jenen kommen, die Englisch auf Muttersprachniveau sprechen, und dass andere der falschen Überzeugung erliegen, die künstliche Intelligenz mache die Kommunikation durch den Wissenschaftler selbst unnötig.

Wissenschaftliches Schreiben lernen wir durch Übung, wissenschaftliches Englisch durch Praxis. KI wird in der Zukunft Zeit sparen, viele Routinearbeiten übernehmen und unser Englisch verfeinern. Wer hofft oder gar spekuliert, dass wir der KI das Schaffen, Schärfen und Präzisieren von Ideen überlassen können, vergisst, was gute, innovative Wissenschaft bedeutet: in neuen, ungedachten Bahnen zu denken. 
"
Artificial Intelligence,Zeit,2023-12-03,https://www.zeit.de/digital/internet/2023-12/kuenstliche-intelligenz-arbeitswelt-automatisierung-carl-frey,"Künstliche Intelligenz in der Arbeitswelt: ""Wir werden Bürojobs sehen, die einfach verschwinden"" | ZEIT ONLINE",
KI,Zeit,2023-12-04,https://www.zeit.de/digital/2023-11/ki-gesetz-eu-parlament-regulierung-bundesregierung,KI-Gesetz der EU: Regulierung oder Innovation? Beides! | ZEIT ONLINE,"Wenn der Verkehrsminister, der auch Digitalminister ist, über die Technologie der Zukunft redet, landet er am Ende doch beim Auto. Kurz nach der Erfindung des Autos, sagte Volker Wissing vergangene Woche auf dem Digitalgipfel in Jena, sei der Menschheit aufgefallen, dass damit Unfälle passieren können. Also habe man Sicherheitsgurte und Führerscheine eingeführt. Aber: ""Wir haben Autos nicht verboten"", sagte er. ""Genauso müssen wir auch mit KI umgehen.""  

Das ist ein erstaunlicher Vergleich. Was Wissing sagt, klingt, als stünde ernsthaft zur Diskussion, künstliche Intelligenz zu verbieten. Das ist aber überhaupt nicht der Fall. Es ist ein alter Trick: Wissing deutet ein Extremszenario an, das gar nicht zur Debatte steht, um sich dann als Kämpfer dagegen zu inszenieren. In Wahrheit wehren sich Wissing und die Bundesregierung gar nicht gegen ein Verbot von KI, sondern gegen eine kleine Handvoll eher bescheidener Regeln.    

Es wird gerade darüber gestritten, welche Regeln für die Anbieter von künstlicher Intelligenz in Europa gelten sollen. Der AI Act, das KI-Gesetz der Europäischen Union, wird derzeit verhandelt und kurz vor dem Ziel gibt es noch eine große offene Frage. Eine, an der im Extremfall sogar das ganze Gesetz nach jahrelangen Verhandlungen noch scheitern könnte.   

Die Frage lautet: Soll es für besonders leistungsfähige Systeme künstlicher Intelligenz besondere Regeln geben? Es geht dabei um sogenannte Basismodelle oder Foundation Models. Darunter fallen unter anderem große Sprachmodelle wie GPT-4. Solche Modelle können Chatbots antreiben, aber sie fließen auch in viele andere Anwendungen ein. So könnten sie etwa in die Software von Krankenhäusern, von Rechtsanwaltskanzleien oder Personalabteilungen eingebaut werden und dort an wichtigen Entscheidungen beteiligt sein.  

Ein Mensch bekommt einen Job nicht, den er verdient hätte, oder eine Person wird nicht so behandelt, wie ihre Krankheit es verlangt: Wenn ein KI-System in einer Personalabteilung oder gar in einem Krankenhaus eine falsche Entscheidung trifft, kann das schwerwiegende Folgen haben. Eine wichtige Frage in der aktuellen Diskussion ist: Wer ist dafür zuständig, das zu verhindern?   

Die Grundlagenmodelle selbst seien nicht gefährlich, reguliert werden müsse allein die Anwendung, also zum Beispiel die Software in der Personalabteilung. So lässt sich die Sichtweise der Bundesregierung zusammenfassen. Die Regierungen von Deutschland, Frankreich und Italien wollen, dass es keine gesetzlichen Vorschriften für die Anbieter von Grundlagenmodellen gibt. Die drei Länder einigten sich vergangene Woche auf eine gemeinsame Position. Geht es nach ihnen, soll es lediglich eine Art Selbstverpflichtung geben.   

Viele KI-Fachleute aber sagen, es sind genau die Grundlagenmodelle selbst, die gefährlich werden könnten: Desinformation, falsche Entscheidungen, Cyberangriffe. Die Liste der möglichen negativen Folgen von KI ist lang. Wenn sie in der Zukunft tatsächlich eintreten, schreibt der KI-Experte Gary Marcus, dann werde man sich vielleicht einmal fragen: ""Hätte das verhindert werden können, wenn Macron und Scholz nicht den besten Versuch des Planeten blockiert hätten, Techfirmen in die Schranken zu weisen?"" 
"
KI,Zeit,2023-12-03,https://www.zeit.de/politik/ausland/2023-12/israel-gaza-hamas-zielauswahl-ki-kollateralschaden,"Krieg in Gaza: Die ""Zielfabrik"" der israelischen Armee | ZEIT ONLINE","Seit dem Überfall der Hamas auf Israel fliegt die israelische Armee (Israel Defense Forces, kurz IDF) zahllose Luftangriffe auf den Gazastreifen. Wie Satellitenbilder zeigen, werden dabei auch ganze Wohnsiedlungen zerstört. Mehreren Medienberichten zufolge sind die enormen Zerstörungen Folge einer veränderten Strategie der israelischen Militärführung. Um Hamas-Mitglieder zu töten, würden inzwischen sehr viel mehr zivile Opfer in Kauf genommen als bei früheren Angriffen, schreiben das israelische Magazin +972 und die Nachrichtenseite Local Call. Außerdem nutzten die IDF für die Auswahl der Bombardierungsziele künstliche Intelligenz, berichten +972, Local Call und der britische Guardian unter Berufung auf Quellen im Militär. Dadurch habe sich die Zahl der potenziellen Ziele vervielfacht.

Israel macht keinen Hehl daraus, dass man versucht, die Hamas vollständig zu zerstören. Raketenstellungen, Bunker, Tunnel und Kommandozentralen werden ebenso angegriffen wie die Wohnhäuser einzelner Hamas-Mitglieder. Letztere werden von der israelischen Luftwaffe auch dann beschossen, wenn sich darin unbeteiligte Zivilisten befinden. In der Sprache des Militärs heißen solche Opfer euphemistisch Kollateralschaden, weil sie ungewollt, aber unvermeidbar seien, um ein Ziel zu erreichen. 

Die Regeln, wie viele zivile Tote dabei in Kauf genommen werden, seien bei dem derzeitigen Krieg gelockert worden, schreibt das Magazin +972. In einem Fall habe die Militärführung die Tötung Hunderter palästinensischer Zivilisten genehmigt, um einen einzigen hochrangigen Hamas-Kommandeur zu treffen. ""Die Zahlen stiegen von Dutzenden von zivilen Todesfällen, die als Kollateralschaden im Rahmen eines Angriffs auf einen hochrangigen Beamten bei früheren Einsätzen zugelassen wurden, auf Hunderte von zivilen Todesfällen als Kollateralschaden"", zitiert das Magazin eine anonyme Quelle.

Die für die Auswahl der Ziele zuständige israelische Einheit nutzt dazu inzwischen auch künstliche Intelligenz. Die seit 2019 existierende Direktion für Ziele, wie die IDF sie nennen, arbeite mit allen Geheimdiensten des Landes zusammen, um Hamas-Strukturen ausfindig zu machen. Dabei würden auch ""automatische Werkzeuge"" und ""künstliche Intelligenzsysteme"" genutzt, heißt es in einer Mitteilung der IDF. Die Armee nennt das System ""Verkündigung"" oder ""Botschaft"" und teilt mit, es habe die Identifizierung von Zielen erheblich beschleunigt und ausgeweitet. Die IDF bezeichnet diese Einheit darin als regelrechte ""Zielfabrik"".

Laut Aviv Kochavi, der bis Januar 2023 Generalstabschef der israelischen Streitkräfte war, wurde das System bereits im elftägigen Krieg Israels mit der Hamas im Mai 2021 eingesetzt. Es habe bereits damals 100 Ziele pro Tag identifiziert, sagte er in einem Interview mit YNet vor Beginn der derzeitigen Kämpfe. ""Um das ins Verhältnis zu setzen: Früher ermittelten wir in Gaza 50 Ziele pro Jahr. Jetzt hat diese Maschine an einem einzigen Tag 100 Ziele identifiziert, von denen 50 Prozent anschließend angegriffen wurden.""

Seit es maschinelle Systeme gibt, die große Datenmengen filtern, darin Muster suchen und aufgrund dessen Entscheidungen treffen, existiert die Sorge, dass sie auch in Kriegen eingesetzt werden. Das ist längst der Fall. Aus Sicht des Militärs ist diese Entwicklung unvermeidbar. Vernetzte Waffen und Überwachungssysteme auf modernen Gefechtsfeldern erzeugen Unmengen von Daten, die für einzelne Menschen nicht mehr durchschaubar sind. Technik soll dabei helfen, in diesen Daten Sinn zu finden. Auch im Krieg in der Ukraine sind KI-Systeme im Einsatz, um Taktiken zu identifizieren und Ziele auszuwählen. 

Das aber erzeugt technische und vor allem ethische Probleme. Denn die Entscheidungen solcher Systeme sind nicht mehr einfach nachvollziehbar. Dafür sind die Datenmengen zu groß und die Verfahren zu komplex. Noch schwieriger wird es, wenn der gesamte Prozess so wie in Israel geheim ist. Eine Beurteilung, ob eine Entscheidung ethisch gerechtfertigt ist oder nicht, ist damit kaum möglich. Auch Fehler der Systeme lassen sich kaum erkennen, im Zweifel auch nicht für jene, die sie bedienen. 

""Andere Staaten schauen zu und lernen"", zitiert der Guardian einen ehemaligen Sicherheitsbeamten des Weißen Hauses, der mit dem Einsatz autonomer Systeme durch das US-Militär vertraut ist. Wenn die IDF künstliche Intelligenz in großem Umfang nutzten, ""um gezielte Entscheidungen zu treffen, die Folgen für Leben und Tod haben"", sei das in den Augen anderer Länder sicher ein wichtiger Moment. ""Von allen technologischen Revolutionen dürfte die künstliche Intelligenz die radikalste sein, im Guten wie im Schlechten"", sagte der 
ehemalige IDF-Chef Kochavi.

Sichtbar ist für externe Beobachter nur das Ergebnis. Die IDF teilten Anfang November mit, ihre Zielsuchabteilung habe ""mehr als 12.000"" Ziele in Gaza identifiziert. Grund dafür scheint neben der KI aber auch die veränderte Strategie der IDF zu sein. 

Das Magazin +972 und Local Call zitieren dazu einen nicht näher identifizierten Menschen, der bei früheren Einsätzen im Gazastreifen an Zielentscheidungen mitgearbeitet habe. Bisher seien Häuser von Nachwuchskräften der Hamas nicht angegriffen worden. Das habe sich nun offenbar geändert und sie würden bombardiert, unabhängig vom Rang der Hamas-Mitglieder, die dort wohnten. Außerdem würden nun auch Hochhäuser angegriffen. Satellitenbilder bestätigen das. Auf aktuellen Bildern des Anbieters Maxar ist beispielsweise zu erkennen, dass in den vergangenen Wochen ganze Hochhausblocks zerstört wurden.

Die genaue Zahl der Opfer dieser Strategie ist unbekannt. Nach Angaben des Gesundheitsministeriums im von der Hamas kontrollierten Gazastreifen wurden bei den Angriffen der IDF bisher mehr als 15.000 Menschen getötet. Überprüft werden kann das derzeit nicht.
"
KI,Zeit,2023-12-02,https://www.zeit.de/news/2023-12/02/neuer-polizeipraesident-bei-verbrechensbekaempfung-auch-ki,Polizei: Neuer Polizeipräsident: Bei Verbrechensbekämpfung auch KI | ZEIT ONLINE,"Hamburgs neuer Polizeipräsident Falk Schnabel setzt bei der Aufklärung von Straftaten auch auf Künstliche Intelligenz (KI). «Kriminalität hat sich vielfach in den virtuellen Raum, ins Internet oder Darknet, verlagert», sagte Schnabel, der Anfang November sein Amt angetreten hat, der «Welt am Sonntag». Da müsse die Polizei Schritt halten. «Wir müssen in der Lage sein, Cybercrime effektiv zu bekämpfen.» Allein im Bereich der Kinderpornografie kämen bei Ermittlungen schnell Terabyte an Daten zusammen. «Um die auszuwerten, brauchen wir Unterstützung auch durch Künstliche Intelligenz, die die riesigen Datenmengen nach strafbaren Inhalten durchforstet.»

Der Nahostkonflikt belastet aus Schnabels Sicht auch die Hamburger Polizei erheblich. Deren Auswirkungen «haben dazu geführt, dass unsere Kolleginnen und Kollegen Zwölf-Stunden-Schichten geschoben haben und kaum noch aus den Stiefeln herausgekommen sind». Doch so konnten aus seiner Sicht Versammlungen, die eine Gefahr für die öffentliche Sicherheit darstellen, überwiegend verhindert werden.

Den Kampf gegen den Antisemitismus und antisemitische Straftaten, wie sie im Zuge propalästinensischer Demonstrationen vorgekommen seien, zählt Schnabel mit zu den wichtigsten Aufgaben. Als Leiter der Staatsanwaltschaft Düsseldorf habe er Kontakt zur dortigen großen jüdischen Gemeinde gehabt. «Zu sehen, was Antisemitismus mit Jüdinnen und Juden macht, wie sehr sie in Angst leben, welche massiven Schutzvorkehrungen nach wie vor erforderlich sind, hat mich sehr bewegt.»

Unzufrieden zeigte er sich mit der geplanten Cannabisfreigabe. «Was geplant ist, wird meines Erachtens dazu führen, dass die Polizei deutlich mehr Arbeit hat, weil die Feinheiten des Gesetzes viel mehr Überprüfung erfordern.» Er wolle die Gefahren, die auch von weichen Drogen ausgehen könnten, nicht noch mal hervorheben. Aber «wir sind gut beraten, intensiv darüber nachzudenken, ob die jetzt vorgesehene Regelung wirklich so kommen sollte», sagte Schnabel.

Seinen Wechsel von der Juristerei in den Polizeidienst nannte er eine «schwere, keineswegs spontane Entscheidung». Es sei ein langer Weg gewesen, auf dem er sich selbst kritisch hinterfragt habe: «Werde ich den Herausforderungen gerecht, da ich kein gelernter Polizeibeamter bin.» Seine Motivation für den Wechsel seien die Gestaltungsmöglichkeiten und die Verantwortung für die Sicherheit der Menschen gewesen. Die Arbeit als Jurist passiere ja meist am Schreibtisch und betrachtet nur den Einzelfall. «Die Polizei ist an den Menschen und den Entwicklungen in einer Stadt oft dichter dran und hat einen umfassenderen Blick.»

© dpa-infocom, dpa:231202-99-154512/2
"
KI,Zeit,2023-12-02,https://www.zeit.de/2023/51/kuenstliche-intelligenz-wissenschaftliches-schreiben-routinearbeit-chatgpt-ersatz,Künstliche Intelligenz: KI kann wissenschaftliches Schreiben nicht ersetzen | ZEIT Arbeit,"Tina Kretschmer ist Professorin für Erziehungswissenschaften an der Universität Groningen, Niederlande.

Anwendungen der künstlichen Intelligenz, die auf sogenannten Large Language Models – großen Sprachmodellen – basieren, werden seit etwa einem Jahr an Universitäten, von Herausgeberinnen von Fachzeitschriften und bei Forschungsförderorganisationen wie der Deutschen Forschungsgemeinschaft kontrovers diskutiert. Dürfen Studierende ChatGPT nutzen, um Essays oder Laborberichte zu schreiben? Sollen Wissenschaftlerinnen sich bald nicht mehr die Nächte um die Ohren schlagen müssen, um Förderanträge zu überarbeiten, sondern die Arbeit einer KI übergeben können? Viele sind überzeugt, dass die verbalen Fähigkeiten von ChatGPT und Co. die von jenen übersteigen, die Englisch nicht als Muttersprache sprechen – so etwa Juan M. Lavista Ferres, Chief Data Scientist bei Microsoft, der im Gastkommentar in der ZEIT dafür plädiert hat, die KI schreiben zu lassen.

Natürlich gibt es KI-Anwendungen, die die Wissenschaft vereinfachen, weil sie stupide, aber zeitintensive Arbeiten übernehmen. Übersetzungsprogramme sind nützlich, wenn einem ein Wort nicht einfällt oder der Satzbau holpert. Doch wenn es darum geht, was wir als Wissenschaftlerinnen akzeptieren sollten und was nicht, ist es wichtig zu wissen, ob die KI wissenschaftliche Texte produziert oder nur verbessert. Diese Unterscheidung wird von Juan M. Lavista Ferres nicht gemacht.

Programme wie ChatGPT tragen nämlich mitnichten dazu bei, dass Nicht-Muttersprachler besseres Englisch lernen. Die Erfahrung zeigt, dass der künstlich geschaffene Text kaum mehr reflektiert wird. ChatGPT erschafft scheinbar flüssige Texte, die jedoch von Worthülsen wie ""Konstrukt"" oder ""Konzept"" nur so wimmeln. Es greift in der ohne Bezahlung zugänglichen Version weder auf neueste Veröffentlichungen zurück noch auf Publikationen hinter Paywalls. Sicher werden diese Schwächen in zukünftigen Versionen verschwinden. Aber die Annahme, Wissenschaft werde besser, wenn man die Kommunikation an Anwendungen der künstlichen Intelligenz auslagert, ist ein gewaltiger Irrtum.

Als Doktormutter und Betreuerin von Bachelor- und Masterarbeiten im sozialwissenschaftlichen Bereich unterrichte ich wissenschaftlichen Nachwuchs darin, Texte gezielt zu rezipieren, eine Synthese des Feldes zu erstellen, Hypothesen abzuleiten und Ergebnisse im Kontext bestehender Forschung zu diskutieren. Die intensive Beschäftigung mit Text, mit Argumenten und Interpretationen ist wesentlicher Bestandteil der wissenschaftlichen Ausbildung und der Wissenschaft selbst. Es mag sein, dass KI in manchen Naturwissenschaften einen Teil der Kommunikation übernehmen kann, in den Geistes- und Sozialwissenschaften würde man damit einen wichtigen Teil der forschenden Tätigkeit an den Computer übergeben.

Wenn ich Texte von Nachwuchswissenschaftlerinnen korrigiere, kommentiere ich sie, mache Vorschläge für Veränderungen, erläutere, wo und warum eine andere Struktur, ein anderes Wort überzeugender oder schlicht deutlicher ist. Bei ChatGPT ist der Lerneffekt für den wissenschaftlichen Nachwuchs gering. Das Argument Juan M. Lavista Ferres’, dass KI die Sprache verbessere, geht nicht auf: Vielleicht ist der Text auf dem Papier lesbarer, aber die Lernenden machen sicher keine Sprachfortschritte.

Die Gefahr ist groß, dass innovative Texte bald nur noch von jenen kommen, die Englisch auf Muttersprachniveau sprechen, und dass andere der falschen Überzeugung erliegen, die künstliche Intelligenz mache die Kommunikation durch den Wissenschaftler selbst unnötig.

Wissenschaftliches Schreiben lernen wir durch Übung, wissenschaftliches Englisch durch Praxis. KI wird in der Zukunft Zeit sparen, viele Routinearbeiten übernehmen und unser Englisch verfeinern. Wer hofft oder gar spekuliert, dass wir der KI das Schaffen, Schärfen und Präzisieren von Ideen überlassen können, vergisst, was gute, innovative Wissenschaft bedeutet: in neuen, ungedachten Bahnen zu denken. 
"
