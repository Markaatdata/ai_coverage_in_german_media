{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac01bf71",
   "metadata": {},
   "source": [
    "# Datensammlung von Nachrichtenseiten per Web Scraping - Sueddeutsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5808309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation Bibliotheken\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "221a9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time period: 2023-12-01 until 2024-04-26 with 37 requests\n"
     ]
    }
   ],
   "source": [
    "#Definition der Parameter für das Web Scraping \n",
    "\n",
    "#Suchwörter in Google Search API\n",
    "search_queries = [\"Künstliche Intelligenz\", \"AI\", \"Artificial Intelligence\", \"KI\"]\n",
    "\n",
    "#Lege ein Startdatum für die automatische Datenerfassung fest\n",
    "last_day_str = \"2024-04-26\"\n",
    "\n",
    "#Zeitintervall von x Tagen vor dem Zieldatum\n",
    "request_days = 3\n",
    "\n",
    "#Limit für API-Anfragen\n",
    "api_request_limit = 37\n",
    "\n",
    "#Pausenzeit des Data Scraping über Beautiful Soup in Sekunden \n",
    "scrap_pause = 5\n",
    "\n",
    "#Bereinigung der Datumsangabe\n",
    "last_day = datetime.strptime(last_day_str, \"%Y-%m-%d\")\n",
    "\n",
    "#Berechnung des letzten Tages\n",
    "first_day = last_day - timedelta(days=request_days*api_request_limit) - ((timedelta(days=1)*api_request_limit)-timedelta(days=1))\n",
    "\n",
    "#Ausgabe des Zeitraums\n",
    "print(f\"Time period: {first_day.strftime('%Y-%m-%d')} until {last_day_str} with {api_request_limit} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "250d9529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Search Query        Quelle       Datum  \\\n",
      "0                         AI  Sueddeutsche  2024-04-23   \n",
      "1    Artificial Intelligence  Sueddeutsche  2024-04-25   \n",
      "2    Artificial Intelligence  Sueddeutsche  2024-04-24   \n",
      "3    Artificial Intelligence  Sueddeutsche  2024-04-23   \n",
      "4    Artificial Intelligence  Sueddeutsche  2024-04-23   \n",
      "..                       ...           ...         ...   \n",
      "336                       AI  Sueddeutsche  2023-12-03   \n",
      "337  Artificial Intelligence  Sueddeutsche  2023-12-04   \n",
      "338                       KI  Sueddeutsche  2023-12-04   \n",
      "339                       KI  Sueddeutsche  2023-12-03   \n",
      "340                       KI  Sueddeutsche  2023-12-02   \n",
      "\n",
      "                                                  Link  \\\n",
      "0    https://www.sueddeutsche.de/wirtschaft/startup...   \n",
      "1    https://www.sueddeutsche.de/panorama/gesetz-ge...   \n",
      "2    https://www.sueddeutsche.de/panorama/glaeubige...   \n",
      "3    https://www.sueddeutsche.de/panorama/mitarbeit...   \n",
      "4    https://www.sueddeutsche.de/wirtschaft/startup...   \n",
      "..                                                 ...   \n",
      "336  https://www.sueddeutsche.de/kultur/ki-popmusik...   \n",
      "337  https://www.sueddeutsche.de/kultur/ki-gesetz-o...   \n",
      "338  https://www.sueddeutsche.de/kultur/ki-gesetz-o...   \n",
      "339  https://www.sueddeutsche.de/kultur/ki-popmusik...   \n",
      "340  https://www.sueddeutsche.de/panorama/polizei-n...   \n",
      "\n",
      "                                                 Titel  \\\n",
      "0    Start-up: Gründer springen auf KI-Hype auf - g...   \n",
      "1    - Gesetz gebilligt: Tiktok aus den USA bald ve...   \n",
      "2    - Gläubiger fordern zwei Milliarden Euro von i...   \n",
      "3    - Mitarbeiter von AfD-Politiker als mutmaßlich...   \n",
      "4    Start-up: Gründer springen auf KI-Hype auf - g...   \n",
      "..                                                 ...   \n",
      "336  Mozart covert Taylor Swift: Wie KI die Popmusi...   \n",
      "337  Kulturwelt kritisiert KI-Gesetz der Regierung:...   \n",
      "338  Kulturwelt kritisiert KI-Gesetz der Regierung:...   \n",
      "339  Mozart covert Taylor Swift: Wie KI die Popmusi...   \n",
      "340  Polizei - Neuer Polizeipräsident: Bei Verbrech...   \n",
      "\n",
      "                                                  Text  \n",
      "0                                                       \n",
      "1                                                       \n",
      "2                                                       \n",
      "3                                                       \n",
      "4                                                       \n",
      "..                                                 ...  \n",
      "336                                                     \n",
      "337                                                     \n",
      "338                                                     \n",
      "339                                                     \n",
      "340  Hamburg (dpa/lno) - Hamburgs neuer Polizeipräs...  \n",
      "\n",
      "[341 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#API-Anfrage\n",
    "\n",
    "#API Schlüssel\n",
    "api_key = \"???\"\n",
    "\n",
    "#DataFrame für die Ergebnisse erstellen\n",
    "df_data_all_queries = []\n",
    "\n",
    "#Berechnung Zeitintervall für jede Schleife\n",
    "loop_interval = timedelta(days=request_days)\n",
    "\n",
    "#Schleife bis zur Erreichung der Grenze an API-Anfragen\n",
    "for i in range(api_request_limit):\n",
    "    #Berechnen des Start- und Enddatums für die aktuelle Schleife\n",
    "    end_date = last_day - (loop_interval * i) - (timedelta(days=1) * i) if i > 0 else last_day - (loop_interval * i)\n",
    "    start_date = end_date - timedelta(days=request_days)\n",
    "\n",
    "    #Bereinigung von Datumsangaben\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    #Schleife über die Suchbegriffe\n",
    "    for search_query in search_queries:\n",
    "        #API-Endpunkt und Parameter\n",
    "        api_endpoint = \"https://www.googleapis.com/customsearch/v1\"\n",
    "        api_params = {\n",
    "            \"key\": api_key,\n",
    "            \"cx\": \"???\",\n",
    "            \"q\": f\"{search_query} before:{end_date_str}\",\n",
    "            \"sort\": \"date\"\n",
    "        }\n",
    "\n",
    "        #API-Aufruf\n",
    "        response = requests.get(api_endpoint, params=api_params)\n",
    "        data = response.json()\n",
    "\n",
    "        #Daten im DataFrame speichern\n",
    "        if \"items\" in data:\n",
    "            for item in data[\"items\"]:\n",
    "                link = item[\"link\"]\n",
    "                title = item[\"title\"]\n",
    "\n",
    "                #Extrahieren von HTML-Inhalten aus der Website\n",
    "                article_response = requests.get(link)\n",
    "\n",
    "                #Pause von x Sekunden\n",
    "                time.sleep(scrap_pause)\n",
    "\n",
    "                soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "                #Extrahieren des Titels aus dem HTML\n",
    "                article_title = soup.title.string.strip() if soup.title else \"\"\n",
    "\n",
    "                #Wenn der Titel leer ist, setzen Sie ihn auf einen Standardwert\n",
    "                if not article_title:\n",
    "                    article_title = \"\"\n",
    "\n",
    "                #Extrahieren des Inhalts\n",
    "                target_paragraphs = soup.find_all('p', class_='css-3zk5y8', attrs={'data-manual': 'paragraph'})\n",
    "                content = ' '.join([paragraph.get_text(separator=' ', strip=True) for paragraph in target_paragraphs])\n",
    "\n",
    "                #Extrahieren des Datums aus dem HTML-Inhalt\n",
    "                article_date = soup.find('time', {'data-manual': 'date'})\n",
    "                if article_date:\n",
    "                    article_date = str(article_date.get('datetime', ''))\n",
    "                else:\n",
    "                    article_date = ''\n",
    "\n",
    "                #Prüfen, ob ein Wert für article_date vorhanden ist\n",
    "                if article_date:\n",
    "                    #Konvertieren des Datums in das gewünschte Format\n",
    "                    formatted_date = datetime.strptime(article_date, \"%Y-%m-%d %H:%M:%S\").strftime(\"%Y-%m-%d\")\n",
    "                else:\n",
    "                    #Wenn article_date nicht vorhanden ist, wird formatted_date auf einen leeren String gesetzt\n",
    "                    formatted_date = \"1900-01-01\"\n",
    "\n",
    "                #Prüfen, ob das Datum mit dem Enddatum übereinstimmt\n",
    "                if datetime.strptime(start_date_str, \"%Y-%m-%d\") <= datetime.strptime(formatted_date, \"%Y-%m-%d\") <= datetime.strptime(end_date_str, \"%Y-%m-%d\"):\n",
    "                    df_data_all_queries.append({\n",
    "                        \"Search Query\": search_query,\n",
    "                        \"Quelle\": \"Sueddeutsche\",\n",
    "                        \"Datum\": formatted_date,\n",
    "                        \"Link\": link,\n",
    "                        \"Titel\": article_title,\n",
    "                        \"Text\": content\n",
    "                    })\n",
    "\n",
    "#DataFrame für alle Suchbegriffe erstellen\n",
    "df_all_queries = pd.DataFrame(df_data_all_queries)\n",
    "\n",
    "#DataFrame anzeigen mit Prüfen, ob der DataFrame leer ist\n",
    "if df_all_queries.empty:\n",
    "    print(\"No entries found!\")\n",
    "else:\n",
    "    print(df_all_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a26df35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neue Daten in einem Hauptdatensatz in CSV speichern\n",
    "\n",
    "#Prüfen, ob die CSV-Datei bereits existiert\n",
    "try:\n",
    "    #Versuche, die vorhandene CSV-Datei zu lesen\n",
    "    existing_df = pd.read_csv(\"2_Daten_Sueddeutsche_V2.csv\")\n",
    "except FileNotFoundError:\n",
    "    #Wenn die Datei nicht existiert, erstelle einen leeren DataFrame\n",
    "    existing_df = pd.DataFrame()\n",
    "\n",
    "#Anhängen der neuen Daten an den bestehenden DataFrame\n",
    "df = pd.DataFrame(df_all_queries)\n",
    "df_to_append = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "#Duplikate auf Basis aller Spalten entfernen\n",
    "df_to_append = df_to_append.drop_duplicates()\n",
    "\n",
    "#Speichern des kombinierten DataFrame in der CSV-Datei\n",
    "df_to_append.to_csv(\"2_Daten_Sueddeutsche_V2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce106ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
