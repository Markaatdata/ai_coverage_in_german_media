{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac01bf71",
   "metadata": {},
   "source": [
    "# Datensammlung von Nachrichtenseiten per Web Scraping - Zeit Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5808309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation Bibliotheken\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "221a9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time period: 2023-12-01 until 2023-12-28 with 7 requests\n"
     ]
    }
   ],
   "source": [
    "#Definition der Parameter für das Web Scraping \n",
    "\n",
    "#Suchwörter in Google Search API\n",
    "search_queries = [\"Künstliche Intelligenz\", \"AI\", \"Artificial Intelligence\", \"KI\"]\n",
    "\n",
    "#Lege ein Startdatum für die automatische Datenerfassung fest\n",
    "last_day_str = \"2023-12-28\"\n",
    "\n",
    "#Zeitintervall von x Tagen vor dem Zieldatum\n",
    "request_days = 3\n",
    "\n",
    "#Limit für API-Anfragen\n",
    "api_request_limit = 7\n",
    "\n",
    "#Pausenzeit des Data Scraping über Beautiful Soup in Sekunden \n",
    "scrap_pause = 3\n",
    "\n",
    "#Bereinigung der Datumsangabe\n",
    "last_day = datetime.strptime(last_day_str, \"%Y-%m-%d\")\n",
    "\n",
    "#Berechnung des letzten Tages\n",
    "first_day = last_day - timedelta(days=request_days*api_request_limit) - ((timedelta(days=1)*api_request_limit)-timedelta(days=1))\n",
    "\n",
    "#Ausgabe des Zeitraums\n",
    "print(f\"Time period: {first_day.strftime('%Y-%m-%d')} until {last_day_str} with {api_request_limit} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "250d9529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Suchbegriff Quelle       Datum  \\\n",
      "0    Künstliche Intelligenz   Zeit  2023-12-27   \n",
      "1    Künstliche Intelligenz   Zeit  2023-12-27   \n",
      "2    Künstliche Intelligenz   Zeit  2023-12-27   \n",
      "3                        AI   Zeit  2023-12-27   \n",
      "4                        AI   Zeit  2023-12-27   \n",
      "..                      ...    ...         ...   \n",
      "74  Artificial Intelligence   Zeit  2023-12-03   \n",
      "75                       KI   Zeit  2023-12-04   \n",
      "76                       KI   Zeit  2023-12-03   \n",
      "77                       KI   Zeit  2023-12-02   \n",
      "78                       KI   Zeit  2023-12-02   \n",
      "\n",
      "                                                 Link  \\\n",
      "0   https://www.zeit.de/digital/2023-12/newyorktim...   \n",
      "1   https://www.zeit.de/kultur/2023-11/kuenstliche...   \n",
      "2   https://www.zeit.de/news/2023-12/27/new-york-t...   \n",
      "3   https://www.zeit.de/digital/2023-12/newyorktim...   \n",
      "4   https://www.zeit.de/kultur/2023-11/kuenstliche...   \n",
      "..                                                ...   \n",
      "74  https://www.zeit.de/digital/internet/2023-12/k...   \n",
      "75  https://www.zeit.de/digital/2023-11/ki-gesetz-...   \n",
      "76  https://www.zeit.de/politik/ausland/2023-12/is...   \n",
      "77  https://www.zeit.de/news/2023-12/02/neuer-poli...   \n",
      "78  https://www.zeit.de/2023/51/kuenstliche-intell...   \n",
      "\n",
      "                                                Titel  \\\n",
      "0   Künstliche Intelligenz: \"New York Times\" reich...   \n",
      "1   Künstliche Intelligenz in der Musik: \"Die Gefa...   \n",
      "2   Künstliche Intelligenz: \"New York Times\" verkl...   \n",
      "3   Künstliche Intelligenz: \"New York Times\" reich...   \n",
      "4   Künstliche Intelligenz in der Musik: \"Die Gefa...   \n",
      "..                                                ...   \n",
      "74  Künstliche Intelligenz in der Arbeitswelt: \"Wi...   \n",
      "75  KI-Gesetz der EU: Regulierung oder Innovation?...   \n",
      "76  Krieg in Gaza: Die \"Zielfabrik\" der israelisch...   \n",
      "77  Polizei: Neuer Polizeipräsident: Bei Verbreche...   \n",
      "78  Künstliche Intelligenz: KI kann wissenschaftli...   \n",
      "\n",
      "                                                 Text  \n",
      "0   Die US-Zeitung New York Times hat das Software...  \n",
      "1   Ein neuer Beatles-Song nach 45 Jahren, ein Fak...  \n",
      "2   Als erste große amerikanische Zeitung hat die ...  \n",
      "3   Die US-Zeitung New York Times hat das Software...  \n",
      "4   Ein neuer Beatles-Song nach 45 Jahren, ein Fak...  \n",
      "..                                                ...  \n",
      "74                                                     \n",
      "75  Wenn der Verkehrsminister, der auch Digitalmin...  \n",
      "76  Seit dem Überfall der Hamas auf Israel fliegt ...  \n",
      "77  Hamburgs neuer Polizeipräsident Falk Schnabel ...  \n",
      "78  Tina Kretschmer ist Professorin für Erziehungs...  \n",
      "\n",
      "[79 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#API-Anfrage\n",
    "\n",
    "#API Schlüssel\n",
    "api_key = \"???\"\n",
    "\n",
    "#DataFrame für die Ergebnisse erstellen\n",
    "df_data_all_queries = []\n",
    "\n",
    "#Berechnung Zeitintervall für jede Schleife\n",
    "loop_interval = timedelta(days=request_days)\n",
    "\n",
    "#Schleife bis zur Erreichung der Grenze an API-Anfragen\n",
    "for i in range(api_request_limit):\n",
    "    #Berechnen des Start- und Enddatums für die aktuelle Schleife\n",
    "    end_date = last_day - (loop_interval * i) - (timedelta(days=1) * i) if i > 0 else last_day - (loop_interval * i)\n",
    "    start_date = end_date - timedelta(days=request_days)\n",
    "\n",
    "    #Bereinigung von Datumsangaben\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    #Schleife über die Suchbegriffe\n",
    "    for search_query in search_queries:\n",
    "        #API-Endpunkt und Parameter\n",
    "        api_endpoint = \"https://www.googleapis.com/customsearch/v1\"\n",
    "        api_params = {\n",
    "            \"key\": api_key,\n",
    "            \"cx\": \"???\",  \n",
    "            \"q\": f\"{search_query} before:{end_date_str}\",\n",
    "            \"sort\": \"date\",\n",
    "        }\n",
    "\n",
    "        #API-Aufruf\n",
    "        response = requests.get(api_endpoint, params=api_params)\n",
    "        data = response.json()\n",
    "\n",
    "        #Daten im DataFrame speichern\n",
    "        if \"items\" in data:\n",
    "            for item in data[\"items\"]:\n",
    "                link = item[\"link\"]\n",
    "                title = item[\"title\"]\n",
    "\n",
    "                #Extrahieren von HTML-Inhalten aus der Website\n",
    "                article_response = requests.get(link)\n",
    "\n",
    "                #Pause von x Sekunden\n",
    "                time.sleep(scrap_pause)\n",
    "\n",
    "                soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "                '''Individueller Teil je Quelle'''\n",
    "\n",
    "                #Extrahieren des Inhalts\n",
    "                #Text aus 'p'-Elementen mit der Klasse 'paragraph article__item' extrahieren\n",
    "                article_text = [p.get_text() for p in soup.find_all('p', class_='paragraph article__item')]\n",
    "\n",
    "                #Verketten der extrahierten Textsegmente zu einem zusammenhängenden Artikeltext\n",
    "                content = '\\n'.join(article_text)\n",
    "\n",
    "                #Extrahieren des Datums aus dem HTML-Inhalt\n",
    "                meta_tag = soup.find('meta', {'name': 'date'})\n",
    "                article_date = meta_tag.get('content', '') if meta_tag else ''\n",
    "\n",
    "                #Prüfen, ob ein Wert für article_date vorhanden ist\n",
    "                if article_date:\n",
    "                    #Konvertieren des Datums in das gewünschte Format\n",
    "                    formatted_date = datetime.strptime(article_date, \"%Y-%m-%dT%H:%M:%S%z\").strftime(\"%Y-%m-%d\")\n",
    "                else:\n",
    "                    #Wenn article_date nicht vorhanden ist, wird formatted_date auf einen leeren String gesetzt.\n",
    "                    formatted_date = \"1900-01-01\"\n",
    "\n",
    "                #Extrahieren des Titels aus der HTML-Datei\n",
    "                article_title = soup.title.text if soup.title else \"\"\n",
    "\n",
    "                '''Individueller Teil je Quelle'''\n",
    "\n",
    "                #Prüfen, ob das Datum mit dem Enddatum übereinstimmt\n",
    "                if datetime.strptime(start_date_str, \"%Y-%m-%d\") <= datetime.strptime(formatted_date, \"%Y-%m-%d\") <= datetime.strptime(end_date_str, \"%Y-%m-%d\"):\n",
    "                    df_data_all_queries.append({\n",
    "                        \"Suchbegriff\": search_query,\n",
    "                        \"Quelle\": \"Zeit\",\n",
    "                        \"Datum\": formatted_date,\n",
    "                        \"Link\": link,\n",
    "                        \"Titel\": article_title,\n",
    "                        \"Text\": content\n",
    "                    })\n",
    "\n",
    "#DataFrame für alle Suchbegriffe erstellen\n",
    "df_all_queries = pd.DataFrame(df_data_all_queries)\n",
    "\n",
    "#DataFrame anzeigen und prüfen, ob er leer ist\n",
    "if df_all_queries.empty:\n",
    "    print(f\"Keine Einträge gefunden!\")\n",
    "else:\n",
    "    print(df_all_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a26df35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neue Daten in einem Hauptdatensatz in CSV speichern\n",
    "\n",
    "#Prüfen, ob die CSV-Datei bereits existiert\n",
    "try:\n",
    "    #Versuche, die vorhandene CSV-Datei zu lesen\n",
    "    existing_df = pd.read_csv(\"2_Daten_Zeit_V2.csv\")\n",
    "except FileNotFoundError:\n",
    "    #Wenn die Datei nicht existiert, erstelle einen leeren DataFrame\n",
    "    existing_df = pd.DataFrame()\n",
    "\n",
    "#Anhängen der neuen Daten an den bestehenden DataFrame\n",
    "df = pd.DataFrame(df_all_queries)\n",
    "df_to_append = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "#Duplikate auf Basis aller Spalten entfernen\n",
    "df_to_append = df_to_append.drop_duplicates()\n",
    "\n",
    "#Speichern des kombinierten DataFrame in der CSV-Datei\n",
    "df_to_append.to_csv(\"2_Daten_Zeit_V2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea900a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
