{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac01bf71",
   "metadata": {},
   "source": [
    "# Datensammlung von Nachrichtenseiten per Web Scraping - Focus Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5808309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation Bibliotheken\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "221a9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time period: 2023-12-01 until 2024-02-10 with 18 requests\n"
     ]
    }
   ],
   "source": [
    "#Definition der Parameter für das Web Scraping \n",
    "\n",
    "#Suchwörter in Google Search API\n",
    "search_queries = [\"Künstliche Intelligenz\", \"AI\", \"Artificial Intelligence\", \"KI\"]\n",
    "\n",
    "#Lege ein Startdatum für die automatische Datenerfassung fest\n",
    "last_day_str = \"2024-02-10\"\n",
    "\n",
    "#Zeitintervall von x Tagen vor dem Zieldatum\n",
    "request_days = 3\n",
    "\n",
    "#Limit für API-Anfragen\n",
    "api_request_limit = 18\n",
    "\n",
    "#Pausenzeit des Data Scraping über Beautiful Soup in Sekunden \n",
    "scrap_pause = 3\n",
    "\n",
    "#Bereinigung der Datumsangabe\n",
    "last_day = datetime.strptime(last_day_str, \"%Y-%m-%d\")\n",
    "\n",
    "#Berechnung des letzten Tages\n",
    "first_day = last_day - timedelta(days=request_days*api_request_limit) - ((timedelta(days=1)*api_request_limit)-timedelta(days=1))\n",
    "\n",
    "#Ausgabe des Zeitraums\n",
    "print(f\"Time period: {first_day.strftime('%Y-%m-%d')} until {last_day_str} with {api_request_limit} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "250d9529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Search Query        Quelle       Datum  \\\n",
      "0    Künstliche Intelligenz  Focus Online  2024-02-09   \n",
      "1    Künstliche Intelligenz  Focus Online  2024-02-09   \n",
      "2    Künstliche Intelligenz  Focus Online  2024-02-09   \n",
      "3    Künstliche Intelligenz  Focus Online  2024-02-09   \n",
      "4    Künstliche Intelligenz  Focus Online  2024-02-07   \n",
      "..                      ...           ...         ...   \n",
      "178                      KI  Focus Online  2023-12-05   \n",
      "179  Künstliche Intelligenz  Focus Online  2023-12-01   \n",
      "180                      AI  Focus Online  2023-12-03   \n",
      "181                      KI  Focus Online  2023-12-04   \n",
      "182                      KI  Focus Online  2023-12-01   \n",
      "\n",
      "                                                  Link  \\\n",
      "0    https://www.focus.de/politik/gastbeitrag-von-g...   \n",
      "1    https://www.focus.de/experts/schluss-mit-gifti...   \n",
      "2    https://www.focus.de/kultur/kino_tv/tv-kolumne...   \n",
      "3    https://www.focus.de/finanzen/boerse/chemiebra...   \n",
      "4    https://www.focus.de/experts/langzeitliebe-das...   \n",
      "..                                                 ...   \n",
      "178  https://www.focus.de/magazin/archiv/nvidia-der...   \n",
      "179  https://www.focus.de/wissen/technik/speerspitz...   \n",
      "180  https://www.focus.de/familie/beziehung/unbewus...   \n",
      "181  https://www.focus.de/panorama/bei-schnee-im-ei...   \n",
      "182  https://www.focus.de/wissen/technik/speerspitz...   \n",
      "\n",
      "                                                 Titel  \\\n",
      "0    Keine Zeit für Smalltalk! Diese Fragen muss Sc...   \n",
      "1    Drei Schritte raus aus der toxischen Partnersc...   \n",
      "2    Selbst in der Kölner Mädchensitzung bekommt Ka...   \n",
      "3    „Wachstum und neue Geschäftsmodelle“: BASF set...   \n",
      "4    Das Geheimnis glücklicher Partnerschaften  - E...   \n",
      "..                                                 ...   \n",
      "178              NVIDIA : Der Lieferant - FOCUS online   \n",
      "179  US-Panzer wird zum „Terminator“: Der „AbramsX“...   \n",
      "180  Wie sitzen Sie auf der Couch? Das verrät es üb...   \n",
      "181  Surfer in München entgeht nur knapp einem umst...   \n",
      "182  US-Panzer wird zum „Terminator“: Der „AbramsX“...   \n",
      "\n",
      "                                                  Text  \n",
      "0    Für Smalltalk werden Olaf Scholz und Joe Biden...  \n",
      "1    Erkennen – der erste Schritt zur Freiheit Das ...  \n",
      "2    „Endlich normale Menschen“, sagt Jörg Runge al...  \n",
      "3    Der BASF-Konzern sieht großes Potenzial für Kü...  \n",
      "4    Auch glückliche Paare streiten. Aber immer mit...  \n",
      "..                                                 ...  \n",
      "178  Die Aktie von Nvidia stürmt nach oben, als ob ...  \n",
      "179  Ein modernes Zielerfassungssystem und ein Lade...  \n",
      "180  Kuscheln Sie abends gern gemeinsam mit dem Par...  \n",
      "181  Bei etwas über 4 Grad in einen Fluss zu spring...  \n",
      "182  Ein modernes Zielerfassungssystem und ein Lade...  \n",
      "\n",
      "[183 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#API-Schlüssel\n",
    "api_key = \"???\"\n",
    "\n",
    "#Liste für die Ergebnisse erstellen\n",
    "df_data_all_queries = []\n",
    "\n",
    "#Berechnung des Zeitintervalls für jede Schleife\n",
    "loop_interval = timedelta(days=request_days)\n",
    "\n",
    "#Schleife bis zur Erreichung der Grenze an API-Anfragen\n",
    "for i in range(api_request_limit):\n",
    "    #Berechnen des Start- und Enddatums für die aktuelle Schleife\n",
    "    end_date = last_day - (loop_interval * i) - (timedelta(days=1) * i) if i > 0 else last_day - (loop_interval * i)\n",
    "    start_date = end_date - timedelta(days=request_days)\n",
    "\n",
    "    #Bereinigung von Datumsangaben\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    #Schleife über die Suchbegriffe\n",
    "    for search_query in search_queries:\n",
    "        #API-Endpunkt und Parameter\n",
    "        api_endpoint = \"https://www.googleapis.com/customsearch/v1\"\n",
    "        api_params = {\n",
    "            \"key\": api_key,\n",
    "            \"cx\": \"???\",\n",
    "            \"q\": f\"{search_query} before:{end_date_str}\",\n",
    "            \"sort\": \"date\",\n",
    "        }\n",
    "\n",
    "        #API-Aufruf\n",
    "        response = requests.get(api_endpoint, params=api_params)\n",
    "        data = response.json()\n",
    "        \n",
    "        #Daten im DataFrame speichern\n",
    "        if \"items\" in data:\n",
    "            for item in data[\"items\"]:\n",
    "                link = item[\"link\"]\n",
    "                title = item[\"title\"]\n",
    "                \n",
    "                #Extrahieren von HTML-Inhalten aus der Website\n",
    "                article_response = requests.get(link)\n",
    "                \n",
    "                #Pause von x Sekunden\n",
    "                time.sleep(scrap_pause)\n",
    "        \n",
    "                soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "                '''Individueller Teil je Quelle'''\n",
    "                #Extrahieren des Titels aus dem HTML\n",
    "                article_title = soup.title.string.strip()\n",
    "                \n",
    "                #Extrahieren des Inhalts mit der Klasse \"textBlock\"\n",
    "                text_blocks = soup.find_all('div', {'class': 'textBlock'})\n",
    "                \n",
    "                #Kombinieren der Textblöcke zu einem einzigen Text\n",
    "                content = ' '.join([block.get_text(separator=' ', strip=True) for block in text_blocks])\n",
    "                \n",
    "                #Extrahieren des Datums aus dem HTML-Inhalt\n",
    "                meta_tag = soup.find('meta', {'name': 'date'})\n",
    "                article_date = meta_tag.get('content', '') if meta_tag else ''\n",
    "        \n",
    "                #Konvertieren des Datums in das gewünschte Format\n",
    "                formatted_date = None\n",
    "                if article_date:\n",
    "                    try:\n",
    "                        #Try parsing with milliseconds and Z\n",
    "                        formatted_date = datetime.strptime(article_date, \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d\")\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            #Try parsing with timezone offset\n",
    "                            formatted_date = datetime.strptime(article_date, \"%Y-%m-%dT%H:%M:%S%z\").strftime(\"%Y-%m-%d\")\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                \n",
    "                if formatted_date:\n",
    "                    '''Individueller Teil je Quelle'''\n",
    "                    #Prüfen, ob das Datum mit dem Enddatum übereinstimmt\n",
    "                    if datetime.strptime(start_date_str, \"%Y-%m-%d\") <= datetime.strptime(formatted_date, \"%Y-%m-%d\") <= datetime.strptime(end_date_str, \"%Y-%m-%d\"):\n",
    "                        df_data_all_queries.append({\n",
    "                            \"Search Query\": search_query,\n",
    "                            \"Quelle\": \"Focus Online\",\n",
    "                            \"Datum\": formatted_date,\n",
    "                            \"Link\": link,\n",
    "                            \"Titel\": article_title,\n",
    "                            \"Text\": content\n",
    "                        })\n",
    "\n",
    "#DataFrame für alle Suchbegriffe erstellen\n",
    "df_all_queries = pd.DataFrame(df_data_all_queries)\n",
    "\n",
    "#DataFrame anzeigen mit Prüfen, ob der DataFrame leer ist\n",
    "if df_all_queries.empty:\n",
    "    print(f\"No entries found!\")\n",
    "else:\n",
    "    print(df_all_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a26df35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neue Daten in einem Hauptdatensatz in CSV speichern\n",
    "\n",
    "#Überprüfen Sie, ob die CSV-Datei bereits existiert\n",
    "try:\n",
    "    #Versuche, die vorhandene CSV-Datei zu lesen\n",
    "    existing_df = pd.read_csv(\"2_Daten_Focus_V2.csv\")\n",
    "except FileNotFoundError:\n",
    "    #Wenn die Datei nicht existiert, erstelle einen leeren DataFrame\n",
    "    existing_df = pd.DataFrame()\n",
    "\n",
    "#Anhängen der neuen Daten an den bestehenden DataFrame\n",
    "df = pd.DataFrame(df_all_queries)\n",
    "df_to_append = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "#Duplikate auf Basis aller Spalten entfernen\n",
    "df_to_append = df_to_append.drop_duplicates()\n",
    "\n",
    "#Speichern des kombinierten DataFrames in der CSV-Datei\n",
    "df_to_append.to_csv(\"2_Daten_Focus_V2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe71c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
