{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac01bf71",
   "metadata": {},
   "source": [
    "# Datensammlung von Nachrichtenseiten per Web Scraping - FAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5808309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation Bibliotheken\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "221a9660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time period: 2023-12-01 until 2024-01-01 with 8 requests\n"
     ]
    }
   ],
   "source": [
    "#Definition der Parameter für das Web Scraping \n",
    "\n",
    "#Suchwörter in Google Search API\n",
    "search_queries = [\"Künstliche Intelligenz\", \"AI\", \"Artificial Intelligence\", \"KI\"]\n",
    "\n",
    "#Lege ein Startdatum für die automatische Datenerfassung fest\n",
    "last_day_str = \"2024-01-01\"\n",
    "\n",
    "#Zeitintervall von x Tagen vor dem Zieldatum\n",
    "request_days = 3\n",
    "\n",
    "#Limit für API-Anfragen\n",
    "api_request_limit = 8\n",
    "\n",
    "#Pausenzeit des Data Scraping über Beautiful Soup in Sekunden \n",
    "scrap_pause = 3\n",
    "\n",
    "#Bereinigung der Datumsangabe\n",
    "last_day = datetime.strptime(last_day_str, \"%Y-%m-%d\")\n",
    "\n",
    "#Berechnung des letzten Tages\n",
    "first_day = last_day - timedelta(days=request_days*api_request_limit) - ((timedelta(days=1)*api_request_limit)-timedelta(days=1))\n",
    "\n",
    "#Ausgabe des Zeitraums\n",
    "print(f\"Time period: {first_day.strftime('%Y-%m-%d')} until {last_day_str} with {api_request_limit} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "250d9529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Search Query Quelle       Datum  \\\n",
      "0     Künstliche Intelligenz    FAZ  2023-12-30   \n",
      "1     Künstliche Intelligenz    FAZ  2023-12-29   \n",
      "2                         AI    FAZ  2023-12-30   \n",
      "3                         KI    FAZ  2023-12-31   \n",
      "4                         KI    FAZ  2023-12-29   \n",
      "..                       ...    ...         ...   \n",
      "108  Artificial Intelligence    FAZ  2023-12-03   \n",
      "109                       KI    FAZ  2023-12-04   \n",
      "110                       KI    FAZ  2023-12-03   \n",
      "111                       KI    FAZ  2023-12-01   \n",
      "112                       KI    FAZ  2023-12-01   \n",
      "\n",
      "                                                  Link  \\\n",
      "0    https://www.faz.net/podcasts/f-a-z-d-economy-p...   \n",
      "1    https://www.faz.net/aktuell/technik-motor/digi...   \n",
      "2    https://www.faz.net/podcasts/f-a-z-d-economy-p...   \n",
      "3    https://www.faz.net/aktuell/gesellschaft/erstm...   \n",
      "4    https://www.faz.net/aktuell/politik/ausland/tr...   \n",
      "..                                                 ...   \n",
      "108  https://www.faz.net/pro/d-economy/kuenstliche-...   \n",
      "109  https://www.faz.net/aktuell/karriere-hochschul...   \n",
      "110  https://www.faz.net/pro/d-economy/kuenstliche-...   \n",
      "111  https://www.faz.net/einspruch/wie-vertrauenssc...   \n",
      "112  https://www.faz.net/aktuell/karriere-hochschul...   \n",
      "\n",
      "                                                 Titel  \\\n",
      "0    Die Künstliche Intelligenz hat das Jahr 2023 g...   \n",
      "1    Was 2024 passiert? Wir haben ChatGPT, Bard und...   \n",
      "2    Die Künstliche Intelligenz hat das Jahr 2023 g...   \n",
      "3    Erstmals auch mit KI: Sydney bereitet Riesenfe...   \n",
      "4        Trumps Ex-Anwalt Cohen gibt Nutzung von KI zu   \n",
      "..                                                 ...   \n",
      "108  ChatGPT, DALL-E und Co.: Das sind die besten k...   \n",
      "109  Die KI in meinem Büro: ChatGPT und Co als digi...   \n",
      "110  ChatGPT, DALL-E und Co.: Das sind die besten k...   \n",
      "111  Wie Vertrauensschutz die KI-Regulierung prägen...   \n",
      "112  Uni schafft Bachelorarbeiten ab: Wegen KI und ...   \n",
      "\n",
      "                                                  Text  \n",
      "0                                                       \n",
      "1    Kein Blick in die Glaskugel, sondern eine abso...  \n",
      "2                                                       \n",
      "3    Das Silvesterfeuerwerk vor der weltberühmten K...  \n",
      "4    Donald Trumps Ex-Anwalt Michael Cohen wollte s...  \n",
      "..                                                 ...  \n",
      "108  Jeder dritte Deutsche fürchtet, dass KI bald s...  \n",
      "109  Mails, Mitarbeitersuche, kreativ sein: Vieles ...  \n",
      "110  Jeder dritte Deutsche fürchtet, dass KI bald s...  \n",
      "111  Die rechtliche Ordnung Künstlicher Intelligenz...  \n",
      "112  In Zeiten von KI ergebe das Anfertigen von Bac...  \n",
      "\n",
      "[113 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#API-Anfrage\n",
    "\n",
    "#API-Schlüssel\n",
    "api_key = \"???\"\n",
    "\n",
    "#DataFrame für die Ergebnisse erstellen\n",
    "df_data_all_queries = []\n",
    "\n",
    "#Berechnung Zeitintervall für jede Schleife\n",
    "loop_interval = timedelta(days=request_days)\n",
    "\n",
    "#Schleife bis zur Erreichung der Grenze an API-Anfragen\n",
    "for i in range(api_request_limit):\n",
    "    #Berechnen des Start- und Enddatums für die aktuelle Schleife\n",
    "    end_date = last_day - (loop_interval * i) - (timedelta(days=1) * i) if i > 0 else last_day - (loop_interval * i)\n",
    "    start_date = end_date - (timedelta(days=request_days))\n",
    "\n",
    "    #Bereinigung von Datumsangaben\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    #Schleife über die Suchbegriffe\n",
    "    for search_query in search_queries:\n",
    "        #API-Endpunkt und Parameter\n",
    "        api_endpoint = \"https://www.googleapis.com/customsearch/v1\"\n",
    "        api_params = {\n",
    "            \"key\": api_key,\n",
    "            \"cx\": \"???\",\n",
    "            \"q\": f\"{search_query} before:{end_date_str}\",\n",
    "            \"sort\": \"date\",\n",
    "        }\n",
    "\n",
    "        #API-Aufruf\n",
    "        response = requests.get(api_endpoint, params=api_params)\n",
    "        data = response.json()\n",
    "        \n",
    "        #Daten im DataFrame speichern\n",
    "        if \"items\" in data:\n",
    "            for item in data[\"items\"]:\n",
    "                link = item[\"link\"]\n",
    "                title = item[\"title\"]\n",
    "\n",
    "                #Extrahieren von HTML-Inhalten aus der Website\n",
    "                article_response = requests.get(link)\n",
    "                \n",
    "                #Pause von x Sekunden\n",
    "                time.sleep(scrap_pause)\n",
    "                \n",
    "                soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "                '''Individueller Teil je Quelle'''\n",
    "                #Extrahieren des Inhalts\n",
    "                #Extrahieren Sie den gewünschten JSON-Abschnitt \n",
    "                json_script_list = soup.find_all('script', {'type': 'application/ld+json'})\n",
    "                json_data = {}\n",
    "\n",
    "                for script in json_script_list:\n",
    "                    try:\n",
    "                        script_data = json.loads(script.string)\n",
    "                        if script_data.get('@type') == 'NewsArticle':\n",
    "                            json_data = script_data\n",
    "                            break\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "\n",
    "                #Extrahieren des Artikeltextes\n",
    "                description = json_data.get(\"description\", \"\")\n",
    "                article_body = json_data.get(\"articleBody\", \"\")\n",
    "\n",
    "                #Schreiben Sie das Ergebnis in die Variable \"content\"\n",
    "                content = description + ' ' + article_body\n",
    "\n",
    "                #Extrahieren des Datums aus dem HTML-Inhalt   \n",
    "                meta_tag = soup.find('time', {'class': 'atc-MetaTime'})\n",
    "                article_date = meta_tag.get('datetime', '') if meta_tag else ''\n",
    "\n",
    "                #Prüfen, ob ein Wert für article_date vorhanden ist\n",
    "                #<time>-Tag finden und datetime-Attribut extrahieren\n",
    "                time_tag = soup.find('time', {'datetime': True})\n",
    "                article_date = time_tag['datetime'] if time_tag else ''\n",
    "\n",
    "                #Datum formatieren\n",
    "                if article_date:\n",
    "                    formatted_date = datetime.strptime(article_date, \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%d\")\n",
    "                else:\n",
    "                    formatted_date = \"1900-01-01\"\n",
    "\n",
    "                #Titel aus dem HTML-Code extrahieren\n",
    "                article_title = soup.title.text\n",
    "                \n",
    "                '''Individueller Teil je Quelle'''\n",
    "                #Prüfen, ob das Datum mit dem Enddatum übereinstimmt\n",
    "                if datetime.strptime(start_date_str, \"%Y-%m-%d\") <= datetime.strptime(formatted_date, \"%Y-%m-%d\") <= datetime.strptime(end_date_str, \"%Y-%m-%d\"):\n",
    "                    df_data_all_queries.append({\n",
    "                        \"Search Query\": search_query,\n",
    "                        \"Quelle\": \"FAZ\",\n",
    "                        \"Datum\": formatted_date,\n",
    "                        \"Link\": link,\n",
    "                        \"Titel\": article_title,\n",
    "                        \"Text\": content\n",
    "                    })\n",
    "\n",
    "#DataFrame für alle Suchbegriffe erstellen\n",
    "df_all_queries = pd.DataFrame(df_data_all_queries)\n",
    "\n",
    "#DataFrame anzeigen mit Prüfen, ob der DataFrame leer ist\n",
    "if df_all_queries.empty:\n",
    "    print(\"No entries found!\")\n",
    "else:\n",
    "    print(df_all_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a26df35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neue Daten in einem Hauptdatensatz in CSV speichern\n",
    "\n",
    "#Prüfen, ob die CSV-Datei bereits existiert\n",
    "try:\n",
    "    #Versuche, die vorhandene CSV-Datei zu lesen\n",
    "    existing_df = pd.read_csv(\"2_Daten_FAZ_V2.csv\")\n",
    "except FileNotFoundError:\n",
    "    #Wenn die Datei nicht existiert, erstelle einen leeren DataFrame\n",
    "    existing_df = pd.DataFrame()\n",
    "\n",
    "#Anhängen der neuen Daten an den bestehenden DataFrame\n",
    "df = pd.DataFrame(df_all_queries)\n",
    "df_to_append = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "#Duplikate auf Basis aller Spalten entfernen\n",
    "df_to_append = df_to_append.drop_duplicates()\n",
    "\n",
    "#Speichern des kombinierten DataFrame in der CSV-Datei\n",
    "df_to_append.to_csv(\"2_Daten_FAZ_V2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf9500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
